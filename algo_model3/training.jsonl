{"text":"Our method finds significant $n$-grams related to a topic, which are then used to help understand and interpret the underlying distribution.","_input_hash":994805695,"_task_hash":1013407051,"tokens":[{"text":"Our","start":0,"end":3,"id":0},{"text":"method","start":4,"end":10,"id":1},{"text":"finds","start":11,"end":16,"id":2},{"text":"significant","start":17,"end":28,"id":3},{"text":"$","start":29,"end":30,"id":4},{"text":"n$-grams","start":30,"end":38,"id":5},{"text":"related","start":39,"end":46,"id":6},{"text":"to","start":47,"end":49,"id":7},{"text":"a","start":50,"end":51,"id":8},{"text":"topic","start":52,"end":57,"id":9},{"text":",","start":57,"end":58,"id":10},{"text":"which","start":59,"end":64,"id":11},{"text":"are","start":65,"end":68,"id":12},{"text":"then","start":69,"end":73,"id":13},{"text":"used","start":74,"end":78,"id":14},{"text":"to","start":79,"end":81,"id":15},{"text":"help","start":82,"end":86,"id":16},{"text":"understand","start":87,"end":97,"id":17},{"text":"and","start":98,"end":101,"id":18},{"text":"interpret","start":102,"end":111,"id":19},{"text":"the","start":112,"end":115,"id":20},{"text":"underlying","start":116,"end":126,"id":21},{"text":"distribution","start":127,"end":139,"id":22},{"text":".","start":139,"end":140,"id":23}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Advanced plotting options are provided by the Rgraphviz package.","_input_hash":-2045713205,"_task_hash":123012275,"tokens":[{"text":"Advanced","start":0,"end":8,"id":0},{"text":"plotting","start":9,"end":17,"id":1},{"text":"options","start":18,"end":25,"id":2},{"text":"are","start":26,"end":29,"id":3},{"text":"provided","start":30,"end":38,"id":4},{"text":"by","start":39,"end":41,"id":5},{"text":"the","start":42,"end":45,"id":6},{"text":"Rgraphviz","start":46,"end":55,"id":7},{"text":"package","start":56,"end":63,"id":8},{"text":".","start":63,"end":64,"id":9}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"The intrinsic underlying structure of the system is modeled by an epsilon-machine and its causal states.","_input_hash":-1157640748,"_task_hash":-1655682722,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"intrinsic","start":4,"end":13,"id":1},{"text":"underlying","start":14,"end":24,"id":2},{"text":"structure","start":25,"end":34,"id":3},{"text":"of","start":35,"end":37,"id":4},{"text":"the","start":38,"end":41,"id":5},{"text":"system","start":42,"end":48,"id":6},{"text":"is","start":49,"end":51,"id":7},{"text":"modeled","start":52,"end":59,"id":8},{"text":"by","start":60,"end":62,"id":9},{"text":"an","start":63,"end":65,"id":10},{"text":"epsilon","start":66,"end":73,"id":11},{"text":"-","start":73,"end":74,"id":12},{"text":"machine","start":74,"end":81,"id":13},{"text":"and","start":82,"end":85,"id":14},{"text":"its","start":86,"end":89,"id":15},{"text":"causal","start":90,"end":96,"id":16},{"text":"states","start":97,"end":103,"id":17},{"text":".","start":103,"end":104,"id":18}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"relational_databases|NOUN","word":"relational databases","sense":"NOUN","meta":{"score":0.7899000049,"sense":"NOUN"},"_input_hash":-2004147165,"_task_hash":867512212,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"relational_databases|NOUN","start":0,"end":25,"id":0}]}
{"text":"Such bounds relate a classifier's risk, assessed with respect to a surrogate loss, to its cost-sensitive classification risk.","_input_hash":-554488191,"_task_hash":-2086478102,"tokens":[{"text":"Such","start":0,"end":4,"id":0},{"text":"bounds","start":5,"end":11,"id":1},{"text":"relate","start":12,"end":18,"id":2},{"text":"a","start":19,"end":20,"id":3},{"text":"classifier","start":21,"end":31,"id":4},{"text":"'s","start":31,"end":33,"id":5},{"text":"risk","start":34,"end":38,"id":6},{"text":",","start":38,"end":39,"id":7},{"text":"assessed","start":40,"end":48,"id":8},{"text":"with","start":49,"end":53,"id":9},{"text":"respect","start":54,"end":61,"id":10},{"text":"to","start":62,"end":64,"id":11},{"text":"a","start":65,"end":66,"id":12},{"text":"surrogate","start":67,"end":76,"id":13},{"text":"loss","start":77,"end":81,"id":14},{"text":",","start":81,"end":82,"id":15},{"text":"to","start":83,"end":85,"id":16},{"text":"its","start":86,"end":89,"id":17},{"text":"cost","start":90,"end":94,"id":18},{"text":"-","start":94,"end":95,"id":19},{"text":"sensitive","start":95,"end":104,"id":20},{"text":"classification","start":105,"end":119,"id":21},{"text":"risk","start":120,"end":124,"id":22},{"text":".","start":124,"end":125,"id":23}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"The resulting algorithms are efficient, and perform well in simulations under stochastic and adversarial inputs.","_input_hash":-843765577,"_task_hash":1419313029,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"resulting","start":4,"end":13,"id":1},{"text":"algorithms","start":14,"end":24,"id":2},{"text":"are","start":25,"end":28,"id":3},{"text":"efficient","start":29,"end":38,"id":4},{"text":",","start":38,"end":39,"id":5},{"text":"and","start":40,"end":43,"id":6},{"text":"perform","start":44,"end":51,"id":7},{"text":"well","start":52,"end":56,"id":8},{"text":"in","start":57,"end":59,"id":9},{"text":"simulations","start":60,"end":71,"id":10},{"text":"under","start":72,"end":77,"id":11},{"text":"stochastic","start":78,"end":88,"id":12},{"text":"and","start":89,"end":92,"id":13},{"text":"adversarial","start":93,"end":104,"id":14},{"text":"inputs","start":105,"end":111,"id":15},{"text":".","start":111,"end":112,"id":16}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":4,"end":24,"token_start":1,"token_end":2,"label":"ALGO","answer":"reject"}],"answer":"reject"}
{"text":"backprop|NOUN","word":"backprop","sense":"NOUN","meta":{"score":0.7919999957,"sense":"NOUN"},"_input_hash":2123728039,"_task_hash":-248598041,"_session_id":null,"_view_id":"html","answer":"accept","spans":[],"tokens":[{"text":"backprop|NOUN","start":0,"end":13,"id":0}]}
{"text":"category_theory|NOUN","word":"category theory","sense":"NOUN","meta":{"score":0.7778000236,"sense":"NOUN"},"_input_hash":-2103438003,"_task_hash":-1829767664,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"category_theory|NOUN","start":0,"end":20,"id":0}]}
{"text":"The kernels we define are semigroup kernels in the sense that they only use the sum of two measures to compare them, and spectral in the sense that they only use the eigenspectrum of the variance matrix of this mixture.","_input_hash":1847661463,"_task_hash":-1645034374,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"kernels","start":4,"end":11,"id":1},{"text":"we","start":12,"end":14,"id":2},{"text":"define","start":15,"end":21,"id":3},{"text":"are","start":22,"end":25,"id":4},{"text":"semigroup","start":26,"end":35,"id":5},{"text":"kernels","start":36,"end":43,"id":6},{"text":"in","start":44,"end":46,"id":7},{"text":"the","start":47,"end":50,"id":8},{"text":"sense","start":51,"end":56,"id":9},{"text":"that","start":57,"end":61,"id":10},{"text":"they","start":62,"end":66,"id":11},{"text":"only","start":67,"end":71,"id":12},{"text":"use","start":72,"end":75,"id":13},{"text":"the","start":76,"end":79,"id":14},{"text":"sum","start":80,"end":83,"id":15},{"text":"of","start":84,"end":86,"id":16},{"text":"two","start":87,"end":90,"id":17},{"text":"measures","start":91,"end":99,"id":18},{"text":"to","start":100,"end":102,"id":19},{"text":"compare","start":103,"end":110,"id":20},{"text":"them","start":111,"end":115,"id":21},{"text":",","start":115,"end":116,"id":22},{"text":"and","start":117,"end":120,"id":23},{"text":"spectral","start":121,"end":129,"id":24},{"text":"in","start":130,"end":132,"id":25},{"text":"the","start":133,"end":136,"id":26},{"text":"sense","start":137,"end":142,"id":27},{"text":"that","start":143,"end":147,"id":28},{"text":"they","start":148,"end":152,"id":29},{"text":"only","start":153,"end":157,"id":30},{"text":"use","start":158,"end":161,"id":31},{"text":"the","start":162,"end":165,"id":32},{"text":"eigenspectrum","start":166,"end":179,"id":33},{"text":"of","start":180,"end":182,"id":34},{"text":"the","start":183,"end":186,"id":35},{"text":"variance","start":187,"end":195,"id":36},{"text":"matrix","start":196,"end":202,"id":37},{"text":"of","start":203,"end":205,"id":38},{"text":"this","start":206,"end":210,"id":39},{"text":"mixture","start":211,"end":218,"id":40},{"text":".","start":218,"end":219,"id":41}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"The main drawbacks of this approach are the associated computational and storage demands.","_input_hash":-1668893154,"_task_hash":-1659586906,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"main","start":4,"end":8,"id":1},{"text":"drawbacks","start":9,"end":18,"id":2},{"text":"of","start":19,"end":21,"id":3},{"text":"this","start":22,"end":26,"id":4},{"text":"approach","start":27,"end":35,"id":5},{"text":"are","start":36,"end":39,"id":6},{"text":"the","start":40,"end":43,"id":7},{"text":"associated","start":44,"end":54,"id":8},{"text":"computational","start":55,"end":68,"id":9},{"text":"and","start":69,"end":72,"id":10},{"text":"storage","start":73,"end":80,"id":11},{"text":"demands","start":81,"end":88,"id":12},{"text":".","start":88,"end":89,"id":13}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"arithmetic|NOUN","word":"arithmetic","sense":"NOUN","meta":{"score":0.7811999917,"sense":"NOUN"},"_input_hash":-66001988,"_task_hash":464920759,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"arithmetic|NOUN","start":0,"end":15,"id":0}]}
{"text":"Security issues are crucial in a number of machine learning applications, especially in scenarios dealing with human activity rather than natural phenomena (e.g., information ranking, spam detection, malware detection, etc.).","_input_hash":-669017834,"_task_hash":877841530,"tokens":[{"text":"Security","start":0,"end":8,"id":0},{"text":"issues","start":9,"end":15,"id":1},{"text":"are","start":16,"end":19,"id":2},{"text":"crucial","start":20,"end":27,"id":3},{"text":"in","start":28,"end":30,"id":4},{"text":"a","start":31,"end":32,"id":5},{"text":"number","start":33,"end":39,"id":6},{"text":"of","start":40,"end":42,"id":7},{"text":"machine","start":43,"end":50,"id":8},{"text":"learning","start":51,"end":59,"id":9},{"text":"applications","start":60,"end":72,"id":10},{"text":",","start":72,"end":73,"id":11},{"text":"especially","start":74,"end":84,"id":12},{"text":"in","start":85,"end":87,"id":13},{"text":"scenarios","start":88,"end":97,"id":14},{"text":"dealing","start":98,"end":105,"id":15},{"text":"with","start":106,"end":110,"id":16},{"text":"human","start":111,"end":116,"id":17},{"text":"activity","start":117,"end":125,"id":18},{"text":"rather","start":126,"end":132,"id":19},{"text":"than","start":133,"end":137,"id":20},{"text":"natural","start":138,"end":145,"id":21},{"text":"phenomena","start":146,"end":155,"id":22},{"text":"(","start":156,"end":157,"id":23},{"text":"e.g.","start":157,"end":161,"id":24},{"text":",","start":161,"end":162,"id":25},{"text":"information","start":163,"end":174,"id":26},{"text":"ranking","start":175,"end":182,"id":27},{"text":",","start":182,"end":183,"id":28},{"text":"spam","start":184,"end":188,"id":29},{"text":"detection","start":189,"end":198,"id":30},{"text":",","start":198,"end":199,"id":31},{"text":"malware","start":200,"end":207,"id":32},{"text":"detection","start":208,"end":217,"id":33},{"text":",","start":217,"end":218,"id":34},{"text":"etc","start":219,"end":222,"id":35},{"text":".","start":222,"end":223,"id":36},{"text":")","start":223,"end":224,"id":37},{"text":".","start":224,"end":225,"id":38}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"A nonparametric kernel-based method for realizing Bayes' rule is proposed, based on representations of probabilities in reproducing kernel Hilbert spaces.","_input_hash":1289696069,"_task_hash":-747759993,"tokens":[{"text":"A","start":0,"end":1,"id":0},{"text":"nonparametric","start":2,"end":15,"id":1},{"text":"kernel","start":16,"end":22,"id":2},{"text":"-","start":22,"end":23,"id":3},{"text":"based","start":23,"end":28,"id":4},{"text":"method","start":29,"end":35,"id":5},{"text":"for","start":36,"end":39,"id":6},{"text":"realizing","start":40,"end":49,"id":7},{"text":"Bayes","start":50,"end":55,"id":8},{"text":"'","start":55,"end":56,"id":9},{"text":"rule","start":57,"end":61,"id":10},{"text":"is","start":62,"end":64,"id":11},{"text":"proposed","start":65,"end":73,"id":12},{"text":",","start":73,"end":74,"id":13},{"text":"based","start":75,"end":80,"id":14},{"text":"on","start":81,"end":83,"id":15},{"text":"representations","start":84,"end":99,"id":16},{"text":"of","start":100,"end":102,"id":17},{"text":"probabilities","start":103,"end":116,"id":18},{"text":"in","start":117,"end":119,"id":19},{"text":"reproducing","start":120,"end":131,"id":20},{"text":"kernel","start":132,"end":138,"id":21},{"text":"Hilbert","start":139,"end":146,"id":22},{"text":"spaces","start":147,"end":153,"id":23},{"text":".","start":153,"end":154,"id":24}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"The f-divergence estimator is then exploited for the two-sample homogeneity test.","_input_hash":-2038338754,"_task_hash":514887218,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"f","start":4,"end":5,"id":1},{"text":"-","start":5,"end":6,"id":2},{"text":"divergence","start":6,"end":16,"id":3},{"text":"estimator","start":17,"end":26,"id":4},{"text":"is","start":27,"end":29,"id":5},{"text":"then","start":30,"end":34,"id":6},{"text":"exploited","start":35,"end":44,"id":7},{"text":"for","start":45,"end":48,"id":8},{"text":"the","start":49,"end":52,"id":9},{"text":"two","start":53,"end":56,"id":10},{"text":"-","start":56,"end":57,"id":11},{"text":"sample","start":57,"end":63,"id":12},{"text":"homogeneity","start":64,"end":75,"id":13},{"text":"test","start":76,"end":80,"id":14},{"text":".","start":80,"end":81,"id":15}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"We formulate the problem as a maximization of $\\ell_1$-regularized surrogate likelihood that allows us to find a sparse solution.","_input_hash":255427274,"_task_hash":-1941332186,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"formulate","start":3,"end":12,"id":1},{"text":"the","start":13,"end":16,"id":2},{"text":"problem","start":17,"end":24,"id":3},{"text":"as","start":25,"end":27,"id":4},{"text":"a","start":28,"end":29,"id":5},{"text":"maximization","start":30,"end":42,"id":6},{"text":"of","start":43,"end":45,"id":7},{"text":"$","start":46,"end":47,"id":8},{"text":"\\ell_1$-regularized","start":47,"end":66,"id":9},{"text":"surrogate","start":67,"end":76,"id":10},{"text":"likelihood","start":77,"end":87,"id":11},{"text":"that","start":88,"end":92,"id":12},{"text":"allows","start":93,"end":99,"id":13},{"text":"us","start":100,"end":102,"id":14},{"text":"to","start":103,"end":105,"id":15},{"text":"find","start":106,"end":110,"id":16},{"text":"a","start":111,"end":112,"id":17},{"text":"sparse","start":113,"end":119,"id":18},{"text":"solution","start":120,"end":128,"id":19},{"text":".","start":128,"end":129,"id":20}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"We present a simple proof for the oracle inequality for the excess risk of structural risk minimizers using a lasso type penalty.","_input_hash":1863823134,"_task_hash":-2000110323,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"present","start":3,"end":10,"id":1},{"text":"a","start":11,"end":12,"id":2},{"text":"simple","start":13,"end":19,"id":3},{"text":"proof","start":20,"end":25,"id":4},{"text":"for","start":26,"end":29,"id":5},{"text":"the","start":30,"end":33,"id":6},{"text":"oracle","start":34,"end":40,"id":7},{"text":"inequality","start":41,"end":51,"id":8},{"text":"for","start":52,"end":55,"id":9},{"text":"the","start":56,"end":59,"id":10},{"text":"excess","start":60,"end":66,"id":11},{"text":"risk","start":67,"end":71,"id":12},{"text":"of","start":72,"end":74,"id":13},{"text":"structural","start":75,"end":85,"id":14},{"text":"risk","start":86,"end":90,"id":15},{"text":"minimizers","start":91,"end":101,"id":16},{"text":"using","start":102,"end":107,"id":17},{"text":"a","start":108,"end":109,"id":18},{"text":"lasso","start":110,"end":115,"id":19},{"text":"type","start":116,"end":120,"id":20},{"text":"penalty","start":121,"end":128,"id":21},{"text":".","start":128,"end":129,"id":22}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Second, we apply our results to the analysis of ERG models.","_input_hash":-1181044587,"_task_hash":215651923,"tokens":[{"text":"Second","start":0,"end":6,"id":0},{"text":",","start":6,"end":7,"id":1},{"text":"we","start":8,"end":10,"id":2},{"text":"apply","start":11,"end":16,"id":3},{"text":"our","start":17,"end":20,"id":4},{"text":"results","start":21,"end":28,"id":5},{"text":"to","start":29,"end":31,"id":6},{"text":"the","start":32,"end":35,"id":7},{"text":"analysis","start":36,"end":44,"id":8},{"text":"of","start":45,"end":47,"id":9},{"text":"ERG","start":48,"end":51,"id":10},{"text":"models","start":52,"end":58,"id":11},{"text":".","start":58,"end":59,"id":12}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"The network is further optimized by implementing Decision Forest.","_input_hash":-191073578,"_task_hash":1861232210,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"network","start":4,"end":11,"id":1},{"text":"is","start":12,"end":14,"id":2},{"text":"further","start":15,"end":22,"id":3},{"text":"optimized","start":23,"end":32,"id":4},{"text":"by","start":33,"end":35,"id":5},{"text":"implementing","start":36,"end":48,"id":6},{"text":"Decision","start":49,"end":57,"id":7},{"text":"Forest","start":58,"end":64,"id":8},{"text":".","start":64,"end":65,"id":9}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":49,"end":64,"token_start":7,"token_end":8,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Recent methods for estimating sparse undirected graphs for real-valued data in high dimensional problems rely heavily on the assumption of normality.","_input_hash":1125546306,"_task_hash":1396191968,"tokens":[{"text":"Recent","start":0,"end":6,"id":0},{"text":"methods","start":7,"end":14,"id":1},{"text":"for","start":15,"end":18,"id":2},{"text":"estimating","start":19,"end":29,"id":3},{"text":"sparse","start":30,"end":36,"id":4},{"text":"undirected","start":37,"end":47,"id":5},{"text":"graphs","start":48,"end":54,"id":6},{"text":"for","start":55,"end":58,"id":7},{"text":"real","start":59,"end":63,"id":8},{"text":"-","start":63,"end":64,"id":9},{"text":"valued","start":64,"end":70,"id":10},{"text":"data","start":71,"end":75,"id":11},{"text":"in","start":76,"end":78,"id":12},{"text":"high","start":79,"end":83,"id":13},{"text":"dimensional","start":84,"end":95,"id":14},{"text":"problems","start":96,"end":104,"id":15},{"text":"rely","start":105,"end":109,"id":16},{"text":"heavily","start":110,"end":117,"id":17},{"text":"on","start":118,"end":120,"id":18},{"text":"the","start":121,"end":124,"id":19},{"text":"assumption","start":125,"end":135,"id":20},{"text":"of","start":136,"end":138,"id":21},{"text":"normality","start":139,"end":148,"id":22},{"text":".","start":148,"end":149,"id":23}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"Changepoints are abrupt variations in the generative parameters of a data sequence.","_input_hash":1519314402,"_task_hash":-980663315,"tokens":[{"text":"Changepoints","start":0,"end":12,"id":0},{"text":"are","start":13,"end":16,"id":1},{"text":"abrupt","start":17,"end":23,"id":2},{"text":"variations","start":24,"end":34,"id":3},{"text":"in","start":35,"end":37,"id":4},{"text":"the","start":38,"end":41,"id":5},{"text":"generative","start":42,"end":52,"id":6},{"text":"parameters","start":53,"end":63,"id":7},{"text":"of","start":64,"end":66,"id":8},{"text":"a","start":67,"end":68,"id":9},{"text":"data","start":69,"end":73,"id":10},{"text":"sequence","start":74,"end":82,"id":11},{"text":".","start":82,"end":83,"id":12}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"em","meta":{"score":0},"_input_hash":1823540398,"_task_hash":-688689720,"_session_id":null,"_view_id":"text","answer":"reject","spans":[],"tokens":[{"text":"em","start":0,"end":2,"id":0}]}
{"text":"Results on vowel recognition data, Parkinson's disease data, and microarray data are also given.","_input_hash":1269753913,"_task_hash":-861504645,"tokens":[{"text":"Results","start":0,"end":7,"id":0},{"text":"on","start":8,"end":10,"id":1},{"text":"vowel","start":11,"end":16,"id":2},{"text":"recognition","start":17,"end":28,"id":3},{"text":"data","start":29,"end":33,"id":4},{"text":",","start":33,"end":34,"id":5},{"text":"Parkinson","start":35,"end":44,"id":6},{"text":"'s","start":44,"end":46,"id":7},{"text":"disease","start":47,"end":54,"id":8},{"text":"data","start":55,"end":59,"id":9},{"text":",","start":59,"end":60,"id":10},{"text":"and","start":61,"end":64,"id":11},{"text":"microarray","start":65,"end":75,"id":12},{"text":"data","start":76,"end":80,"id":13},{"text":"are","start":81,"end":84,"id":14},{"text":"also","start":85,"end":89,"id":15},{"text":"given","start":90,"end":95,"id":16},{"text":".","start":95,"end":96,"id":17}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"The GPLVM consists of a set of points in a low dimensional latent space, and a stochastic map to the observed space.","_input_hash":867554865,"_task_hash":582911741,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"GPLVM","start":4,"end":9,"id":1},{"text":"consists","start":10,"end":18,"id":2},{"text":"of","start":19,"end":21,"id":3},{"text":"a","start":22,"end":23,"id":4},{"text":"set","start":24,"end":27,"id":5},{"text":"of","start":28,"end":30,"id":6},{"text":"points","start":31,"end":37,"id":7},{"text":"in","start":38,"end":40,"id":8},{"text":"a","start":41,"end":42,"id":9},{"text":"low","start":43,"end":46,"id":10},{"text":"dimensional","start":47,"end":58,"id":11},{"text":"latent","start":59,"end":65,"id":12},{"text":"space","start":66,"end":71,"id":13},{"text":",","start":71,"end":72,"id":14},{"text":"and","start":73,"end":76,"id":15},{"text":"a","start":77,"end":78,"id":16},{"text":"stochastic","start":79,"end":89,"id":17},{"text":"map","start":90,"end":93,"id":18},{"text":"to","start":94,"end":96,"id":19},{"text":"the","start":97,"end":100,"id":20},{"text":"observed","start":101,"end":109,"id":21},{"text":"space","start":110,"end":115,"id":22},{"text":".","start":115,"end":116,"id":23}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Viewing the tree size as a complexity parameter, we then select a forest using data splitting, and prove bounds on excess risk and structure selection consistency of the procedure.","_input_hash":-1610309224,"_task_hash":-893889697,"tokens":[{"text":"Viewing","start":0,"end":7,"id":0},{"text":"the","start":8,"end":11,"id":1},{"text":"tree","start":12,"end":16,"id":2},{"text":"size","start":17,"end":21,"id":3},{"text":"as","start":22,"end":24,"id":4},{"text":"a","start":25,"end":26,"id":5},{"text":"complexity","start":27,"end":37,"id":6},{"text":"parameter","start":38,"end":47,"id":7},{"text":",","start":47,"end":48,"id":8},{"text":"we","start":49,"end":51,"id":9},{"text":"then","start":52,"end":56,"id":10},{"text":"select","start":57,"end":63,"id":11},{"text":"a","start":64,"end":65,"id":12},{"text":"forest","start":66,"end":72,"id":13},{"text":"using","start":73,"end":78,"id":14},{"text":"data","start":79,"end":83,"id":15},{"text":"splitting","start":84,"end":93,"id":16},{"text":",","start":93,"end":94,"id":17},{"text":"and","start":95,"end":98,"id":18},{"text":"prove","start":99,"end":104,"id":19},{"text":"bounds","start":105,"end":111,"id":20},{"text":"on","start":112,"end":114,"id":21},{"text":"excess","start":115,"end":121,"id":22},{"text":"risk","start":122,"end":126,"id":23},{"text":"and","start":127,"end":130,"id":24},{"text":"structure","start":131,"end":140,"id":25},{"text":"selection","start":141,"end":150,"id":26},{"text":"consistency","start":151,"end":162,"id":27},{"text":"of","start":163,"end":165,"id":28},{"text":"the","start":166,"end":169,"id":29},{"text":"procedure","start":170,"end":179,"id":30},{"text":".","start":179,"end":180,"id":31}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":66,"end":72,"token_start":13,"token_end":13,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We construct a framework for studying clustering algorithms, which includes two key ideas:","_input_hash":748612535,"_task_hash":-410756363,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"construct","start":3,"end":12,"id":1},{"text":"a","start":13,"end":14,"id":2},{"text":"framework","start":15,"end":24,"id":3},{"text":"for","start":25,"end":28,"id":4},{"text":"studying","start":29,"end":37,"id":5},{"text":"clustering","start":38,"end":48,"id":6},{"text":"algorithms","start":49,"end":59,"id":7},{"text":",","start":59,"end":60,"id":8},{"text":"which","start":61,"end":66,"id":9},{"text":"includes","start":67,"end":75,"id":10},{"text":"two","start":76,"end":79,"id":11},{"text":"key","start":80,"end":83,"id":12},{"text":"ideas","start":84,"end":89,"id":13},{"text":":","start":89,"end":90,"id":14}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":38,"end":59,"token_start":6,"token_end":7,"label":"ALGO","answer":"reject"}],"answer":"reject"}
{"text":"We also give an example of use of aggregation to achieve minimax adaptation over anisotropic Besov spaces, which was not previously known in minimax theory (in regression on a random design).","_input_hash":-1727036382,"_task_hash":341697707,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"also","start":3,"end":7,"id":1},{"text":"give","start":8,"end":12,"id":2},{"text":"an","start":13,"end":15,"id":3},{"text":"example","start":16,"end":23,"id":4},{"text":"of","start":24,"end":26,"id":5},{"text":"use","start":27,"end":30,"id":6},{"text":"of","start":31,"end":33,"id":7},{"text":"aggregation","start":34,"end":45,"id":8},{"text":"to","start":46,"end":48,"id":9},{"text":"achieve","start":49,"end":56,"id":10},{"text":"minimax","start":57,"end":64,"id":11},{"text":"adaptation","start":65,"end":75,"id":12},{"text":"over","start":76,"end":80,"id":13},{"text":"anisotropic","start":81,"end":92,"id":14},{"text":"Besov","start":93,"end":98,"id":15},{"text":"spaces","start":99,"end":105,"id":16},{"text":",","start":105,"end":106,"id":17},{"text":"which","start":107,"end":112,"id":18},{"text":"was","start":113,"end":116,"id":19},{"text":"not","start":117,"end":120,"id":20},{"text":"previously","start":121,"end":131,"id":21},{"text":"known","start":132,"end":137,"id":22},{"text":"in","start":138,"end":140,"id":23},{"text":"minimax","start":141,"end":148,"id":24},{"text":"theory","start":149,"end":155,"id":25},{"text":"(","start":156,"end":157,"id":26},{"text":"in","start":157,"end":159,"id":27},{"text":"regression","start":160,"end":170,"id":28},{"text":"on","start":171,"end":173,"id":29},{"text":"a","start":174,"end":175,"id":30},{"text":"random","start":176,"end":182,"id":31},{"text":"design","start":183,"end":189,"id":32},{"text":")","start":189,"end":190,"id":33},{"text":".","start":190,"end":191,"id":34}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":34,"end":45,"token_start":8,"token_end":8,"label":"ALGO","answer":"reject"}],"answer":"reject"}
{"text":"mathematical_model|NOUN","word":"mathematical model","sense":"NOUN","meta":{"score":0.7804999948,"sense":"NOUN"},"_input_hash":-2032882010,"_task_hash":-715595245,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"mathematical_model|NOUN","start":0,"end":23,"id":0}]}
{"text":"Prediction problems motivate this research:","_input_hash":298200160,"_task_hash":1586548048,"tokens":[{"text":"Prediction","start":0,"end":10,"id":0},{"text":"problems","start":11,"end":19,"id":1},{"text":"motivate","start":20,"end":28,"id":2},{"text":"this","start":29,"end":33,"id":3},{"text":"research","start":34,"end":42,"id":4},{"text":":","start":42,"end":43,"id":5}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Using this, we argue that there are situations in which the excess risk of our method is of order 1/n, while the excess risk of empirical risk minimization is of order 1/sqrt/{n}.","_input_hash":-1371932534,"_task_hash":1519199756,"tokens":[{"text":"Using","start":0,"end":5,"id":0},{"text":"this","start":6,"end":10,"id":1},{"text":",","start":10,"end":11,"id":2},{"text":"we","start":12,"end":14,"id":3},{"text":"argue","start":15,"end":20,"id":4},{"text":"that","start":21,"end":25,"id":5},{"text":"there","start":26,"end":31,"id":6},{"text":"are","start":32,"end":35,"id":7},{"text":"situations","start":36,"end":46,"id":8},{"text":"in","start":47,"end":49,"id":9},{"text":"which","start":50,"end":55,"id":10},{"text":"the","start":56,"end":59,"id":11},{"text":"excess","start":60,"end":66,"id":12},{"text":"risk","start":67,"end":71,"id":13},{"text":"of","start":72,"end":74,"id":14},{"text":"our","start":75,"end":78,"id":15},{"text":"method","start":79,"end":85,"id":16},{"text":"is","start":86,"end":88,"id":17},{"text":"of","start":89,"end":91,"id":18},{"text":"order","start":92,"end":97,"id":19},{"text":"1","start":98,"end":99,"id":20},{"text":"/","start":99,"end":100,"id":21},{"text":"n","start":100,"end":101,"id":22},{"text":",","start":101,"end":102,"id":23},{"text":"while","start":103,"end":108,"id":24},{"text":"the","start":109,"end":112,"id":25},{"text":"excess","start":113,"end":119,"id":26},{"text":"risk","start":120,"end":124,"id":27},{"text":"of","start":125,"end":127,"id":28},{"text":"empirical","start":128,"end":137,"id":29},{"text":"risk","start":138,"end":142,"id":30},{"text":"minimization","start":143,"end":155,"id":31},{"text":"is","start":156,"end":158,"id":32},{"text":"of","start":159,"end":161,"id":33},{"text":"order","start":162,"end":167,"id":34},{"text":"1","start":168,"end":169,"id":35},{"text":"/","start":169,"end":170,"id":36},{"text":"sqrt/{n","start":170,"end":177,"id":37},{"text":"}","start":177,"end":178,"id":38},{"text":".","start":178,"end":179,"id":39}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"Furthermore, we illustrate the regularizer can also be applied to deep Boltzmann machines, which lead to sparse group deep Boltzmann machines.","_input_hash":-1701629641,"_task_hash":-297231635,"tokens":[{"text":"Furthermore","start":0,"end":11,"id":0},{"text":",","start":11,"end":12,"id":1},{"text":"we","start":13,"end":15,"id":2},{"text":"illustrate","start":16,"end":26,"id":3},{"text":"the","start":27,"end":30,"id":4},{"text":"regularizer","start":31,"end":42,"id":5},{"text":"can","start":43,"end":46,"id":6},{"text":"also","start":47,"end":51,"id":7},{"text":"be","start":52,"end":54,"id":8},{"text":"applied","start":55,"end":62,"id":9},{"text":"to","start":63,"end":65,"id":10},{"text":"deep","start":66,"end":70,"id":11},{"text":"Boltzmann","start":71,"end":80,"id":12},{"text":"machines","start":81,"end":89,"id":13},{"text":",","start":89,"end":90,"id":14},{"text":"which","start":91,"end":96,"id":15},{"text":"lead","start":97,"end":101,"id":16},{"text":"to","start":102,"end":104,"id":17},{"text":"sparse","start":105,"end":111,"id":18},{"text":"group","start":112,"end":117,"id":19},{"text":"deep","start":118,"end":122,"id":20},{"text":"Boltzmann","start":123,"end":132,"id":21},{"text":"machines","start":133,"end":141,"id":22},{"text":".","start":141,"end":142,"id":23}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":71,"end":89,"token_start":12,"token_end":13,"label":"ALGO","answer":"accept"},{"start":123,"end":141,"token_start":21,"token_end":22,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"In this report, we derive a non-negative series expansion for the Jensen-Shannon divergence (JSD) between two probability distributions.","_input_hash":-379261663,"_task_hash":-1352715429,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"this","start":3,"end":7,"id":1},{"text":"report","start":8,"end":14,"id":2},{"text":",","start":14,"end":15,"id":3},{"text":"we","start":16,"end":18,"id":4},{"text":"derive","start":19,"end":25,"id":5},{"text":"a","start":26,"end":27,"id":6},{"text":"non","start":28,"end":31,"id":7},{"text":"-","start":31,"end":32,"id":8},{"text":"negative","start":32,"end":40,"id":9},{"text":"series","start":41,"end":47,"id":10},{"text":"expansion","start":48,"end":57,"id":11},{"text":"for","start":58,"end":61,"id":12},{"text":"the","start":62,"end":65,"id":13},{"text":"Jensen","start":66,"end":72,"id":14},{"text":"-","start":72,"end":73,"id":15},{"text":"Shannon","start":73,"end":80,"id":16},{"text":"divergence","start":81,"end":91,"id":17},{"text":"(","start":92,"end":93,"id":18},{"text":"JSD","start":93,"end":96,"id":19},{"text":")","start":96,"end":97,"id":20},{"text":"between","start":98,"end":105,"id":21},{"text":"two","start":106,"end":109,"id":22},{"text":"probability","start":110,"end":121,"id":23},{"text":"distributions","start":122,"end":135,"id":24},{"text":".","start":135,"end":136,"id":25}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"One approach to account for non-trivial correlations between outputs employs convolution processes.","_input_hash":1942995204,"_task_hash":-997009107,"tokens":[{"text":"One","start":0,"end":3,"id":0},{"text":"approach","start":4,"end":12,"id":1},{"text":"to","start":13,"end":15,"id":2},{"text":"account","start":16,"end":23,"id":3},{"text":"for","start":24,"end":27,"id":4},{"text":"non","start":28,"end":31,"id":5},{"text":"-","start":31,"end":32,"id":6},{"text":"trivial","start":32,"end":39,"id":7},{"text":"correlations","start":40,"end":52,"id":8},{"text":"between","start":53,"end":60,"id":9},{"text":"outputs","start":61,"end":68,"id":10},{"text":"employs","start":69,"end":76,"id":11},{"text":"convolution","start":77,"end":88,"id":12},{"text":"processes","start":89,"end":98,"id":13},{"text":".","start":98,"end":99,"id":14}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We present some evidence to show that BKNN still significantly underestimates model uncertainty.","_input_hash":-437162734,"_task_hash":-754743438,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"present","start":3,"end":10,"id":1},{"text":"some","start":11,"end":15,"id":2},{"text":"evidence","start":16,"end":24,"id":3},{"text":"to","start":25,"end":27,"id":4},{"text":"show","start":28,"end":32,"id":5},{"text":"that","start":33,"end":37,"id":6},{"text":"BKNN","start":38,"end":42,"id":7},{"text":"still","start":43,"end":48,"id":8},{"text":"significantly","start":49,"end":62,"id":9},{"text":"underestimates","start":63,"end":77,"id":10},{"text":"model","start":78,"end":83,"id":11},{"text":"uncertainty","start":84,"end":95,"id":12},{"text":".","start":95,"end":96,"id":13}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"For density estimation, we do not assume the true distribution corresponds to a forest;","_input_hash":-1307895784,"_task_hash":638218156,"tokens":[{"text":"For","start":0,"end":3,"id":0},{"text":"density","start":4,"end":11,"id":1},{"text":"estimation","start":12,"end":22,"id":2},{"text":",","start":22,"end":23,"id":3},{"text":"we","start":24,"end":26,"id":4},{"text":"do","start":27,"end":29,"id":5},{"text":"not","start":30,"end":33,"id":6},{"text":"assume","start":34,"end":40,"id":7},{"text":"the","start":41,"end":44,"id":8},{"text":"true","start":45,"end":49,"id":9},{"text":"distribution","start":50,"end":62,"id":10},{"text":"corresponds","start":63,"end":74,"id":11},{"text":"to","start":75,"end":77,"id":12},{"text":"a","start":78,"end":79,"id":13},{"text":"forest","start":80,"end":86,"id":14},{"text":";","start":86,"end":87,"id":15}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":80,"end":86,"token_start":14,"token_end":14,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"One approach to the problem is to search for a lower dimensional manifold which captures the main characteristics of the data.","_input_hash":-503766215,"_task_hash":-1754268838,"tokens":[{"text":"One","start":0,"end":3,"id":0},{"text":"approach","start":4,"end":12,"id":1},{"text":"to","start":13,"end":15,"id":2},{"text":"the","start":16,"end":19,"id":3},{"text":"problem","start":20,"end":27,"id":4},{"text":"is","start":28,"end":30,"id":5},{"text":"to","start":31,"end":33,"id":6},{"text":"search","start":34,"end":40,"id":7},{"text":"for","start":41,"end":44,"id":8},{"text":"a","start":45,"end":46,"id":9},{"text":"lower","start":47,"end":52,"id":10},{"text":"dimensional","start":53,"end":64,"id":11},{"text":"manifold","start":65,"end":73,"id":12},{"text":"which","start":74,"end":79,"id":13},{"text":"captures","start":80,"end":88,"id":14},{"text":"the","start":89,"end":92,"id":15},{"text":"main","start":93,"end":97,"id":16},{"text":"characteristics","start":98,"end":113,"id":17},{"text":"of","start":114,"end":116,"id":18},{"text":"the","start":117,"end":120,"id":19},{"text":"data","start":121,"end":125,"id":20},{"text":".","start":125,"end":126,"id":21}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Feature path realizations also reveal important non-linear correlations among features that prove useful in determining a subset of significant variables.","_input_hash":-1968131365,"_task_hash":283339050,"tokens":[{"text":"Feature","start":0,"end":7,"id":0},{"text":"path","start":8,"end":12,"id":1},{"text":"realizations","start":13,"end":25,"id":2},{"text":"also","start":26,"end":30,"id":3},{"text":"reveal","start":31,"end":37,"id":4},{"text":"important","start":38,"end":47,"id":5},{"text":"non","start":48,"end":51,"id":6},{"text":"-","start":51,"end":52,"id":7},{"text":"linear","start":52,"end":58,"id":8},{"text":"correlations","start":59,"end":71,"id":9},{"text":"among","start":72,"end":77,"id":10},{"text":"features","start":78,"end":86,"id":11},{"text":"that","start":87,"end":91,"id":12},{"text":"prove","start":92,"end":97,"id":13},{"text":"useful","start":98,"end":104,"id":14},{"text":"in","start":105,"end":107,"id":15},{"text":"determining","start":108,"end":119,"id":16},{"text":"a","start":120,"end":121,"id":17},{"text":"subset","start":122,"end":128,"id":18},{"text":"of","start":129,"end":131,"id":19},{"text":"significant","start":132,"end":143,"id":20},{"text":"variables","start":144,"end":153,"id":21},{"text":".","start":153,"end":154,"id":22}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We show that the reconstruction weights computed by LLE capture the high-dimensional structure of the neighborhoods, and not the low-dimensional manifold structure.","_input_hash":-1196170794,"_task_hash":282924980,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"show","start":3,"end":7,"id":1},{"text":"that","start":8,"end":12,"id":2},{"text":"the","start":13,"end":16,"id":3},{"text":"reconstruction","start":17,"end":31,"id":4},{"text":"weights","start":32,"end":39,"id":5},{"text":"computed","start":40,"end":48,"id":6},{"text":"by","start":49,"end":51,"id":7},{"text":"LLE","start":52,"end":55,"id":8},{"text":"capture","start":56,"end":63,"id":9},{"text":"the","start":64,"end":67,"id":10},{"text":"high","start":68,"end":72,"id":11},{"text":"-","start":72,"end":73,"id":12},{"text":"dimensional","start":73,"end":84,"id":13},{"text":"structure","start":85,"end":94,"id":14},{"text":"of","start":95,"end":97,"id":15},{"text":"the","start":98,"end":101,"id":16},{"text":"neighborhoods","start":102,"end":115,"id":17},{"text":",","start":115,"end":116,"id":18},{"text":"and","start":117,"end":120,"id":19},{"text":"not","start":121,"end":124,"id":20},{"text":"the","start":125,"end":128,"id":21},{"text":"low","start":129,"end":132,"id":22},{"text":"-","start":132,"end":133,"id":23},{"text":"dimensional","start":133,"end":144,"id":24},{"text":"manifold","start":145,"end":153,"id":25},{"text":"structure","start":154,"end":163,"id":26},{"text":".","start":163,"end":164,"id":27}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We show how to use a semiparametric Gaussian copula--or \"nonparanormal\"--for high dimensional inference.","_input_hash":-1958123589,"_task_hash":-2056500758,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"show","start":3,"end":7,"id":1},{"text":"how","start":8,"end":11,"id":2},{"text":"to","start":12,"end":14,"id":3},{"text":"use","start":15,"end":18,"id":4},{"text":"a","start":19,"end":20,"id":5},{"text":"semiparametric","start":21,"end":35,"id":6},{"text":"Gaussian","start":36,"end":44,"id":7},{"text":"copula","start":45,"end":51,"id":8},{"text":"--","start":51,"end":53,"id":9},{"text":"or","start":53,"end":55,"id":10},{"text":"\"","start":56,"end":57,"id":11},{"text":"nonparanormal\"--for","start":57,"end":76,"id":12},{"text":"high","start":77,"end":81,"id":13},{"text":"dimensional","start":82,"end":93,"id":14},{"text":"inference","start":94,"end":103,"id":15},{"text":".","start":103,"end":104,"id":16}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"This monograph deals with adaptive supervised classification, using tools borrowed from statistical mechanics and information theory, stemming from the PACBayesian approach pioneered by David McAllester and applied to a conception of statistical learning theory forged by Vladimir Vapnik.","_input_hash":291673665,"_task_hash":1261677885,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"monograph","start":5,"end":14,"id":1},{"text":"deals","start":15,"end":20,"id":2},{"text":"with","start":21,"end":25,"id":3},{"text":"adaptive","start":26,"end":34,"id":4},{"text":"supervised","start":35,"end":45,"id":5},{"text":"classification","start":46,"end":60,"id":6},{"text":",","start":60,"end":61,"id":7},{"text":"using","start":62,"end":67,"id":8},{"text":"tools","start":68,"end":73,"id":9},{"text":"borrowed","start":74,"end":82,"id":10},{"text":"from","start":83,"end":87,"id":11},{"text":"statistical","start":88,"end":99,"id":12},{"text":"mechanics","start":100,"end":109,"id":13},{"text":"and","start":110,"end":113,"id":14},{"text":"information","start":114,"end":125,"id":15},{"text":"theory","start":126,"end":132,"id":16},{"text":",","start":132,"end":133,"id":17},{"text":"stemming","start":134,"end":142,"id":18},{"text":"from","start":143,"end":147,"id":19},{"text":"the","start":148,"end":151,"id":20},{"text":"PACBayesian","start":152,"end":163,"id":21},{"text":"approach","start":164,"end":172,"id":22},{"text":"pioneered","start":173,"end":182,"id":23},{"text":"by","start":183,"end":185,"id":24},{"text":"David","start":186,"end":191,"id":25},{"text":"McAllester","start":192,"end":202,"id":26},{"text":"and","start":203,"end":206,"id":27},{"text":"applied","start":207,"end":214,"id":28},{"text":"to","start":215,"end":217,"id":29},{"text":"a","start":218,"end":219,"id":30},{"text":"conception","start":220,"end":230,"id":31},{"text":"of","start":231,"end":233,"id":32},{"text":"statistical","start":234,"end":245,"id":33},{"text":"learning","start":246,"end":254,"id":34},{"text":"theory","start":255,"end":261,"id":35},{"text":"forged","start":262,"end":268,"id":36},{"text":"by","start":269,"end":271,"id":37},{"text":"Vladimir","start":272,"end":280,"id":38},{"text":"Vapnik","start":281,"end":287,"id":39},{"text":".","start":287,"end":288,"id":40}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Thereby, it is able to discover the most relevant features, even if their appearance in the training data is entirely prevented by noise.","_input_hash":836817907,"_task_hash":167990887,"tokens":[{"text":"Thereby","start":0,"end":7,"id":0},{"text":",","start":7,"end":8,"id":1},{"text":"it","start":9,"end":11,"id":2},{"text":"is","start":12,"end":14,"id":3},{"text":"able","start":15,"end":19,"id":4},{"text":"to","start":20,"end":22,"id":5},{"text":"discover","start":23,"end":31,"id":6},{"text":"the","start":32,"end":35,"id":7},{"text":"most","start":36,"end":40,"id":8},{"text":"relevant","start":41,"end":49,"id":9},{"text":"features","start":50,"end":58,"id":10},{"text":",","start":58,"end":59,"id":11},{"text":"even","start":60,"end":64,"id":12},{"text":"if","start":65,"end":67,"id":13},{"text":"their","start":68,"end":73,"id":14},{"text":"appearance","start":74,"end":84,"id":15},{"text":"in","start":85,"end":87,"id":16},{"text":"the","start":88,"end":91,"id":17},{"text":"training","start":92,"end":100,"id":18},{"text":"data","start":101,"end":105,"id":19},{"text":"is","start":106,"end":108,"id":20},{"text":"entirely","start":109,"end":117,"id":21},{"text":"prevented","start":118,"end":127,"id":22},{"text":"by","start":128,"end":130,"id":23},{"text":"noise","start":131,"end":136,"id":24},{"text":".","start":136,"end":137,"id":25}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"parallelization|NOUN","word":"parallelization","sense":"NOUN","meta":{"score":0.7779999971,"sense":"NOUN"},"_input_hash":-1534185698,"_task_hash":-1177330319,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"parallelization|NOUN","start":0,"end":20,"id":0}]}
{"text":"We have developed an efficient algorithm for the maximum likelihood joint tracking and association problem in a strong clutter for GMTI data.","_input_hash":2013596882,"_task_hash":1101493405,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"have","start":3,"end":7,"id":1},{"text":"developed","start":8,"end":17,"id":2},{"text":"an","start":18,"end":20,"id":3},{"text":"efficient","start":21,"end":30,"id":4},{"text":"algorithm","start":31,"end":40,"id":5},{"text":"for","start":41,"end":44,"id":6},{"text":"the","start":45,"end":48,"id":7},{"text":"maximum","start":49,"end":56,"id":8},{"text":"likelihood","start":57,"end":67,"id":9},{"text":"joint","start":68,"end":73,"id":10},{"text":"tracking","start":74,"end":82,"id":11},{"text":"and","start":83,"end":86,"id":12},{"text":"association","start":87,"end":98,"id":13},{"text":"problem","start":99,"end":106,"id":14},{"text":"in","start":107,"end":109,"id":15},{"text":"a","start":110,"end":111,"id":16},{"text":"strong","start":112,"end":118,"id":17},{"text":"clutter","start":119,"end":126,"id":18},{"text":"for","start":127,"end":130,"id":19},{"text":"GMTI","start":131,"end":135,"id":20},{"text":"data","start":136,"end":140,"id":21},{"text":".","start":140,"end":141,"id":22}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":21,"end":40,"token_start":4,"token_end":5,"label":"ALGO","answer":"reject"}],"answer":"reject"}
{"text":"set_theory|NOUN","word":"set theory","sense":"NOUN","meta":{"score":0.7789999843,"sense":"NOUN"},"_input_hash":1603971363,"_task_hash":-279327291,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"set_theory|NOUN","start":0,"end":15,"id":0}]}
{"text":"numerical_analysis|NOUN","word":"numerical analysis","sense":"NOUN","meta":{"score":0.7752000093,"sense":"NOUN"},"_input_hash":-890635509,"_task_hash":1990036306,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"numerical_analysis|NOUN","start":0,"end":23,"id":0}]}
{"text":"Geographical flow data, properly conditioned, illustrate the procedure as well as visualization aspects.","_input_hash":184435809,"_task_hash":2001808480,"tokens":[{"text":"Geographical","start":0,"end":12,"id":0},{"text":"flow","start":13,"end":17,"id":1},{"text":"data","start":18,"end":22,"id":2},{"text":",","start":22,"end":23,"id":3},{"text":"properly","start":24,"end":32,"id":4},{"text":"conditioned","start":33,"end":44,"id":5},{"text":",","start":44,"end":45,"id":6},{"text":"illustrate","start":46,"end":56,"id":7},{"text":"the","start":57,"end":60,"id":8},{"text":"procedure","start":61,"end":70,"id":9},{"text":"as","start":71,"end":73,"id":10},{"text":"well","start":74,"end":78,"id":11},{"text":"as","start":79,"end":81,"id":12},{"text":"visualization","start":82,"end":95,"id":13},{"text":"aspects","start":96,"end":103,"id":14},{"text":".","start":103,"end":104,"id":15}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"It is shown how this can be used to build a heuristic approximation to the maximum relationship over a finite set of Gaussian variables, allowing approximate inference by Expectation Propagation on such quantities.","_input_hash":1454766528,"_task_hash":-341878961,"tokens":[{"text":"It","start":0,"end":2,"id":0},{"text":"is","start":3,"end":5,"id":1},{"text":"shown","start":6,"end":11,"id":2},{"text":"how","start":12,"end":15,"id":3},{"text":"this","start":16,"end":20,"id":4},{"text":"can","start":21,"end":24,"id":5},{"text":"be","start":25,"end":27,"id":6},{"text":"used","start":28,"end":32,"id":7},{"text":"to","start":33,"end":35,"id":8},{"text":"build","start":36,"end":41,"id":9},{"text":"a","start":42,"end":43,"id":10},{"text":"heuristic","start":44,"end":53,"id":11},{"text":"approximation","start":54,"end":67,"id":12},{"text":"to","start":68,"end":70,"id":13},{"text":"the","start":71,"end":74,"id":14},{"text":"maximum","start":75,"end":82,"id":15},{"text":"relationship","start":83,"end":95,"id":16},{"text":"over","start":96,"end":100,"id":17},{"text":"a","start":101,"end":102,"id":18},{"text":"finite","start":103,"end":109,"id":19},{"text":"set","start":110,"end":113,"id":20},{"text":"of","start":114,"end":116,"id":21},{"text":"Gaussian","start":117,"end":125,"id":22},{"text":"variables","start":126,"end":135,"id":23},{"text":",","start":135,"end":136,"id":24},{"text":"allowing","start":137,"end":145,"id":25},{"text":"approximate","start":146,"end":157,"id":26},{"text":"inference","start":158,"end":167,"id":27},{"text":"by","start":168,"end":170,"id":28},{"text":"Expectation","start":171,"end":182,"id":29},{"text":"Propagation","start":183,"end":194,"id":30},{"text":"on","start":195,"end":197,"id":31},{"text":"such","start":198,"end":202,"id":32},{"text":"quantities","start":203,"end":213,"id":33},{"text":".","start":213,"end":214,"id":34}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":171,"end":194,"token_start":29,"token_end":30,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"neural_activity|NOUN","word":"neural activity","sense":"NOUN","meta":{"score":0.7541000247,"sense":"NOUN"},"_input_hash":-1307763834,"_task_hash":1072183409,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"neural_activity|NOUN","start":0,"end":20,"id":0}]}
{"text":"Here, we introduce the Feature Importance Ranking Measure (FIRM), which by retrospective analysis of arbitrary learning machines allows to achieve both excellent predictive performance and superior interpretation.","_input_hash":1541985135,"_task_hash":-2034018295,"tokens":[{"text":"Here","start":0,"end":4,"id":0},{"text":",","start":4,"end":5,"id":1},{"text":"we","start":6,"end":8,"id":2},{"text":"introduce","start":9,"end":18,"id":3},{"text":"the","start":19,"end":22,"id":4},{"text":"Feature","start":23,"end":30,"id":5},{"text":"Importance","start":31,"end":41,"id":6},{"text":"Ranking","start":42,"end":49,"id":7},{"text":"Measure","start":50,"end":57,"id":8},{"text":"(","start":58,"end":59,"id":9},{"text":"FIRM","start":59,"end":63,"id":10},{"text":")","start":63,"end":64,"id":11},{"text":",","start":64,"end":65,"id":12},{"text":"which","start":66,"end":71,"id":13},{"text":"by","start":72,"end":74,"id":14},{"text":"retrospective","start":75,"end":88,"id":15},{"text":"analysis","start":89,"end":97,"id":16},{"text":"of","start":98,"end":100,"id":17},{"text":"arbitrary","start":101,"end":110,"id":18},{"text":"learning","start":111,"end":119,"id":19},{"text":"machines","start":120,"end":128,"id":20},{"text":"allows","start":129,"end":135,"id":21},{"text":"to","start":136,"end":138,"id":22},{"text":"achieve","start":139,"end":146,"id":23},{"text":"both","start":147,"end":151,"id":24},{"text":"excellent","start":152,"end":161,"id":25},{"text":"predictive","start":162,"end":172,"id":26},{"text":"performance","start":173,"end":184,"id":27},{"text":"and","start":185,"end":188,"id":28},{"text":"superior","start":189,"end":197,"id":29},{"text":"interpretation","start":198,"end":212,"id":30},{"text":".","start":212,"end":213,"id":31}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"might","meta":{"score":0},"_input_hash":1322507247,"_task_hash":-1787926338,"_session_id":null,"_view_id":"text","answer":"reject","spans":[],"tokens":[{"text":"might","start":0,"end":5,"id":0}]}
{"text":"The different views are assumed to be connected by having paired samples;","_input_hash":702098655,"_task_hash":1770841791,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"different","start":4,"end":13,"id":1},{"text":"views","start":14,"end":19,"id":2},{"text":"are","start":20,"end":23,"id":3},{"text":"assumed","start":24,"end":31,"id":4},{"text":"to","start":32,"end":34,"id":5},{"text":"be","start":35,"end":37,"id":6},{"text":"connected","start":38,"end":47,"id":7},{"text":"by","start":48,"end":50,"id":8},{"text":"having","start":51,"end":57,"id":9},{"text":"paired","start":58,"end":64,"id":10},{"text":"samples","start":65,"end":72,"id":11},{"text":";","start":72,"end":73,"id":12}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Some representative applications of the kernel Bayes' rule are presented, including Baysian computation without likelihood and filtering with a nonparametric state-space model.","_input_hash":-728004888,"_task_hash":1299069463,"tokens":[{"text":"Some","start":0,"end":4,"id":0},{"text":"representative","start":5,"end":19,"id":1},{"text":"applications","start":20,"end":32,"id":2},{"text":"of","start":33,"end":35,"id":3},{"text":"the","start":36,"end":39,"id":4},{"text":"kernel","start":40,"end":46,"id":5},{"text":"Bayes","start":47,"end":52,"id":6},{"text":"'","start":52,"end":53,"id":7},{"text":"rule","start":54,"end":58,"id":8},{"text":"are","start":59,"end":62,"id":9},{"text":"presented","start":63,"end":72,"id":10},{"text":",","start":72,"end":73,"id":11},{"text":"including","start":74,"end":83,"id":12},{"text":"Baysian","start":84,"end":91,"id":13},{"text":"computation","start":92,"end":103,"id":14},{"text":"without","start":104,"end":111,"id":15},{"text":"likelihood","start":112,"end":122,"id":16},{"text":"and","start":123,"end":126,"id":17},{"text":"filtering","start":127,"end":136,"id":18},{"text":"with","start":137,"end":141,"id":19},{"text":"a","start":142,"end":143,"id":20},{"text":"nonparametric","start":144,"end":157,"id":21},{"text":"state","start":158,"end":163,"id":22},{"text":"-","start":163,"end":164,"id":23},{"text":"space","start":164,"end":169,"id":24},{"text":"model","start":170,"end":175,"id":25},{"text":".","start":175,"end":176,"id":26}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"In particular, by means of a detailed example, we provide some characterization of the properties of ERG models, and, in particular, of certain behaviors of ERG models known as degeneracy.","_input_hash":1206154583,"_task_hash":318287214,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"particular","start":3,"end":13,"id":1},{"text":",","start":13,"end":14,"id":2},{"text":"by","start":15,"end":17,"id":3},{"text":"means","start":18,"end":23,"id":4},{"text":"of","start":24,"end":26,"id":5},{"text":"a","start":27,"end":28,"id":6},{"text":"detailed","start":29,"end":37,"id":7},{"text":"example","start":38,"end":45,"id":8},{"text":",","start":45,"end":46,"id":9},{"text":"we","start":47,"end":49,"id":10},{"text":"provide","start":50,"end":57,"id":11},{"text":"some","start":58,"end":62,"id":12},{"text":"characterization","start":63,"end":79,"id":13},{"text":"of","start":80,"end":82,"id":14},{"text":"the","start":83,"end":86,"id":15},{"text":"properties","start":87,"end":97,"id":16},{"text":"of","start":98,"end":100,"id":17},{"text":"ERG","start":101,"end":104,"id":18},{"text":"models","start":105,"end":111,"id":19},{"text":",","start":111,"end":112,"id":20},{"text":"and","start":113,"end":116,"id":21},{"text":",","start":116,"end":117,"id":22},{"text":"in","start":118,"end":120,"id":23},{"text":"particular","start":121,"end":131,"id":24},{"text":",","start":131,"end":132,"id":25},{"text":"of","start":133,"end":135,"id":26},{"text":"certain","start":136,"end":143,"id":27},{"text":"behaviors","start":144,"end":153,"id":28},{"text":"of","start":154,"end":156,"id":29},{"text":"ERG","start":157,"end":160,"id":30},{"text":"models","start":161,"end":167,"id":31},{"text":"known","start":168,"end":173,"id":32},{"text":"as","start":174,"end":176,"id":33},{"text":"degeneracy","start":177,"end":187,"id":34},{"text":".","start":187,"end":188,"id":35}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"Our methodology provides new insight into similarities and differences between sparse reconstruction and approximate Bayesian inference, and has important implications for compressive sensing of real-world images.","_input_hash":85836900,"_task_hash":1595739516,"tokens":[{"text":"Our","start":0,"end":3,"id":0},{"text":"methodology","start":4,"end":15,"id":1},{"text":"provides","start":16,"end":24,"id":2},{"text":"new","start":25,"end":28,"id":3},{"text":"insight","start":29,"end":36,"id":4},{"text":"into","start":37,"end":41,"id":5},{"text":"similarities","start":42,"end":54,"id":6},{"text":"and","start":55,"end":58,"id":7},{"text":"differences","start":59,"end":70,"id":8},{"text":"between","start":71,"end":78,"id":9},{"text":"sparse","start":79,"end":85,"id":10},{"text":"reconstruction","start":86,"end":100,"id":11},{"text":"and","start":101,"end":104,"id":12},{"text":"approximate","start":105,"end":116,"id":13},{"text":"Bayesian","start":117,"end":125,"id":14},{"text":"inference","start":126,"end":135,"id":15},{"text":",","start":135,"end":136,"id":16},{"text":"and","start":137,"end":140,"id":17},{"text":"has","start":141,"end":144,"id":18},{"text":"important","start":145,"end":154,"id":19},{"text":"implications","start":155,"end":167,"id":20},{"text":"for","start":168,"end":171,"id":21},{"text":"compressive","start":172,"end":183,"id":22},{"text":"sensing","start":184,"end":191,"id":23},{"text":"of","start":192,"end":194,"id":24},{"text":"real","start":195,"end":199,"id":25},{"text":"-","start":199,"end":200,"id":26},{"text":"world","start":200,"end":205,"id":27},{"text":"images","start":206,"end":212,"id":28},{"text":".","start":212,"end":213,"id":29}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"let","meta":{"score":0},"_input_hash":-718232832,"_task_hash":-76390953,"_session_id":null,"_view_id":"text","answer":"reject","spans":[],"tokens":[{"text":"let","start":0,"end":3,"id":0}]}
{"text":"We show how to associate to any posterior distribution an effective temperature relating it to the Gibbs prior distribution with the same level of expected error rate, and how to estimate this effective temperature from data, resulting in an estimator whose expected error rate converges according to the best possible power of the sample size adaptively under any margin and parametric complexity assumptions.","_input_hash":1038190572,"_task_hash":655357634,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"show","start":3,"end":7,"id":1},{"text":"how","start":8,"end":11,"id":2},{"text":"to","start":12,"end":14,"id":3},{"text":"associate","start":15,"end":24,"id":4},{"text":"to","start":25,"end":27,"id":5},{"text":"any","start":28,"end":31,"id":6},{"text":"posterior","start":32,"end":41,"id":7},{"text":"distribution","start":42,"end":54,"id":8},{"text":"an","start":55,"end":57,"id":9},{"text":"effective","start":58,"end":67,"id":10},{"text":"temperature","start":68,"end":79,"id":11},{"text":"relating","start":80,"end":88,"id":12},{"text":"it","start":89,"end":91,"id":13},{"text":"to","start":92,"end":94,"id":14},{"text":"the","start":95,"end":98,"id":15},{"text":"Gibbs","start":99,"end":104,"id":16},{"text":"prior","start":105,"end":110,"id":17},{"text":"distribution","start":111,"end":123,"id":18},{"text":"with","start":124,"end":128,"id":19},{"text":"the","start":129,"end":132,"id":20},{"text":"same","start":133,"end":137,"id":21},{"text":"level","start":138,"end":143,"id":22},{"text":"of","start":144,"end":146,"id":23},{"text":"expected","start":147,"end":155,"id":24},{"text":"error","start":156,"end":161,"id":25},{"text":"rate","start":162,"end":166,"id":26},{"text":",","start":166,"end":167,"id":27},{"text":"and","start":168,"end":171,"id":28},{"text":"how","start":172,"end":175,"id":29},{"text":"to","start":176,"end":178,"id":30},{"text":"estimate","start":179,"end":187,"id":31},{"text":"this","start":188,"end":192,"id":32},{"text":"effective","start":193,"end":202,"id":33},{"text":"temperature","start":203,"end":214,"id":34},{"text":"from","start":215,"end":219,"id":35},{"text":"data","start":220,"end":224,"id":36},{"text":",","start":224,"end":225,"id":37},{"text":"resulting","start":226,"end":235,"id":38},{"text":"in","start":236,"end":238,"id":39},{"text":"an","start":239,"end":241,"id":40},{"text":"estimator","start":242,"end":251,"id":41},{"text":"whose","start":252,"end":257,"id":42},{"text":"expected","start":258,"end":266,"id":43},{"text":"error","start":267,"end":272,"id":44},{"text":"rate","start":273,"end":277,"id":45},{"text":"converges","start":278,"end":287,"id":46},{"text":"according","start":288,"end":297,"id":47},{"text":"to","start":298,"end":300,"id":48},{"text":"the","start":301,"end":304,"id":49},{"text":"best","start":305,"end":309,"id":50},{"text":"possible","start":310,"end":318,"id":51},{"text":"power","start":319,"end":324,"id":52},{"text":"of","start":325,"end":327,"id":53},{"text":"the","start":328,"end":331,"id":54},{"text":"sample","start":332,"end":338,"id":55},{"text":"size","start":339,"end":343,"id":56},{"text":"adaptively","start":344,"end":354,"id":57},{"text":"under","start":355,"end":360,"id":58},{"text":"any","start":361,"end":364,"id":59},{"text":"margin","start":365,"end":371,"id":60},{"text":"and","start":372,"end":375,"id":61},{"text":"parametric","start":376,"end":386,"id":62},{"text":"complexity","start":387,"end":397,"id":63},{"text":"assumptions","start":398,"end":409,"id":64},{"text":".","start":409,"end":410,"id":65}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Simulation results demonstrate the utility of KNIFE for both kernel regression and support vector machines with a variety of kernels.","_input_hash":-111780878,"_task_hash":-2012910529,"tokens":[{"text":"Simulation","start":0,"end":10,"id":0},{"text":"results","start":11,"end":18,"id":1},{"text":"demonstrate","start":19,"end":30,"id":2},{"text":"the","start":31,"end":34,"id":3},{"text":"utility","start":35,"end":42,"id":4},{"text":"of","start":43,"end":45,"id":5},{"text":"KNIFE","start":46,"end":51,"id":6},{"text":"for","start":52,"end":55,"id":7},{"text":"both","start":56,"end":60,"id":8},{"text":"kernel","start":61,"end":67,"id":9},{"text":"regression","start":68,"end":78,"id":10},{"text":"and","start":79,"end":82,"id":11},{"text":"support","start":83,"end":90,"id":12},{"text":"vector","start":91,"end":97,"id":13},{"text":"machines","start":98,"end":106,"id":14},{"text":"with","start":107,"end":111,"id":15},{"text":"a","start":112,"end":113,"id":16},{"text":"variety","start":114,"end":121,"id":17},{"text":"of","start":122,"end":124,"id":18},{"text":"kernels","start":125,"end":132,"id":19},{"text":".","start":132,"end":133,"id":20}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":61,"end":78,"token_start":9,"token_end":10,"label":"ALGO","answer":"accept"},{"start":83,"end":106,"token_start":12,"token_end":14,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"numerical_methods|NOUN","word":"numerical methods","sense":"NOUN","meta":{"score":0.7896000147,"sense":"NOUN"},"_input_hash":1376937719,"_task_hash":-1794277090,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"numerical_methods|NOUN","start":0,"end":22,"id":0}]}
{"text":"Many statistical methods have been proposed to estimate causal models in classical situations with fewer variables than observations (p<n, p:","_input_hash":815758402,"_task_hash":1579647097,"tokens":[{"text":"Many","start":0,"end":4,"id":0},{"text":"statistical","start":5,"end":16,"id":1},{"text":"methods","start":17,"end":24,"id":2},{"text":"have","start":25,"end":29,"id":3},{"text":"been","start":30,"end":34,"id":4},{"text":"proposed","start":35,"end":43,"id":5},{"text":"to","start":44,"end":46,"id":6},{"text":"estimate","start":47,"end":55,"id":7},{"text":"causal","start":56,"end":62,"id":8},{"text":"models","start":63,"end":69,"id":9},{"text":"in","start":70,"end":72,"id":10},{"text":"classical","start":73,"end":82,"id":11},{"text":"situations","start":83,"end":93,"id":12},{"text":"with","start":94,"end":98,"id":13},{"text":"fewer","start":99,"end":104,"id":14},{"text":"variables","start":105,"end":114,"id":15},{"text":"than","start":115,"end":119,"id":16},{"text":"observations","start":120,"end":132,"id":17},{"text":"(","start":133,"end":134,"id":18},{"text":"p","start":134,"end":135,"id":19},{"text":"<","start":135,"end":136,"id":20},{"text":"n","start":136,"end":137,"id":21},{"text":",","start":137,"end":138,"id":22},{"text":"p","start":139,"end":140,"id":23},{"text":":","start":140,"end":141,"id":24}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":5,"end":24,"token_start":1,"token_end":2,"label":"ALGO","answer":"reject"},{"start":56,"end":69,"token_start":8,"token_end":9,"label":"ALGO","answer":"reject"}],"answer":"reject"}
{"text":"We present a computationally efficient algorithm, with provable numerical convergence properties, for optimizing the penalized likelihood.","_input_hash":-1310496817,"_task_hash":1987461370,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"present","start":3,"end":10,"id":1},{"text":"a","start":11,"end":12,"id":2},{"text":"computationally","start":13,"end":28,"id":3},{"text":"efficient","start":29,"end":38,"id":4},{"text":"algorithm","start":39,"end":48,"id":5},{"text":",","start":48,"end":49,"id":6},{"text":"with","start":50,"end":54,"id":7},{"text":"provable","start":55,"end":63,"id":8},{"text":"numerical","start":64,"end":73,"id":9},{"text":"convergence","start":74,"end":85,"id":10},{"text":"properties","start":86,"end":96,"id":11},{"text":",","start":96,"end":97,"id":12},{"text":"for","start":98,"end":101,"id":13},{"text":"optimizing","start":102,"end":112,"id":14},{"text":"the","start":113,"end":116,"id":15},{"text":"penalized","start":117,"end":126,"id":16},{"text":"likelihood","start":127,"end":137,"id":17},{"text":".","start":137,"end":138,"id":18}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":29,"end":48,"token_start":4,"token_end":5,"label":"ALGO","answer":"reject"}],"answer":"reject"}
{"text":"practical_applications|NOUN","word":"practical applications","sense":"NOUN","meta":{"score":0.7858999968,"sense":"NOUN"},"_input_hash":79761643,"_task_hash":319103928,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"practical_applications|NOUN","start":0,"end":27,"id":0}]}
{"text":"iteratively|ADV","word":"iteratively","sense":"ADV","meta":{"score":0.77700001,"sense":"ADV"},"_input_hash":1988134512,"_task_hash":465857894,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"iteratively|ADV","start":0,"end":15,"id":0}]}
{"text":"We obtain an index of the complexity of a random sequence by allowing the role of the measure in classical probability theory to be played by a function we call the generating mechanism.","_input_hash":293701925,"_task_hash":-213590300,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"obtain","start":3,"end":9,"id":1},{"text":"an","start":10,"end":12,"id":2},{"text":"index","start":13,"end":18,"id":3},{"text":"of","start":19,"end":21,"id":4},{"text":"the","start":22,"end":25,"id":5},{"text":"complexity","start":26,"end":36,"id":6},{"text":"of","start":37,"end":39,"id":7},{"text":"a","start":40,"end":41,"id":8},{"text":"random","start":42,"end":48,"id":9},{"text":"sequence","start":49,"end":57,"id":10},{"text":"by","start":58,"end":60,"id":11},{"text":"allowing","start":61,"end":69,"id":12},{"text":"the","start":70,"end":73,"id":13},{"text":"role","start":74,"end":78,"id":14},{"text":"of","start":79,"end":81,"id":15},{"text":"the","start":82,"end":85,"id":16},{"text":"measure","start":86,"end":93,"id":17},{"text":"in","start":94,"end":96,"id":18},{"text":"classical","start":97,"end":106,"id":19},{"text":"probability","start":107,"end":118,"id":20},{"text":"theory","start":119,"end":125,"id":21},{"text":"to","start":126,"end":128,"id":22},{"text":"be","start":129,"end":131,"id":23},{"text":"played","start":132,"end":138,"id":24},{"text":"by","start":139,"end":141,"id":25},{"text":"a","start":142,"end":143,"id":26},{"text":"function","start":144,"end":152,"id":27},{"text":"we","start":153,"end":155,"id":28},{"text":"call","start":156,"end":160,"id":29},{"text":"the","start":161,"end":164,"id":30},{"text":"generating","start":165,"end":175,"id":31},{"text":"mechanism","start":176,"end":185,"id":32},{"text":".","start":185,"end":186,"id":33}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"does","meta":{"score":0},"_input_hash":-2079885618,"_task_hash":-1922061064,"_session_id":null,"_view_id":"text","answer":"reject","spans":[],"tokens":[{"text":"does","start":0,"end":4,"id":0}]}
{"text":"This interpretation requires essentially no conditions.","_input_hash":-1174012755,"_task_hash":1113905910,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"interpretation","start":5,"end":19,"id":1},{"text":"requires","start":20,"end":28,"id":2},{"text":"essentially","start":29,"end":40,"id":3},{"text":"no","start":41,"end":43,"id":4},{"text":"conditions","start":44,"end":54,"id":5},{"text":".","start":54,"end":55,"id":6}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"got","meta":{"score":0},"_input_hash":1518210131,"_task_hash":223461793,"_session_id":null,"_view_id":"text","answer":"reject","spans":[],"tokens":[{"text":"got","start":0,"end":3,"id":0}]}
{"text":"linear_systems|NOUN","word":"linear systems","sense":"NOUN","meta":{"score":0.7813000083,"sense":"NOUN"},"_input_hash":1944198125,"_task_hash":420574911,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"linear_systems|NOUN","start":0,"end":19,"id":0}]}
{"text":"Our procedure has a complexity linear, or close to linear, in the number of atoms, and allows the use of accelerated gradient techniques to solve the tree-structured sparse approximation problem at the same computational cost as traditional ones using the L1-norm.","_input_hash":-1480593943,"_task_hash":-1751024069,"tokens":[{"text":"Our","start":0,"end":3,"id":0},{"text":"procedure","start":4,"end":13,"id":1},{"text":"has","start":14,"end":17,"id":2},{"text":"a","start":18,"end":19,"id":3},{"text":"complexity","start":20,"end":30,"id":4},{"text":"linear","start":31,"end":37,"id":5},{"text":",","start":37,"end":38,"id":6},{"text":"or","start":39,"end":41,"id":7},{"text":"close","start":42,"end":47,"id":8},{"text":"to","start":48,"end":50,"id":9},{"text":"linear","start":51,"end":57,"id":10},{"text":",","start":57,"end":58,"id":11},{"text":"in","start":59,"end":61,"id":12},{"text":"the","start":62,"end":65,"id":13},{"text":"number","start":66,"end":72,"id":14},{"text":"of","start":73,"end":75,"id":15},{"text":"atoms","start":76,"end":81,"id":16},{"text":",","start":81,"end":82,"id":17},{"text":"and","start":83,"end":86,"id":18},{"text":"allows","start":87,"end":93,"id":19},{"text":"the","start":94,"end":97,"id":20},{"text":"use","start":98,"end":101,"id":21},{"text":"of","start":102,"end":104,"id":22},{"text":"accelerated","start":105,"end":116,"id":23},{"text":"gradient","start":117,"end":125,"id":24},{"text":"techniques","start":126,"end":136,"id":25},{"text":"to","start":137,"end":139,"id":26},{"text":"solve","start":140,"end":145,"id":27},{"text":"the","start":146,"end":149,"id":28},{"text":"tree","start":150,"end":154,"id":29},{"text":"-","start":154,"end":155,"id":30},{"text":"structured","start":155,"end":165,"id":31},{"text":"sparse","start":166,"end":172,"id":32},{"text":"approximation","start":173,"end":186,"id":33},{"text":"problem","start":187,"end":194,"id":34},{"text":"at","start":195,"end":197,"id":35},{"text":"the","start":198,"end":201,"id":36},{"text":"same","start":202,"end":206,"id":37},{"text":"computational","start":207,"end":220,"id":38},{"text":"cost","start":221,"end":225,"id":39},{"text":"as","start":226,"end":228,"id":40},{"text":"traditional","start":229,"end":240,"id":41},{"text":"ones","start":241,"end":245,"id":42},{"text":"using","start":246,"end":251,"id":43},{"text":"the","start":252,"end":255,"id":44},{"text":"L1-norm","start":256,"end":263,"id":45},{"text":".","start":263,"end":264,"id":46}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"machine_learning_algorithms|NOUN","word":"machine learning algorithms","sense":"NOUN","meta":{"score":0.8026000261,"sense":"NOUN"},"_input_hash":-1933158904,"_task_hash":1881051005,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"machine_learning_algorithms|NOUN","start":0,"end":32,"id":0}]}
{"text":"It arises from a combinatorial optimization problem which we cast as a variant of the set cover problem.","_input_hash":-2021799529,"_task_hash":105061103,"tokens":[{"text":"It","start":0,"end":2,"id":0},{"text":"arises","start":3,"end":9,"id":1},{"text":"from","start":10,"end":14,"id":2},{"text":"a","start":15,"end":16,"id":3},{"text":"combinatorial","start":17,"end":30,"id":4},{"text":"optimization","start":31,"end":43,"id":5},{"text":"problem","start":44,"end":51,"id":6},{"text":"which","start":52,"end":57,"id":7},{"text":"we","start":58,"end":60,"id":8},{"text":"cast","start":61,"end":65,"id":9},{"text":"as","start":66,"end":68,"id":10},{"text":"a","start":69,"end":70,"id":11},{"text":"variant","start":71,"end":78,"id":12},{"text":"of","start":79,"end":81,"id":13},{"text":"the","start":82,"end":85,"id":14},{"text":"set","start":86,"end":89,"id":15},{"text":"cover","start":90,"end":95,"id":16},{"text":"problem","start":96,"end":103,"id":17},{"text":".","start":103,"end":104,"id":18}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We give an explicit construction of kernels - and thus of their RKHSs - which leads in combination with a Lipschitz continuous loss function to consistent and statistically robust SMVs for additive models.","_input_hash":1823417886,"_task_hash":-1782603285,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"give","start":3,"end":7,"id":1},{"text":"an","start":8,"end":10,"id":2},{"text":"explicit","start":11,"end":19,"id":3},{"text":"construction","start":20,"end":32,"id":4},{"text":"of","start":33,"end":35,"id":5},{"text":"kernels","start":36,"end":43,"id":6},{"text":"-","start":44,"end":45,"id":7},{"text":"and","start":46,"end":49,"id":8},{"text":"thus","start":50,"end":54,"id":9},{"text":"of","start":55,"end":57,"id":10},{"text":"their","start":58,"end":63,"id":11},{"text":"RKHSs","start":64,"end":69,"id":12},{"text":"-","start":70,"end":71,"id":13},{"text":"which","start":72,"end":77,"id":14},{"text":"leads","start":78,"end":83,"id":15},{"text":"in","start":84,"end":86,"id":16},{"text":"combination","start":87,"end":98,"id":17},{"text":"with","start":99,"end":103,"id":18},{"text":"a","start":104,"end":105,"id":19},{"text":"Lipschitz","start":106,"end":115,"id":20},{"text":"continuous","start":116,"end":126,"id":21},{"text":"loss","start":127,"end":131,"id":22},{"text":"function","start":132,"end":140,"id":23},{"text":"to","start":141,"end":143,"id":24},{"text":"consistent","start":144,"end":154,"id":25},{"text":"and","start":155,"end":158,"id":26},{"text":"statistically","start":159,"end":172,"id":27},{"text":"robust","start":173,"end":179,"id":28},{"text":"SMVs","start":180,"end":184,"id":29},{"text":"for","start":185,"end":188,"id":30},{"text":"additive","start":189,"end":197,"id":31},{"text":"models","start":198,"end":204,"id":32},{"text":".","start":204,"end":205,"id":33}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"artificial_intelligence|NOUN","word":"artificial intelligence","sense":"NOUN","meta":{"score":0.8001999855,"sense":"NOUN"},"_input_hash":-846084962,"_task_hash":-1759305868,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"artificial_intelligence|NOUN","start":0,"end":28,"id":0}]}
{"text":"The former finds both disassortative and assortative structure, while the alternative assumes assortativity and finds community-like structures like the earlier methods motivated by physics.","_input_hash":251252844,"_task_hash":-328374854,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"former","start":4,"end":10,"id":1},{"text":"finds","start":11,"end":16,"id":2},{"text":"both","start":17,"end":21,"id":3},{"text":"disassortative","start":22,"end":36,"id":4},{"text":"and","start":37,"end":40,"id":5},{"text":"assortative","start":41,"end":52,"id":6},{"text":"structure","start":53,"end":62,"id":7},{"text":",","start":62,"end":63,"id":8},{"text":"while","start":64,"end":69,"id":9},{"text":"the","start":70,"end":73,"id":10},{"text":"alternative","start":74,"end":85,"id":11},{"text":"assumes","start":86,"end":93,"id":12},{"text":"assortativity","start":94,"end":107,"id":13},{"text":"and","start":108,"end":111,"id":14},{"text":"finds","start":112,"end":117,"id":15},{"text":"community","start":118,"end":127,"id":16},{"text":"-","start":127,"end":128,"id":17},{"text":"like","start":128,"end":132,"id":18},{"text":"structures","start":133,"end":143,"id":19},{"text":"like","start":144,"end":148,"id":20},{"text":"the","start":149,"end":152,"id":21},{"text":"earlier","start":153,"end":160,"id":22},{"text":"methods","start":161,"end":168,"id":23},{"text":"motivated","start":169,"end":178,"id":24},{"text":"by","start":179,"end":181,"id":25},{"text":"physics","start":182,"end":189,"id":26},{"text":".","start":189,"end":190,"id":27}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"When adapted to the MNIST data set, a two-layer sparse group Boltzmann machine achieves an error rate of $0.84\\%$, which is, to our knowledge, the best published result on the permutation-invariant version of the MNIST task.","_input_hash":-648323920,"_task_hash":-1628056821,"tokens":[{"text":"When","start":0,"end":4,"id":0},{"text":"adapted","start":5,"end":12,"id":1},{"text":"to","start":13,"end":15,"id":2},{"text":"the","start":16,"end":19,"id":3},{"text":"MNIST","start":20,"end":25,"id":4},{"text":"data","start":26,"end":30,"id":5},{"text":"set","start":31,"end":34,"id":6},{"text":",","start":34,"end":35,"id":7},{"text":"a","start":36,"end":37,"id":8},{"text":"two","start":38,"end":41,"id":9},{"text":"-","start":41,"end":42,"id":10},{"text":"layer","start":42,"end":47,"id":11},{"text":"sparse","start":48,"end":54,"id":12},{"text":"group","start":55,"end":60,"id":13},{"text":"Boltzmann","start":61,"end":70,"id":14},{"text":"machine","start":71,"end":78,"id":15},{"text":"achieves","start":79,"end":87,"id":16},{"text":"an","start":88,"end":90,"id":17},{"text":"error","start":91,"end":96,"id":18},{"text":"rate","start":97,"end":101,"id":19},{"text":"of","start":102,"end":104,"id":20},{"text":"$","start":105,"end":106,"id":21},{"text":"0.84\\%$","start":106,"end":113,"id":22},{"text":",","start":113,"end":114,"id":23},{"text":"which","start":115,"end":120,"id":24},{"text":"is","start":121,"end":123,"id":25},{"text":",","start":123,"end":124,"id":26},{"text":"to","start":125,"end":127,"id":27},{"text":"our","start":128,"end":131,"id":28},{"text":"knowledge","start":132,"end":141,"id":29},{"text":",","start":141,"end":142,"id":30},{"text":"the","start":143,"end":146,"id":31},{"text":"best","start":147,"end":151,"id":32},{"text":"published","start":152,"end":161,"id":33},{"text":"result","start":162,"end":168,"id":34},{"text":"on","start":169,"end":171,"id":35},{"text":"the","start":172,"end":175,"id":36},{"text":"permutation","start":176,"end":187,"id":37},{"text":"-","start":187,"end":188,"id":38},{"text":"invariant","start":188,"end":197,"id":39},{"text":"version","start":198,"end":205,"id":40},{"text":"of","start":206,"end":208,"id":41},{"text":"the","start":209,"end":212,"id":42},{"text":"MNIST","start":213,"end":218,"id":43},{"text":"task","start":219,"end":223,"id":44},{"text":".","start":223,"end":224,"id":45}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We consider the problem of jointly estimating the parameters as well as the structure of binary valued Markov Random Fields, in contrast to earlier work that focus on one of the two problems.","_input_hash":680968016,"_task_hash":2041940297,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"consider","start":3,"end":11,"id":1},{"text":"the","start":12,"end":15,"id":2},{"text":"problem","start":16,"end":23,"id":3},{"text":"of","start":24,"end":26,"id":4},{"text":"jointly","start":27,"end":34,"id":5},{"text":"estimating","start":35,"end":45,"id":6},{"text":"the","start":46,"end":49,"id":7},{"text":"parameters","start":50,"end":60,"id":8},{"text":"as","start":61,"end":63,"id":9},{"text":"well","start":64,"end":68,"id":10},{"text":"as","start":69,"end":71,"id":11},{"text":"the","start":72,"end":75,"id":12},{"text":"structure","start":76,"end":85,"id":13},{"text":"of","start":86,"end":88,"id":14},{"text":"binary","start":89,"end":95,"id":15},{"text":"valued","start":96,"end":102,"id":16},{"text":"Markov","start":103,"end":109,"id":17},{"text":"Random","start":110,"end":116,"id":18},{"text":"Fields","start":117,"end":123,"id":19},{"text":",","start":123,"end":124,"id":20},{"text":"in","start":125,"end":127,"id":21},{"text":"contrast","start":128,"end":136,"id":22},{"text":"to","start":137,"end":139,"id":23},{"text":"earlier","start":140,"end":147,"id":24},{"text":"work","start":148,"end":152,"id":25},{"text":"that","start":153,"end":157,"id":26},{"text":"focus","start":158,"end":163,"id":27},{"text":"on","start":164,"end":166,"id":28},{"text":"one","start":167,"end":170,"id":29},{"text":"of","start":171,"end":173,"id":30},{"text":"the","start":174,"end":177,"id":31},{"text":"two","start":178,"end":181,"id":32},{"text":"problems","start":182,"end":190,"id":33},{"text":".","start":190,"end":191,"id":34}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"We propose to investigate test statistics for testing homogeneity in reproducing kernel Hilbert spaces.","_input_hash":-1337820549,"_task_hash":-1191157355,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"propose","start":3,"end":10,"id":1},{"text":"to","start":11,"end":13,"id":2},{"text":"investigate","start":14,"end":25,"id":3},{"text":"test","start":26,"end":30,"id":4},{"text":"statistics","start":31,"end":41,"id":5},{"text":"for","start":42,"end":45,"id":6},{"text":"testing","start":46,"end":53,"id":7},{"text":"homogeneity","start":54,"end":65,"id":8},{"text":"in","start":66,"end":68,"id":9},{"text":"reproducing","start":69,"end":80,"id":10},{"text":"kernel","start":81,"end":87,"id":11},{"text":"Hilbert","start":88,"end":95,"id":12},{"text":"spaces","start":96,"end":102,"id":13},{"text":".","start":102,"end":103,"id":14}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"With Dirichlet Process priors and an efficient implementation the models are highly scalable, as demonstrated with a social network from the Last.fm web site, with 670,000 nodes and 1.89 million links.","_input_hash":913960638,"_task_hash":-1919846162,"tokens":[{"text":"With","start":0,"end":4,"id":0},{"text":"Dirichlet","start":5,"end":14,"id":1},{"text":"Process","start":15,"end":22,"id":2},{"text":"priors","start":23,"end":29,"id":3},{"text":"and","start":30,"end":33,"id":4},{"text":"an","start":34,"end":36,"id":5},{"text":"efficient","start":37,"end":46,"id":6},{"text":"implementation","start":47,"end":61,"id":7},{"text":"the","start":62,"end":65,"id":8},{"text":"models","start":66,"end":72,"id":9},{"text":"are","start":73,"end":76,"id":10},{"text":"highly","start":77,"end":83,"id":11},{"text":"scalable","start":84,"end":92,"id":12},{"text":",","start":92,"end":93,"id":13},{"text":"as","start":94,"end":96,"id":14},{"text":"demonstrated","start":97,"end":109,"id":15},{"text":"with","start":110,"end":114,"id":16},{"text":"a","start":115,"end":116,"id":17},{"text":"social","start":117,"end":123,"id":18},{"text":"network","start":124,"end":131,"id":19},{"text":"from","start":132,"end":136,"id":20},{"text":"the","start":137,"end":140,"id":21},{"text":"Last.fm","start":141,"end":148,"id":22},{"text":"web","start":149,"end":152,"id":23},{"text":"site","start":153,"end":157,"id":24},{"text":",","start":157,"end":158,"id":25},{"text":"with","start":159,"end":163,"id":26},{"text":"670,000","start":164,"end":171,"id":27},{"text":"nodes","start":172,"end":177,"id":28},{"text":"and","start":178,"end":181,"id":29},{"text":"1.89","start":182,"end":186,"id":30},{"text":"million","start":187,"end":194,"id":31},{"text":"links","start":195,"end":200,"id":32},{"text":".","start":200,"end":201,"id":33}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"programming_languages|NOUN","word":"programming languages","sense":"NOUN","meta":{"score":0.7784000039,"sense":"NOUN"},"_input_hash":-666989736,"_task_hash":1354618083,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"programming_languages|NOUN","start":0,"end":26,"id":0}]}
{"text":"computer_programs|NOUN","word":"computer programs","sense":"NOUN","meta":{"score":0.8090999722,"sense":"NOUN"},"_input_hash":1030041860,"_task_hash":505873811,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"computer_programs|NOUN","start":0,"end":22,"id":0}]}
{"text":"the number of observations).","_input_hash":-1297366295,"_task_hash":-818741412,"tokens":[{"text":"the","start":0,"end":3,"id":0},{"text":"number","start":4,"end":10,"id":1},{"text":"of","start":11,"end":13,"id":2},{"text":"observations","start":14,"end":26,"id":3},{"text":")","start":26,"end":27,"id":4},{"text":".","start":27,"end":28,"id":5}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"This has been motivated partly by frameworks like multitask learning, multisensor networks or structured output data.","_input_hash":1009388369,"_task_hash":2124680217,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"has","start":5,"end":8,"id":1},{"text":"been","start":9,"end":13,"id":2},{"text":"motivated","start":14,"end":23,"id":3},{"text":"partly","start":24,"end":30,"id":4},{"text":"by","start":31,"end":33,"id":5},{"text":"frameworks","start":34,"end":44,"id":6},{"text":"like","start":45,"end":49,"id":7},{"text":"multitask","start":50,"end":59,"id":8},{"text":"learning","start":60,"end":68,"id":9},{"text":",","start":68,"end":69,"id":10},{"text":"multisensor","start":70,"end":81,"id":11},{"text":"networks","start":82,"end":90,"id":12},{"text":"or","start":91,"end":93,"id":13},{"text":"structured","start":94,"end":104,"id":14},{"text":"output","start":105,"end":111,"id":15},{"text":"data","start":112,"end":116,"id":16},{"text":".","start":116,"end":117,"id":17}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Therefore, the probability of selecting proper class for a given input data, can be estimated even without the prior knowledge of its affiliation.","_input_hash":432004175,"_task_hash":-455672980,"tokens":[{"text":"Therefore","start":0,"end":9,"id":0},{"text":",","start":9,"end":10,"id":1},{"text":"the","start":11,"end":14,"id":2},{"text":"probability","start":15,"end":26,"id":3},{"text":"of","start":27,"end":29,"id":4},{"text":"selecting","start":30,"end":39,"id":5},{"text":"proper","start":40,"end":46,"id":6},{"text":"class","start":47,"end":52,"id":7},{"text":"for","start":53,"end":56,"id":8},{"text":"a","start":57,"end":58,"id":9},{"text":"given","start":59,"end":64,"id":10},{"text":"input","start":65,"end":70,"id":11},{"text":"data","start":71,"end":75,"id":12},{"text":",","start":75,"end":76,"id":13},{"text":"can","start":77,"end":80,"id":14},{"text":"be","start":81,"end":83,"id":15},{"text":"estimated","start":84,"end":93,"id":16},{"text":"even","start":94,"end":98,"id":17},{"text":"without","start":99,"end":106,"id":18},{"text":"the","start":107,"end":110,"id":19},{"text":"prior","start":111,"end":116,"id":20},{"text":"knowledge","start":117,"end":126,"id":21},{"text":"of","start":127,"end":129,"id":22},{"text":"its","start":130,"end":133,"id":23},{"text":"affiliation","start":134,"end":145,"id":24},{"text":".","start":145,"end":146,"id":25}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We speed up training by using an early stopping strategy based on the prediction on unlabeled data or, if available, on labeled validation examples.","_input_hash":-117015718,"_task_hash":-1906945971,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"speed","start":3,"end":8,"id":1},{"text":"up","start":9,"end":11,"id":2},{"text":"training","start":12,"end":20,"id":3},{"text":"by","start":21,"end":23,"id":4},{"text":"using","start":24,"end":29,"id":5},{"text":"an","start":30,"end":32,"id":6},{"text":"early","start":33,"end":38,"id":7},{"text":"stopping","start":39,"end":47,"id":8},{"text":"strategy","start":48,"end":56,"id":9},{"text":"based","start":57,"end":62,"id":10},{"text":"on","start":63,"end":65,"id":11},{"text":"the","start":66,"end":69,"id":12},{"text":"prediction","start":70,"end":80,"id":13},{"text":"on","start":81,"end":83,"id":14},{"text":"unlabeled","start":84,"end":93,"id":15},{"text":"data","start":94,"end":98,"id":16},{"text":"or","start":99,"end":101,"id":17},{"text":",","start":101,"end":102,"id":18},{"text":"if","start":103,"end":105,"id":19},{"text":"available","start":106,"end":115,"id":20},{"text":",","start":115,"end":116,"id":21},{"text":"on","start":117,"end":119,"id":22},{"text":"labeled","start":120,"end":127,"id":23},{"text":"validation","start":128,"end":138,"id":24},{"text":"examples","start":139,"end":147,"id":25},{"text":".","start":147,"end":148,"id":26}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We then consider four specific covariance models, including a full-rank unconstrained model.","_input_hash":-2040857313,"_task_hash":1137042928,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"then","start":3,"end":7,"id":1},{"text":"consider","start":8,"end":16,"id":2},{"text":"four","start":17,"end":21,"id":3},{"text":"specific","start":22,"end":30,"id":4},{"text":"covariance","start":31,"end":41,"id":5},{"text":"models","start":42,"end":48,"id":6},{"text":",","start":48,"end":49,"id":7},{"text":"including","start":50,"end":59,"id":8},{"text":"a","start":60,"end":61,"id":9},{"text":"full","start":62,"end":66,"id":10},{"text":"-","start":66,"end":67,"id":11},{"text":"rank","start":67,"end":71,"id":12},{"text":"unconstrained","start":72,"end":85,"id":13},{"text":"model","start":86,"end":91,"id":14},{"text":".","start":91,"end":92,"id":15}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Incorporating this information into the learning method may lead to a significant decrease of the estimation error.","_input_hash":-1272222384,"_task_hash":813786115,"tokens":[{"text":"Incorporating","start":0,"end":13,"id":0},{"text":"this","start":14,"end":18,"id":1},{"text":"information","start":19,"end":30,"id":2},{"text":"into","start":31,"end":35,"id":3},{"text":"the","start":36,"end":39,"id":4},{"text":"learning","start":40,"end":48,"id":5},{"text":"method","start":49,"end":55,"id":6},{"text":"may","start":56,"end":59,"id":7},{"text":"lead","start":60,"end":64,"id":8},{"text":"to","start":65,"end":67,"id":9},{"text":"a","start":68,"end":69,"id":10},{"text":"significant","start":70,"end":81,"id":11},{"text":"decrease","start":82,"end":90,"id":12},{"text":"of","start":91,"end":93,"id":13},{"text":"the","start":94,"end":97,"id":14},{"text":"estimation","start":98,"end":108,"id":15},{"text":"error","start":109,"end":114,"id":16},{"text":".","start":114,"end":115,"id":17}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"backpropagation|NOUN","word":"backpropagation","sense":"NOUN","meta":{"score":0.796299994,"sense":"NOUN"},"_input_hash":1657507157,"_task_hash":-231231002,"_session_id":null,"_view_id":"html","answer":"accept","spans":[],"tokens":[{"text":"backpropagation|NOUN","start":0,"end":20,"id":0}]}
{"text":"Part of the attraction is the variable selection effect:","_input_hash":-1062851974,"_task_hash":1647961294,"tokens":[{"text":"Part","start":0,"end":4,"id":0},{"text":"of","start":5,"end":7,"id":1},{"text":"the","start":8,"end":11,"id":2},{"text":"attraction","start":12,"end":22,"id":3},{"text":"is","start":23,"end":25,"id":4},{"text":"the","start":26,"end":29,"id":5},{"text":"variable","start":30,"end":38,"id":6},{"text":"selection","start":39,"end":48,"id":7},{"text":"effect","start":49,"end":55,"id":8},{"text":":","start":55,"end":56,"id":9}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"This article introduces both a new algorithm for reconstructing epsilon-machines from data, as well as the decisional states.","_input_hash":-318982839,"_task_hash":-230969820,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"article","start":5,"end":12,"id":1},{"text":"introduces","start":13,"end":23,"id":2},{"text":"both","start":24,"end":28,"id":3},{"text":"a","start":29,"end":30,"id":4},{"text":"new","start":31,"end":34,"id":5},{"text":"algorithm","start":35,"end":44,"id":6},{"text":"for","start":45,"end":48,"id":7},{"text":"reconstructing","start":49,"end":63,"id":8},{"text":"epsilon","start":64,"end":71,"id":9},{"text":"-","start":71,"end":72,"id":10},{"text":"machines","start":72,"end":80,"id":11},{"text":"from","start":81,"end":85,"id":12},{"text":"data","start":86,"end":90,"id":13},{"text":",","start":90,"end":91,"id":14},{"text":"as","start":92,"end":94,"id":15},{"text":"well","start":95,"end":99,"id":16},{"text":"as","start":100,"end":102,"id":17},{"text":"the","start":103,"end":106,"id":18},{"text":"decisional","start":107,"end":117,"id":19},{"text":"states","start":118,"end":124,"id":20},{"text":".","start":124,"end":125,"id":21}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"RNN|NOUN","word":"RNN","sense":"NOUN","meta":{"score":0.7872999907,"sense":"NOUN"},"_input_hash":1378741512,"_task_hash":-1339757557,"_session_id":null,"_view_id":"html","answer":"accept","spans":[],"tokens":[{"text":"RNN|NOUN","start":0,"end":8,"id":0}]}
{"text":"Several network scores and conditional independence algorithms are available for both the learning algorithms and independent use.","_input_hash":1248275622,"_task_hash":425483013,"tokens":[{"text":"Several","start":0,"end":7,"id":0},{"text":"network","start":8,"end":15,"id":1},{"text":"scores","start":16,"end":22,"id":2},{"text":"and","start":23,"end":26,"id":3},{"text":"conditional","start":27,"end":38,"id":4},{"text":"independence","start":39,"end":51,"id":5},{"text":"algorithms","start":52,"end":62,"id":6},{"text":"are","start":63,"end":66,"id":7},{"text":"available","start":67,"end":76,"id":8},{"text":"for","start":77,"end":80,"id":9},{"text":"both","start":81,"end":85,"id":10},{"text":"the","start":86,"end":89,"id":11},{"text":"learning","start":90,"end":98,"id":12},{"text":"algorithms","start":99,"end":109,"id":13},{"text":"and","start":110,"end":113,"id":14},{"text":"independent","start":114,"end":125,"id":15},{"text":"use","start":126,"end":129,"id":16},{"text":".","start":129,"end":130,"id":17}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":27,"end":62,"token_start":4,"token_end":6,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We use the results of this paper for partially justifying recent effective algorithms for modeling data by mixtures of multiple subspaces as well as for discussing the effect of using variants of lp minimizations in RANSAC-type strategies for single subspace recovery.","_input_hash":-1979514270,"_task_hash":332997941,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"use","start":3,"end":6,"id":1},{"text":"the","start":7,"end":10,"id":2},{"text":"results","start":11,"end":18,"id":3},{"text":"of","start":19,"end":21,"id":4},{"text":"this","start":22,"end":26,"id":5},{"text":"paper","start":27,"end":32,"id":6},{"text":"for","start":33,"end":36,"id":7},{"text":"partially","start":37,"end":46,"id":8},{"text":"justifying","start":47,"end":57,"id":9},{"text":"recent","start":58,"end":64,"id":10},{"text":"effective","start":65,"end":74,"id":11},{"text":"algorithms","start":75,"end":85,"id":12},{"text":"for","start":86,"end":89,"id":13},{"text":"modeling","start":90,"end":98,"id":14},{"text":"data","start":99,"end":103,"id":15},{"text":"by","start":104,"end":106,"id":16},{"text":"mixtures","start":107,"end":115,"id":17},{"text":"of","start":116,"end":118,"id":18},{"text":"multiple","start":119,"end":127,"id":19},{"text":"subspaces","start":128,"end":137,"id":20},{"text":"as","start":138,"end":140,"id":21},{"text":"well","start":141,"end":145,"id":22},{"text":"as","start":146,"end":148,"id":23},{"text":"for","start":149,"end":152,"id":24},{"text":"discussing","start":153,"end":163,"id":25},{"text":"the","start":164,"end":167,"id":26},{"text":"effect","start":168,"end":174,"id":27},{"text":"of","start":175,"end":177,"id":28},{"text":"using","start":178,"end":183,"id":29},{"text":"variants","start":184,"end":192,"id":30},{"text":"of","start":193,"end":195,"id":31},{"text":"lp","start":196,"end":198,"id":32},{"text":"minimizations","start":199,"end":212,"id":33},{"text":"in","start":213,"end":215,"id":34},{"text":"RANSAC","start":216,"end":222,"id":35},{"text":"-","start":222,"end":223,"id":36},{"text":"type","start":223,"end":227,"id":37},{"text":"strategies","start":228,"end":238,"id":38},{"text":"for","start":239,"end":242,"id":39},{"text":"single","start":243,"end":249,"id":40},{"text":"subspace","start":250,"end":258,"id":41},{"text":"recovery","start":259,"end":267,"id":42},{"text":".","start":267,"end":268,"id":43}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"A key feature of the method is that, once a tree ensemble is fitted, no further tuning parameter needs to be selected.","_input_hash":35365933,"_task_hash":-323187463,"tokens":[{"text":"A","start":0,"end":1,"id":0},{"text":"key","start":2,"end":5,"id":1},{"text":"feature","start":6,"end":13,"id":2},{"text":"of","start":14,"end":16,"id":3},{"text":"the","start":17,"end":20,"id":4},{"text":"method","start":21,"end":27,"id":5},{"text":"is","start":28,"end":30,"id":6},{"text":"that","start":31,"end":35,"id":7},{"text":",","start":35,"end":36,"id":8},{"text":"once","start":37,"end":41,"id":9},{"text":"a","start":42,"end":43,"id":10},{"text":"tree","start":44,"end":48,"id":11},{"text":"ensemble","start":49,"end":57,"id":12},{"text":"is","start":58,"end":60,"id":13},{"text":"fitted","start":61,"end":67,"id":14},{"text":",","start":67,"end":68,"id":15},{"text":"no","start":69,"end":71,"id":16},{"text":"further","start":72,"end":79,"id":17},{"text":"tuning","start":80,"end":86,"id":18},{"text":"parameter","start":87,"end":96,"id":19},{"text":"needs","start":97,"end":102,"id":20},{"text":"to","start":103,"end":105,"id":21},{"text":"be","start":106,"end":108,"id":22},{"text":"selected","start":109,"end":117,"id":23},{"text":".","start":117,"end":118,"id":24}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":44,"end":57,"token_start":11,"token_end":12,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"In terms of predictive power, however, these regularized linear models are often slightly inferior to machine learning procedures like tree ensembles.","_input_hash":849111586,"_task_hash":1744191789,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"terms","start":3,"end":8,"id":1},{"text":"of","start":9,"end":11,"id":2},{"text":"predictive","start":12,"end":22,"id":3},{"text":"power","start":23,"end":28,"id":4},{"text":",","start":28,"end":29,"id":5},{"text":"however","start":30,"end":37,"id":6},{"text":",","start":37,"end":38,"id":7},{"text":"these","start":39,"end":44,"id":8},{"text":"regularized","start":45,"end":56,"id":9},{"text":"linear","start":57,"end":63,"id":10},{"text":"models","start":64,"end":70,"id":11},{"text":"are","start":71,"end":74,"id":12},{"text":"often","start":75,"end":80,"id":13},{"text":"slightly","start":81,"end":89,"id":14},{"text":"inferior","start":90,"end":98,"id":15},{"text":"to","start":99,"end":101,"id":16},{"text":"machine","start":102,"end":109,"id":17},{"text":"learning","start":110,"end":118,"id":18},{"text":"procedures","start":119,"end":129,"id":19},{"text":"like","start":130,"end":134,"id":20},{"text":"tree","start":135,"end":139,"id":21},{"text":"ensembles","start":140,"end":149,"id":22},{"text":".","start":149,"end":150,"id":23}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":135,"end":149,"token_start":21,"token_end":22,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"bnlearn is an R package which includes several algorithms for learning the structure of Bayesian networks with either discrete or continuous variables.","_input_hash":-1603445648,"_task_hash":1197650306,"tokens":[{"text":"bnlearn","start":0,"end":7,"id":0},{"text":"is","start":8,"end":10,"id":1},{"text":"an","start":11,"end":13,"id":2},{"text":"R","start":14,"end":15,"id":3},{"text":"package","start":16,"end":23,"id":4},{"text":"which","start":24,"end":29,"id":5},{"text":"includes","start":30,"end":38,"id":6},{"text":"several","start":39,"end":46,"id":7},{"text":"algorithms","start":47,"end":57,"id":8},{"text":"for","start":58,"end":61,"id":9},{"text":"learning","start":62,"end":70,"id":10},{"text":"the","start":71,"end":74,"id":11},{"text":"structure","start":75,"end":84,"id":12},{"text":"of","start":85,"end":87,"id":13},{"text":"Bayesian","start":88,"end":96,"id":14},{"text":"networks","start":97,"end":105,"id":15},{"text":"with","start":106,"end":110,"id":16},{"text":"either","start":111,"end":117,"id":17},{"text":"discrete","start":118,"end":126,"id":18},{"text":"or","start":127,"end":129,"id":19},{"text":"continuous","start":130,"end":140,"id":20},{"text":"variables","start":141,"end":150,"id":21},{"text":".","start":150,"end":151,"id":22}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"The network performance is determined by calculating the mean square error of the network prediction.","_input_hash":1246371537,"_task_hash":-64210889,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"network","start":4,"end":11,"id":1},{"text":"performance","start":12,"end":23,"id":2},{"text":"is","start":24,"end":26,"id":3},{"text":"determined","start":27,"end":37,"id":4},{"text":"by","start":38,"end":40,"id":5},{"text":"calculating","start":41,"end":52,"id":6},{"text":"the","start":53,"end":56,"id":7},{"text":"mean","start":57,"end":61,"id":8},{"text":"square","start":62,"end":68,"id":9},{"text":"error","start":69,"end":74,"id":10},{"text":"of","start":75,"end":77,"id":11},{"text":"the","start":78,"end":81,"id":12},{"text":"network","start":82,"end":89,"id":13},{"text":"prediction","start":90,"end":100,"id":14},{"text":".","start":100,"end":101,"id":15}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We assume data independently sampled from a mixture distribution on the unit ball of the D-dimensional Euclidean space with K+1 components:","_input_hash":279266441,"_task_hash":-368282575,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"assume","start":3,"end":9,"id":1},{"text":"data","start":10,"end":14,"id":2},{"text":"independently","start":15,"end":28,"id":3},{"text":"sampled","start":29,"end":36,"id":4},{"text":"from","start":37,"end":41,"id":5},{"text":"a","start":42,"end":43,"id":6},{"text":"mixture","start":44,"end":51,"id":7},{"text":"distribution","start":52,"end":64,"id":8},{"text":"on","start":65,"end":67,"id":9},{"text":"the","start":68,"end":71,"id":10},{"text":"unit","start":72,"end":76,"id":11},{"text":"ball","start":77,"end":81,"id":12},{"text":"of","start":82,"end":84,"id":13},{"text":"the","start":85,"end":88,"id":14},{"text":"D","start":89,"end":90,"id":15},{"text":"-","start":90,"end":91,"id":16},{"text":"dimensional","start":91,"end":102,"id":17},{"text":"Euclidean","start":103,"end":112,"id":18},{"text":"space","start":113,"end":118,"id":19},{"text":"with","start":119,"end":123,"id":20},{"text":"K+1","start":124,"end":127,"id":21},{"text":"components","start":128,"end":138,"id":22},{"text":":","start":138,"end":139,"id":23}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"We extend multi-way, multivariate ANOVA-type analysis to cases where one covariate is the view, with features of each view coming from different, high-dimensional domains.","_input_hash":-1034263990,"_task_hash":-1703169187,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"extend","start":3,"end":9,"id":1},{"text":"multi","start":10,"end":15,"id":2},{"text":"-","start":15,"end":16,"id":3},{"text":"way","start":16,"end":19,"id":4},{"text":",","start":19,"end":20,"id":5},{"text":"multivariate","start":21,"end":33,"id":6},{"text":"ANOVA","start":34,"end":39,"id":7},{"text":"-","start":39,"end":40,"id":8},{"text":"type","start":40,"end":44,"id":9},{"text":"analysis","start":45,"end":53,"id":10},{"text":"to","start":54,"end":56,"id":11},{"text":"cases","start":57,"end":62,"id":12},{"text":"where","start":63,"end":68,"id":13},{"text":"one","start":69,"end":72,"id":14},{"text":"covariate","start":73,"end":82,"id":15},{"text":"is","start":83,"end":85,"id":16},{"text":"the","start":86,"end":89,"id":17},{"text":"view","start":90,"end":94,"id":18},{"text":",","start":94,"end":95,"id":19},{"text":"with","start":96,"end":100,"id":20},{"text":"features","start":101,"end":109,"id":21},{"text":"of","start":110,"end":112,"id":22},{"text":"each","start":113,"end":117,"id":23},{"text":"view","start":118,"end":122,"id":24},{"text":"coming","start":123,"end":129,"id":25},{"text":"from","start":130,"end":134,"id":26},{"text":"different","start":135,"end":144,"id":27},{"text":",","start":144,"end":145,"id":28},{"text":"high","start":146,"end":150,"id":29},{"text":"-","start":150,"end":151,"id":30},{"text":"dimensional","start":151,"end":162,"id":31},{"text":"domains","start":163,"end":170,"id":32},{"text":".","start":170,"end":171,"id":33}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"A Garrote-style convex penalty for trees ensembles, in particular Random Forests, is proposed.","_input_hash":782473757,"_task_hash":2107645131,"tokens":[{"text":"A","start":0,"end":1,"id":0},{"text":"Garrote","start":2,"end":9,"id":1},{"text":"-","start":9,"end":10,"id":2},{"text":"style","start":10,"end":15,"id":3},{"text":"convex","start":16,"end":22,"id":4},{"text":"penalty","start":23,"end":30,"id":5},{"text":"for","start":31,"end":34,"id":6},{"text":"trees","start":35,"end":40,"id":7},{"text":"ensembles","start":41,"end":50,"id":8},{"text":",","start":50,"end":51,"id":9},{"text":"in","start":52,"end":54,"id":10},{"text":"particular","start":55,"end":65,"id":11},{"text":"Random","start":66,"end":72,"id":12},{"text":"Forests","start":73,"end":80,"id":13},{"text":",","start":80,"end":81,"id":14},{"text":"is","start":82,"end":84,"id":15},{"text":"proposed","start":85,"end":93,"id":16},{"text":".","start":93,"end":94,"id":17}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":35,"end":50,"token_start":7,"token_end":8,"label":"ALGO","answer":"accept"},{"start":66,"end":80,"token_start":12,"token_end":13,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"natural_language|NOUN","word":"natural language","sense":"NOUN","meta":{"score":0.7588999867,"sense":"NOUN"},"_input_hash":-1656388173,"_task_hash":355516253,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"natural_language|NOUN","start":0,"end":21,"id":0}]}
{"text":"large_datasets|NOUN","word":"large datasets","sense":"NOUN","meta":{"score":0.77700001,"sense":"NOUN"},"_input_hash":-1782610135,"_task_hash":643832336,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"large_datasets|NOUN","start":0,"end":19,"id":0}]}
{"text":"We prove conditions for the asymptotic unbiasedness of the DP-GLM regression mean function estimate.","_input_hash":-190543059,"_task_hash":-988772315,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"prove","start":3,"end":8,"id":1},{"text":"conditions","start":9,"end":19,"id":2},{"text":"for","start":20,"end":23,"id":3},{"text":"the","start":24,"end":27,"id":4},{"text":"asymptotic","start":28,"end":38,"id":5},{"text":"unbiasedness","start":39,"end":51,"id":6},{"text":"of","start":52,"end":54,"id":7},{"text":"the","start":55,"end":58,"id":8},{"text":"DP","start":59,"end":61,"id":9},{"text":"-","start":61,"end":62,"id":10},{"text":"GLM","start":62,"end":65,"id":11},{"text":"regression","start":66,"end":76,"id":12},{"text":"mean","start":77,"end":81,"id":13},{"text":"function","start":82,"end":90,"id":14},{"text":"estimate","start":91,"end":99,"id":15},{"text":".","start":99,"end":100,"id":16}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"However, the GPLVM is not trained as a density model and therefore yields bad density estimates.","_input_hash":1037458206,"_task_hash":-2085319297,"tokens":[{"text":"However","start":0,"end":7,"id":0},{"text":",","start":7,"end":8,"id":1},{"text":"the","start":9,"end":12,"id":2},{"text":"GPLVM","start":13,"end":18,"id":3},{"text":"is","start":19,"end":21,"id":4},{"text":"not","start":22,"end":25,"id":5},{"text":"trained","start":26,"end":33,"id":6},{"text":"as","start":34,"end":36,"id":7},{"text":"a","start":37,"end":38,"id":8},{"text":"density","start":39,"end":46,"id":9},{"text":"model","start":47,"end":52,"id":10},{"text":"and","start":53,"end":56,"id":11},{"text":"therefore","start":57,"end":66,"id":12},{"text":"yields","start":67,"end":73,"id":13},{"text":"bad","start":74,"end":77,"id":14},{"text":"density","start":78,"end":85,"id":15},{"text":"estimates","start":86,"end":95,"id":16},{"text":".","start":95,"end":96,"id":17}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"stochastic|ADJ","word":"stochastic","sense":"ADJ","meta":{"score":0.7663999796,"sense":"ADJ"},"_input_hash":285063480,"_task_hash":-365658871,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"stochastic|ADJ","start":0,"end":14,"id":0}]}
{"text":"A novel method for deflationary ICA, referred to as RobustICA, is put forward in this paper.","_input_hash":-1464039596,"_task_hash":1293334276,"tokens":[{"text":"A","start":0,"end":1,"id":0},{"text":"novel","start":2,"end":7,"id":1},{"text":"method","start":8,"end":14,"id":2},{"text":"for","start":15,"end":18,"id":3},{"text":"deflationary","start":19,"end":31,"id":4},{"text":"ICA","start":32,"end":35,"id":5},{"text":",","start":35,"end":36,"id":6},{"text":"referred","start":37,"end":45,"id":7},{"text":"to","start":46,"end":48,"id":8},{"text":"as","start":49,"end":51,"id":9},{"text":"RobustICA","start":52,"end":61,"id":10},{"text":",","start":61,"end":62,"id":11},{"text":"is","start":63,"end":65,"id":12},{"text":"put","start":66,"end":69,"id":13},{"text":"forward","start":70,"end":77,"id":14},{"text":"in","start":78,"end":80,"id":15},{"text":"this","start":81,"end":85,"id":16},{"text":"paper","start":86,"end":91,"id":17},{"text":".","start":91,"end":92,"id":18}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":52,"end":61,"token_start":10,"token_end":10,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"fuzzy_logic|NOUN","word":"fuzzy logic","sense":"NOUN","meta":{"score":0.7619000077,"sense":"NOUN"},"_input_hash":-1082277471,"_task_hash":-1737381379,"_session_id":null,"_view_id":"html","answer":"accept","spans":[],"tokens":[{"text":"fuzzy_logic|NOUN","start":0,"end":16,"id":0}]}
{"text":"We derive a family of iterative expectationmaximization (EM) algorithms to estimate the parameters of each model and propose suitable procedures to initialize the parameters and to align the order of the estimated sources across all frequency bins based on their estimated directions of arrival (DOA).","_input_hash":-473871123,"_task_hash":980455228,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"derive","start":3,"end":9,"id":1},{"text":"a","start":10,"end":11,"id":2},{"text":"family","start":12,"end":18,"id":3},{"text":"of","start":19,"end":21,"id":4},{"text":"iterative","start":22,"end":31,"id":5},{"text":"expectationmaximization","start":32,"end":55,"id":6},{"text":"(","start":56,"end":57,"id":7},{"text":"EM","start":57,"end":59,"id":8},{"text":")","start":59,"end":60,"id":9},{"text":"algorithms","start":61,"end":71,"id":10},{"text":"to","start":72,"end":74,"id":11},{"text":"estimate","start":75,"end":83,"id":12},{"text":"the","start":84,"end":87,"id":13},{"text":"parameters","start":88,"end":98,"id":14},{"text":"of","start":99,"end":101,"id":15},{"text":"each","start":102,"end":106,"id":16},{"text":"model","start":107,"end":112,"id":17},{"text":"and","start":113,"end":116,"id":18},{"text":"propose","start":117,"end":124,"id":19},{"text":"suitable","start":125,"end":133,"id":20},{"text":"procedures","start":134,"end":144,"id":21},{"text":"to","start":145,"end":147,"id":22},{"text":"initialize","start":148,"end":158,"id":23},{"text":"the","start":159,"end":162,"id":24},{"text":"parameters","start":163,"end":173,"id":25},{"text":"and","start":174,"end":177,"id":26},{"text":"to","start":178,"end":180,"id":27},{"text":"align","start":181,"end":186,"id":28},{"text":"the","start":187,"end":190,"id":29},{"text":"order","start":191,"end":196,"id":30},{"text":"of","start":197,"end":199,"id":31},{"text":"the","start":200,"end":203,"id":32},{"text":"estimated","start":204,"end":213,"id":33},{"text":"sources","start":214,"end":221,"id":34},{"text":"across","start":222,"end":228,"id":35},{"text":"all","start":229,"end":232,"id":36},{"text":"frequency","start":233,"end":242,"id":37},{"text":"bins","start":243,"end":247,"id":38},{"text":"based","start":248,"end":253,"id":39},{"text":"on","start":254,"end":256,"id":40},{"text":"their","start":257,"end":262,"id":41},{"text":"estimated","start":263,"end":272,"id":42},{"text":"directions","start":273,"end":283,"id":43},{"text":"of","start":284,"end":286,"id":44},{"text":"arrival","start":287,"end":294,"id":45},{"text":"(","start":295,"end":296,"id":46},{"text":"DOA","start":296,"end":299,"id":47},{"text":")","start":299,"end":300,"id":48},{"text":".","start":300,"end":301,"id":49}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":32,"end":55,"token_start":6,"token_end":6,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"From a Gaussian processes perspective, the problem reduces to specifying an appropriate covariance function that, whilst being positive semi-definite, captures the dependencies between all the data points and across all the outputs.","_input_hash":-2121883516,"_task_hash":393969558,"tokens":[{"text":"From","start":0,"end":4,"id":0},{"text":"a","start":5,"end":6,"id":1},{"text":"Gaussian","start":7,"end":15,"id":2},{"text":"processes","start":16,"end":25,"id":3},{"text":"perspective","start":26,"end":37,"id":4},{"text":",","start":37,"end":38,"id":5},{"text":"the","start":39,"end":42,"id":6},{"text":"problem","start":43,"end":50,"id":7},{"text":"reduces","start":51,"end":58,"id":8},{"text":"to","start":59,"end":61,"id":9},{"text":"specifying","start":62,"end":72,"id":10},{"text":"an","start":73,"end":75,"id":11},{"text":"appropriate","start":76,"end":87,"id":12},{"text":"covariance","start":88,"end":98,"id":13},{"text":"function","start":99,"end":107,"id":14},{"text":"that","start":108,"end":112,"id":15},{"text":",","start":112,"end":113,"id":16},{"text":"whilst","start":114,"end":120,"id":17},{"text":"being","start":121,"end":126,"id":18},{"text":"positive","start":127,"end":135,"id":19},{"text":"semi","start":136,"end":140,"id":20},{"text":"-","start":140,"end":141,"id":21},{"text":"definite","start":141,"end":149,"id":22},{"text":",","start":149,"end":150,"id":23},{"text":"captures","start":151,"end":159,"id":24},{"text":"the","start":160,"end":163,"id":25},{"text":"dependencies","start":164,"end":176,"id":26},{"text":"between","start":177,"end":184,"id":27},{"text":"all","start":185,"end":188,"id":28},{"text":"the","start":189,"end":192,"id":29},{"text":"data","start":193,"end":197,"id":30},{"text":"points","start":198,"end":204,"id":31},{"text":"and","start":205,"end":208,"id":32},{"text":"across","start":209,"end":215,"id":33},{"text":"all","start":216,"end":219,"id":34},{"text":"the","start":220,"end":223,"id":35},{"text":"outputs","start":224,"end":231,"id":36},{"text":".","start":231,"end":232,"id":37}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We present different sparse approximations for dependent output Gaussian processes constructed through the convolution formalism.","_input_hash":-1711602771,"_task_hash":-1376056328,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"present","start":3,"end":10,"id":1},{"text":"different","start":11,"end":20,"id":2},{"text":"sparse","start":21,"end":27,"id":3},{"text":"approximations","start":28,"end":42,"id":4},{"text":"for","start":43,"end":46,"id":5},{"text":"dependent","start":47,"end":56,"id":6},{"text":"output","start":57,"end":63,"id":7},{"text":"Gaussian","start":64,"end":72,"id":8},{"text":"processes","start":73,"end":82,"id":9},{"text":"constructed","start":83,"end":94,"id":10},{"text":"through","start":95,"end":102,"id":11},{"text":"the","start":103,"end":106,"id":12},{"text":"convolution","start":107,"end":118,"id":13},{"text":"formalism","start":119,"end":128,"id":14},{"text":".","start":128,"end":129,"id":15}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We compare approximation error to batch AdaBoost on synthetic datasets and generalization error on face datasets and the MNIST dataset.","_input_hash":1258227701,"_task_hash":-333339323,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"compare","start":3,"end":10,"id":1},{"text":"approximation","start":11,"end":24,"id":2},{"text":"error","start":25,"end":30,"id":3},{"text":"to","start":31,"end":33,"id":4},{"text":"batch","start":34,"end":39,"id":5},{"text":"AdaBoost","start":40,"end":48,"id":6},{"text":"on","start":49,"end":51,"id":7},{"text":"synthetic","start":52,"end":61,"id":8},{"text":"datasets","start":62,"end":70,"id":9},{"text":"and","start":71,"end":74,"id":10},{"text":"generalization","start":75,"end":89,"id":11},{"text":"error","start":90,"end":95,"id":12},{"text":"on","start":96,"end":98,"id":13},{"text":"face","start":99,"end":103,"id":14},{"text":"datasets","start":104,"end":112,"id":15},{"text":"and","start":113,"end":116,"id":16},{"text":"the","start":117,"end":120,"id":17},{"text":"MNIST","start":121,"end":126,"id":18},{"text":"dataset","start":127,"end":134,"id":19},{"text":".","start":134,"end":135,"id":20}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":40,"end":48,"token_start":6,"token_end":6,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"physical_systems|NOUN","word":"physical systems","sense":"NOUN","meta":{"score":0.7871999741,"sense":"NOUN"},"_input_hash":1687211955,"_task_hash":-1226377612,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"physical_systems|NOUN","start":0,"end":21,"id":0}]}
{"text":"evolutionary_algorithms|NOUN","word":"evolutionary algorithms","sense":"NOUN","meta":{"score":0.7714999914,"sense":"NOUN"},"_input_hash":-1469470725,"_task_hash":-1728494677,"_session_id":null,"_view_id":"html","answer":"accept","spans":[],"tokens":[{"text":"evolutionary_algorithms|NOUN","start":0,"end":28,"id":0}]}
{"text":"When many of the features are irrelevant, kernel methods such as the support vector machine and kernel ridge regression can sometimes perform poorly.","_input_hash":1399878476,"_task_hash":130876173,"tokens":[{"text":"When","start":0,"end":4,"id":0},{"text":"many","start":5,"end":9,"id":1},{"text":"of","start":10,"end":12,"id":2},{"text":"the","start":13,"end":16,"id":3},{"text":"features","start":17,"end":25,"id":4},{"text":"are","start":26,"end":29,"id":5},{"text":"irrelevant","start":30,"end":40,"id":6},{"text":",","start":40,"end":41,"id":7},{"text":"kernel","start":42,"end":48,"id":8},{"text":"methods","start":49,"end":56,"id":9},{"text":"such","start":57,"end":61,"id":10},{"text":"as","start":62,"end":64,"id":11},{"text":"the","start":65,"end":68,"id":12},{"text":"support","start":69,"end":76,"id":13},{"text":"vector","start":77,"end":83,"id":14},{"text":"machine","start":84,"end":91,"id":15},{"text":"and","start":92,"end":95,"id":16},{"text":"kernel","start":96,"end":102,"id":17},{"text":"ridge","start":103,"end":108,"id":18},{"text":"regression","start":109,"end":119,"id":19},{"text":"can","start":120,"end":123,"id":20},{"text":"sometimes","start":124,"end":133,"id":21},{"text":"perform","start":134,"end":141,"id":22},{"text":"poorly","start":142,"end":148,"id":23},{"text":".","start":148,"end":149,"id":24}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":69,"end":91,"token_start":13,"token_end":15,"label":"ALGO","answer":"accept"},{"start":96,"end":119,"token_start":17,"token_end":19,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"training_data|NOUN","word":"training data","sense":"NOUN","meta":{"score":0.8237000108,"sense":"NOUN"},"_input_hash":127235364,"_task_hash":-1650291400,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"training_data|NOUN","start":0,"end":18,"id":0}]}
{"text":"protein_folding|NOUN","word":"protein folding","sense":"NOUN","meta":{"score":0.769299984,"sense":"NOUN"},"_input_hash":-122488253,"_task_hash":1837785341,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"protein_folding|NOUN","start":0,"end":20,"id":0}]}
{"text":"neurons|NOUN","word":"neurons","sense":"NOUN","meta":{"score":0.7627999783,"sense":"NOUN"},"_input_hash":2117136566,"_task_hash":-1401853756,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"neurons|NOUN","start":0,"end":12,"id":0}]}
{"text":"Their Bayesian KNN (BKNN) approach uses a pseudo-likelihood function, and standard Markov chain Monte Carlo (MCMC) techniques to draw posterior samples.","_input_hash":-1316639529,"_task_hash":-719874719,"tokens":[{"text":"Their","start":0,"end":5,"id":0},{"text":"Bayesian","start":6,"end":14,"id":1},{"text":"KNN","start":15,"end":18,"id":2},{"text":"(","start":19,"end":20,"id":3},{"text":"BKNN","start":20,"end":24,"id":4},{"text":")","start":24,"end":25,"id":5},{"text":"approach","start":26,"end":34,"id":6},{"text":"uses","start":35,"end":39,"id":7},{"text":"a","start":40,"end":41,"id":8},{"text":"pseudo","start":42,"end":48,"id":9},{"text":"-","start":48,"end":49,"id":10},{"text":"likelihood","start":49,"end":59,"id":11},{"text":"function","start":60,"end":68,"id":12},{"text":",","start":68,"end":69,"id":13},{"text":"and","start":70,"end":73,"id":14},{"text":"standard","start":74,"end":82,"id":15},{"text":"Markov","start":83,"end":89,"id":16},{"text":"chain","start":90,"end":95,"id":17},{"text":"Monte","start":96,"end":101,"id":18},{"text":"Carlo","start":102,"end":107,"id":19},{"text":"(","start":108,"end":109,"id":20},{"text":"MCMC","start":109,"end":113,"id":21},{"text":")","start":113,"end":114,"id":22},{"text":"techniques","start":115,"end":125,"id":23},{"text":"to","start":126,"end":128,"id":24},{"text":"draw","start":129,"end":133,"id":25},{"text":"posterior","start":134,"end":143,"id":26},{"text":"samples","start":144,"end":151,"id":27},{"text":".","start":151,"end":152,"id":28}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":83,"end":107,"token_start":16,"token_end":19,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We propose weighting the features within a kernel with a sparse set of weights that are estimated in conjunction with the original classification or regression problem.","_input_hash":200208298,"_task_hash":374697797,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"propose","start":3,"end":10,"id":1},{"text":"weighting","start":11,"end":20,"id":2},{"text":"the","start":21,"end":24,"id":3},{"text":"features","start":25,"end":33,"id":4},{"text":"within","start":34,"end":40,"id":5},{"text":"a","start":41,"end":42,"id":6},{"text":"kernel","start":43,"end":49,"id":7},{"text":"with","start":50,"end":54,"id":8},{"text":"a","start":55,"end":56,"id":9},{"text":"sparse","start":57,"end":63,"id":10},{"text":"set","start":64,"end":67,"id":11},{"text":"of","start":68,"end":70,"id":12},{"text":"weights","start":71,"end":78,"id":13},{"text":"that","start":79,"end":83,"id":14},{"text":"are","start":84,"end":87,"id":15},{"text":"estimated","start":88,"end":97,"id":16},{"text":"in","start":98,"end":100,"id":17},{"text":"conjunction","start":101,"end":112,"id":18},{"text":"with","start":113,"end":117,"id":19},{"text":"the","start":118,"end":121,"id":20},{"text":"original","start":122,"end":130,"id":21},{"text":"classification","start":131,"end":145,"id":22},{"text":"or","start":146,"end":148,"id":23},{"text":"regression","start":149,"end":159,"id":24},{"text":"problem","start":160,"end":167,"id":25},{"text":".","start":167,"end":168,"id":26}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We establish the basic properties of these penalty functions and discuss some examples where they can be computed explicitly.","_input_hash":-2019039687,"_task_hash":-2138102470,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"establish","start":3,"end":12,"id":1},{"text":"the","start":13,"end":16,"id":2},{"text":"basic","start":17,"end":22,"id":3},{"text":"properties","start":23,"end":33,"id":4},{"text":"of","start":34,"end":36,"id":5},{"text":"these","start":37,"end":42,"id":6},{"text":"penalty","start":43,"end":50,"id":7},{"text":"functions","start":51,"end":60,"id":8},{"text":"and","start":61,"end":64,"id":9},{"text":"discuss","start":65,"end":72,"id":10},{"text":"some","start":73,"end":77,"id":11},{"text":"examples","start":78,"end":86,"id":12},{"text":"where","start":87,"end":92,"id":13},{"text":"they","start":93,"end":97,"id":14},{"text":"can","start":98,"end":101,"id":15},{"text":"be","start":102,"end":104,"id":16},{"text":"computed","start":105,"end":113,"id":17},{"text":"explicitly","start":114,"end":124,"id":18},{"text":".","start":124,"end":125,"id":19}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"i","meta":{"score":0},"_input_hash":33216936,"_task_hash":1090601529,"_session_id":null,"_view_id":"text","answer":"reject","spans":[],"tokens":[{"text":"i","start":0,"end":1,"id":0}]}
{"text":"In this paper, we introduce the cascading Indian buffet process (CIBP), which provides a nonparametric prior on the structure of a layered, directed belief network that is unbounded in both depth and width, yet allows tractable inference.","_input_hash":1429126629,"_task_hash":-1099096889,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"this","start":3,"end":7,"id":1},{"text":"paper","start":8,"end":13,"id":2},{"text":",","start":13,"end":14,"id":3},{"text":"we","start":15,"end":17,"id":4},{"text":"introduce","start":18,"end":27,"id":5},{"text":"the","start":28,"end":31,"id":6},{"text":"cascading","start":32,"end":41,"id":7},{"text":"Indian","start":42,"end":48,"id":8},{"text":"buffet","start":49,"end":55,"id":9},{"text":"process","start":56,"end":63,"id":10},{"text":"(","start":64,"end":65,"id":11},{"text":"CIBP","start":65,"end":69,"id":12},{"text":")","start":69,"end":70,"id":13},{"text":",","start":70,"end":71,"id":14},{"text":"which","start":72,"end":77,"id":15},{"text":"provides","start":78,"end":86,"id":16},{"text":"a","start":87,"end":88,"id":17},{"text":"nonparametric","start":89,"end":102,"id":18},{"text":"prior","start":103,"end":108,"id":19},{"text":"on","start":109,"end":111,"id":20},{"text":"the","start":112,"end":115,"id":21},{"text":"structure","start":116,"end":125,"id":22},{"text":"of","start":126,"end":128,"id":23},{"text":"a","start":129,"end":130,"id":24},{"text":"layered","start":131,"end":138,"id":25},{"text":",","start":138,"end":139,"id":26},{"text":"directed","start":140,"end":148,"id":27},{"text":"belief","start":149,"end":155,"id":28},{"text":"network","start":156,"end":163,"id":29},{"text":"that","start":164,"end":168,"id":30},{"text":"is","start":169,"end":171,"id":31},{"text":"unbounded","start":172,"end":181,"id":32},{"text":"in","start":182,"end":184,"id":33},{"text":"both","start":185,"end":189,"id":34},{"text":"depth","start":190,"end":195,"id":35},{"text":"and","start":196,"end":199,"id":36},{"text":"width","start":200,"end":205,"id":37},{"text":",","start":205,"end":206,"id":38},{"text":"yet","start":207,"end":210,"id":39},{"text":"allows","start":211,"end":217,"id":40},{"text":"tractable","start":218,"end":227,"id":41},{"text":"inference","start":228,"end":237,"id":42},{"text":".","start":237,"end":238,"id":43}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":42,"end":63,"token_start":8,"token_end":10,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"The present contribution suggests the use of a multidimensional scaling (MDS) algorithm as a visualization tool for manifold-valued elements.","_input_hash":-27624822,"_task_hash":776772633,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"present","start":4,"end":11,"id":1},{"text":"contribution","start":12,"end":24,"id":2},{"text":"suggests","start":25,"end":33,"id":3},{"text":"the","start":34,"end":37,"id":4},{"text":"use","start":38,"end":41,"id":5},{"text":"of","start":42,"end":44,"id":6},{"text":"a","start":45,"end":46,"id":7},{"text":"multidimensional","start":47,"end":63,"id":8},{"text":"scaling","start":64,"end":71,"id":9},{"text":"(","start":72,"end":73,"id":10},{"text":"MDS","start":73,"end":76,"id":11},{"text":")","start":76,"end":77,"id":12},{"text":"algorithm","start":78,"end":87,"id":13},{"text":"as","start":88,"end":90,"id":14},{"text":"a","start":91,"end":92,"id":15},{"text":"visualization","start":93,"end":106,"id":16},{"text":"tool","start":107,"end":111,"id":17},{"text":"for","start":112,"end":115,"id":18},{"text":"manifold","start":116,"end":124,"id":19},{"text":"-","start":124,"end":125,"id":20},{"text":"valued","start":125,"end":131,"id":21},{"text":"elements","start":132,"end":140,"id":22},{"text":".","start":140,"end":141,"id":23}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"While frequentist methods have yielded online filtering and prediction techniques, most Bayesian papers have focused on the retrospective segmentation problem.","_input_hash":-1258359525,"_task_hash":2022667785,"tokens":[{"text":"While","start":0,"end":5,"id":0},{"text":"frequentist","start":6,"end":17,"id":1},{"text":"methods","start":18,"end":25,"id":2},{"text":"have","start":26,"end":30,"id":3},{"text":"yielded","start":31,"end":38,"id":4},{"text":"online","start":39,"end":45,"id":5},{"text":"filtering","start":46,"end":55,"id":6},{"text":"and","start":56,"end":59,"id":7},{"text":"prediction","start":60,"end":70,"id":8},{"text":"techniques","start":71,"end":81,"id":9},{"text":",","start":81,"end":82,"id":10},{"text":"most","start":83,"end":87,"id":11},{"text":"Bayesian","start":88,"end":96,"id":12},{"text":"papers","start":97,"end":103,"id":13},{"text":"have","start":104,"end":108,"id":14},{"text":"focused","start":109,"end":116,"id":15},{"text":"on","start":117,"end":119,"id":16},{"text":"the","start":120,"end":123,"id":17},{"text":"retrospective","start":124,"end":137,"id":18},{"text":"segmentation","start":138,"end":150,"id":19},{"text":"problem","start":151,"end":158,"id":20},{"text":".","start":158,"end":159,"id":21}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"Furthermore, a way to design these kernels is given.","_input_hash":1566328038,"_task_hash":-1975561419,"tokens":[{"text":"Furthermore","start":0,"end":11,"id":0},{"text":",","start":11,"end":12,"id":1},{"text":"a","start":13,"end":14,"id":2},{"text":"way","start":15,"end":18,"id":3},{"text":"to","start":19,"end":21,"id":4},{"text":"design","start":22,"end":28,"id":5},{"text":"these","start":29,"end":34,"id":6},{"text":"kernels","start":35,"end":42,"id":7},{"text":"is","start":43,"end":45,"id":8},{"text":"given","start":46,"end":51,"id":9},{"text":".","start":51,"end":52,"id":10}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":35,"end":42,"token_start":7,"token_end":7,"label":"ALGO","answer":"reject"}],"answer":"reject"}
{"text":"We first discuss some properties of positive definite kernels as well as reproducing kernel Hibert spaces, the natural extension of the set of functions $\\{k(x,\\cdot),x\\in\\mathcal{X}\\}$ associated with a kernel $k$ defined on a space $\\mathcal{X}$. We discuss at length the construction of kernel functions that take advantage of well-known statistical models.","_input_hash":-198045816,"_task_hash":-1780433255,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"first","start":3,"end":8,"id":1},{"text":"discuss","start":9,"end":16,"id":2},{"text":"some","start":17,"end":21,"id":3},{"text":"properties","start":22,"end":32,"id":4},{"text":"of","start":33,"end":35,"id":5},{"text":"positive","start":36,"end":44,"id":6},{"text":"definite","start":45,"end":53,"id":7},{"text":"kernels","start":54,"end":61,"id":8},{"text":"as","start":62,"end":64,"id":9},{"text":"well","start":65,"end":69,"id":10},{"text":"as","start":70,"end":72,"id":11},{"text":"reproducing","start":73,"end":84,"id":12},{"text":"kernel","start":85,"end":91,"id":13},{"text":"Hibert","start":92,"end":98,"id":14},{"text":"spaces","start":99,"end":105,"id":15},{"text":",","start":105,"end":106,"id":16},{"text":"the","start":107,"end":110,"id":17},{"text":"natural","start":111,"end":118,"id":18},{"text":"extension","start":119,"end":128,"id":19},{"text":"of","start":129,"end":131,"id":20},{"text":"the","start":132,"end":135,"id":21},{"text":"set","start":136,"end":139,"id":22},{"text":"of","start":140,"end":142,"id":23},{"text":"functions","start":143,"end":152,"id":24},{"text":"$","start":153,"end":154,"id":25},{"text":"\\{k(x,\\cdot),x\\in\\mathcal{X}\\}$","start":154,"end":185,"id":26},{"text":"associated","start":186,"end":196,"id":27},{"text":"with","start":197,"end":201,"id":28},{"text":"a","start":202,"end":203,"id":29},{"text":"kernel","start":204,"end":210,"id":30},{"text":"$","start":211,"end":212,"id":31},{"text":"k$","start":212,"end":214,"id":32},{"text":"defined","start":215,"end":222,"id":33},{"text":"on","start":223,"end":225,"id":34},{"text":"a","start":226,"end":227,"id":35},{"text":"space","start":228,"end":233,"id":36},{"text":"$","start":234,"end":235,"id":37},{"text":"\\mathcal{X}$.","start":235,"end":248,"id":38},{"text":"We","start":249,"end":251,"id":39},{"text":"discuss","start":252,"end":259,"id":40},{"text":"at","start":260,"end":262,"id":41},{"text":"length","start":263,"end":269,"id":42},{"text":"the","start":270,"end":273,"id":43},{"text":"construction","start":274,"end":286,"id":44},{"text":"of","start":287,"end":289,"id":45},{"text":"kernel","start":290,"end":296,"id":46},{"text":"functions","start":297,"end":306,"id":47},{"text":"that","start":307,"end":311,"id":48},{"text":"take","start":312,"end":316,"id":49},{"text":"advantage","start":317,"end":326,"id":50},{"text":"of","start":327,"end":329,"id":51},{"text":"well","start":330,"end":334,"id":52},{"text":"-","start":334,"end":335,"id":53},{"text":"known","start":335,"end":340,"id":54},{"text":"statistical","start":341,"end":352,"id":55},{"text":"models","start":353,"end":359,"id":56},{"text":".","start":359,"end":360,"id":57}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":341,"end":359,"token_start":55,"token_end":56,"label":"ALGO","answer":"reject"}],"answer":"reject"}
{"text":"This leads to a form of the covariance similar in spirit to the so called PITC and FITC approximations for a single output.","_input_hash":2080976227,"_task_hash":1854135906,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"leads","start":5,"end":10,"id":1},{"text":"to","start":11,"end":13,"id":2},{"text":"a","start":14,"end":15,"id":3},{"text":"form","start":16,"end":20,"id":4},{"text":"of","start":21,"end":23,"id":5},{"text":"the","start":24,"end":27,"id":6},{"text":"covariance","start":28,"end":38,"id":7},{"text":"similar","start":39,"end":46,"id":8},{"text":"in","start":47,"end":49,"id":9},{"text":"spirit","start":50,"end":56,"id":10},{"text":"to","start":57,"end":59,"id":11},{"text":"the","start":60,"end":63,"id":12},{"text":"so","start":64,"end":66,"id":13},{"text":"called","start":67,"end":73,"id":14},{"text":"PITC","start":74,"end":78,"id":15},{"text":"and","start":79,"end":82,"id":16},{"text":"FITC","start":83,"end":87,"id":17},{"text":"approximations","start":88,"end":102,"id":18},{"text":"for","start":103,"end":106,"id":19},{"text":"a","start":107,"end":108,"id":20},{"text":"single","start":109,"end":115,"id":21},{"text":"output","start":116,"end":122,"id":22},{"text":".","start":122,"end":123,"id":23}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Finally, experimental evidence of the performance of the proposed approach on both artificial data and a speaker verification task is provided.","_input_hash":-1272997971,"_task_hash":799308236,"tokens":[{"text":"Finally","start":0,"end":7,"id":0},{"text":",","start":7,"end":8,"id":1},{"text":"experimental","start":9,"end":21,"id":2},{"text":"evidence","start":22,"end":30,"id":3},{"text":"of","start":31,"end":33,"id":4},{"text":"the","start":34,"end":37,"id":5},{"text":"performance","start":38,"end":49,"id":6},{"text":"of","start":50,"end":52,"id":7},{"text":"the","start":53,"end":56,"id":8},{"text":"proposed","start":57,"end":65,"id":9},{"text":"approach","start":66,"end":74,"id":10},{"text":"on","start":75,"end":77,"id":11},{"text":"both","start":78,"end":82,"id":12},{"text":"artificial","start":83,"end":93,"id":13},{"text":"data","start":94,"end":98,"id":14},{"text":"and","start":99,"end":102,"id":15},{"text":"a","start":103,"end":104,"id":16},{"text":"speaker","start":105,"end":112,"id":17},{"text":"verification","start":113,"end":125,"id":18},{"text":"task","start":126,"end":130,"id":19},{"text":"is","start":131,"end":133,"id":20},{"text":"provided","start":134,"end":142,"id":21},{"text":".","start":142,"end":143,"id":22}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"We propose an efficient and simple optimization procedure to solve this problem.","_input_hash":-799536345,"_task_hash":1234176831,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"propose","start":3,"end":10,"id":1},{"text":"an","start":11,"end":13,"id":2},{"text":"efficient","start":14,"end":23,"id":3},{"text":"and","start":24,"end":27,"id":4},{"text":"simple","start":28,"end":34,"id":5},{"text":"optimization","start":35,"end":47,"id":6},{"text":"procedure","start":48,"end":57,"id":7},{"text":"to","start":58,"end":60,"id":8},{"text":"solve","start":61,"end":66,"id":9},{"text":"this","start":67,"end":71,"id":10},{"text":"problem","start":72,"end":79,"id":11},{"text":".","start":79,"end":80,"id":12}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"computational|ADJ","word":"computational","sense":"ADJ","meta":{"score":0.7961999774,"sense":"ADJ"},"_input_hash":1605231035,"_task_hash":1890015207,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"computational|ADJ","start":0,"end":17,"id":0}]}
{"text":"dare","meta":{"score":0},"_input_hash":62560757,"_task_hash":-1743593307,"_session_id":null,"_view_id":"text","answer":"reject","spans":[],"tokens":[{"text":"dare","start":0,"end":4,"id":0}]}
{"text":"machine_vision|NOUN","word":"machine vision","sense":"NOUN","meta":{"score":0.7954999804,"sense":"NOUN"},"_input_hash":1624311700,"_task_hash":-1894252790,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"machine_vision|NOUN","start":0,"end":19,"id":0}]}
{"text":"Fourier_analysis|NOUN","word":"Fourier analysis","sense":"NOUN","meta":{"score":0.7764999866,"sense":"NOUN"},"_input_hash":1870765505,"_task_hash":498210409,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"Fourier_analysis|NOUN","start":0,"end":21,"id":0}]}
{"text":"Using this example, we describe that sometimes a causal hypothesis must be rejected because P(effect|cause) and P(cause) share algorithmic information (which is untypical if they are chosen independently).","_input_hash":1988888483,"_task_hash":-38613420,"tokens":[{"text":"Using","start":0,"end":5,"id":0},{"text":"this","start":6,"end":10,"id":1},{"text":"example","start":11,"end":18,"id":2},{"text":",","start":18,"end":19,"id":3},{"text":"we","start":20,"end":22,"id":4},{"text":"describe","start":23,"end":31,"id":5},{"text":"that","start":32,"end":36,"id":6},{"text":"sometimes","start":37,"end":46,"id":7},{"text":"a","start":47,"end":48,"id":8},{"text":"causal","start":49,"end":55,"id":9},{"text":"hypothesis","start":56,"end":66,"id":10},{"text":"must","start":67,"end":71,"id":11},{"text":"be","start":72,"end":74,"id":12},{"text":"rejected","start":75,"end":83,"id":13},{"text":"because","start":84,"end":91,"id":14},{"text":"P(effect|cause","start":92,"end":106,"id":15},{"text":")","start":106,"end":107,"id":16},{"text":"and","start":108,"end":111,"id":17},{"text":"P(cause","start":112,"end":119,"id":18},{"text":")","start":119,"end":120,"id":19},{"text":"share","start":121,"end":126,"id":20},{"text":"algorithmic","start":127,"end":138,"id":21},{"text":"information","start":139,"end":150,"id":22},{"text":"(","start":151,"end":152,"id":23},{"text":"which","start":152,"end":157,"id":24},{"text":"is","start":158,"end":160,"id":25},{"text":"untypical","start":161,"end":170,"id":26},{"text":"if","start":171,"end":173,"id":27},{"text":"they","start":174,"end":178,"id":28},{"text":"are","start":179,"end":182,"id":29},{"text":"chosen","start":183,"end":189,"id":30},{"text":"independently","start":190,"end":203,"id":31},{"text":")","start":203,"end":204,"id":32},{"text":".","start":204,"end":205,"id":33}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":49,"end":66,"token_start":9,"token_end":10,"label":"ALGO","answer":"reject"},{"start":121,"end":138,"token_start":20,"token_end":21,"label":"ALGO","answer":"reject"}],"answer":"reject"}
{"text":"We assume that the weak hypotheses were selected beforehand, and only their weights are updated during online boosting.","_input_hash":366728403,"_task_hash":-219993,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"assume","start":3,"end":9,"id":1},{"text":"that","start":10,"end":14,"id":2},{"text":"the","start":15,"end":18,"id":3},{"text":"weak","start":19,"end":23,"id":4},{"text":"hypotheses","start":24,"end":34,"id":5},{"text":"were","start":35,"end":39,"id":6},{"text":"selected","start":40,"end":48,"id":7},{"text":"beforehand","start":49,"end":59,"id":8},{"text":",","start":59,"end":60,"id":9},{"text":"and","start":61,"end":64,"id":10},{"text":"only","start":65,"end":69,"id":11},{"text":"their","start":70,"end":75,"id":12},{"text":"weights","start":76,"end":83,"id":13},{"text":"are","start":84,"end":87,"id":14},{"text":"updated","start":88,"end":95,"id":15},{"text":"during","start":96,"end":102,"id":16},{"text":"online","start":103,"end":109,"id":17},{"text":"boosting","start":110,"end":118,"id":18},{"text":".","start":118,"end":119,"id":19}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"ol","meta":{"score":0},"_input_hash":-345826933,"_task_hash":952210073,"_session_id":null,"_view_id":"text","answer":"reject","spans":[],"tokens":[{"text":"ol","start":0,"end":2,"id":0}]}
{"text":"We derive a well-principled set of codes for both parameters and error residuals along with smooth approximations to lengths of these codes as to allow gradient descent optimization of description length, and go on to show that sparsification and feature selection using our approach is faster than the LASSO on several datasets from the UCI and StatLib repositories, with favorable generalization accuracy, while being fully automatic, requiring neither cross-validation nor tuning of regularization hyper-parameters, allowing even for a nonlinear expansion of the feature set followed by sparsification.","_input_hash":-1916215688,"_task_hash":259178487,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"derive","start":3,"end":9,"id":1},{"text":"a","start":10,"end":11,"id":2},{"text":"well","start":12,"end":16,"id":3},{"text":"-","start":16,"end":17,"id":4},{"text":"principled","start":17,"end":27,"id":5},{"text":"set","start":28,"end":31,"id":6},{"text":"of","start":32,"end":34,"id":7},{"text":"codes","start":35,"end":40,"id":8},{"text":"for","start":41,"end":44,"id":9},{"text":"both","start":45,"end":49,"id":10},{"text":"parameters","start":50,"end":60,"id":11},{"text":"and","start":61,"end":64,"id":12},{"text":"error","start":65,"end":70,"id":13},{"text":"residuals","start":71,"end":80,"id":14},{"text":"along","start":81,"end":86,"id":15},{"text":"with","start":87,"end":91,"id":16},{"text":"smooth","start":92,"end":98,"id":17},{"text":"approximations","start":99,"end":113,"id":18},{"text":"to","start":114,"end":116,"id":19},{"text":"lengths","start":117,"end":124,"id":20},{"text":"of","start":125,"end":127,"id":21},{"text":"these","start":128,"end":133,"id":22},{"text":"codes","start":134,"end":139,"id":23},{"text":"as","start":140,"end":142,"id":24},{"text":"to","start":143,"end":145,"id":25},{"text":"allow","start":146,"end":151,"id":26},{"text":"gradient","start":152,"end":160,"id":27},{"text":"descent","start":161,"end":168,"id":28},{"text":"optimization","start":169,"end":181,"id":29},{"text":"of","start":182,"end":184,"id":30},{"text":"description","start":185,"end":196,"id":31},{"text":"length","start":197,"end":203,"id":32},{"text":",","start":203,"end":204,"id":33},{"text":"and","start":205,"end":208,"id":34},{"text":"go","start":209,"end":211,"id":35},{"text":"on","start":212,"end":214,"id":36},{"text":"to","start":215,"end":217,"id":37},{"text":"show","start":218,"end":222,"id":38},{"text":"that","start":223,"end":227,"id":39},{"text":"sparsification","start":228,"end":242,"id":40},{"text":"and","start":243,"end":246,"id":41},{"text":"feature","start":247,"end":254,"id":42},{"text":"selection","start":255,"end":264,"id":43},{"text":"using","start":265,"end":270,"id":44},{"text":"our","start":271,"end":274,"id":45},{"text":"approach","start":275,"end":283,"id":46},{"text":"is","start":284,"end":286,"id":47},{"text":"faster","start":287,"end":293,"id":48},{"text":"than","start":294,"end":298,"id":49},{"text":"the","start":299,"end":302,"id":50},{"text":"LASSO","start":303,"end":308,"id":51},{"text":"on","start":309,"end":311,"id":52},{"text":"several","start":312,"end":319,"id":53},{"text":"datasets","start":320,"end":328,"id":54},{"text":"from","start":329,"end":333,"id":55},{"text":"the","start":334,"end":337,"id":56},{"text":"UCI","start":338,"end":341,"id":57},{"text":"and","start":342,"end":345,"id":58},{"text":"StatLib","start":346,"end":353,"id":59},{"text":"repositories","start":354,"end":366,"id":60},{"text":",","start":366,"end":367,"id":61},{"text":"with","start":368,"end":372,"id":62},{"text":"favorable","start":373,"end":382,"id":63},{"text":"generalization","start":383,"end":397,"id":64},{"text":"accuracy","start":398,"end":406,"id":65},{"text":",","start":406,"end":407,"id":66},{"text":"while","start":408,"end":413,"id":67},{"text":"being","start":414,"end":419,"id":68},{"text":"fully","start":420,"end":425,"id":69},{"text":"automatic","start":426,"end":435,"id":70},{"text":",","start":435,"end":436,"id":71},{"text":"requiring","start":437,"end":446,"id":72},{"text":"neither","start":447,"end":454,"id":73},{"text":"cross","start":455,"end":460,"id":74},{"text":"-","start":460,"end":461,"id":75},{"text":"validation","start":461,"end":471,"id":76},{"text":"nor","start":472,"end":475,"id":77},{"text":"tuning","start":476,"end":482,"id":78},{"text":"of","start":483,"end":485,"id":79},{"text":"regularization","start":486,"end":500,"id":80},{"text":"hyper","start":501,"end":506,"id":81},{"text":"-","start":506,"end":507,"id":82},{"text":"parameters","start":507,"end":517,"id":83},{"text":",","start":517,"end":518,"id":84},{"text":"allowing","start":519,"end":527,"id":85},{"text":"even","start":528,"end":532,"id":86},{"text":"for","start":533,"end":536,"id":87},{"text":"a","start":537,"end":538,"id":88},{"text":"nonlinear","start":539,"end":548,"id":89},{"text":"expansion","start":549,"end":558,"id":90},{"text":"of","start":559,"end":561,"id":91},{"text":"the","start":562,"end":565,"id":92},{"text":"feature","start":566,"end":573,"id":93},{"text":"set","start":574,"end":577,"id":94},{"text":"followed","start":578,"end":586,"id":95},{"text":"by","start":587,"end":589,"id":96},{"text":"sparsification","start":590,"end":604,"id":97},{"text":".","start":604,"end":605,"id":98}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":152,"end":168,"token_start":27,"token_end":28,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"did","meta":{"score":0},"_input_hash":-1284011795,"_task_hash":-1838880058,"_session_id":null,"_view_id":"text","answer":"reject","spans":[],"tokens":[{"text":"did","start":0,"end":3,"id":0}]}
{"text":"The Johnson-Lindenstrauss Lemma allows for the projection of $n$ points in $p-$dimensional Euclidean space onto a $k-$dimensional Euclidean space, with $k \\ge \\frac{24\\ln \\emph{n}}{3\\epsilon^2-2\\epsilon^3}$, so that the pairwise distances are preserved within a factor of $1\\pm\\epsilon$. Here, working directly with the distributions of the random distances rather than resorting to the moment generating function technique, an improvement on the lower bound for $k$ is obtained.","_input_hash":-651631983,"_task_hash":-1244763824,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"Johnson","start":4,"end":11,"id":1},{"text":"-","start":11,"end":12,"id":2},{"text":"Lindenstrauss","start":12,"end":25,"id":3},{"text":"Lemma","start":26,"end":31,"id":4},{"text":"allows","start":32,"end":38,"id":5},{"text":"for","start":39,"end":42,"id":6},{"text":"the","start":43,"end":46,"id":7},{"text":"projection","start":47,"end":57,"id":8},{"text":"of","start":58,"end":60,"id":9},{"text":"$","start":61,"end":62,"id":10},{"text":"n$","start":62,"end":64,"id":11},{"text":"points","start":65,"end":71,"id":12},{"text":"in","start":72,"end":74,"id":13},{"text":"$","start":75,"end":76,"id":14},{"text":"p-$dimensional","start":76,"end":90,"id":15},{"text":"Euclidean","start":91,"end":100,"id":16},{"text":"space","start":101,"end":106,"id":17},{"text":"onto","start":107,"end":111,"id":18},{"text":"a","start":112,"end":113,"id":19},{"text":"$","start":114,"end":115,"id":20},{"text":"k-$dimensional","start":115,"end":129,"id":21},{"text":"Euclidean","start":130,"end":139,"id":22},{"text":"space","start":140,"end":145,"id":23},{"text":",","start":145,"end":146,"id":24},{"text":"with","start":147,"end":151,"id":25},{"text":"$","start":152,"end":153,"id":26},{"text":"k","start":153,"end":154,"id":27},{"text":"\\ge","start":155,"end":158,"id":28},{"text":"\\frac{24\\ln","start":159,"end":170,"id":29},{"text":"\\emph{n}}{3\\epsilon^2","start":171,"end":192,"id":30},{"text":"-","start":192,"end":193,"id":31},{"text":"2\\epsilon^3}$","start":193,"end":206,"id":32},{"text":",","start":206,"end":207,"id":33},{"text":"so","start":208,"end":210,"id":34},{"text":"that","start":211,"end":215,"id":35},{"text":"the","start":216,"end":219,"id":36},{"text":"pairwise","start":220,"end":228,"id":37},{"text":"distances","start":229,"end":238,"id":38},{"text":"are","start":239,"end":242,"id":39},{"text":"preserved","start":243,"end":252,"id":40},{"text":"within","start":253,"end":259,"id":41},{"text":"a","start":260,"end":261,"id":42},{"text":"factor","start":262,"end":268,"id":43},{"text":"of","start":269,"end":271,"id":44},{"text":"$","start":272,"end":273,"id":45},{"text":"1\\pm\\epsilon$.","start":273,"end":287,"id":46},{"text":"Here","start":288,"end":292,"id":47},{"text":",","start":292,"end":293,"id":48},{"text":"working","start":294,"end":301,"id":49},{"text":"directly","start":302,"end":310,"id":50},{"text":"with","start":311,"end":315,"id":51},{"text":"the","start":316,"end":319,"id":52},{"text":"distributions","start":320,"end":333,"id":53},{"text":"of","start":334,"end":336,"id":54},{"text":"the","start":337,"end":340,"id":55},{"text":"random","start":341,"end":347,"id":56},{"text":"distances","start":348,"end":357,"id":57},{"text":"rather","start":358,"end":364,"id":58},{"text":"than","start":365,"end":369,"id":59},{"text":"resorting","start":370,"end":379,"id":60},{"text":"to","start":380,"end":382,"id":61},{"text":"the","start":383,"end":386,"id":62},{"text":"moment","start":387,"end":393,"id":63},{"text":"generating","start":394,"end":404,"id":64},{"text":"function","start":405,"end":413,"id":65},{"text":"technique","start":414,"end":423,"id":66},{"text":",","start":423,"end":424,"id":67},{"text":"an","start":425,"end":427,"id":68},{"text":"improvement","start":428,"end":439,"id":69},{"text":"on","start":440,"end":442,"id":70},{"text":"the","start":443,"end":446,"id":71},{"text":"lower","start":447,"end":452,"id":72},{"text":"bound","start":453,"end":458,"id":73},{"text":"for","start":459,"end":462,"id":74},{"text":"$","start":463,"end":464,"id":75},{"text":"k$","start":464,"end":466,"id":76},{"text":"is","start":467,"end":469,"id":77},{"text":"obtained","start":470,"end":478,"id":78},{"text":".","start":478,"end":479,"id":79}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"The algorithm exploits a close connection between Kernel PLS and the Lanczos algorithm for approximating the eigenvalues of symmetric matrices, and uses this approximation to compute the trace of powers of the kernel matrix in quadratic runtime.","_input_hash":1727047642,"_task_hash":-1150333975,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"algorithm","start":4,"end":13,"id":1},{"text":"exploits","start":14,"end":22,"id":2},{"text":"a","start":23,"end":24,"id":3},{"text":"close","start":25,"end":30,"id":4},{"text":"connection","start":31,"end":41,"id":5},{"text":"between","start":42,"end":49,"id":6},{"text":"Kernel","start":50,"end":56,"id":7},{"text":"PLS","start":57,"end":60,"id":8},{"text":"and","start":61,"end":64,"id":9},{"text":"the","start":65,"end":68,"id":10},{"text":"Lanczos","start":69,"end":76,"id":11},{"text":"algorithm","start":77,"end":86,"id":12},{"text":"for","start":87,"end":90,"id":13},{"text":"approximating","start":91,"end":104,"id":14},{"text":"the","start":105,"end":108,"id":15},{"text":"eigenvalues","start":109,"end":120,"id":16},{"text":"of","start":121,"end":123,"id":17},{"text":"symmetric","start":124,"end":133,"id":18},{"text":"matrices","start":134,"end":142,"id":19},{"text":",","start":142,"end":143,"id":20},{"text":"and","start":144,"end":147,"id":21},{"text":"uses","start":148,"end":152,"id":22},{"text":"this","start":153,"end":157,"id":23},{"text":"approximation","start":158,"end":171,"id":24},{"text":"to","start":172,"end":174,"id":25},{"text":"compute","start":175,"end":182,"id":26},{"text":"the","start":183,"end":186,"id":27},{"text":"trace","start":187,"end":192,"id":28},{"text":"of","start":193,"end":195,"id":29},{"text":"powers","start":196,"end":202,"id":30},{"text":"of","start":203,"end":205,"id":31},{"text":"the","start":206,"end":209,"id":32},{"text":"kernel","start":210,"end":216,"id":33},{"text":"matrix","start":217,"end":223,"id":34},{"text":"in","start":224,"end":226,"id":35},{"text":"quadratic","start":227,"end":236,"id":36},{"text":"runtime","start":237,"end":244,"id":37},{"text":".","start":244,"end":245,"id":38}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":0,"end":13,"token_start":0,"token_end":1,"label":"ALGO","answer":"reject"}],"answer":"reject"}
{"text":"We illustrate this modularity by demonstrating the algorithm on three different real-world data sets.","_input_hash":-995745589,"_task_hash":-996205726,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"illustrate","start":3,"end":13,"id":1},{"text":"this","start":14,"end":18,"id":2},{"text":"modularity","start":19,"end":29,"id":3},{"text":"by","start":30,"end":32,"id":4},{"text":"demonstrating","start":33,"end":46,"id":5},{"text":"the","start":47,"end":50,"id":6},{"text":"algorithm","start":51,"end":60,"id":7},{"text":"on","start":61,"end":63,"id":8},{"text":"three","start":64,"end":69,"id":9},{"text":"different","start":70,"end":79,"id":10},{"text":"real","start":80,"end":84,"id":11},{"text":"-","start":84,"end":85,"id":12},{"text":"world","start":85,"end":90,"id":13},{"text":"data","start":91,"end":95,"id":14},{"text":"sets","start":96,"end":100,"id":15},{"text":".","start":100,"end":101,"id":16}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":47,"end":60,"token_start":6,"token_end":7,"label":"ALGO","answer":"reject"}],"answer":"reject"}
{"text":"The proof is done by an application of the functional delta-method and by showing that the SVM-functional is suitably Hadamard-differentiable.","_input_hash":540185100,"_task_hash":-1772772296,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"proof","start":4,"end":9,"id":1},{"text":"is","start":10,"end":12,"id":2},{"text":"done","start":13,"end":17,"id":3},{"text":"by","start":18,"end":20,"id":4},{"text":"an","start":21,"end":23,"id":5},{"text":"application","start":24,"end":35,"id":6},{"text":"of","start":36,"end":38,"id":7},{"text":"the","start":39,"end":42,"id":8},{"text":"functional","start":43,"end":53,"id":9},{"text":"delta","start":54,"end":59,"id":10},{"text":"-","start":59,"end":60,"id":11},{"text":"method","start":60,"end":66,"id":12},{"text":"and","start":67,"end":70,"id":13},{"text":"by","start":71,"end":73,"id":14},{"text":"showing","start":74,"end":81,"id":15},{"text":"that","start":82,"end":86,"id":16},{"text":"the","start":87,"end":90,"id":17},{"text":"SVM","start":91,"end":94,"id":18},{"text":"-","start":94,"end":95,"id":19},{"text":"functional","start":95,"end":105,"id":20},{"text":"is","start":106,"end":108,"id":21},{"text":"suitably","start":109,"end":117,"id":22},{"text":"Hadamard","start":118,"end":126,"id":23},{"text":"-","start":126,"end":127,"id":24},{"text":"differentiable","start":127,"end":141,"id":25},{"text":".","start":141,"end":142,"id":26}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":91,"end":94,"token_start":18,"token_end":18,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"The new REMAPF algorithm estimates both the epsilon-machine and the decisional states from data.","_input_hash":-1763963857,"_task_hash":-1169858327,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"new","start":4,"end":7,"id":1},{"text":"REMAPF","start":8,"end":14,"id":2},{"text":"algorithm","start":15,"end":24,"id":3},{"text":"estimates","start":25,"end":34,"id":4},{"text":"both","start":35,"end":39,"id":5},{"text":"the","start":40,"end":43,"id":6},{"text":"epsilon","start":44,"end":51,"id":7},{"text":"-","start":51,"end":52,"id":8},{"text":"machine","start":52,"end":59,"id":9},{"text":"and","start":60,"end":63,"id":10},{"text":"the","start":64,"end":67,"id":11},{"text":"decisional","start":68,"end":78,"id":12},{"text":"states","start":79,"end":85,"id":13},{"text":"from","start":86,"end":90,"id":14},{"text":"data","start":91,"end":95,"id":15},{"text":".","start":95,"end":96,"id":16}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"As in other formulations such as the LASSO and forward step-wise regression we are interested in sparsifying the feature set while preserving generalization ability.","_input_hash":872316592,"_task_hash":1445313325,"tokens":[{"text":"As","start":0,"end":2,"id":0},{"text":"in","start":3,"end":5,"id":1},{"text":"other","start":6,"end":11,"id":2},{"text":"formulations","start":12,"end":24,"id":3},{"text":"such","start":25,"end":29,"id":4},{"text":"as","start":30,"end":32,"id":5},{"text":"the","start":33,"end":36,"id":6},{"text":"LASSO","start":37,"end":42,"id":7},{"text":"and","start":43,"end":46,"id":8},{"text":"forward","start":47,"end":54,"id":9},{"text":"step","start":55,"end":59,"id":10},{"text":"-","start":59,"end":60,"id":11},{"text":"wise","start":60,"end":64,"id":12},{"text":"regression","start":65,"end":75,"id":13},{"text":"we","start":76,"end":78,"id":14},{"text":"are","start":79,"end":82,"id":15},{"text":"interested","start":83,"end":93,"id":16},{"text":"in","start":94,"end":96,"id":17},{"text":"sparsifying","start":97,"end":108,"id":18},{"text":"the","start":109,"end":112,"id":19},{"text":"feature","start":113,"end":120,"id":20},{"text":"set","start":121,"end":124,"id":21},{"text":"while","start":125,"end":130,"id":22},{"text":"preserving","start":131,"end":141,"id":23},{"text":"generalization","start":142,"end":156,"id":24},{"text":"ability","start":157,"end":164,"id":25},{"text":".","start":164,"end":165,"id":26}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":47,"end":75,"token_start":9,"token_end":13,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We generate a set of biased sequences by applying a finite state automata with a specified number, $m$, of states to the set of all binary sequences.","_input_hash":1039097680,"_task_hash":1277283597,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"generate","start":3,"end":11,"id":1},{"text":"a","start":12,"end":13,"id":2},{"text":"set","start":14,"end":17,"id":3},{"text":"of","start":18,"end":20,"id":4},{"text":"biased","start":21,"end":27,"id":5},{"text":"sequences","start":28,"end":37,"id":6},{"text":"by","start":38,"end":40,"id":7},{"text":"applying","start":41,"end":49,"id":8},{"text":"a","start":50,"end":51,"id":9},{"text":"finite","start":52,"end":58,"id":10},{"text":"state","start":59,"end":64,"id":11},{"text":"automata","start":65,"end":73,"id":12},{"text":"with","start":74,"end":78,"id":13},{"text":"a","start":79,"end":80,"id":14},{"text":"specified","start":81,"end":90,"id":15},{"text":"number","start":91,"end":97,"id":16},{"text":",","start":97,"end":98,"id":17},{"text":"$","start":99,"end":100,"id":18},{"text":"m$","start":100,"end":102,"id":19},{"text":",","start":102,"end":103,"id":20},{"text":"of","start":104,"end":106,"id":21},{"text":"states","start":107,"end":113,"id":22},{"text":"to","start":114,"end":116,"id":23},{"text":"the","start":117,"end":120,"id":24},{"text":"set","start":121,"end":124,"id":25},{"text":"of","start":125,"end":127,"id":26},{"text":"all","start":128,"end":131,"id":27},{"text":"binary","start":132,"end":138,"id":28},{"text":"sequences","start":139,"end":148,"id":29},{"text":".","start":148,"end":149,"id":30}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"The measure also serves as a natural tool when choosing dimension-reduction parameters.","_input_hash":-1067289464,"_task_hash":2026893910,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"measure","start":4,"end":11,"id":1},{"text":"also","start":12,"end":16,"id":2},{"text":"serves","start":17,"end":23,"id":3},{"text":"as","start":24,"end":26,"id":4},{"text":"a","start":27,"end":28,"id":5},{"text":"natural","start":29,"end":36,"id":6},{"text":"tool","start":37,"end":41,"id":7},{"text":"when","start":42,"end":46,"id":8},{"text":"choosing","start":47,"end":55,"id":9},{"text":"dimension","start":56,"end":65,"id":10},{"text":"-","start":65,"end":66,"id":11},{"text":"reduction","start":66,"end":75,"id":12},{"text":"parameters","start":76,"end":86,"id":13},{"text":".","start":86,"end":87,"id":14}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"The utility function encodes some a priori knowledge external to the system, it quantifies how bad it is to make mistakes.","_input_hash":651532569,"_task_hash":-1655137091,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"utility","start":4,"end":11,"id":1},{"text":"function","start":12,"end":20,"id":2},{"text":"encodes","start":21,"end":28,"id":3},{"text":"some","start":29,"end":33,"id":4},{"text":"a","start":34,"end":35,"id":5},{"text":"priori","start":36,"end":42,"id":6},{"text":"knowledge","start":43,"end":52,"id":7},{"text":"external","start":53,"end":61,"id":8},{"text":"to","start":62,"end":64,"id":9},{"text":"the","start":65,"end":68,"id":10},{"text":"system","start":69,"end":75,"id":11},{"text":",","start":75,"end":76,"id":12},{"text":"it","start":77,"end":79,"id":13},{"text":"quantifies","start":80,"end":90,"id":14},{"text":"how","start":91,"end":94,"id":15},{"text":"bad","start":95,"end":98,"id":16},{"text":"it","start":99,"end":101,"id":17},{"text":"is","start":102,"end":104,"id":18},{"text":"to","start":105,"end":107,"id":19},{"text":"make","start":108,"end":112,"id":20},{"text":"mistakes","start":113,"end":121,"id":21},{"text":".","start":121,"end":122,"id":22}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"topology|NOUN","word":"topology","sense":"NOUN","meta":{"score":0.7960000038,"sense":"NOUN"},"_input_hash":1183486010,"_task_hash":414799710,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"topology|NOUN","start":0,"end":13,"id":0}]}
{"text":"computational_models|NOUN","word":"computational models","sense":"NOUN","meta":{"score":0.779399991,"sense":"NOUN"},"_input_hash":268456910,"_task_hash":-1368491268,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"computational_models|NOUN","start":0,"end":25,"id":0}]}
{"text":"We introduce an alternative, interaction component model for communities (ICMc), where the whole network is a bag of links, stemming from different components.","_input_hash":-485852851,"_task_hash":1709948878,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"introduce","start":3,"end":12,"id":1},{"text":"an","start":13,"end":15,"id":2},{"text":"alternative","start":16,"end":27,"id":3},{"text":",","start":27,"end":28,"id":4},{"text":"interaction","start":29,"end":40,"id":5},{"text":"component","start":41,"end":50,"id":6},{"text":"model","start":51,"end":56,"id":7},{"text":"for","start":57,"end":60,"id":8},{"text":"communities","start":61,"end":72,"id":9},{"text":"(","start":73,"end":74,"id":10},{"text":"ICMc","start":74,"end":78,"id":11},{"text":")","start":78,"end":79,"id":12},{"text":",","start":79,"end":80,"id":13},{"text":"where","start":81,"end":86,"id":14},{"text":"the","start":87,"end":90,"id":15},{"text":"whole","start":91,"end":96,"id":16},{"text":"network","start":97,"end":104,"id":17},{"text":"is","start":105,"end":107,"id":18},{"text":"a","start":108,"end":109,"id":19},{"text":"bag","start":110,"end":113,"id":20},{"text":"of","start":114,"end":116,"id":21},{"text":"links","start":117,"end":122,"id":22},{"text":",","start":122,"end":123,"id":23},{"text":"stemming","start":124,"end":132,"id":24},{"text":"from","start":133,"end":137,"id":25},{"text":"different","start":138,"end":147,"id":26},{"text":"components","start":148,"end":158,"id":27},{"text":".","start":158,"end":159,"id":28}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":29,"end":56,"token_start":5,"token_end":7,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Although some previous work addressed the handling of malicious data in the context of supervised learning, very little is known about the behavior of anomaly detection methods in such scenarios.","_input_hash":-29242478,"_task_hash":-1043603022,"tokens":[{"text":"Although","start":0,"end":8,"id":0},{"text":"some","start":9,"end":13,"id":1},{"text":"previous","start":14,"end":22,"id":2},{"text":"work","start":23,"end":27,"id":3},{"text":"addressed","start":28,"end":37,"id":4},{"text":"the","start":38,"end":41,"id":5},{"text":"handling","start":42,"end":50,"id":6},{"text":"of","start":51,"end":53,"id":7},{"text":"malicious","start":54,"end":63,"id":8},{"text":"data","start":64,"end":68,"id":9},{"text":"in","start":69,"end":71,"id":10},{"text":"the","start":72,"end":75,"id":11},{"text":"context","start":76,"end":83,"id":12},{"text":"of","start":84,"end":86,"id":13},{"text":"supervised","start":87,"end":97,"id":14},{"text":"learning","start":98,"end":106,"id":15},{"text":",","start":106,"end":107,"id":16},{"text":"very","start":108,"end":112,"id":17},{"text":"little","start":113,"end":119,"id":18},{"text":"is","start":120,"end":122,"id":19},{"text":"known","start":123,"end":128,"id":20},{"text":"about","start":129,"end":134,"id":21},{"text":"the","start":135,"end":138,"id":22},{"text":"behavior","start":139,"end":147,"id":23},{"text":"of","start":148,"end":150,"id":24},{"text":"anomaly","start":151,"end":158,"id":25},{"text":"detection","start":159,"end":168,"id":26},{"text":"methods","start":169,"end":176,"id":27},{"text":"in","start":177,"end":179,"id":28},{"text":"such","start":180,"end":184,"id":29},{"text":"scenarios","start":185,"end":194,"id":30},{"text":".","start":194,"end":195,"id":31}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"The transitions between these decisional states correspond to events that lead to a change of decision.","_input_hash":-752918440,"_task_hash":-1884347967,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"transitions","start":4,"end":15,"id":1},{"text":"between","start":16,"end":23,"id":2},{"text":"these","start":24,"end":29,"id":3},{"text":"decisional","start":30,"end":40,"id":4},{"text":"states","start":41,"end":47,"id":5},{"text":"correspond","start":48,"end":58,"id":6},{"text":"to","start":59,"end":61,"id":7},{"text":"events","start":62,"end":68,"id":8},{"text":"that","start":69,"end":73,"id":9},{"text":"lead","start":74,"end":78,"id":10},{"text":"to","start":79,"end":81,"id":11},{"text":"a","start":82,"end":83,"id":12},{"text":"change","start":84,"end":90,"id":13},{"text":"of","start":91,"end":93,"id":14},{"text":"decision","start":94,"end":102,"id":15},{"text":".","start":102,"end":103,"id":16}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"We apply this theory to the class of uneven margin losses, and characterize when these losses are properly calibrated.","_input_hash":1803974302,"_task_hash":-398803600,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"apply","start":3,"end":8,"id":1},{"text":"this","start":9,"end":13,"id":2},{"text":"theory","start":14,"end":20,"id":3},{"text":"to","start":21,"end":23,"id":4},{"text":"the","start":24,"end":27,"id":5},{"text":"class","start":28,"end":33,"id":6},{"text":"of","start":34,"end":36,"id":7},{"text":"uneven","start":37,"end":43,"id":8},{"text":"margin","start":44,"end":50,"id":9},{"text":"losses","start":51,"end":57,"id":10},{"text":",","start":57,"end":58,"id":11},{"text":"and","start":59,"end":62,"id":12},{"text":"characterize","start":63,"end":75,"id":13},{"text":"when","start":76,"end":80,"id":14},{"text":"these","start":81,"end":86,"id":15},{"text":"losses","start":87,"end":93,"id":16},{"text":"are","start":94,"end":97,"id":17},{"text":"properly","start":98,"end":106,"id":18},{"text":"calibrated","start":107,"end":117,"id":19},{"text":".","start":117,"end":118,"id":20}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"A density ratio is defined by the ratio of two probability densities.","_input_hash":-428174293,"_task_hash":309688397,"tokens":[{"text":"A","start":0,"end":1,"id":0},{"text":"density","start":2,"end":9,"id":1},{"text":"ratio","start":10,"end":15,"id":2},{"text":"is","start":16,"end":18,"id":3},{"text":"defined","start":19,"end":26,"id":4},{"text":"by","start":27,"end":29,"id":5},{"text":"the","start":30,"end":33,"id":6},{"text":"ratio","start":34,"end":39,"id":7},{"text":"of","start":40,"end":42,"id":8},{"text":"two","start":43,"end":46,"id":9},{"text":"probability","start":47,"end":58,"id":10},{"text":"densities","start":59,"end":68,"id":11},{"text":".","start":68,"end":69,"id":12}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"We present a procedure for effective estimation of entropy and mutual information from small-sample data, and apply it to the problem of inferring high-dimensional gene association networks.","_input_hash":1305692642,"_task_hash":408214070,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"present","start":3,"end":10,"id":1},{"text":"a","start":11,"end":12,"id":2},{"text":"procedure","start":13,"end":22,"id":3},{"text":"for","start":23,"end":26,"id":4},{"text":"effective","start":27,"end":36,"id":5},{"text":"estimation","start":37,"end":47,"id":6},{"text":"of","start":48,"end":50,"id":7},{"text":"entropy","start":51,"end":58,"id":8},{"text":"and","start":59,"end":62,"id":9},{"text":"mutual","start":63,"end":69,"id":10},{"text":"information","start":70,"end":81,"id":11},{"text":"from","start":82,"end":86,"id":12},{"text":"small","start":87,"end":92,"id":13},{"text":"-","start":92,"end":93,"id":14},{"text":"sample","start":93,"end":99,"id":15},{"text":"data","start":100,"end":104,"id":16},{"text":",","start":104,"end":105,"id":17},{"text":"and","start":106,"end":109,"id":18},{"text":"apply","start":110,"end":115,"id":19},{"text":"it","start":116,"end":118,"id":20},{"text":"to","start":119,"end":121,"id":21},{"text":"the","start":122,"end":125,"id":22},{"text":"problem","start":126,"end":133,"id":23},{"text":"of","start":134,"end":136,"id":24},{"text":"inferring","start":137,"end":146,"id":25},{"text":"high","start":147,"end":151,"id":26},{"text":"-","start":151,"end":152,"id":27},{"text":"dimensional","start":152,"end":163,"id":28},{"text":"gene","start":164,"end":168,"id":29},{"text":"association","start":169,"end":180,"id":30},{"text":"networks","start":181,"end":189,"id":31},{"text":".","start":189,"end":190,"id":32}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"This problem is of particular interest when $F$ contains many irrelevant functions that should not appear in $\\tilde{f}$. We provide an exact oracle inequality for $\\tilde f$, where only two coefficients are non-zero, that entails $\\tilde f$ to be an optimal aggregation algorithm.","_input_hash":-671656956,"_task_hash":-887917405,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"problem","start":5,"end":12,"id":1},{"text":"is","start":13,"end":15,"id":2},{"text":"of","start":16,"end":18,"id":3},{"text":"particular","start":19,"end":29,"id":4},{"text":"interest","start":30,"end":38,"id":5},{"text":"when","start":39,"end":43,"id":6},{"text":"$","start":44,"end":45,"id":7},{"text":"F$","start":45,"end":47,"id":8},{"text":"contains","start":48,"end":56,"id":9},{"text":"many","start":57,"end":61,"id":10},{"text":"irrelevant","start":62,"end":72,"id":11},{"text":"functions","start":73,"end":82,"id":12},{"text":"that","start":83,"end":87,"id":13},{"text":"should","start":88,"end":94,"id":14},{"text":"not","start":95,"end":98,"id":15},{"text":"appear","start":99,"end":105,"id":16},{"text":"in","start":106,"end":108,"id":17},{"text":"$","start":109,"end":110,"id":18},{"text":"\\tilde{f}$.","start":110,"end":121,"id":19},{"text":"We","start":122,"end":124,"id":20},{"text":"provide","start":125,"end":132,"id":21},{"text":"an","start":133,"end":135,"id":22},{"text":"exact","start":136,"end":141,"id":23},{"text":"oracle","start":142,"end":148,"id":24},{"text":"inequality","start":149,"end":159,"id":25},{"text":"for","start":160,"end":163,"id":26},{"text":"$","start":164,"end":165,"id":27},{"text":"\\tilde","start":165,"end":171,"id":28},{"text":"f$","start":172,"end":174,"id":29},{"text":",","start":174,"end":175,"id":30},{"text":"where","start":176,"end":181,"id":31},{"text":"only","start":182,"end":186,"id":32},{"text":"two","start":187,"end":190,"id":33},{"text":"coefficients","start":191,"end":203,"id":34},{"text":"are","start":204,"end":207,"id":35},{"text":"non","start":208,"end":211,"id":36},{"text":"-","start":211,"end":212,"id":37},{"text":"zero","start":212,"end":216,"id":38},{"text":",","start":216,"end":217,"id":39},{"text":"that","start":218,"end":222,"id":40},{"text":"entails","start":223,"end":230,"id":41},{"text":"$","start":231,"end":232,"id":42},{"text":"\\tilde","start":232,"end":238,"id":43},{"text":"f$","start":239,"end":241,"id":44},{"text":"to","start":242,"end":244,"id":45},{"text":"be","start":245,"end":247,"id":46},{"text":"an","start":248,"end":250,"id":47},{"text":"optimal","start":251,"end":258,"id":48},{"text":"aggregation","start":259,"end":270,"id":49},{"text":"algorithm","start":271,"end":280,"id":50},{"text":".","start":280,"end":281,"id":51}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"We propose a scalable algorithmic framework, with which SLM posteriors over full, high-resolution images can be approximated for the first time, solving a variational optimization problem which is convex iff posterior mode finding is convex.","_input_hash":-1049062672,"_task_hash":-1627809004,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"propose","start":3,"end":10,"id":1},{"text":"a","start":11,"end":12,"id":2},{"text":"scalable","start":13,"end":21,"id":3},{"text":"algorithmic","start":22,"end":33,"id":4},{"text":"framework","start":34,"end":43,"id":5},{"text":",","start":43,"end":44,"id":6},{"text":"with","start":45,"end":49,"id":7},{"text":"which","start":50,"end":55,"id":8},{"text":"SLM","start":56,"end":59,"id":9},{"text":"posteriors","start":60,"end":70,"id":10},{"text":"over","start":71,"end":75,"id":11},{"text":"full","start":76,"end":80,"id":12},{"text":",","start":80,"end":81,"id":13},{"text":"high","start":82,"end":86,"id":14},{"text":"-","start":86,"end":87,"id":15},{"text":"resolution","start":87,"end":97,"id":16},{"text":"images","start":98,"end":104,"id":17},{"text":"can","start":105,"end":108,"id":18},{"text":"be","start":109,"end":111,"id":19},{"text":"approximated","start":112,"end":124,"id":20},{"text":"for","start":125,"end":128,"id":21},{"text":"the","start":129,"end":132,"id":22},{"text":"first","start":133,"end":138,"id":23},{"text":"time","start":139,"end":143,"id":24},{"text":",","start":143,"end":144,"id":25},{"text":"solving","start":145,"end":152,"id":26},{"text":"a","start":153,"end":154,"id":27},{"text":"variational","start":155,"end":166,"id":28},{"text":"optimization","start":167,"end":179,"id":29},{"text":"problem","start":180,"end":187,"id":30},{"text":"which","start":188,"end":193,"id":31},{"text":"is","start":194,"end":196,"id":32},{"text":"convex","start":197,"end":203,"id":33},{"text":"iff","start":204,"end":207,"id":34},{"text":"posterior","start":208,"end":217,"id":35},{"text":"mode","start":218,"end":222,"id":36},{"text":"finding","start":223,"end":230,"id":37},{"text":"is","start":231,"end":233,"id":38},{"text":"convex","start":234,"end":240,"id":39},{"text":".","start":240,"end":241,"id":40}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Despite its simplicity, we show that it outperforms eight other entropy estimation procedures across a diverse range of sampling scenarios and data-generating models, even in cases of severe undersampling.","_input_hash":1789338829,"_task_hash":-1307448222,"tokens":[{"text":"Despite","start":0,"end":7,"id":0},{"text":"its","start":8,"end":11,"id":1},{"text":"simplicity","start":12,"end":22,"id":2},{"text":",","start":22,"end":23,"id":3},{"text":"we","start":24,"end":26,"id":4},{"text":"show","start":27,"end":31,"id":5},{"text":"that","start":32,"end":36,"id":6},{"text":"it","start":37,"end":39,"id":7},{"text":"outperforms","start":40,"end":51,"id":8},{"text":"eight","start":52,"end":57,"id":9},{"text":"other","start":58,"end":63,"id":10},{"text":"entropy","start":64,"end":71,"id":11},{"text":"estimation","start":72,"end":82,"id":12},{"text":"procedures","start":83,"end":93,"id":13},{"text":"across","start":94,"end":100,"id":14},{"text":"a","start":101,"end":102,"id":15},{"text":"diverse","start":103,"end":110,"id":16},{"text":"range","start":111,"end":116,"id":17},{"text":"of","start":117,"end":119,"id":18},{"text":"sampling","start":120,"end":128,"id":19},{"text":"scenarios","start":129,"end":138,"id":20},{"text":"and","start":139,"end":142,"id":21},{"text":"data","start":143,"end":147,"id":22},{"text":"-","start":147,"end":148,"id":23},{"text":"generating","start":148,"end":158,"id":24},{"text":"models","start":159,"end":165,"id":25},{"text":",","start":165,"end":166,"id":26},{"text":"even","start":167,"end":171,"id":27},{"text":"in","start":172,"end":174,"id":28},{"text":"cases","start":175,"end":180,"id":29},{"text":"of","start":181,"end":183,"id":30},{"text":"severe","start":184,"end":190,"id":31},{"text":"undersampling","start":191,"end":204,"id":32},{"text":".","start":204,"end":205,"id":33}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"artificial_neural_networks|NOUN","word":"artificial neural networks","sense":"NOUN","meta":{"score":0.7613000274,"sense":"NOUN"},"_input_hash":-1724947595,"_task_hash":-1046508247,"_session_id":null,"_view_id":"html","answer":"accept","spans":[],"tokens":[{"text":"artificial_neural_networks|NOUN","start":0,"end":31,"id":0}]}
{"text":"We analyze when different initializations lead to the same local optimum, and when they lead to different local optima.","_input_hash":1958327346,"_task_hash":-2001187100,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"analyze","start":3,"end":10,"id":1},{"text":"when","start":11,"end":15,"id":2},{"text":"different","start":16,"end":25,"id":3},{"text":"initializations","start":26,"end":41,"id":4},{"text":"lead","start":42,"end":46,"id":5},{"text":"to","start":47,"end":49,"id":6},{"text":"the","start":50,"end":53,"id":7},{"text":"same","start":54,"end":58,"id":8},{"text":"local","start":59,"end":64,"id":9},{"text":"optimum","start":65,"end":72,"id":10},{"text":",","start":72,"end":73,"id":11},{"text":"and","start":74,"end":77,"id":12},{"text":"when","start":78,"end":82,"id":13},{"text":"they","start":83,"end":87,"id":14},{"text":"lead","start":88,"end":92,"id":15},{"text":"to","start":93,"end":95,"id":16},{"text":"different","start":96,"end":105,"id":17},{"text":"local","start":106,"end":111,"id":18},{"text":"optima","start":112,"end":118,"id":19},{"text":".","start":118,"end":119,"id":20}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"a new stability-based method for choosing the regularization parameter in high dimensional inference for undirected graphs.","_input_hash":-1506303016,"_task_hash":-288976320,"tokens":[{"text":"a","start":0,"end":1,"id":0},{"text":"new","start":2,"end":5,"id":1},{"text":"stability","start":6,"end":15,"id":2},{"text":"-","start":15,"end":16,"id":3},{"text":"based","start":16,"end":21,"id":4},{"text":"method","start":22,"end":28,"id":5},{"text":"for","start":29,"end":32,"id":6},{"text":"choosing","start":33,"end":41,"id":7},{"text":"the","start":42,"end":45,"id":8},{"text":"regularization","start":46,"end":60,"id":9},{"text":"parameter","start":61,"end":70,"id":10},{"text":"in","start":71,"end":73,"id":11},{"text":"high","start":74,"end":78,"id":12},{"text":"dimensional","start":79,"end":90,"id":13},{"text":"inference","start":91,"end":100,"id":14},{"text":"for","start":101,"end":104,"id":15},{"text":"undirected","start":105,"end":115,"id":16},{"text":"graphs","start":116,"end":122,"id":17},{"text":".","start":122,"end":123,"id":18}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"We provide explicit formulas suited for applications and present preliminary experiments to illustrate the interest of the approach.","_input_hash":-2125450756,"_task_hash":1774286172,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"provide","start":3,"end":10,"id":1},{"text":"explicit","start":11,"end":19,"id":2},{"text":"formulas","start":20,"end":28,"id":3},{"text":"suited","start":29,"end":35,"id":4},{"text":"for","start":36,"end":39,"id":5},{"text":"applications","start":40,"end":52,"id":6},{"text":"and","start":53,"end":56,"id":7},{"text":"present","start":57,"end":64,"id":8},{"text":"preliminary","start":65,"end":76,"id":9},{"text":"experiments","start":77,"end":88,"id":10},{"text":"to","start":89,"end":91,"id":11},{"text":"illustrate","start":92,"end":102,"id":12},{"text":"the","start":103,"end":106,"id":13},{"text":"interest","start":107,"end":115,"id":14},{"text":"of","start":116,"end":118,"id":15},{"text":"the","start":119,"end":122,"id":16},{"text":"approach","start":123,"end":131,"id":17},{"text":".","start":131,"end":132,"id":18}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"We present the Procrustes measure, a novel measure based on Procrustes rotation that enables quantitative comparison of the output of manifold-based embedding algorithms (such as LLE (Roweis and Saul, 2000) and Isomap (Tenenbaum et al, 2000)).","_input_hash":1911758319,"_task_hash":123835621,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"present","start":3,"end":10,"id":1},{"text":"the","start":11,"end":14,"id":2},{"text":"Procrustes","start":15,"end":25,"id":3},{"text":"measure","start":26,"end":33,"id":4},{"text":",","start":33,"end":34,"id":5},{"text":"a","start":35,"end":36,"id":6},{"text":"novel","start":37,"end":42,"id":7},{"text":"measure","start":43,"end":50,"id":8},{"text":"based","start":51,"end":56,"id":9},{"text":"on","start":57,"end":59,"id":10},{"text":"Procrustes","start":60,"end":70,"id":11},{"text":"rotation","start":71,"end":79,"id":12},{"text":"that","start":80,"end":84,"id":13},{"text":"enables","start":85,"end":92,"id":14},{"text":"quantitative","start":93,"end":105,"id":15},{"text":"comparison","start":106,"end":116,"id":16},{"text":"of","start":117,"end":119,"id":17},{"text":"the","start":120,"end":123,"id":18},{"text":"output","start":124,"end":130,"id":19},{"text":"of","start":131,"end":133,"id":20},{"text":"manifold","start":134,"end":142,"id":21},{"text":"-","start":142,"end":143,"id":22},{"text":"based","start":143,"end":148,"id":23},{"text":"embedding","start":149,"end":158,"id":24},{"text":"algorithms","start":159,"end":169,"id":25},{"text":"(","start":170,"end":171,"id":26},{"text":"such","start":171,"end":175,"id":27},{"text":"as","start":176,"end":178,"id":28},{"text":"LLE","start":179,"end":182,"id":29},{"text":"(","start":183,"end":184,"id":30},{"text":"Roweis","start":184,"end":190,"id":31},{"text":"and","start":191,"end":194,"id":32},{"text":"Saul","start":195,"end":199,"id":33},{"text":",","start":199,"end":200,"id":34},{"text":"2000","start":201,"end":205,"id":35},{"text":")","start":205,"end":206,"id":36},{"text":"and","start":207,"end":210,"id":37},{"text":"Isomap","start":211,"end":217,"id":38},{"text":"(","start":218,"end":219,"id":39},{"text":"Tenenbaum","start":219,"end":228,"id":40},{"text":"et","start":229,"end":231,"id":41},{"text":"al","start":232,"end":234,"id":42},{"text":",","start":234,"end":235,"id":43},{"text":"2000","start":236,"end":240,"id":44},{"text":")","start":240,"end":241,"id":45},{"text":")","start":241,"end":242,"id":46},{"text":".","start":242,"end":243,"id":47}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":134,"end":169,"token_start":21,"token_end":25,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"computational_methods|NOUN","word":"computational methods","sense":"NOUN","meta":{"score":0.7881000042,"sense":"NOUN"},"_input_hash":-2102461886,"_task_hash":-1529882859,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"computational_methods|NOUN","start":0,"end":26,"id":0}]}
{"text":"sensory_data|NOUN","word":"sensory data","sense":"NOUN","meta":{"score":0.7610999942,"sense":"NOUN"},"_input_hash":-1747845465,"_task_hash":490821631,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"sensory_data|NOUN","start":0,"end":17,"id":0}]}
{"text":"Since selectors are suboptimal aggregation procedures, this proves that 2 is the minimal number of elements of $F$ required for the construction of an optimal aggregation procedures in every situations.","_input_hash":293426480,"_task_hash":-1564923605,"tokens":[{"text":"Since","start":0,"end":5,"id":0},{"text":"selectors","start":6,"end":15,"id":1},{"text":"are","start":16,"end":19,"id":2},{"text":"suboptimal","start":20,"end":30,"id":3},{"text":"aggregation","start":31,"end":42,"id":4},{"text":"procedures","start":43,"end":53,"id":5},{"text":",","start":53,"end":54,"id":6},{"text":"this","start":55,"end":59,"id":7},{"text":"proves","start":60,"end":66,"id":8},{"text":"that","start":67,"end":71,"id":9},{"text":"2","start":72,"end":73,"id":10},{"text":"is","start":74,"end":76,"id":11},{"text":"the","start":77,"end":80,"id":12},{"text":"minimal","start":81,"end":88,"id":13},{"text":"number","start":89,"end":95,"id":14},{"text":"of","start":96,"end":98,"id":15},{"text":"elements","start":99,"end":107,"id":16},{"text":"of","start":108,"end":110,"id":17},{"text":"$","start":111,"end":112,"id":18},{"text":"F$","start":112,"end":114,"id":19},{"text":"required","start":115,"end":123,"id":20},{"text":"for","start":124,"end":127,"id":21},{"text":"the","start":128,"end":131,"id":22},{"text":"construction","start":132,"end":144,"id":23},{"text":"of","start":145,"end":147,"id":24},{"text":"an","start":148,"end":150,"id":25},{"text":"optimal","start":151,"end":158,"id":26},{"text":"aggregation","start":159,"end":170,"id":27},{"text":"procedures","start":171,"end":181,"id":28},{"text":"in","start":182,"end":184,"id":29},{"text":"every","start":185,"end":190,"id":30},{"text":"situations","start":191,"end":201,"id":31},{"text":".","start":201,"end":202,"id":32}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"They are simple, computationally fast and scalable, interpretable, and admit nonparametric priors.","_input_hash":438256361,"_task_hash":333327110,"tokens":[{"text":"They","start":0,"end":4,"id":0},{"text":"are","start":5,"end":8,"id":1},{"text":"simple","start":9,"end":15,"id":2},{"text":",","start":15,"end":16,"id":3},{"text":"computationally","start":17,"end":32,"id":4},{"text":"fast","start":33,"end":37,"id":5},{"text":"and","start":38,"end":41,"id":6},{"text":"scalable","start":42,"end":50,"id":7},{"text":",","start":50,"end":51,"id":8},{"text":"interpretable","start":52,"end":65,"id":9},{"text":",","start":65,"end":66,"id":10},{"text":"and","start":67,"end":70,"id":11},{"text":"admit","start":71,"end":76,"id":12},{"text":"nonparametric","start":77,"end":90,"id":13},{"text":"priors","start":91,"end":97,"id":14},{"text":".","start":97,"end":98,"id":15}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Typically, flow cytometric data analysis is performed through a series of 2-dimensional projections onto the axes of the data set.","_input_hash":-1351284336,"_task_hash":-39802717,"tokens":[{"text":"Typically","start":0,"end":9,"id":0},{"text":",","start":9,"end":10,"id":1},{"text":"flow","start":11,"end":15,"id":2},{"text":"cytometric","start":16,"end":26,"id":3},{"text":"data","start":27,"end":31,"id":4},{"text":"analysis","start":32,"end":40,"id":5},{"text":"is","start":41,"end":43,"id":6},{"text":"performed","start":44,"end":53,"id":7},{"text":"through","start":54,"end":61,"id":8},{"text":"a","start":62,"end":63,"id":9},{"text":"series","start":64,"end":70,"id":10},{"text":"of","start":71,"end":73,"id":11},{"text":"2-dimensional","start":74,"end":87,"id":12},{"text":"projections","start":88,"end":99,"id":13},{"text":"onto","start":100,"end":104,"id":14},{"text":"the","start":105,"end":108,"id":15},{"text":"axes","start":109,"end":113,"id":16},{"text":"of","start":114,"end":116,"id":17},{"text":"the","start":117,"end":120,"id":18},{"text":"data","start":121,"end":125,"id":19},{"text":"set","start":126,"end":129,"id":20},{"text":".","start":129,"end":130,"id":21}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"quantum_computing|NOUN","word":"quantum computing","sense":"NOUN","meta":{"score":0.7876999974,"sense":"NOUN"},"_input_hash":1105399114,"_task_hash":-412935212,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"quantum_computing|NOUN","start":0,"end":22,"id":0}]}
{"text":"We show that this method outperforms the more standard use of $\\chi^2$ and likelihood ratio tests.","_input_hash":-131603167,"_task_hash":-657724470,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"show","start":3,"end":7,"id":1},{"text":"that","start":8,"end":12,"id":2},{"text":"this","start":13,"end":17,"id":3},{"text":"method","start":18,"end":24,"id":4},{"text":"outperforms","start":25,"end":36,"id":5},{"text":"the","start":37,"end":40,"id":6},{"text":"more","start":41,"end":45,"id":7},{"text":"standard","start":46,"end":54,"id":8},{"text":"use","start":55,"end":58,"id":9},{"text":"of","start":59,"end":61,"id":10},{"text":"$","start":62,"end":63,"id":11},{"text":"\\chi^2","start":63,"end":69,"id":12},{"text":"$","start":69,"end":70,"id":13},{"text":"and","start":71,"end":74,"id":14},{"text":"likelihood","start":75,"end":85,"id":15},{"text":"ratio","start":86,"end":91,"id":16},{"text":"tests","start":92,"end":97,"id":17},{"text":".","start":97,"end":98,"id":18}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"individual_neurons|NOUN","word":"individual neurons","sense":"NOUN","meta":{"score":0.8046000004,"sense":"NOUN"},"_input_hash":-1478547349,"_task_hash":-1663430496,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"individual_neurons|NOUN","start":0,"end":23,"id":0}]}
{"text":"In recent years, kernel density estimation has been exploited by computer scientists to model machine learning problems.","_input_hash":-74833356,"_task_hash":1868453665,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"recent","start":3,"end":9,"id":1},{"text":"years","start":10,"end":15,"id":2},{"text":",","start":15,"end":16,"id":3},{"text":"kernel","start":17,"end":23,"id":4},{"text":"density","start":24,"end":31,"id":5},{"text":"estimation","start":32,"end":42,"id":6},{"text":"has","start":43,"end":46,"id":7},{"text":"been","start":47,"end":51,"id":8},{"text":"exploited","start":52,"end":61,"id":9},{"text":"by","start":62,"end":64,"id":10},{"text":"computer","start":65,"end":73,"id":11},{"text":"scientists","start":74,"end":84,"id":12},{"text":"to","start":85,"end":87,"id":13},{"text":"model","start":88,"end":93,"id":14},{"text":"machine","start":94,"end":101,"id":15},{"text":"learning","start":102,"end":110,"id":16},{"text":"problems","start":111,"end":119,"id":17},{"text":".","start":119,"end":120,"id":18}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"There has been an explosion of interest in statistical models for analyzing network data, and considerable interest in the class of exponential random graph (ERG) models, especially in connection with difficulties in computing maximum likelihood estimates.","_input_hash":1399533402,"_task_hash":442581205,"tokens":[{"text":"There","start":0,"end":5,"id":0},{"text":"has","start":6,"end":9,"id":1},{"text":"been","start":10,"end":14,"id":2},{"text":"an","start":15,"end":17,"id":3},{"text":"explosion","start":18,"end":27,"id":4},{"text":"of","start":28,"end":30,"id":5},{"text":"interest","start":31,"end":39,"id":6},{"text":"in","start":40,"end":42,"id":7},{"text":"statistical","start":43,"end":54,"id":8},{"text":"models","start":55,"end":61,"id":9},{"text":"for","start":62,"end":65,"id":10},{"text":"analyzing","start":66,"end":75,"id":11},{"text":"network","start":76,"end":83,"id":12},{"text":"data","start":84,"end":88,"id":13},{"text":",","start":88,"end":89,"id":14},{"text":"and","start":90,"end":93,"id":15},{"text":"considerable","start":94,"end":106,"id":16},{"text":"interest","start":107,"end":115,"id":17},{"text":"in","start":116,"end":118,"id":18},{"text":"the","start":119,"end":122,"id":19},{"text":"class","start":123,"end":128,"id":20},{"text":"of","start":129,"end":131,"id":21},{"text":"exponential","start":132,"end":143,"id":22},{"text":"random","start":144,"end":150,"id":23},{"text":"graph","start":151,"end":156,"id":24},{"text":"(","start":157,"end":158,"id":25},{"text":"ERG","start":158,"end":161,"id":26},{"text":")","start":161,"end":162,"id":27},{"text":"models","start":163,"end":169,"id":28},{"text":",","start":169,"end":170,"id":29},{"text":"especially","start":171,"end":181,"id":30},{"text":"in","start":182,"end":184,"id":31},{"text":"connection","start":185,"end":195,"id":32},{"text":"with","start":196,"end":200,"id":33},{"text":"difficulties","start":201,"end":213,"id":34},{"text":"in","start":214,"end":216,"id":35},{"text":"computing","start":217,"end":226,"id":36},{"text":"maximum","start":227,"end":234,"id":37},{"text":"likelihood","start":235,"end":245,"id":38},{"text":"estimates","start":246,"end":255,"id":39},{"text":".","start":255,"end":256,"id":40}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":43,"end":61,"token_start":8,"token_end":9,"label":"ALGO","answer":"reject"}],"answer":"reject"}
{"text":"This norm leads to regularized problems that are difficult to optimize, and we propose in this paper efficient algorithms for solving them.","_input_hash":677465210,"_task_hash":197435254,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"norm","start":5,"end":9,"id":1},{"text":"leads","start":10,"end":15,"id":2},{"text":"to","start":16,"end":18,"id":3},{"text":"regularized","start":19,"end":30,"id":4},{"text":"problems","start":31,"end":39,"id":5},{"text":"that","start":40,"end":44,"id":6},{"text":"are","start":45,"end":48,"id":7},{"text":"difficult","start":49,"end":58,"id":8},{"text":"to","start":59,"end":61,"id":9},{"text":"optimize","start":62,"end":70,"id":10},{"text":",","start":70,"end":71,"id":11},{"text":"and","start":72,"end":75,"id":12},{"text":"we","start":76,"end":78,"id":13},{"text":"propose","start":79,"end":86,"id":14},{"text":"in","start":87,"end":89,"id":15},{"text":"this","start":90,"end":94,"id":16},{"text":"paper","start":95,"end":100,"id":17},{"text":"efficient","start":101,"end":110,"id":18},{"text":"algorithms","start":111,"end":121,"id":19},{"text":"for","start":122,"end":125,"id":20},{"text":"solving","start":126,"end":133,"id":21},{"text":"them","start":134,"end":138,"id":22},{"text":".","start":138,"end":139,"id":23}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"machine_learning|NOUN","word":"machine learning","sense":"NOUN","meta":{"score":0.8349999785,"sense":"NOUN"},"_input_hash":-2107036536,"_task_hash":97148454,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"machine_learning|NOUN","start":0,"end":21,"id":0}]}
{"text":"LLE first linearly reconstructs each input point from its nearest neighbors and then preserves these neighborhood relations in the low-dimensional embedding.","_input_hash":-258775660,"_task_hash":1429270643,"tokens":[{"text":"LLE","start":0,"end":3,"id":0},{"text":"first","start":4,"end":9,"id":1},{"text":"linearly","start":10,"end":18,"id":2},{"text":"reconstructs","start":19,"end":31,"id":3},{"text":"each","start":32,"end":36,"id":4},{"text":"input","start":37,"end":42,"id":5},{"text":"point","start":43,"end":48,"id":6},{"text":"from","start":49,"end":53,"id":7},{"text":"its","start":54,"end":57,"id":8},{"text":"nearest","start":58,"end":65,"id":9},{"text":"neighbors","start":66,"end":75,"id":10},{"text":"and","start":76,"end":79,"id":11},{"text":"then","start":80,"end":84,"id":12},{"text":"preserves","start":85,"end":94,"id":13},{"text":"these","start":95,"end":100,"id":14},{"text":"neighborhood","start":101,"end":113,"id":15},{"text":"relations","start":114,"end":123,"id":16},{"text":"in","start":124,"end":126,"id":17},{"text":"the","start":127,"end":130,"id":18},{"text":"low","start":131,"end":134,"id":19},{"text":"-","start":134,"end":135,"id":20},{"text":"dimensional","start":135,"end":146,"id":21},{"text":"embedding","start":147,"end":156,"id":22},{"text":".","start":156,"end":157,"id":23}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We call RBMs trained with the regularizer \\emph{sparse group} RBMs.","_input_hash":184226534,"_task_hash":88774851,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"call","start":3,"end":7,"id":1},{"text":"RBMs","start":8,"end":12,"id":2},{"text":"trained","start":13,"end":20,"id":3},{"text":"with","start":21,"end":25,"id":4},{"text":"the","start":26,"end":29,"id":5},{"text":"regularizer","start":30,"end":41,"id":6},{"text":"\\emph{sparse","start":42,"end":54,"id":7},{"text":"group","start":55,"end":60,"id":8},{"text":"}","start":60,"end":61,"id":9},{"text":"RBMs","start":62,"end":66,"id":10},{"text":".","start":66,"end":67,"id":11}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"not","meta":{"score":0},"_input_hash":-441412227,"_task_hash":1543392104,"_session_id":null,"_view_id":"text","answer":"reject","spans":[],"tokens":[{"text":"not","start":0,"end":3,"id":0}]}
{"text":"Experiments with simulated data and microarray data indicate that the methods are a practical alternative to Gaussian graphical models.","_input_hash":-1805975362,"_task_hash":-1378503022,"tokens":[{"text":"Experiments","start":0,"end":11,"id":0},{"text":"with","start":12,"end":16,"id":1},{"text":"simulated","start":17,"end":26,"id":2},{"text":"data","start":27,"end":31,"id":3},{"text":"and","start":32,"end":35,"id":4},{"text":"microarray","start":36,"end":46,"id":5},{"text":"data","start":47,"end":51,"id":6},{"text":"indicate","start":52,"end":60,"id":7},{"text":"that","start":61,"end":65,"id":8},{"text":"the","start":66,"end":69,"id":9},{"text":"methods","start":70,"end":77,"id":10},{"text":"are","start":78,"end":81,"id":11},{"text":"a","start":82,"end":83,"id":12},{"text":"practical","start":84,"end":93,"id":13},{"text":"alternative","start":94,"end":105,"id":14},{"text":"to","start":106,"end":108,"id":15},{"text":"Gaussian","start":109,"end":117,"id":16},{"text":"graphical","start":118,"end":127,"id":17},{"text":"models","start":128,"end":134,"id":18},{"text":".","start":134,"end":135,"id":19}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":109,"end":134,"token_start":16,"token_end":18,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Empirically, the performance of StARS is compared with the state-of-the-art model selection procedures, including $K$-CV, AIC, and BIC, on both synthetic data and a real microarray dataset.","_input_hash":-570864727,"_task_hash":788245219,"tokens":[{"text":"Empirically","start":0,"end":11,"id":0},{"text":",","start":11,"end":12,"id":1},{"text":"the","start":13,"end":16,"id":2},{"text":"performance","start":17,"end":28,"id":3},{"text":"of","start":29,"end":31,"id":4},{"text":"StARS","start":32,"end":37,"id":5},{"text":"is","start":38,"end":40,"id":6},{"text":"compared","start":41,"end":49,"id":7},{"text":"with","start":50,"end":54,"id":8},{"text":"the","start":55,"end":58,"id":9},{"text":"state","start":59,"end":64,"id":10},{"text":"-","start":64,"end":65,"id":11},{"text":"of","start":65,"end":67,"id":12},{"text":"-","start":67,"end":68,"id":13},{"text":"the","start":68,"end":71,"id":14},{"text":"-","start":71,"end":72,"id":15},{"text":"art","start":72,"end":75,"id":16},{"text":"model","start":76,"end":81,"id":17},{"text":"selection","start":82,"end":91,"id":18},{"text":"procedures","start":92,"end":102,"id":19},{"text":",","start":102,"end":103,"id":20},{"text":"including","start":104,"end":113,"id":21},{"text":"$","start":114,"end":115,"id":22},{"text":"K$-CV","start":115,"end":120,"id":23},{"text":",","start":120,"end":121,"id":24},{"text":"AIC","start":122,"end":125,"id":25},{"text":",","start":125,"end":126,"id":26},{"text":"and","start":127,"end":130,"id":27},{"text":"BIC","start":131,"end":134,"id":28},{"text":",","start":134,"end":135,"id":29},{"text":"on","start":136,"end":138,"id":30},{"text":"both","start":139,"end":143,"id":31},{"text":"synthetic","start":144,"end":153,"id":32},{"text":"data","start":154,"end":158,"id":33},{"text":"and","start":159,"end":162,"id":34},{"text":"a","start":163,"end":164,"id":35},{"text":"real","start":165,"end":169,"id":36},{"text":"microarray","start":170,"end":180,"id":37},{"text":"dataset","start":181,"end":188,"id":38},{"text":".","start":188,"end":189,"id":39}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"Variable selection for high-dimensional linear models has received a lot of attention lately, mostly in the context of l1-regularization.","_input_hash":412488295,"_task_hash":1372798332,"tokens":[{"text":"Variable","start":0,"end":8,"id":0},{"text":"selection","start":9,"end":18,"id":1},{"text":"for","start":19,"end":22,"id":2},{"text":"high","start":23,"end":27,"id":3},{"text":"-","start":27,"end":28,"id":4},{"text":"dimensional","start":28,"end":39,"id":5},{"text":"linear","start":40,"end":46,"id":6},{"text":"models","start":47,"end":53,"id":7},{"text":"has","start":54,"end":57,"id":8},{"text":"received","start":58,"end":66,"id":9},{"text":"a","start":67,"end":68,"id":10},{"text":"lot","start":69,"end":72,"id":11},{"text":"of","start":73,"end":75,"id":12},{"text":"attention","start":76,"end":85,"id":13},{"text":"lately","start":86,"end":92,"id":14},{"text":",","start":92,"end":93,"id":15},{"text":"mostly","start":94,"end":100,"id":16},{"text":"in","start":101,"end":103,"id":17},{"text":"the","start":104,"end":107,"id":18},{"text":"context","start":108,"end":115,"id":19},{"text":"of","start":116,"end":118,"id":20},{"text":"l1-regularization","start":119,"end":136,"id":21},{"text":".","start":136,"end":137,"id":22}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"It first aims at confirming, known but sparse, advice for using random forests and at proposing some complementary remarks for both standard problems as well as high dimensional ones for which the number of variables hugely exceeds the sample size.","_input_hash":649819897,"_task_hash":-848801109,"tokens":[{"text":"It","start":0,"end":2,"id":0},{"text":"first","start":3,"end":8,"id":1},{"text":"aims","start":9,"end":13,"id":2},{"text":"at","start":14,"end":16,"id":3},{"text":"confirming","start":17,"end":27,"id":4},{"text":",","start":27,"end":28,"id":5},{"text":"known","start":29,"end":34,"id":6},{"text":"but","start":35,"end":38,"id":7},{"text":"sparse","start":39,"end":45,"id":8},{"text":",","start":45,"end":46,"id":9},{"text":"advice","start":47,"end":53,"id":10},{"text":"for","start":54,"end":57,"id":11},{"text":"using","start":58,"end":63,"id":12},{"text":"random","start":64,"end":70,"id":13},{"text":"forests","start":71,"end":78,"id":14},{"text":"and","start":79,"end":82,"id":15},{"text":"at","start":83,"end":85,"id":16},{"text":"proposing","start":86,"end":95,"id":17},{"text":"some","start":96,"end":100,"id":18},{"text":"complementary","start":101,"end":114,"id":19},{"text":"remarks","start":115,"end":122,"id":20},{"text":"for","start":123,"end":126,"id":21},{"text":"both","start":127,"end":131,"id":22},{"text":"standard","start":132,"end":140,"id":23},{"text":"problems","start":141,"end":149,"id":24},{"text":"as","start":150,"end":152,"id":25},{"text":"well","start":153,"end":157,"id":26},{"text":"as","start":158,"end":160,"id":27},{"text":"high","start":161,"end":165,"id":28},{"text":"dimensional","start":166,"end":177,"id":29},{"text":"ones","start":178,"end":182,"id":30},{"text":"for","start":183,"end":186,"id":31},{"text":"which","start":187,"end":192,"id":32},{"text":"the","start":193,"end":196,"id":33},{"text":"number","start":197,"end":203,"id":34},{"text":"of","start":204,"end":206,"id":35},{"text":"variables","start":207,"end":216,"id":36},{"text":"hugely","start":217,"end":223,"id":37},{"text":"exceeds","start":224,"end":231,"id":38},{"text":"the","start":232,"end":235,"id":39},{"text":"sample","start":236,"end":242,"id":40},{"text":"size","start":243,"end":247,"id":41},{"text":".","start":247,"end":248,"id":42}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":64,"end":78,"token_start":13,"token_end":14,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Unlike other lp minimization problems, this minimization is non-convex for all p>0 and thus requires different methods for its analysis.","_input_hash":460153296,"_task_hash":1612676887,"tokens":[{"text":"Unlike","start":0,"end":6,"id":0},{"text":"other","start":7,"end":12,"id":1},{"text":"lp","start":13,"end":15,"id":2},{"text":"minimization","start":16,"end":28,"id":3},{"text":"problems","start":29,"end":37,"id":4},{"text":",","start":37,"end":38,"id":5},{"text":"this","start":39,"end":43,"id":6},{"text":"minimization","start":44,"end":56,"id":7},{"text":"is","start":57,"end":59,"id":8},{"text":"non","start":60,"end":63,"id":9},{"text":"-","start":63,"end":64,"id":10},{"text":"convex","start":64,"end":70,"id":11},{"text":"for","start":71,"end":74,"id":12},{"text":"all","start":75,"end":78,"id":13},{"text":"p>0","start":79,"end":82,"id":14},{"text":"and","start":83,"end":86,"id":15},{"text":"thus","start":87,"end":91,"id":16},{"text":"requires","start":92,"end":100,"id":17},{"text":"different","start":101,"end":110,"id":18},{"text":"methods","start":111,"end":118,"id":19},{"text":"for","start":119,"end":122,"id":20},{"text":"its","start":123,"end":126,"id":21},{"text":"analysis","start":127,"end":135,"id":22},{"text":".","start":135,"end":136,"id":23}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"Our approach is based on a language model of arbitrary length expressions, for which we develop a new methodology based on nested permutation tests to find significant phrases.","_input_hash":1958017045,"_task_hash":2136400857,"tokens":[{"text":"Our","start":0,"end":3,"id":0},{"text":"approach","start":4,"end":12,"id":1},{"text":"is","start":13,"end":15,"id":2},{"text":"based","start":16,"end":21,"id":3},{"text":"on","start":22,"end":24,"id":4},{"text":"a","start":25,"end":26,"id":5},{"text":"language","start":27,"end":35,"id":6},{"text":"model","start":36,"end":41,"id":7},{"text":"of","start":42,"end":44,"id":8},{"text":"arbitrary","start":45,"end":54,"id":9},{"text":"length","start":55,"end":61,"id":10},{"text":"expressions","start":62,"end":73,"id":11},{"text":",","start":73,"end":74,"id":12},{"text":"for","start":75,"end":78,"id":13},{"text":"which","start":79,"end":84,"id":14},{"text":"we","start":85,"end":87,"id":15},{"text":"develop","start":88,"end":95,"id":16},{"text":"a","start":96,"end":97,"id":17},{"text":"new","start":98,"end":101,"id":18},{"text":"methodology","start":102,"end":113,"id":19},{"text":"based","start":114,"end":119,"id":20},{"text":"on","start":120,"end":122,"id":21},{"text":"nested","start":123,"end":129,"id":22},{"text":"permutation","start":130,"end":141,"id":23},{"text":"tests","start":142,"end":147,"id":24},{"text":"to","start":148,"end":150,"id":25},{"text":"find","start":151,"end":155,"id":26},{"text":"significant","start":156,"end":167,"id":27},{"text":"phrases","start":168,"end":175,"id":28},{"text":".","start":175,"end":176,"id":29}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"biological_systems|NOUN","word":"biological systems","sense":"NOUN","meta":{"score":0.7876999974,"sense":"NOUN"},"_input_hash":-1551101839,"_task_hash":-1016162776,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"biological_systems|NOUN","start":0,"end":23,"id":0}]}
{"text":"We present an extensive experimental evaluation on real world data showing the benefits of the proposed approach.","_input_hash":804669926,"_task_hash":-791853834,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"present","start":3,"end":10,"id":1},{"text":"an","start":11,"end":13,"id":2},{"text":"extensive","start":14,"end":23,"id":3},{"text":"experimental","start":24,"end":36,"id":4},{"text":"evaluation","start":37,"end":47,"id":5},{"text":"on","start":48,"end":50,"id":6},{"text":"real","start":51,"end":55,"id":7},{"text":"world","start":56,"end":61,"id":8},{"text":"data","start":62,"end":66,"id":9},{"text":"showing","start":67,"end":74,"id":10},{"text":"the","start":75,"end":78,"id":11},{"text":"benefits","start":79,"end":87,"id":12},{"text":"of","start":88,"end":90,"id":13},{"text":"the","start":91,"end":94,"id":14},{"text":"proposed","start":95,"end":103,"id":15},{"text":"approach","start":104,"end":112,"id":16},{"text":".","start":112,"end":113,"id":17}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Recently there has been an increasing interest in methods that deal with multiple outputs.","_input_hash":922042019,"_task_hash":231578561,"tokens":[{"text":"Recently","start":0,"end":8,"id":0},{"text":"there","start":9,"end":14,"id":1},{"text":"has","start":15,"end":18,"id":2},{"text":"been","start":19,"end":23,"id":3},{"text":"an","start":24,"end":26,"id":4},{"text":"increasing","start":27,"end":37,"id":5},{"text":"interest","start":38,"end":46,"id":6},{"text":"in","start":47,"end":49,"id":7},{"text":"methods","start":50,"end":57,"id":8},{"text":"that","start":58,"end":62,"id":9},{"text":"deal","start":63,"end":67,"id":10},{"text":"with","start":68,"end":72,"id":11},{"text":"multiple","start":73,"end":81,"id":12},{"text":"outputs","start":82,"end":89,"id":13},{"text":".","start":89,"end":90,"id":14}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We derive a method for estimating the nonparanormal, study the method's theoretical properties, and show that it works well in many examples.","_input_hash":639694872,"_task_hash":1933702875,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"derive","start":3,"end":9,"id":1},{"text":"a","start":10,"end":11,"id":2},{"text":"method","start":12,"end":18,"id":3},{"text":"for","start":19,"end":22,"id":4},{"text":"estimating","start":23,"end":33,"id":5},{"text":"the","start":34,"end":37,"id":6},{"text":"nonparanormal","start":38,"end":51,"id":7},{"text":",","start":51,"end":52,"id":8},{"text":"study","start":53,"end":58,"id":9},{"text":"the","start":59,"end":62,"id":10},{"text":"method","start":63,"end":69,"id":11},{"text":"'s","start":69,"end":71,"id":12},{"text":"theoretical","start":72,"end":83,"id":13},{"text":"properties","start":84,"end":94,"id":14},{"text":",","start":94,"end":95,"id":15},{"text":"and","start":96,"end":99,"id":16},{"text":"show","start":100,"end":104,"id":17},{"text":"that","start":105,"end":109,"id":18},{"text":"it","start":110,"end":112,"id":19},{"text":"works","start":113,"end":118,"id":20},{"text":"well","start":119,"end":123,"id":21},{"text":"in","start":124,"end":126,"id":22},{"text":"many","start":127,"end":131,"id":23},{"text":"examples","start":132,"end":140,"id":24},{"text":".","start":140,"end":141,"id":25}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"The empirical performance is demonstrated on a wide array of datasets.","_input_hash":-180388319,"_task_hash":1952199026,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"empirical","start":4,"end":13,"id":1},{"text":"performance","start":14,"end":25,"id":2},{"text":"is","start":26,"end":28,"id":3},{"text":"demonstrated","start":29,"end":41,"id":4},{"text":"on","start":42,"end":44,"id":5},{"text":"a","start":45,"end":46,"id":6},{"text":"wide","start":47,"end":51,"id":7},{"text":"array","start":52,"end":57,"id":8},{"text":"of","start":58,"end":60,"id":9},{"text":"datasets","start":61,"end":69,"id":10},{"text":".","start":69,"end":70,"id":11}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"random_processes|NOUN","word":"random processes","sense":"NOUN","meta":{"score":0.7592999935,"sense":"NOUN"},"_input_hash":436421327,"_task_hash":-1899807773,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"random_processes|NOUN","start":0,"end":21,"id":0}]}
{"text":"Typically, this generating mechanism will be a finite automata.","_input_hash":-1179024221,"_task_hash":-1078314631,"tokens":[{"text":"Typically","start":0,"end":9,"id":0},{"text":",","start":9,"end":10,"id":1},{"text":"this","start":11,"end":15,"id":2},{"text":"generating","start":16,"end":26,"id":3},{"text":"mechanism","start":27,"end":36,"id":4},{"text":"will","start":37,"end":41,"id":5},{"text":"be","start":42,"end":44,"id":6},{"text":"a","start":45,"end":46,"id":7},{"text":"finite","start":47,"end":53,"id":8},{"text":"automata","start":54,"end":62,"id":9},{"text":".","start":62,"end":63,"id":10}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"We then discuss relative bounds, comparing the generalization error of two classification rules, showing how the margin assumption of Mammen and Tsybakov can be replaced with some empirical measure of the covariance structure of the classification model.","_input_hash":-1789333198,"_task_hash":76277052,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"then","start":3,"end":7,"id":1},{"text":"discuss","start":8,"end":15,"id":2},{"text":"relative","start":16,"end":24,"id":3},{"text":"bounds","start":25,"end":31,"id":4},{"text":",","start":31,"end":32,"id":5},{"text":"comparing","start":33,"end":42,"id":6},{"text":"the","start":43,"end":46,"id":7},{"text":"generalization","start":47,"end":61,"id":8},{"text":"error","start":62,"end":67,"id":9},{"text":"of","start":68,"end":70,"id":10},{"text":"two","start":71,"end":74,"id":11},{"text":"classification","start":75,"end":89,"id":12},{"text":"rules","start":90,"end":95,"id":13},{"text":",","start":95,"end":96,"id":14},{"text":"showing","start":97,"end":104,"id":15},{"text":"how","start":105,"end":108,"id":16},{"text":"the","start":109,"end":112,"id":17},{"text":"margin","start":113,"end":119,"id":18},{"text":"assumption","start":120,"end":130,"id":19},{"text":"of","start":131,"end":133,"id":20},{"text":"Mammen","start":134,"end":140,"id":21},{"text":"and","start":141,"end":144,"id":22},{"text":"Tsybakov","start":145,"end":153,"id":23},{"text":"can","start":154,"end":157,"id":24},{"text":"be","start":158,"end":160,"id":25},{"text":"replaced","start":161,"end":169,"id":26},{"text":"with","start":170,"end":174,"id":27},{"text":"some","start":175,"end":179,"id":28},{"text":"empirical","start":180,"end":189,"id":29},{"text":"measure","start":190,"end":197,"id":30},{"text":"of","start":198,"end":200,"id":31},{"text":"the","start":201,"end":204,"id":32},{"text":"covariance","start":205,"end":215,"id":33},{"text":"structure","start":216,"end":225,"id":34},{"text":"of","start":226,"end":228,"id":35},{"text":"the","start":229,"end":232,"id":36},{"text":"classification","start":233,"end":247,"id":37},{"text":"model","start":248,"end":253,"id":38},{"text":".","start":253,"end":254,"id":39}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"It is to be expected in such cases that learning algorithms will have to deal with manipulated data aimed at hampering decision making.","_input_hash":-363845449,"_task_hash":371133315,"tokens":[{"text":"It","start":0,"end":2,"id":0},{"text":"is","start":3,"end":5,"id":1},{"text":"to","start":6,"end":8,"id":2},{"text":"be","start":9,"end":11,"id":3},{"text":"expected","start":12,"end":20,"id":4},{"text":"in","start":21,"end":23,"id":5},{"text":"such","start":24,"end":28,"id":6},{"text":"cases","start":29,"end":34,"id":7},{"text":"that","start":35,"end":39,"id":8},{"text":"learning","start":40,"end":48,"id":9},{"text":"algorithms","start":49,"end":59,"id":10},{"text":"will","start":60,"end":64,"id":11},{"text":"have","start":65,"end":69,"id":12},{"text":"to","start":70,"end":72,"id":13},{"text":"deal","start":73,"end":77,"id":14},{"text":"with","start":78,"end":82,"id":15},{"text":"manipulated","start":83,"end":94,"id":16},{"text":"data","start":95,"end":99,"id":17},{"text":"aimed","start":100,"end":105,"id":18},{"text":"at","start":106,"end":108,"id":19},{"text":"hampering","start":109,"end":118,"id":20},{"text":"decision","start":119,"end":127,"id":21},{"text":"making","start":128,"end":134,"id":22},{"text":".","start":134,"end":135,"id":23}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"This approach was motivated by the Divisive Information-Theoretic Feature Clustering model in probabilistic space with Kullback-Leibler divergence which may be regarded as a special case within the Clustering Minimisation framework.","_input_hash":109449691,"_task_hash":-1740736266,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"approach","start":5,"end":13,"id":1},{"text":"was","start":14,"end":17,"id":2},{"text":"motivated","start":18,"end":27,"id":3},{"text":"by","start":28,"end":30,"id":4},{"text":"the","start":31,"end":34,"id":5},{"text":"Divisive","start":35,"end":43,"id":6},{"text":"Information","start":44,"end":55,"id":7},{"text":"-","start":55,"end":56,"id":8},{"text":"Theoretic","start":56,"end":65,"id":9},{"text":"Feature","start":66,"end":73,"id":10},{"text":"Clustering","start":74,"end":84,"id":11},{"text":"model","start":85,"end":90,"id":12},{"text":"in","start":91,"end":93,"id":13},{"text":"probabilistic","start":94,"end":107,"id":14},{"text":"space","start":108,"end":113,"id":15},{"text":"with","start":114,"end":118,"id":16},{"text":"Kullback","start":119,"end":127,"id":17},{"text":"-","start":127,"end":128,"id":18},{"text":"Leibler","start":128,"end":135,"id":19},{"text":"divergence","start":136,"end":146,"id":20},{"text":"which","start":147,"end":152,"id":21},{"text":"may","start":153,"end":156,"id":22},{"text":"be","start":157,"end":159,"id":23},{"text":"regarded","start":160,"end":168,"id":24},{"text":"as","start":169,"end":171,"id":25},{"text":"a","start":172,"end":173,"id":26},{"text":"special","start":174,"end":181,"id":27},{"text":"case","start":182,"end":186,"id":28},{"text":"within","start":187,"end":193,"id":29},{"text":"the","start":194,"end":197,"id":30},{"text":"Clustering","start":198,"end":208,"id":31},{"text":"Minimisation","start":209,"end":221,"id":32},{"text":"framework","start":222,"end":231,"id":33},{"text":".","start":231,"end":232,"id":34}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":74,"end":90,"token_start":11,"token_end":12,"label":"ALGO","answer":"reject"},{"start":198,"end":208,"token_start":31,"token_end":31,"label":"ALGO","answer":"reject"}],"answer":"reject"}
{"text":"It is well known that a linear regression can benefit from knowledge that the underlying regression vector is sparse.","_input_hash":1438029261,"_task_hash":-132180548,"tokens":[{"text":"It","start":0,"end":2,"id":0},{"text":"is","start":3,"end":5,"id":1},{"text":"well","start":6,"end":10,"id":2},{"text":"known","start":11,"end":16,"id":3},{"text":"that","start":17,"end":21,"id":4},{"text":"a","start":22,"end":23,"id":5},{"text":"linear","start":24,"end":30,"id":6},{"text":"regression","start":31,"end":41,"id":7},{"text":"can","start":42,"end":45,"id":8},{"text":"benefit","start":46,"end":53,"id":9},{"text":"from","start":54,"end":58,"id":10},{"text":"knowledge","start":59,"end":68,"id":11},{"text":"that","start":69,"end":73,"id":12},{"text":"the","start":74,"end":77,"id":13},{"text":"underlying","start":78,"end":88,"id":14},{"text":"regression","start":89,"end":99,"id":15},{"text":"vector","start":100,"end":106,"id":16},{"text":"is","start":107,"end":109,"id":17},{"text":"sparse","start":110,"end":116,"id":18},{"text":".","start":116,"end":117,"id":19}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We also contribute a new way of deriving the online algorithm that ties together previous online boosting work.","_input_hash":-821263039,"_task_hash":1271882590,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"also","start":3,"end":7,"id":1},{"text":"contribute","start":8,"end":18,"id":2},{"text":"a","start":19,"end":20,"id":3},{"text":"new","start":21,"end":24,"id":4},{"text":"way","start":25,"end":28,"id":5},{"text":"of","start":29,"end":31,"id":6},{"text":"deriving","start":32,"end":40,"id":7},{"text":"the","start":41,"end":44,"id":8},{"text":"online","start":45,"end":51,"id":9},{"text":"algorithm","start":52,"end":61,"id":10},{"text":"that","start":62,"end":66,"id":11},{"text":"ties","start":67,"end":71,"id":12},{"text":"together","start":72,"end":80,"id":13},{"text":"previous","start":81,"end":89,"id":14},{"text":"online","start":90,"end":96,"id":15},{"text":"boosting","start":97,"end":105,"id":16},{"text":"work","start":106,"end":110,"id":17},{"text":".","start":110,"end":111,"id":18}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":90,"end":105,"token_start":15,"token_end":16,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"ta","meta":{"score":0},"_input_hash":719943344,"_task_hash":-289430011,"_session_id":null,"_view_id":"text","answer":"reject","spans":[],"tokens":[{"text":"ta","start":0,"end":2,"id":0}]}
{"text":"This survey is an introduction to positive definite kernels and the set of methods they have inspired in the machine learning literature, namely kernel methods.","_input_hash":-1965380037,"_task_hash":1194215130,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"survey","start":5,"end":11,"id":1},{"text":"is","start":12,"end":14,"id":2},{"text":"an","start":15,"end":17,"id":3},{"text":"introduction","start":18,"end":30,"id":4},{"text":"to","start":31,"end":33,"id":5},{"text":"positive","start":34,"end":42,"id":6},{"text":"definite","start":43,"end":51,"id":7},{"text":"kernels","start":52,"end":59,"id":8},{"text":"and","start":60,"end":63,"id":9},{"text":"the","start":64,"end":67,"id":10},{"text":"set","start":68,"end":71,"id":11},{"text":"of","start":72,"end":74,"id":12},{"text":"methods","start":75,"end":82,"id":13},{"text":"they","start":83,"end":87,"id":14},{"text":"have","start":88,"end":92,"id":15},{"text":"inspired","start":93,"end":101,"id":16},{"text":"in","start":102,"end":104,"id":17},{"text":"the","start":105,"end":108,"id":18},{"text":"machine","start":109,"end":116,"id":19},{"text":"learning","start":117,"end":125,"id":20},{"text":"literature","start":126,"end":136,"id":21},{"text":",","start":136,"end":137,"id":22},{"text":"namely","start":138,"end":144,"id":23},{"text":"kernel","start":145,"end":151,"id":24},{"text":"methods","start":152,"end":159,"id":25},{"text":".","start":159,"end":160,"id":26}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"Therefore, one often resorts to linear models in combination with variable selection, thereby sacrificing some predictive power for presumptive interpretability.","_input_hash":-1652173087,"_task_hash":248702778,"tokens":[{"text":"Therefore","start":0,"end":9,"id":0},{"text":",","start":9,"end":10,"id":1},{"text":"one","start":11,"end":14,"id":2},{"text":"often","start":15,"end":20,"id":3},{"text":"resorts","start":21,"end":28,"id":4},{"text":"to","start":29,"end":31,"id":5},{"text":"linear","start":32,"end":38,"id":6},{"text":"models","start":39,"end":45,"id":7},{"text":"in","start":46,"end":48,"id":8},{"text":"combination","start":49,"end":60,"id":9},{"text":"with","start":61,"end":65,"id":10},{"text":"variable","start":66,"end":74,"id":11},{"text":"selection","start":75,"end":84,"id":12},{"text":",","start":84,"end":85,"id":13},{"text":"thereby","start":86,"end":93,"id":14},{"text":"sacrificing","start":94,"end":105,"id":15},{"text":"some","start":106,"end":110,"id":16},{"text":"predictive","start":111,"end":121,"id":17},{"text":"power","start":122,"end":127,"id":18},{"text":"for","start":128,"end":131,"id":19},{"text":"presumptive","start":132,"end":143,"id":20},{"text":"interpretability","start":144,"end":160,"id":21},{"text":".","start":160,"end":161,"id":22}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"Finally, an adaptive version of our sparsity-smoothness penalized approach yields large additional performance gains.","_input_hash":835233476,"_task_hash":1281631127,"tokens":[{"text":"Finally","start":0,"end":7,"id":0},{"text":",","start":7,"end":8,"id":1},{"text":"an","start":9,"end":11,"id":2},{"text":"adaptive","start":12,"end":20,"id":3},{"text":"version","start":21,"end":28,"id":4},{"text":"of","start":29,"end":31,"id":5},{"text":"our","start":32,"end":35,"id":6},{"text":"sparsity","start":36,"end":44,"id":7},{"text":"-","start":44,"end":45,"id":8},{"text":"smoothness","start":45,"end":55,"id":9},{"text":"penalized","start":56,"end":65,"id":10},{"text":"approach","start":66,"end":74,"id":11},{"text":"yields","start":75,"end":81,"id":12},{"text":"large","start":82,"end":87,"id":13},{"text":"additional","start":88,"end":98,"id":14},{"text":"performance","start":99,"end":110,"id":15},{"text":"gains","start":111,"end":116,"id":16},{"text":".","start":116,"end":117,"id":17}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"you","meta":{"score":0},"_input_hash":731198247,"_task_hash":437080569,"_session_id":null,"_view_id":"text","answer":"reject","spans":[],"tokens":[{"text":"you","start":0,"end":3,"id":0}]}
{"text":"In many applications, some qualitative prior knowledge of the distribution P or of the unknown function f to be estimated is present or the prediction function with a good interpretability is desired, such that a semiparametric model or an additive model is of interest.","_input_hash":931512951,"_task_hash":1507941777,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"many","start":3,"end":7,"id":1},{"text":"applications","start":8,"end":20,"id":2},{"text":",","start":20,"end":21,"id":3},{"text":"some","start":22,"end":26,"id":4},{"text":"qualitative","start":27,"end":38,"id":5},{"text":"prior","start":39,"end":44,"id":6},{"text":"knowledge","start":45,"end":54,"id":7},{"text":"of","start":55,"end":57,"id":8},{"text":"the","start":58,"end":61,"id":9},{"text":"distribution","start":62,"end":74,"id":10},{"text":"P","start":75,"end":76,"id":11},{"text":"or","start":77,"end":79,"id":12},{"text":"of","start":80,"end":82,"id":13},{"text":"the","start":83,"end":86,"id":14},{"text":"unknown","start":87,"end":94,"id":15},{"text":"function","start":95,"end":103,"id":16},{"text":"f","start":104,"end":105,"id":17},{"text":"to","start":106,"end":108,"id":18},{"text":"be","start":109,"end":111,"id":19},{"text":"estimated","start":112,"end":121,"id":20},{"text":"is","start":122,"end":124,"id":21},{"text":"present","start":125,"end":132,"id":22},{"text":"or","start":133,"end":135,"id":23},{"text":"the","start":136,"end":139,"id":24},{"text":"prediction","start":140,"end":150,"id":25},{"text":"function","start":151,"end":159,"id":26},{"text":"with","start":160,"end":164,"id":27},{"text":"a","start":165,"end":166,"id":28},{"text":"good","start":167,"end":171,"id":29},{"text":"interpretability","start":172,"end":188,"id":30},{"text":"is","start":189,"end":191,"id":31},{"text":"desired","start":192,"end":199,"id":32},{"text":",","start":199,"end":200,"id":33},{"text":"such","start":201,"end":205,"id":34},{"text":"that","start":206,"end":210,"id":35},{"text":"a","start":211,"end":212,"id":36},{"text":"semiparametric","start":213,"end":227,"id":37},{"text":"model","start":228,"end":233,"id":38},{"text":"or","start":234,"end":236,"id":39},{"text":"an","start":237,"end":239,"id":40},{"text":"additive","start":240,"end":248,"id":41},{"text":"model","start":249,"end":254,"id":42},{"text":"is","start":255,"end":257,"id":43},{"text":"of","start":258,"end":260,"id":44},{"text":"interest","start":261,"end":269,"id":45},{"text":".","start":269,"end":270,"id":46}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Compared to existing techniques, the DP-GLM provides a single model (and corresponding inference algorithms) that performs well in many regression settings.","_input_hash":131905234,"_task_hash":-813022950,"tokens":[{"text":"Compared","start":0,"end":8,"id":0},{"text":"to","start":9,"end":11,"id":1},{"text":"existing","start":12,"end":20,"id":2},{"text":"techniques","start":21,"end":31,"id":3},{"text":",","start":31,"end":32,"id":4},{"text":"the","start":33,"end":36,"id":5},{"text":"DP","start":37,"end":39,"id":6},{"text":"-","start":39,"end":40,"id":7},{"text":"GLM","start":40,"end":43,"id":8},{"text":"provides","start":44,"end":52,"id":9},{"text":"a","start":53,"end":54,"id":10},{"text":"single","start":55,"end":61,"id":11},{"text":"model","start":62,"end":67,"id":12},{"text":"(","start":68,"end":69,"id":13},{"text":"and","start":69,"end":72,"id":14},{"text":"corresponding","start":73,"end":86,"id":15},{"text":"inference","start":87,"end":96,"id":16},{"text":"algorithms","start":97,"end":107,"id":17},{"text":")","start":107,"end":108,"id":18},{"text":"that","start":109,"end":113,"id":19},{"text":"performs","start":114,"end":122,"id":20},{"text":"well","start":123,"end":127,"id":21},{"text":"in","start":128,"end":130,"id":22},{"text":"many","start":131,"end":135,"id":23},{"text":"regression","start":136,"end":146,"id":24},{"text":"settings","start":147,"end":155,"id":25},{"text":".","start":155,"end":156,"id":26}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"In an abstract sense, regularized kernel methods (simply called SVMs here) can be seen as regularized M-estimators for a parameter in a (typically infinite dimensional) reproducing kernel Hilbert space.","_input_hash":-690500307,"_task_hash":955188851,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"an","start":3,"end":5,"id":1},{"text":"abstract","start":6,"end":14,"id":2},{"text":"sense","start":15,"end":20,"id":3},{"text":",","start":20,"end":21,"id":4},{"text":"regularized","start":22,"end":33,"id":5},{"text":"kernel","start":34,"end":40,"id":6},{"text":"methods","start":41,"end":48,"id":7},{"text":"(","start":49,"end":50,"id":8},{"text":"simply","start":50,"end":56,"id":9},{"text":"called","start":57,"end":63,"id":10},{"text":"SVMs","start":64,"end":68,"id":11},{"text":"here","start":69,"end":73,"id":12},{"text":")","start":73,"end":74,"id":13},{"text":"can","start":75,"end":78,"id":14},{"text":"be","start":79,"end":81,"id":15},{"text":"seen","start":82,"end":86,"id":16},{"text":"as","start":87,"end":89,"id":17},{"text":"regularized","start":90,"end":101,"id":18},{"text":"M","start":102,"end":103,"id":19},{"text":"-","start":103,"end":104,"id":20},{"text":"estimators","start":104,"end":114,"id":21},{"text":"for","start":115,"end":118,"id":22},{"text":"a","start":119,"end":120,"id":23},{"text":"parameter","start":121,"end":130,"id":24},{"text":"in","start":131,"end":133,"id":25},{"text":"a","start":134,"end":135,"id":26},{"text":"(","start":136,"end":137,"id":27},{"text":"typically","start":137,"end":146,"id":28},{"text":"infinite","start":147,"end":155,"id":29},{"text":"dimensional","start":156,"end":167,"id":30},{"text":")","start":167,"end":168,"id":31},{"text":"reproducing","start":169,"end":180,"id":32},{"text":"kernel","start":181,"end":187,"id":33},{"text":"Hilbert","start":188,"end":195,"id":34},{"text":"space","start":196,"end":201,"id":35},{"text":".","start":201,"end":202,"id":36}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":64,"end":68,"token_start":11,"token_end":11,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"The key idea is to identify which variables are exogenous based on non-Gaussianity instead of estimating the entire structure of the model.","_input_hash":-2077789567,"_task_hash":-912162644,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"key","start":4,"end":7,"id":1},{"text":"idea","start":8,"end":12,"id":2},{"text":"is","start":13,"end":15,"id":3},{"text":"to","start":16,"end":18,"id":4},{"text":"identify","start":19,"end":27,"id":5},{"text":"which","start":28,"end":33,"id":6},{"text":"variables","start":34,"end":43,"id":7},{"text":"are","start":44,"end":47,"id":8},{"text":"exogenous","start":48,"end":57,"id":9},{"text":"based","start":58,"end":63,"id":10},{"text":"on","start":64,"end":66,"id":11},{"text":"non","start":67,"end":70,"id":12},{"text":"-","start":70,"end":71,"id":13},{"text":"Gaussianity","start":71,"end":82,"id":14},{"text":"instead","start":83,"end":90,"id":15},{"text":"of","start":91,"end":93,"id":16},{"text":"estimating","start":94,"end":104,"id":17},{"text":"the","start":105,"end":108,"id":18},{"text":"entire","start":109,"end":115,"id":19},{"text":"structure","start":116,"end":125,"id":20},{"text":"of","start":126,"end":128,"id":21},{"text":"the","start":129,"end":132,"id":22},{"text":"model","start":133,"end":138,"id":23},{"text":".","start":138,"end":139,"id":24}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"could","meta":{"score":0},"_input_hash":-1084682554,"_task_hash":1315461653,"_session_id":null,"_view_id":"text","answer":"reject","spans":[],"tokens":[{"text":"could","start":0,"end":5,"id":0}]}
{"text":"While classical sparse priors only deal with \\textit{cardinality}, the regularization we use encodes higher-order information about the data.","_input_hash":-1360787246,"_task_hash":732544823,"tokens":[{"text":"While","start":0,"end":5,"id":0},{"text":"classical","start":6,"end":15,"id":1},{"text":"sparse","start":16,"end":22,"id":2},{"text":"priors","start":23,"end":29,"id":3},{"text":"only","start":30,"end":34,"id":4},{"text":"deal","start":35,"end":39,"id":5},{"text":"with","start":40,"end":44,"id":6},{"text":"\\textit{cardinality","start":45,"end":64,"id":7},{"text":"}","start":64,"end":65,"id":8},{"text":",","start":65,"end":66,"id":9},{"text":"the","start":67,"end":70,"id":10},{"text":"regularization","start":71,"end":85,"id":11},{"text":"we","start":86,"end":88,"id":12},{"text":"use","start":89,"end":92,"id":13},{"text":"encodes","start":93,"end":100,"id":14},{"text":"higher","start":101,"end":107,"id":15},{"text":"-","start":107,"end":108,"id":16},{"text":"order","start":108,"end":113,"id":17},{"text":"information","start":114,"end":125,"id":18},{"text":"about","start":126,"end":131,"id":19},{"text":"the","start":132,"end":135,"id":20},{"text":"data","start":136,"end":140,"id":21},{"text":".","start":140,"end":141,"id":22}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Experiments demonstrate the superior scalability of the proposed approach, when compared to the fastest existing RankSVM implementations.","_input_hash":357139733,"_task_hash":-377767318,"tokens":[{"text":"Experiments","start":0,"end":11,"id":0},{"text":"demonstrate","start":12,"end":23,"id":1},{"text":"the","start":24,"end":27,"id":2},{"text":"superior","start":28,"end":36,"id":3},{"text":"scalability","start":37,"end":48,"id":4},{"text":"of","start":49,"end":51,"id":5},{"text":"the","start":52,"end":55,"id":6},{"text":"proposed","start":56,"end":64,"id":7},{"text":"approach","start":65,"end":73,"id":8},{"text":",","start":73,"end":74,"id":9},{"text":"when","start":75,"end":79,"id":10},{"text":"compared","start":80,"end":88,"id":11},{"text":"to","start":89,"end":91,"id":12},{"text":"the","start":92,"end":95,"id":13},{"text":"fastest","start":96,"end":103,"id":14},{"text":"existing","start":104,"end":112,"id":15},{"text":"RankSVM","start":113,"end":120,"id":16},{"text":"implementations","start":121,"end":136,"id":17},{"text":".","start":136,"end":137,"id":18}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Our method is efficient and scales gracefully to millions of variables, which we illustrate in two types of applications:","_input_hash":-416997197,"_task_hash":-2096850310,"tokens":[{"text":"Our","start":0,"end":3,"id":0},{"text":"method","start":4,"end":10,"id":1},{"text":"is","start":11,"end":13,"id":2},{"text":"efficient","start":14,"end":23,"id":3},{"text":"and","start":24,"end":27,"id":4},{"text":"scales","start":28,"end":34,"id":5},{"text":"gracefully","start":35,"end":45,"id":6},{"text":"to","start":46,"end":48,"id":7},{"text":"millions","start":49,"end":57,"id":8},{"text":"of","start":58,"end":60,"id":9},{"text":"variables","start":61,"end":70,"id":10},{"text":",","start":70,"end":71,"id":11},{"text":"which","start":72,"end":77,"id":12},{"text":"we","start":78,"end":80,"id":13},{"text":"illustrate","start":81,"end":91,"id":14},{"text":"in","start":92,"end":94,"id":15},{"text":"two","start":95,"end":98,"id":16},{"text":"types","start":99,"end":104,"id":17},{"text":"of","start":105,"end":107,"id":18},{"text":"applications","start":108,"end":120,"id":19},{"text":":","start":120,"end":121,"id":20}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"Most accurate predictions are typically obtained by learning machines with complex feature spaces (as e.g. induced by kernels).","_input_hash":886146820,"_task_hash":924847000,"tokens":[{"text":"Most","start":0,"end":4,"id":0},{"text":"accurate","start":5,"end":13,"id":1},{"text":"predictions","start":14,"end":25,"id":2},{"text":"are","start":26,"end":29,"id":3},{"text":"typically","start":30,"end":39,"id":4},{"text":"obtained","start":40,"end":48,"id":5},{"text":"by","start":49,"end":51,"id":6},{"text":"learning","start":52,"end":60,"id":7},{"text":"machines","start":61,"end":69,"id":8},{"text":"with","start":70,"end":74,"id":9},{"text":"complex","start":75,"end":82,"id":10},{"text":"feature","start":83,"end":90,"id":11},{"text":"spaces","start":91,"end":97,"id":12},{"text":"(","start":98,"end":99,"id":13},{"text":"as","start":99,"end":101,"id":14},{"text":"e.g.","start":102,"end":106,"id":15},{"text":"induced","start":107,"end":114,"id":16},{"text":"by","start":115,"end":117,"id":17},{"text":"kernels","start":118,"end":125,"id":18},{"text":")","start":125,"end":126,"id":19},{"text":".","start":126,"end":127,"id":20}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"We introduce an efficient method for training the linear ranking support vector machine.","_input_hash":-385118110,"_task_hash":865479839,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"introduce","start":3,"end":12,"id":1},{"text":"an","start":13,"end":15,"id":2},{"text":"efficient","start":16,"end":25,"id":3},{"text":"method","start":26,"end":32,"id":4},{"text":"for","start":33,"end":36,"id":5},{"text":"training","start":37,"end":45,"id":6},{"text":"the","start":46,"end":49,"id":7},{"text":"linear","start":50,"end":56,"id":8},{"text":"ranking","start":57,"end":64,"id":9},{"text":"support","start":65,"end":72,"id":10},{"text":"vector","start":73,"end":79,"id":11},{"text":"machine","start":80,"end":87,"id":12},{"text":".","start":87,"end":88,"id":13}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":65,"end":87,"token_start":10,"token_end":12,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We show how this stochastic process can be used as a prior distribution in a Bayesian nonparametric model of document collections.","_input_hash":-1051689315,"_task_hash":1569391408,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"show","start":3,"end":7,"id":1},{"text":"how","start":8,"end":11,"id":2},{"text":"this","start":12,"end":16,"id":3},{"text":"stochastic","start":17,"end":27,"id":4},{"text":"process","start":28,"end":35,"id":5},{"text":"can","start":36,"end":39,"id":6},{"text":"be","start":40,"end":42,"id":7},{"text":"used","start":43,"end":47,"id":8},{"text":"as","start":48,"end":50,"id":9},{"text":"a","start":51,"end":52,"id":10},{"text":"prior","start":53,"end":58,"id":11},{"text":"distribution","start":59,"end":71,"id":12},{"text":"in","start":72,"end":74,"id":13},{"text":"a","start":75,"end":76,"id":14},{"text":"Bayesian","start":77,"end":85,"id":15},{"text":"nonparametric","start":86,"end":99,"id":16},{"text":"model","start":100,"end":105,"id":17},{"text":"of","start":106,"end":108,"id":18},{"text":"document","start":109,"end":117,"id":19},{"text":"collections","start":118,"end":129,"id":20},{"text":".","start":129,"end":130,"id":21}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We also provide a short cookbook of different kernels which are particularly useful for certain data-types such as images, graphs or speech segments.","_input_hash":-2019130877,"_task_hash":1483528114,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"also","start":3,"end":7,"id":1},{"text":"provide","start":8,"end":15,"id":2},{"text":"a","start":16,"end":17,"id":3},{"text":"short","start":18,"end":23,"id":4},{"text":"cookbook","start":24,"end":32,"id":5},{"text":"of","start":33,"end":35,"id":6},{"text":"different","start":36,"end":45,"id":7},{"text":"kernels","start":46,"end":53,"id":8},{"text":"which","start":54,"end":59,"id":9},{"text":"are","start":60,"end":63,"id":10},{"text":"particularly","start":64,"end":76,"id":11},{"text":"useful","start":77,"end":83,"id":12},{"text":"for","start":84,"end":87,"id":13},{"text":"certain","start":88,"end":95,"id":14},{"text":"data","start":96,"end":100,"id":15},{"text":"-","start":100,"end":101,"id":16},{"text":"types","start":101,"end":106,"id":17},{"text":"such","start":107,"end":111,"id":18},{"text":"as","start":112,"end":114,"id":19},{"text":"images","start":115,"end":121,"id":20},{"text":",","start":121,"end":122,"id":21},{"text":"graphs","start":123,"end":129,"id":22},{"text":"or","start":130,"end":132,"id":23},{"text":"speech","start":133,"end":139,"id":24},{"text":"segments","start":140,"end":148,"id":25},{"text":".","start":148,"end":149,"id":26}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"decision_trees|NOUN","word":"decision trees","sense":"NOUN","meta":{"score":0.7677999735,"sense":"NOUN"},"_input_hash":1733690317,"_task_hash":-2039106751,"_session_id":null,"_view_id":"html","answer":"accept","spans":[],"tokens":[{"text":"decision_trees|NOUN","start":0,"end":19,"id":0}]}
{"text":"has","meta":{"score":0},"_input_hash":1073603555,"_task_hash":-41363608,"_session_id":null,"_view_id":"text","answer":"reject","spans":[],"tokens":[{"text":"has","start":0,"end":3,"id":0}]}
{"text":"In this paper we present a means of determining a low-dimensional projection which maintains the high-dimensional relationships (i.e. information) between differing oncological data sets.","_input_hash":1027560018,"_task_hash":989776101,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"this","start":3,"end":7,"id":1},{"text":"paper","start":8,"end":13,"id":2},{"text":"we","start":14,"end":16,"id":3},{"text":"present","start":17,"end":24,"id":4},{"text":"a","start":25,"end":26,"id":5},{"text":"means","start":27,"end":32,"id":6},{"text":"of","start":33,"end":35,"id":7},{"text":"determining","start":36,"end":47,"id":8},{"text":"a","start":48,"end":49,"id":9},{"text":"low","start":50,"end":53,"id":10},{"text":"-","start":53,"end":54,"id":11},{"text":"dimensional","start":54,"end":65,"id":12},{"text":"projection","start":66,"end":76,"id":13},{"text":"which","start":77,"end":82,"id":14},{"text":"maintains","start":83,"end":92,"id":15},{"text":"the","start":93,"end":96,"id":16},{"text":"high","start":97,"end":101,"id":17},{"text":"-","start":101,"end":102,"id":18},{"text":"dimensional","start":102,"end":113,"id":19},{"text":"relationships","start":114,"end":127,"id":20},{"text":"(","start":128,"end":129,"id":21},{"text":"i.e.","start":129,"end":133,"id":22},{"text":"information","start":134,"end":145,"id":23},{"text":")","start":145,"end":146,"id":24},{"text":"between","start":147,"end":154,"id":25},{"text":"differing","start":155,"end":164,"id":26},{"text":"oncological","start":165,"end":176,"id":27},{"text":"data","start":177,"end":181,"id":28},{"text":"sets","start":182,"end":186,"id":29},{"text":".","start":186,"end":187,"id":30}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"We present and analyse three online algorithms for learning in discrete Hidden Markov Models (HMMs) and compare them with the Baldi-Chauvin Algorithm.","_input_hash":451561862,"_task_hash":1609269552,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"present","start":3,"end":10,"id":1},{"text":"and","start":11,"end":14,"id":2},{"text":"analyse","start":15,"end":22,"id":3},{"text":"three","start":23,"end":28,"id":4},{"text":"online","start":29,"end":35,"id":5},{"text":"algorithms","start":36,"end":46,"id":6},{"text":"for","start":47,"end":50,"id":7},{"text":"learning","start":51,"end":59,"id":8},{"text":"in","start":60,"end":62,"id":9},{"text":"discrete","start":63,"end":71,"id":10},{"text":"Hidden","start":72,"end":78,"id":11},{"text":"Markov","start":79,"end":85,"id":12},{"text":"Models","start":86,"end":92,"id":13},{"text":"(","start":93,"end":94,"id":14},{"text":"HMMs","start":94,"end":98,"id":15},{"text":")","start":98,"end":99,"id":16},{"text":"and","start":100,"end":103,"id":17},{"text":"compare","start":104,"end":111,"id":18},{"text":"them","start":112,"end":116,"id":19},{"text":"with","start":117,"end":121,"id":20},{"text":"the","start":122,"end":125,"id":21},{"text":"Baldi","start":126,"end":131,"id":22},{"text":"-","start":131,"end":132,"id":23},{"text":"Chauvin","start":132,"end":139,"id":24},{"text":"Algorithm","start":140,"end":149,"id":25},{"text":".","start":149,"end":150,"id":26}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":72,"end":92,"token_start":11,"token_end":13,"label":"ALGO","answer":"accept"},{"start":126,"end":149,"token_start":22,"token_end":25,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"By only viewing a series of 2-dimensional projections, the high-dimensional nature of the data is rarely exploited.","_input_hash":608820998,"_task_hash":452229493,"tokens":[{"text":"By","start":0,"end":2,"id":0},{"text":"only","start":3,"end":7,"id":1},{"text":"viewing","start":8,"end":15,"id":2},{"text":"a","start":16,"end":17,"id":3},{"text":"series","start":18,"end":24,"id":4},{"text":"of","start":25,"end":27,"id":5},{"text":"2-dimensional","start":28,"end":41,"id":6},{"text":"projections","start":42,"end":53,"id":7},{"text":",","start":53,"end":54,"id":8},{"text":"the","start":55,"end":58,"id":9},{"text":"high","start":59,"end":63,"id":10},{"text":"-","start":63,"end":64,"id":11},{"text":"dimensional","start":64,"end":75,"id":12},{"text":"nature","start":76,"end":82,"id":13},{"text":"of","start":83,"end":85,"id":14},{"text":"the","start":86,"end":89,"id":15},{"text":"data","start":90,"end":94,"id":16},{"text":"is","start":95,"end":97,"id":17},{"text":"rarely","start":98,"end":104,"id":18},{"text":"exploited","start":105,"end":114,"id":19},{"text":".","start":114,"end":115,"id":20}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"The method is compatible with any dissimilarity measure, making it amenable to situations in which the data are not embedded in an underlying feature space or in which using a non-Euclidean metric is desirable.","_input_hash":-792706932,"_task_hash":1584051152,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"method","start":4,"end":10,"id":1},{"text":"is","start":11,"end":13,"id":2},{"text":"compatible","start":14,"end":24,"id":3},{"text":"with","start":25,"end":29,"id":4},{"text":"any","start":30,"end":33,"id":5},{"text":"dissimilarity","start":34,"end":47,"id":6},{"text":"measure","start":48,"end":55,"id":7},{"text":",","start":55,"end":56,"id":8},{"text":"making","start":57,"end":63,"id":9},{"text":"it","start":64,"end":66,"id":10},{"text":"amenable","start":67,"end":75,"id":11},{"text":"to","start":76,"end":78,"id":12},{"text":"situations","start":79,"end":89,"id":13},{"text":"in","start":90,"end":92,"id":14},{"text":"which","start":93,"end":98,"id":15},{"text":"the","start":99,"end":102,"id":16},{"text":"data","start":103,"end":107,"id":17},{"text":"are","start":108,"end":111,"id":18},{"text":"not","start":112,"end":115,"id":19},{"text":"embedded","start":116,"end":124,"id":20},{"text":"in","start":125,"end":127,"id":21},{"text":"an","start":128,"end":130,"id":22},{"text":"underlying","start":131,"end":141,"id":23},{"text":"feature","start":142,"end":149,"id":24},{"text":"space","start":150,"end":155,"id":25},{"text":"or","start":156,"end":158,"id":26},{"text":"in","start":159,"end":161,"id":27},{"text":"which","start":162,"end":167,"id":28},{"text":"using","start":168,"end":173,"id":29},{"text":"a","start":174,"end":175,"id":30},{"text":"non","start":176,"end":179,"id":31},{"text":"-","start":179,"end":180,"id":32},{"text":"Euclidean","start":180,"end":189,"id":33},{"text":"metric","start":190,"end":196,"id":34},{"text":"is","start":197,"end":199,"id":35},{"text":"desirable","start":200,"end":209,"id":36},{"text":".","start":209,"end":210,"id":37}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We exploit the conditional independencies present naturally in the model.","_input_hash":1482967409,"_task_hash":-1911340019,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"exploit","start":3,"end":10,"id":1},{"text":"the","start":11,"end":14,"id":2},{"text":"conditional","start":15,"end":26,"id":3},{"text":"independencies","start":27,"end":41,"id":4},{"text":"present","start":42,"end":49,"id":5},{"text":"naturally","start":50,"end":59,"id":6},{"text":"in","start":60,"end":62,"id":7},{"text":"the","start":63,"end":66,"id":8},{"text":"model","start":67,"end":72,"id":9},{"text":".","start":72,"end":73,"id":10}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We illustrate the benefits of sLDA versus modern regularized regression, as well as versus an unsupervised LDA analysis followed by a separate regression.","_input_hash":1070269309,"_task_hash":2017059354,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"illustrate","start":3,"end":13,"id":1},{"text":"the","start":14,"end":17,"id":2},{"text":"benefits","start":18,"end":26,"id":3},{"text":"of","start":27,"end":29,"id":4},{"text":"sLDA","start":30,"end":34,"id":5},{"text":"versus","start":35,"end":41,"id":6},{"text":"modern","start":42,"end":48,"id":7},{"text":"regularized","start":49,"end":60,"id":8},{"text":"regression","start":61,"end":71,"id":9},{"text":",","start":71,"end":72,"id":10},{"text":"as","start":73,"end":75,"id":11},{"text":"well","start":76,"end":80,"id":12},{"text":"as","start":81,"end":83,"id":13},{"text":"versus","start":84,"end":90,"id":14},{"text":"an","start":91,"end":93,"id":15},{"text":"unsupervised","start":94,"end":106,"id":16},{"text":"LDA","start":107,"end":110,"id":17},{"text":"analysis","start":111,"end":119,"id":18},{"text":"followed","start":120,"end":128,"id":19},{"text":"by","start":129,"end":131,"id":20},{"text":"a","start":132,"end":133,"id":21},{"text":"separate","start":134,"end":142,"id":22},{"text":"regression","start":143,"end":153,"id":23},{"text":".","start":153,"end":154,"id":24}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"In this paper we present two strategies to solve the primal LapSVM problem, in order to overcome some issues of the original dual formulation.","_input_hash":-233113015,"_task_hash":-1118113622,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"this","start":3,"end":7,"id":1},{"text":"paper","start":8,"end":13,"id":2},{"text":"we","start":14,"end":16,"id":3},{"text":"present","start":17,"end":24,"id":4},{"text":"two","start":25,"end":28,"id":5},{"text":"strategies","start":29,"end":39,"id":6},{"text":"to","start":40,"end":42,"id":7},{"text":"solve","start":43,"end":48,"id":8},{"text":"the","start":49,"end":52,"id":9},{"text":"primal","start":53,"end":59,"id":10},{"text":"LapSVM","start":60,"end":66,"id":11},{"text":"problem","start":67,"end":74,"id":12},{"text":",","start":74,"end":75,"id":13},{"text":"in","start":76,"end":78,"id":14},{"text":"order","start":79,"end":84,"id":15},{"text":"to","start":85,"end":87,"id":16},{"text":"overcome","start":88,"end":96,"id":17},{"text":"some","start":97,"end":101,"id":18},{"text":"issues","start":102,"end":108,"id":19},{"text":"of","start":109,"end":111,"id":20},{"text":"the","start":112,"end":115,"id":21},{"text":"original","start":116,"end":124,"id":22},{"text":"dual","start":125,"end":129,"id":23},{"text":"formulation","start":130,"end":141,"id":24},{"text":".","start":141,"end":142,"id":25}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Examples are quantile regression based on the pinball loss function, regression based on the epsilon-insensitive loss function, and classification based on the hinge loss function.","_input_hash":1506673819,"_task_hash":-1762896888,"tokens":[{"text":"Examples","start":0,"end":8,"id":0},{"text":"are","start":9,"end":12,"id":1},{"text":"quantile","start":13,"end":21,"id":2},{"text":"regression","start":22,"end":32,"id":3},{"text":"based","start":33,"end":38,"id":4},{"text":"on","start":39,"end":41,"id":5},{"text":"the","start":42,"end":45,"id":6},{"text":"pinball","start":46,"end":53,"id":7},{"text":"loss","start":54,"end":58,"id":8},{"text":"function","start":59,"end":67,"id":9},{"text":",","start":67,"end":68,"id":10},{"text":"regression","start":69,"end":79,"id":11},{"text":"based","start":80,"end":85,"id":12},{"text":"on","start":86,"end":88,"id":13},{"text":"the","start":89,"end":92,"id":14},{"text":"epsilon","start":93,"end":100,"id":15},{"text":"-","start":100,"end":101,"id":16},{"text":"insensitive","start":101,"end":112,"id":17},{"text":"loss","start":113,"end":117,"id":18},{"text":"function","start":118,"end":126,"id":19},{"text":",","start":126,"end":127,"id":20},{"text":"and","start":128,"end":131,"id":21},{"text":"classification","start":132,"end":146,"id":22},{"text":"based","start":147,"end":152,"id":23},{"text":"on","start":153,"end":155,"id":24},{"text":"the","start":156,"end":159,"id":25},{"text":"hinge","start":160,"end":165,"id":26},{"text":"loss","start":166,"end":170,"id":27},{"text":"function","start":171,"end":179,"id":28},{"text":".","start":179,"end":180,"id":29}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"d","meta":{"score":0},"_input_hash":-1656380824,"_task_hash":840228544,"_session_id":null,"_view_id":"text","answer":"reject","spans":[],"tokens":[{"text":"d","start":0,"end":1,"id":0}]}
{"text":"The runtime for Kernel Partial Least Squares (KPLS) to compute the fit is quadratic in the number of examples.","_input_hash":-1440221630,"_task_hash":925807616,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"runtime","start":4,"end":11,"id":1},{"text":"for","start":12,"end":15,"id":2},{"text":"Kernel","start":16,"end":22,"id":3},{"text":"Partial","start":23,"end":30,"id":4},{"text":"Least","start":31,"end":36,"id":5},{"text":"Squares","start":37,"end":44,"id":6},{"text":"(","start":45,"end":46,"id":7},{"text":"KPLS","start":46,"end":50,"id":8},{"text":")","start":50,"end":51,"id":9},{"text":"to","start":52,"end":54,"id":10},{"text":"compute","start":55,"end":62,"id":11},{"text":"the","start":63,"end":66,"id":12},{"text":"fit","start":67,"end":70,"id":13},{"text":"is","start":71,"end":73,"id":14},{"text":"quadratic","start":74,"end":83,"id":15},{"text":"in","start":84,"end":86,"id":16},{"text":"the","start":87,"end":90,"id":17},{"text":"number","start":91,"end":97,"id":18},{"text":"of","start":98,"end":100,"id":19},{"text":"examples","start":101,"end":109,"id":20},{"text":".","start":109,"end":110,"id":21}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"2006], who focus on margin-based losses and cost-insensitive classification, while the second adopts the framework of Steinwart [2007] based on calibration functions.","_input_hash":104294326,"_task_hash":-901203550,"tokens":[{"text":"2006","start":0,"end":4,"id":0},{"text":"]","start":4,"end":5,"id":1},{"text":",","start":5,"end":6,"id":2},{"text":"who","start":7,"end":10,"id":3},{"text":"focus","start":11,"end":16,"id":4},{"text":"on","start":17,"end":19,"id":5},{"text":"margin","start":20,"end":26,"id":6},{"text":"-","start":26,"end":27,"id":7},{"text":"based","start":27,"end":32,"id":8},{"text":"losses","start":33,"end":39,"id":9},{"text":"and","start":40,"end":43,"id":10},{"text":"cost","start":44,"end":48,"id":11},{"text":"-","start":48,"end":49,"id":12},{"text":"insensitive","start":49,"end":60,"id":13},{"text":"classification","start":61,"end":75,"id":14},{"text":",","start":75,"end":76,"id":15},{"text":"while","start":77,"end":82,"id":16},{"text":"the","start":83,"end":86,"id":17},{"text":"second","start":87,"end":93,"id":18},{"text":"adopts","start":94,"end":100,"id":19},{"text":"the","start":101,"end":104,"id":20},{"text":"framework","start":105,"end":114,"id":21},{"text":"of","start":115,"end":117,"id":22},{"text":"Steinwart","start":118,"end":127,"id":23},{"text":"[","start":128,"end":129,"id":24},{"text":"2007","start":129,"end":133,"id":25},{"text":"]","start":133,"end":134,"id":26},{"text":"based","start":135,"end":140,"id":27},{"text":"on","start":141,"end":143,"id":28},{"text":"calibration","start":144,"end":155,"id":29},{"text":"functions","start":156,"end":165,"id":30},{"text":".","start":165,"end":166,"id":31}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"This provides an aid in diagnosing similar forms of cancer, as well as a means for variable selection in exploratory flow cytometric research.","_input_hash":1247429372,"_task_hash":-623487225,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"provides","start":5,"end":13,"id":1},{"text":"an","start":14,"end":16,"id":2},{"text":"aid","start":17,"end":20,"id":3},{"text":"in","start":21,"end":23,"id":4},{"text":"diagnosing","start":24,"end":34,"id":5},{"text":"similar","start":35,"end":42,"id":6},{"text":"forms","start":43,"end":48,"id":7},{"text":"of","start":49,"end":51,"id":8},{"text":"cancer","start":52,"end":58,"id":9},{"text":",","start":58,"end":59,"id":10},{"text":"as","start":60,"end":62,"id":11},{"text":"well","start":63,"end":67,"id":12},{"text":"as","start":68,"end":70,"id":13},{"text":"a","start":71,"end":72,"id":14},{"text":"means","start":73,"end":78,"id":15},{"text":"for","start":79,"end":82,"id":16},{"text":"variable","start":83,"end":91,"id":17},{"text":"selection","start":92,"end":101,"id":18},{"text":"in","start":102,"end":104,"id":19},{"text":"exploratory","start":105,"end":116,"id":20},{"text":"flow","start":117,"end":121,"id":21},{"text":"cytometric","start":122,"end":132,"id":22},{"text":"research","start":133,"end":141,"id":23},{"text":".","start":141,"end":142,"id":24}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"Numerical simulations highlight the benefit of structured sparsity and the advantage offered by our approach over the Lasso method and other related methods.","_input_hash":-1360195889,"_task_hash":1258898590,"tokens":[{"text":"Numerical","start":0,"end":9,"id":0},{"text":"simulations","start":10,"end":21,"id":1},{"text":"highlight","start":22,"end":31,"id":2},{"text":"the","start":32,"end":35,"id":3},{"text":"benefit","start":36,"end":43,"id":4},{"text":"of","start":44,"end":46,"id":5},{"text":"structured","start":47,"end":57,"id":6},{"text":"sparsity","start":58,"end":66,"id":7},{"text":"and","start":67,"end":70,"id":8},{"text":"the","start":71,"end":74,"id":9},{"text":"advantage","start":75,"end":84,"id":10},{"text":"offered","start":85,"end":92,"id":11},{"text":"by","start":93,"end":95,"id":12},{"text":"our","start":96,"end":99,"id":13},{"text":"approach","start":100,"end":108,"id":14},{"text":"over","start":109,"end":113,"id":15},{"text":"the","start":114,"end":117,"id":16},{"text":"Lasso","start":118,"end":123,"id":17},{"text":"method","start":124,"end":130,"id":18},{"text":"and","start":131,"end":134,"id":19},{"text":"other","start":135,"end":140,"id":20},{"text":"related","start":141,"end":148,"id":21},{"text":"methods","start":149,"end":156,"id":22},{"text":".","start":156,"end":157,"id":23}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"how","meta":{"score":0},"_input_hash":1360335703,"_task_hash":-445517711,"_session_id":null,"_view_id":"text","answer":"reject","spans":[],"tokens":[{"text":"how","start":0,"end":3,"id":0}]}
{"text":"We first cover the case where $\\Xcal$ is Euclidian, and focus on kernels which take into account the variance matrix of the mixture of two measures to compute their similarity.","_input_hash":1864943426,"_task_hash":1466318706,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"first","start":3,"end":8,"id":1},{"text":"cover","start":9,"end":14,"id":2},{"text":"the","start":15,"end":18,"id":3},{"text":"case","start":19,"end":23,"id":4},{"text":"where","start":24,"end":29,"id":5},{"text":"$","start":30,"end":31,"id":6},{"text":"\\Xcal$","start":31,"end":37,"id":7},{"text":"is","start":38,"end":40,"id":8},{"text":"Euclidian","start":41,"end":50,"id":9},{"text":",","start":50,"end":51,"id":10},{"text":"and","start":52,"end":55,"id":11},{"text":"focus","start":56,"end":61,"id":12},{"text":"on","start":62,"end":64,"id":13},{"text":"kernels","start":65,"end":72,"id":14},{"text":"which","start":73,"end":78,"id":15},{"text":"take","start":79,"end":83,"id":16},{"text":"into","start":84,"end":88,"id":17},{"text":"account","start":89,"end":96,"id":18},{"text":"the","start":97,"end":100,"id":19},{"text":"variance","start":101,"end":109,"id":20},{"text":"matrix","start":110,"end":116,"id":21},{"text":"of","start":117,"end":119,"id":22},{"text":"the","start":120,"end":123,"id":23},{"text":"mixture","start":124,"end":131,"id":24},{"text":"of","start":132,"end":134,"id":25},{"text":"two","start":135,"end":138,"id":26},{"text":"measures","start":139,"end":147,"id":27},{"text":"to","start":148,"end":150,"id":28},{"text":"compute","start":151,"end":158,"id":29},{"text":"their","start":159,"end":164,"id":30},{"text":"similarity","start":165,"end":175,"id":31},{"text":".","start":175,"end":176,"id":32}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"  In particular, we focus on proving the consistency of the various cross-validation procedures.","_input_hash":-1032825242,"_task_hash":1620179809,"tokens":[{"text":"  ","start":0,"end":2,"id":0},{"text":"In","start":2,"end":4,"id":1},{"text":"particular","start":5,"end":15,"id":2},{"text":",","start":15,"end":16,"id":3},{"text":"we","start":17,"end":19,"id":4},{"text":"focus","start":20,"end":25,"id":5},{"text":"on","start":26,"end":28,"id":6},{"text":"proving","start":29,"end":36,"id":7},{"text":"the","start":37,"end":40,"id":8},{"text":"consistency","start":41,"end":52,"id":9},{"text":"of","start":53,"end":55,"id":10},{"text":"the","start":56,"end":59,"id":11},{"text":"various","start":60,"end":67,"id":12},{"text":"cross","start":68,"end":73,"id":13},{"text":"-","start":73,"end":74,"id":14},{"text":"validation","start":74,"end":84,"id":15},{"text":"procedures","start":85,"end":95,"id":16},{"text":".","start":95,"end":96,"id":17}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"We present numerical examples demonstrating both the perturbation and linear projection problems, and the improved outputs using the low-dimensional neighborhood representation.","_input_hash":1302518157,"_task_hash":-1290208708,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"present","start":3,"end":10,"id":1},{"text":"numerical","start":11,"end":20,"id":2},{"text":"examples","start":21,"end":29,"id":3},{"text":"demonstrating","start":30,"end":43,"id":4},{"text":"both","start":44,"end":48,"id":5},{"text":"the","start":49,"end":52,"id":6},{"text":"perturbation","start":53,"end":65,"id":7},{"text":"and","start":66,"end":69,"id":8},{"text":"linear","start":70,"end":76,"id":9},{"text":"projection","start":77,"end":87,"id":10},{"text":"problems","start":88,"end":96,"id":11},{"text":",","start":96,"end":97,"id":12},{"text":"and","start":98,"end":101,"id":13},{"text":"the","start":102,"end":105,"id":14},{"text":"improved","start":106,"end":114,"id":15},{"text":"outputs","start":115,"end":122,"id":16},{"text":"using","start":123,"end":128,"id":17},{"text":"the","start":129,"end":132,"id":18},{"text":"low","start":133,"end":136,"id":19},{"text":"-","start":136,"end":137,"id":20},{"text":"dimensional","start":137,"end":148,"id":21},{"text":"neighborhood","start":149,"end":161,"id":22},{"text":"representation","start":162,"end":176,"id":23},{"text":".","start":176,"end":177,"id":24}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"computer_simulations|NOUN","word":"computer simulations","sense":"NOUN","meta":{"score":0.7804999948,"sense":"NOUN"},"_input_hash":-2106370398,"_task_hash":731897998,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"computer_simulations|NOUN","start":0,"end":25,"id":0}]}
{"text":"Consequently, the weight vectors are highly sensitive to noise.","_input_hash":-1963221515,"_task_hash":508945984,"tokens":[{"text":"Consequently","start":0,"end":12,"id":0},{"text":",","start":12,"end":13,"id":1},{"text":"the","start":14,"end":17,"id":2},{"text":"weight","start":18,"end":24,"id":3},{"text":"vectors","start":25,"end":32,"id":4},{"text":"are","start":33,"end":36,"id":5},{"text":"highly","start":37,"end":43,"id":6},{"text":"sensitive","start":44,"end":53,"id":7},{"text":"to","start":54,"end":56,"id":8},{"text":"noise","start":57,"end":62,"id":9},{"text":".","start":62,"end":63,"id":10}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"By focusing further on functions which are invariant to the addition of a null eigenvalue to the spectrum of the variance matrix, we can define kernels between atomic measures on arbitrary spaces $\\Xcal$ endowed with a kernel $\\kappa$ by using directly the eigenvalues of the centered Gram matrix of the joined support of the compared measures.","_input_hash":-1208808103,"_task_hash":-878755707,"tokens":[{"text":"By","start":0,"end":2,"id":0},{"text":"focusing","start":3,"end":11,"id":1},{"text":"further","start":12,"end":19,"id":2},{"text":"on","start":20,"end":22,"id":3},{"text":"functions","start":23,"end":32,"id":4},{"text":"which","start":33,"end":38,"id":5},{"text":"are","start":39,"end":42,"id":6},{"text":"invariant","start":43,"end":52,"id":7},{"text":"to","start":53,"end":55,"id":8},{"text":"the","start":56,"end":59,"id":9},{"text":"addition","start":60,"end":68,"id":10},{"text":"of","start":69,"end":71,"id":11},{"text":"a","start":72,"end":73,"id":12},{"text":"null","start":74,"end":78,"id":13},{"text":"eigenvalue","start":79,"end":89,"id":14},{"text":"to","start":90,"end":92,"id":15},{"text":"the","start":93,"end":96,"id":16},{"text":"spectrum","start":97,"end":105,"id":17},{"text":"of","start":106,"end":108,"id":18},{"text":"the","start":109,"end":112,"id":19},{"text":"variance","start":113,"end":121,"id":20},{"text":"matrix","start":122,"end":128,"id":21},{"text":",","start":128,"end":129,"id":22},{"text":"we","start":130,"end":132,"id":23},{"text":"can","start":133,"end":136,"id":24},{"text":"define","start":137,"end":143,"id":25},{"text":"kernels","start":144,"end":151,"id":26},{"text":"between","start":152,"end":159,"id":27},{"text":"atomic","start":160,"end":166,"id":28},{"text":"measures","start":167,"end":175,"id":29},{"text":"on","start":176,"end":178,"id":30},{"text":"arbitrary","start":179,"end":188,"id":31},{"text":"spaces","start":189,"end":195,"id":32},{"text":"$","start":196,"end":197,"id":33},{"text":"\\Xcal$","start":197,"end":203,"id":34},{"text":"endowed","start":204,"end":211,"id":35},{"text":"with","start":212,"end":216,"id":36},{"text":"a","start":217,"end":218,"id":37},{"text":"kernel","start":219,"end":225,"id":38},{"text":"$","start":226,"end":227,"id":39},{"text":"\\kappa$","start":227,"end":234,"id":40},{"text":"by","start":235,"end":237,"id":41},{"text":"using","start":238,"end":243,"id":42},{"text":"directly","start":244,"end":252,"id":43},{"text":"the","start":253,"end":256,"id":44},{"text":"eigenvalues","start":257,"end":268,"id":45},{"text":"of","start":269,"end":271,"id":46},{"text":"the","start":272,"end":275,"id":47},{"text":"centered","start":276,"end":284,"id":48},{"text":"Gram","start":285,"end":289,"id":49},{"text":"matrix","start":290,"end":296,"id":50},{"text":"of","start":297,"end":299,"id":51},{"text":"the","start":300,"end":303,"id":52},{"text":"joined","start":304,"end":310,"id":53},{"text":"support","start":311,"end":318,"id":54},{"text":"of","start":319,"end":321,"id":55},{"text":"the","start":322,"end":325,"id":56},{"text":"compared","start":326,"end":334,"id":57},{"text":"measures","start":335,"end":343,"id":58},{"text":".","start":343,"end":344,"id":59}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"sha","meta":{"score":0},"_input_hash":-868404387,"_task_hash":43158969,"_session_id":null,"_view_id":"text","answer":"reject","spans":[],"tokens":[{"text":"sha","start":0,"end":3,"id":0}]}
{"text":"The estimator for the expectation of a function of the posterior is derived, and rates of consistency are shown.","_input_hash":1763479016,"_task_hash":1598080471,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"estimator","start":4,"end":13,"id":1},{"text":"for","start":14,"end":17,"id":2},{"text":"the","start":18,"end":21,"id":3},{"text":"expectation","start":22,"end":33,"id":4},{"text":"of","start":34,"end":36,"id":5},{"text":"a","start":37,"end":38,"id":6},{"text":"function","start":39,"end":47,"id":7},{"text":"of","start":48,"end":50,"id":8},{"text":"the","start":51,"end":54,"id":9},{"text":"posterior","start":55,"end":64,"id":10},{"text":"is","start":65,"end":67,"id":11},{"text":"derived","start":68,"end":75,"id":12},{"text":",","start":75,"end":76,"id":13},{"text":"and","start":77,"end":80,"id":14},{"text":"rates","start":81,"end":86,"id":15},{"text":"of","start":87,"end":89,"id":16},{"text":"consistency","start":90,"end":101,"id":17},{"text":"are","start":102,"end":105,"id":18},{"text":"shown","start":106,"end":111,"id":19},{"text":".","start":111,"end":112,"id":20}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"The class can be constructed form the \"raw coordinates\" encountered in spectral clustering, and can be extended by means of higher-dimensional embeddings (Schoenberg transformations).","_input_hash":1711228091,"_task_hash":-1406764672,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"class","start":4,"end":9,"id":1},{"text":"can","start":10,"end":13,"id":2},{"text":"be","start":14,"end":16,"id":3},{"text":"constructed","start":17,"end":28,"id":4},{"text":"form","start":29,"end":33,"id":5},{"text":"the","start":34,"end":37,"id":6},{"text":"\"","start":38,"end":39,"id":7},{"text":"raw","start":39,"end":42,"id":8},{"text":"coordinates","start":43,"end":54,"id":9},{"text":"\"","start":54,"end":55,"id":10},{"text":"encountered","start":56,"end":67,"id":11},{"text":"in","start":68,"end":70,"id":12},{"text":"spectral","start":71,"end":79,"id":13},{"text":"clustering","start":80,"end":90,"id":14},{"text":",","start":90,"end":91,"id":15},{"text":"and","start":92,"end":95,"id":16},{"text":"can","start":96,"end":99,"id":17},{"text":"be","start":100,"end":102,"id":18},{"text":"extended","start":103,"end":111,"id":19},{"text":"by","start":112,"end":114,"id":20},{"text":"means","start":115,"end":120,"id":21},{"text":"of","start":121,"end":123,"id":22},{"text":"higher","start":124,"end":130,"id":23},{"text":"-","start":130,"end":131,"id":24},{"text":"dimensional","start":131,"end":142,"id":25},{"text":"embeddings","start":143,"end":153,"id":26},{"text":"(","start":154,"end":155,"id":27},{"text":"Schoenberg","start":155,"end":165,"id":28},{"text":"transformations","start":166,"end":181,"id":29},{"text":")","start":181,"end":182,"id":30},{"text":".","start":182,"end":183,"id":31}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"When applied to text documents, our method learns hierarchies of topics, thus providing a competitive alternative to probabilistic topic models.","_input_hash":774547266,"_task_hash":-1488149939,"tokens":[{"text":"When","start":0,"end":4,"id":0},{"text":"applied","start":5,"end":12,"id":1},{"text":"to","start":13,"end":15,"id":2},{"text":"text","start":16,"end":20,"id":3},{"text":"documents","start":21,"end":30,"id":4},{"text":",","start":30,"end":31,"id":5},{"text":"our","start":32,"end":35,"id":6},{"text":"method","start":36,"end":42,"id":7},{"text":"learns","start":43,"end":49,"id":8},{"text":"hierarchies","start":50,"end":61,"id":9},{"text":"of","start":62,"end":64,"id":10},{"text":"topics","start":65,"end":71,"id":11},{"text":",","start":71,"end":72,"id":12},{"text":"thus","start":73,"end":77,"id":13},{"text":"providing","start":78,"end":87,"id":14},{"text":"a","start":88,"end":89,"id":15},{"text":"competitive","start":90,"end":101,"id":16},{"text":"alternative","start":102,"end":113,"id":17},{"text":"to","start":114,"end":116,"id":18},{"text":"probabilistic","start":117,"end":130,"id":19},{"text":"topic","start":131,"end":136,"id":20},{"text":"models","start":137,"end":143,"id":21},{"text":".","start":143,"end":144,"id":22}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"Just as additive models extend linear models by replacing linear functions with a set of one-dimensional smooth functions, the nonparanormal extends the normal by transforming the variables by smooth functions.","_input_hash":652053292,"_task_hash":375498781,"tokens":[{"text":"Just","start":0,"end":4,"id":0},{"text":"as","start":5,"end":7,"id":1},{"text":"additive","start":8,"end":16,"id":2},{"text":"models","start":17,"end":23,"id":3},{"text":"extend","start":24,"end":30,"id":4},{"text":"linear","start":31,"end":37,"id":5},{"text":"models","start":38,"end":44,"id":6},{"text":"by","start":45,"end":47,"id":7},{"text":"replacing","start":48,"end":57,"id":8},{"text":"linear","start":58,"end":64,"id":9},{"text":"functions","start":65,"end":74,"id":10},{"text":"with","start":75,"end":79,"id":11},{"text":"a","start":80,"end":81,"id":12},{"text":"set","start":82,"end":85,"id":13},{"text":"of","start":86,"end":88,"id":14},{"text":"one","start":89,"end":92,"id":15},{"text":"-","start":92,"end":93,"id":16},{"text":"dimensional","start":93,"end":104,"id":17},{"text":"smooth","start":105,"end":111,"id":18},{"text":"functions","start":112,"end":121,"id":19},{"text":",","start":121,"end":122,"id":20},{"text":"the","start":123,"end":126,"id":21},{"text":"nonparanormal","start":127,"end":140,"id":22},{"text":"extends","start":141,"end":148,"id":23},{"text":"the","start":149,"end":152,"id":24},{"text":"normal","start":153,"end":159,"id":25},{"text":"by","start":160,"end":162,"id":26},{"text":"transforming","start":163,"end":175,"id":27},{"text":"the","start":176,"end":179,"id":28},{"text":"variables","start":180,"end":189,"id":29},{"text":"by","start":190,"end":192,"id":30},{"text":"smooth","start":193,"end":199,"id":31},{"text":"functions","start":200,"end":209,"id":32},{"text":".","start":209,"end":210,"id":33}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"computer_vision|NOUN","word":"computer vision","sense":"NOUN","meta":{"score":0.8059999943,"sense":"NOUN"},"_input_hash":-585629917,"_task_hash":-1990857468,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"computer_vision|NOUN","start":0,"end":20,"id":0}]}
{"text":"We consider the problem of learning a binary classifier from a training set of positive and unlabeled examples, both in the inductive and in the transductive setting.","_input_hash":-131897280,"_task_hash":-1993608256,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"consider","start":3,"end":11,"id":1},{"text":"the","start":12,"end":15,"id":2},{"text":"problem","start":16,"end":23,"id":3},{"text":"of","start":24,"end":26,"id":4},{"text":"learning","start":27,"end":35,"id":5},{"text":"a","start":36,"end":37,"id":6},{"text":"binary","start":38,"end":44,"id":7},{"text":"classifier","start":45,"end":55,"id":8},{"text":"from","start":56,"end":60,"id":9},{"text":"a","start":61,"end":62,"id":10},{"text":"training","start":63,"end":71,"id":11},{"text":"set","start":72,"end":75,"id":12},{"text":"of","start":76,"end":78,"id":13},{"text":"positive","start":79,"end":87,"id":14},{"text":"and","start":88,"end":91,"id":15},{"text":"unlabeled","start":92,"end":101,"id":16},{"text":"examples","start":102,"end":110,"id":17},{"text":",","start":110,"end":111,"id":18},{"text":"both","start":112,"end":116,"id":19},{"text":"in","start":117,"end":119,"id":20},{"text":"the","start":120,"end":123,"id":21},{"text":"inductive","start":124,"end":133,"id":22},{"text":"and","start":134,"end":137,"id":23},{"text":"in","start":138,"end":140,"id":24},{"text":"the","start":141,"end":144,"id":25},{"text":"transductive","start":145,"end":157,"id":26},{"text":"setting","start":158,"end":165,"id":27},{"text":".","start":165,"end":166,"id":28}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":38,"end":55,"token_start":7,"token_end":8,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"the first component is a uniform distribution on that ball representing outliers and the other K components are uniform distributions along K d-dimensional linear subspaces restricted to that ball.","_input_hash":1359244882,"_task_hash":1340857627,"tokens":[{"text":"the","start":0,"end":3,"id":0},{"text":"first","start":4,"end":9,"id":1},{"text":"component","start":10,"end":19,"id":2},{"text":"is","start":20,"end":22,"id":3},{"text":"a","start":23,"end":24,"id":4},{"text":"uniform","start":25,"end":32,"id":5},{"text":"distribution","start":33,"end":45,"id":6},{"text":"on","start":46,"end":48,"id":7},{"text":"that","start":49,"end":53,"id":8},{"text":"ball","start":54,"end":58,"id":9},{"text":"representing","start":59,"end":71,"id":10},{"text":"outliers","start":72,"end":80,"id":11},{"text":"and","start":81,"end":84,"id":12},{"text":"the","start":85,"end":88,"id":13},{"text":"other","start":89,"end":94,"id":14},{"text":"K","start":95,"end":96,"id":15},{"text":"components","start":97,"end":107,"id":16},{"text":"are","start":108,"end":111,"id":17},{"text":"uniform","start":112,"end":119,"id":18},{"text":"distributions","start":120,"end":133,"id":19},{"text":"along","start":134,"end":139,"id":20},{"text":"K","start":140,"end":141,"id":21},{"text":"d","start":142,"end":143,"id":22},{"text":"-","start":143,"end":144,"id":23},{"text":"dimensional","start":144,"end":155,"id":24},{"text":"linear","start":156,"end":162,"id":25},{"text":"subspaces","start":163,"end":172,"id":26},{"text":"restricted","start":173,"end":183,"id":27},{"text":"to","start":184,"end":186,"id":28},{"text":"that","start":187,"end":191,"id":29},{"text":"ball","start":192,"end":196,"id":30},{"text":".","start":196,"end":197,"id":31}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"We illustrate the approach by analyzing E. coli gene expression data and computing an entropy-based gene-association network from gene expression data.","_input_hash":-1542246212,"_task_hash":1128590524,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"illustrate","start":3,"end":13,"id":1},{"text":"the","start":14,"end":17,"id":2},{"text":"approach","start":18,"end":26,"id":3},{"text":"by","start":27,"end":29,"id":4},{"text":"analyzing","start":30,"end":39,"id":5},{"text":"E.","start":40,"end":42,"id":6},{"text":"coli","start":43,"end":47,"id":7},{"text":"gene","start":48,"end":52,"id":8},{"text":"expression","start":53,"end":63,"id":9},{"text":"data","start":64,"end":68,"id":10},{"text":"and","start":69,"end":72,"id":11},{"text":"computing","start":73,"end":82,"id":12},{"text":"an","start":83,"end":85,"id":13},{"text":"entropy","start":86,"end":93,"id":14},{"text":"-","start":93,"end":94,"id":15},{"text":"based","start":94,"end":99,"id":16},{"text":"gene","start":100,"end":104,"id":17},{"text":"-","start":104,"end":105,"id":18},{"text":"association","start":105,"end":116,"id":19},{"text":"network","start":117,"end":124,"id":20},{"text":"from","start":125,"end":129,"id":21},{"text":"gene","start":130,"end":134,"id":22},{"text":"expression","start":135,"end":145,"id":23},{"text":"data","start":146,"end":150,"id":24},{"text":".","start":150,"end":151,"id":25}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"An estimation curve with transition phases depending on the cross-validation procedure and not only on the percentage of observations in the test sample gives a simple rule on how to choose the cross-validation.","_input_hash":1106405742,"_task_hash":-1583822556,"tokens":[{"text":"An","start":0,"end":2,"id":0},{"text":"estimation","start":3,"end":13,"id":1},{"text":"curve","start":14,"end":19,"id":2},{"text":"with","start":20,"end":24,"id":3},{"text":"transition","start":25,"end":35,"id":4},{"text":"phases","start":36,"end":42,"id":5},{"text":"depending","start":43,"end":52,"id":6},{"text":"on","start":53,"end":55,"id":7},{"text":"the","start":56,"end":59,"id":8},{"text":"cross","start":60,"end":65,"id":9},{"text":"-","start":65,"end":66,"id":10},{"text":"validation","start":66,"end":76,"id":11},{"text":"procedure","start":77,"end":86,"id":12},{"text":"and","start":87,"end":90,"id":13},{"text":"not","start":91,"end":94,"id":14},{"text":"only","start":95,"end":99,"id":15},{"text":"on","start":100,"end":102,"id":16},{"text":"the","start":103,"end":106,"id":17},{"text":"percentage","start":107,"end":117,"id":18},{"text":"of","start":118,"end":120,"id":19},{"text":"observations","start":121,"end":133,"id":20},{"text":"in","start":134,"end":136,"id":21},{"text":"the","start":137,"end":140,"id":22},{"text":"test","start":141,"end":145,"id":23},{"text":"sample","start":146,"end":152,"id":24},{"text":"gives","start":153,"end":158,"id":25},{"text":"a","start":159,"end":160,"id":26},{"text":"simple","start":161,"end":167,"id":27},{"text":"rule","start":168,"end":172,"id":28},{"text":"on","start":173,"end":175,"id":29},{"text":"how","start":176,"end":179,"id":30},{"text":"to","start":180,"end":182,"id":31},{"text":"choose","start":183,"end":189,"id":32},{"text":"the","start":190,"end":193,"id":33},{"text":"cross","start":194,"end":199,"id":34},{"text":"-","start":199,"end":200,"id":35},{"text":"validation","start":200,"end":210,"id":36},{"text":".","start":210,"end":211,"id":37}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"This simple technique consists of performing exact line search optimization of the kurtosis contrast function.","_input_hash":1234467529,"_task_hash":-485536131,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"simple","start":5,"end":11,"id":1},{"text":"technique","start":12,"end":21,"id":2},{"text":"consists","start":22,"end":30,"id":3},{"text":"of","start":31,"end":33,"id":4},{"text":"performing","start":34,"end":44,"id":5},{"text":"exact","start":45,"end":50,"id":6},{"text":"line","start":51,"end":55,"id":7},{"text":"search","start":56,"end":62,"id":8},{"text":"optimization","start":63,"end":75,"id":9},{"text":"of","start":76,"end":78,"id":10},{"text":"the","start":79,"end":82,"id":11},{"text":"kurtosis","start":83,"end":91,"id":12},{"text":"contrast","start":92,"end":100,"id":13},{"text":"function","start":101,"end":109,"id":14},{"text":".","start":109,"end":110,"id":15}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"In this paper we address these issues.","_input_hash":-1631506436,"_task_hash":2017089648,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"this","start":3,"end":7,"id":1},{"text":"paper","start":8,"end":13,"id":2},{"text":"we","start":14,"end":16,"id":3},{"text":"address","start":17,"end":24,"id":4},{"text":"these","start":25,"end":30,"id":5},{"text":"issues","start":31,"end":37,"id":6},{"text":".","start":37,"end":38,"id":7}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"The local linear embedding algorithm (LLE) is a non-linear dimension-reducing technique, widely used due to its computational simplicity and intuitive approach.","_input_hash":-1398033812,"_task_hash":-521611702,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"local","start":4,"end":9,"id":1},{"text":"linear","start":10,"end":16,"id":2},{"text":"embedding","start":17,"end":26,"id":3},{"text":"algorithm","start":27,"end":36,"id":4},{"text":"(","start":37,"end":38,"id":5},{"text":"LLE","start":38,"end":41,"id":6},{"text":")","start":41,"end":42,"id":7},{"text":"is","start":43,"end":45,"id":8},{"text":"a","start":46,"end":47,"id":9},{"text":"non","start":48,"end":51,"id":10},{"text":"-","start":51,"end":52,"id":11},{"text":"linear","start":52,"end":58,"id":12},{"text":"dimension","start":59,"end":68,"id":13},{"text":"-","start":68,"end":69,"id":14},{"text":"reducing","start":69,"end":77,"id":15},{"text":"technique","start":78,"end":87,"id":16},{"text":",","start":87,"end":88,"id":17},{"text":"widely","start":89,"end":95,"id":18},{"text":"used","start":96,"end":100,"id":19},{"text":"due","start":101,"end":104,"id":20},{"text":"to","start":105,"end":107,"id":21},{"text":"its","start":108,"end":111,"id":22},{"text":"computational","start":112,"end":125,"id":23},{"text":"simplicity","start":126,"end":136,"id":24},{"text":"and","start":137,"end":140,"id":25},{"text":"intuitive","start":141,"end":150,"id":26},{"text":"approach","start":151,"end":159,"id":27},{"text":".","start":159,"end":160,"id":28}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":4,"end":36,"token_start":1,"token_end":4,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Thus, the $l_1/l_2$ regularization on RBMs yields sparsity at both the group and the hidden unit levels.","_input_hash":-115310504,"_task_hash":631214118,"tokens":[{"text":"Thus","start":0,"end":4,"id":0},{"text":",","start":4,"end":5,"id":1},{"text":"the","start":6,"end":9,"id":2},{"text":"$","start":10,"end":11,"id":3},{"text":"l_1","start":11,"end":14,"id":4},{"text":"/","start":14,"end":15,"id":5},{"text":"l_2","start":15,"end":18,"id":6},{"text":"$","start":18,"end":19,"id":7},{"text":"regularization","start":20,"end":34,"id":8},{"text":"on","start":35,"end":37,"id":9},{"text":"RBMs","start":38,"end":42,"id":10},{"text":"yields","start":43,"end":49,"id":11},{"text":"sparsity","start":50,"end":58,"id":12},{"text":"at","start":59,"end":61,"id":13},{"text":"both","start":62,"end":66,"id":14},{"text":"the","start":67,"end":70,"id":15},{"text":"group","start":71,"end":76,"id":16},{"text":"and","start":77,"end":80,"id":17},{"text":"the","start":81,"end":84,"id":18},{"text":"hidden","start":85,"end":91,"id":19},{"text":"unit","start":92,"end":96,"id":20},{"text":"levels","start":97,"end":103,"id":21},{"text":".","start":103,"end":104,"id":22}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"unsupervised_learning|NOUN","word":"unsupervised learning","sense":"NOUN","meta":{"score":0.793200016,"sense":"NOUN"},"_input_hash":581862214,"_task_hash":-1758279962,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"unsupervised_learning|NOUN","start":0,"end":26,"id":0}]}
{"text":"We derive bounds on the effectiveness of a poisoning attack against centroid anomaly under different conditions:","_input_hash":-254283982,"_task_hash":-1123992711,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"derive","start":3,"end":9,"id":1},{"text":"bounds","start":10,"end":16,"id":2},{"text":"on","start":17,"end":19,"id":3},{"text":"the","start":20,"end":23,"id":4},{"text":"effectiveness","start":24,"end":37,"id":5},{"text":"of","start":38,"end":40,"id":6},{"text":"a","start":41,"end":42,"id":7},{"text":"poisoning","start":43,"end":52,"id":8},{"text":"attack","start":53,"end":59,"id":9},{"text":"against","start":60,"end":67,"id":10},{"text":"centroid","start":68,"end":76,"id":11},{"text":"anomaly","start":77,"end":84,"id":12},{"text":"under","start":85,"end":90,"id":13},{"text":"different","start":91,"end":100,"id":14},{"text":"conditions","start":101,"end":111,"id":15},{"text":":","start":111,"end":112,"id":16}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"state_machines|NOUN","word":"state machines","sense":"NOUN","meta":{"score":0.804099977,"sense":"NOUN"},"_input_hash":1643825329,"_task_hash":502184038,"_session_id":null,"_view_id":"html","answer":"accept","spans":[],"tokens":[{"text":"state_machines|NOUN","start":0,"end":19,"id":0}]}
{"text":"We introduce a new Bayesian model for hierarchical clustering based on a prior over trees called Kingman's coalescent.","_input_hash":2068280802,"_task_hash":-1791110143,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"introduce","start":3,"end":12,"id":1},{"text":"a","start":13,"end":14,"id":2},{"text":"new","start":15,"end":18,"id":3},{"text":"Bayesian","start":19,"end":27,"id":4},{"text":"model","start":28,"end":33,"id":5},{"text":"for","start":34,"end":37,"id":6},{"text":"hierarchical","start":38,"end":50,"id":7},{"text":"clustering","start":51,"end":61,"id":8},{"text":"based","start":62,"end":67,"id":9},{"text":"on","start":68,"end":70,"id":10},{"text":"a","start":71,"end":72,"id":11},{"text":"prior","start":73,"end":78,"id":12},{"text":"over","start":79,"end":83,"id":13},{"text":"trees","start":84,"end":89,"id":14},{"text":"called","start":90,"end":96,"id":15},{"text":"Kingman","start":97,"end":104,"id":16},{"text":"'s","start":104,"end":106,"id":17},{"text":"coalescent","start":107,"end":117,"id":18},{"text":".","start":117,"end":118,"id":19}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"To illustrate this framework some examples of such reproducing sets and kernels are given.","_input_hash":870541094,"_task_hash":-871632937,"tokens":[{"text":"To","start":0,"end":2,"id":0},{"text":"illustrate","start":3,"end":13,"id":1},{"text":"this","start":14,"end":18,"id":2},{"text":"framework","start":19,"end":28,"id":3},{"text":"some","start":29,"end":33,"id":4},{"text":"examples","start":34,"end":42,"id":5},{"text":"of","start":43,"end":45,"id":6},{"text":"such","start":46,"end":50,"id":7},{"text":"reproducing","start":51,"end":62,"id":8},{"text":"sets","start":63,"end":67,"id":9},{"text":"and","start":68,"end":71,"id":10},{"text":"kernels","start":72,"end":79,"id":11},{"text":"are","start":80,"end":83,"id":12},{"text":"given","start":84,"end":89,"id":13},{"text":".","start":89,"end":90,"id":14}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":84,"end":89,"token_start":13,"token_end":13,"label":"ALGO","answer":"reject"}],"answer":"reject"}
{"text":"The cellular automata network simulates the higher level integration of information acquired from the independent learning trials.","_input_hash":-1149983967,"_task_hash":5255362,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"cellular","start":4,"end":12,"id":1},{"text":"automata","start":13,"end":21,"id":2},{"text":"network","start":22,"end":29,"id":3},{"text":"simulates","start":30,"end":39,"id":4},{"text":"the","start":40,"end":43,"id":5},{"text":"higher","start":44,"end":50,"id":6},{"text":"level","start":51,"end":56,"id":7},{"text":"integration","start":57,"end":68,"id":8},{"text":"of","start":69,"end":71,"id":9},{"text":"information","start":72,"end":83,"id":10},{"text":"acquired","start":84,"end":92,"id":11},{"text":"from","start":93,"end":97,"id":12},{"text":"the","start":98,"end":101,"id":13},{"text":"independent","start":102,"end":113,"id":14},{"text":"learning","start":114,"end":122,"id":15},{"text":"trials","start":123,"end":129,"id":16},{"text":".","start":129,"end":130,"id":17}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"After explaining the definitions, the concept of classifiers is discussed in the context of moduli spaces, and upper bounds for the number of hidden vertices in dendrograms are given.","_input_hash":1788297260,"_task_hash":1090886645,"tokens":[{"text":"After","start":0,"end":5,"id":0},{"text":"explaining","start":6,"end":16,"id":1},{"text":"the","start":17,"end":20,"id":2},{"text":"definitions","start":21,"end":32,"id":3},{"text":",","start":32,"end":33,"id":4},{"text":"the","start":34,"end":37,"id":5},{"text":"concept","start":38,"end":45,"id":6},{"text":"of","start":46,"end":48,"id":7},{"text":"classifiers","start":49,"end":60,"id":8},{"text":"is","start":61,"end":63,"id":9},{"text":"discussed","start":64,"end":73,"id":10},{"text":"in","start":74,"end":76,"id":11},{"text":"the","start":77,"end":80,"id":12},{"text":"context","start":81,"end":88,"id":13},{"text":"of","start":89,"end":91,"id":14},{"text":"moduli","start":92,"end":98,"id":15},{"text":"spaces","start":99,"end":105,"id":16},{"text":",","start":105,"end":106,"id":17},{"text":"and","start":107,"end":110,"id":18},{"text":"upper","start":111,"end":116,"id":19},{"text":"bounds","start":117,"end":123,"id":20},{"text":"for","start":124,"end":127,"id":21},{"text":"the","start":128,"end":131,"id":22},{"text":"number","start":132,"end":138,"id":23},{"text":"of","start":139,"end":141,"id":24},{"text":"hidden","start":142,"end":148,"id":25},{"text":"vertices","start":149,"end":157,"id":26},{"text":"in","start":158,"end":160,"id":27},{"text":"dendrograms","start":161,"end":172,"id":28},{"text":"are","start":173,"end":176,"id":29},{"text":"given","start":177,"end":182,"id":30},{"text":".","start":182,"end":183,"id":31}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"SVMs can informally be described as a kind of regularized M-estimators for functions and have demonstrated their usefulness in many complicated real-life problems.","_input_hash":-1291340098,"_task_hash":-1764603687,"tokens":[{"text":"SVMs","start":0,"end":4,"id":0},{"text":"can","start":5,"end":8,"id":1},{"text":"informally","start":9,"end":19,"id":2},{"text":"be","start":20,"end":22,"id":3},{"text":"described","start":23,"end":32,"id":4},{"text":"as","start":33,"end":35,"id":5},{"text":"a","start":36,"end":37,"id":6},{"text":"kind","start":38,"end":42,"id":7},{"text":"of","start":43,"end":45,"id":8},{"text":"regularized","start":46,"end":57,"id":9},{"text":"M","start":58,"end":59,"id":10},{"text":"-","start":59,"end":60,"id":11},{"text":"estimators","start":60,"end":70,"id":12},{"text":"for","start":71,"end":74,"id":13},{"text":"functions","start":75,"end":84,"id":14},{"text":"and","start":85,"end":88,"id":15},{"text":"have","start":89,"end":93,"id":16},{"text":"demonstrated","start":94,"end":106,"id":17},{"text":"their","start":107,"end":112,"id":18},{"text":"usefulness","start":113,"end":123,"id":19},{"text":"in","start":124,"end":126,"id":20},{"text":"many","start":127,"end":131,"id":21},{"text":"complicated","start":132,"end":143,"id":22},{"text":"real","start":144,"end":148,"id":23},{"text":"-","start":148,"end":149,"id":24},{"text":"life","start":149,"end":153,"id":25},{"text":"problems","start":154,"end":162,"id":26},{"text":".","start":162,"end":163,"id":27}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":0,"end":4,"token_start":0,"token_end":0,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"sufficient_complexity|NOUN","word":"sufficient complexity","sense":"NOUN","meta":{"score":0.7619000077,"sense":"NOUN"},"_input_hash":1199185788,"_task_hash":1873041531,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"sufficient_complexity|NOUN","start":0,"end":26,"id":0}]}
{"text":"These could be as simple as monotone functions of individual predictor variables.","_input_hash":-446331697,"_task_hash":-781571654,"tokens":[{"text":"These","start":0,"end":5,"id":0},{"text":"could","start":6,"end":11,"id":1},{"text":"be","start":12,"end":14,"id":2},{"text":"as","start":15,"end":17,"id":3},{"text":"simple","start":18,"end":24,"id":4},{"text":"as","start":25,"end":27,"id":5},{"text":"monotone","start":28,"end":36,"id":6},{"text":"functions","start":37,"end":46,"id":7},{"text":"of","start":47,"end":49,"id":8},{"text":"individual","start":50,"end":60,"id":9},{"text":"predictor","start":61,"end":70,"id":10},{"text":"variables","start":71,"end":80,"id":11},{"text":".","start":80,"end":81,"id":12}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We illustrate the topic presentations on corpora of scientific abstracts and news articles.","_input_hash":180928036,"_task_hash":-1441474605,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"illustrate","start":3,"end":13,"id":1},{"text":"the","start":14,"end":17,"id":2},{"text":"topic","start":18,"end":23,"id":3},{"text":"presentations","start":24,"end":37,"id":4},{"text":"on","start":38,"end":40,"id":5},{"text":"corpora","start":41,"end":48,"id":6},{"text":"of","start":49,"end":51,"id":7},{"text":"scientific","start":52,"end":62,"id":8},{"text":"abstracts","start":63,"end":72,"id":9},{"text":"and","start":73,"end":76,"id":10},{"text":"news","start":77,"end":81,"id":11},{"text":"articles","start":82,"end":90,"id":12},{"text":".","start":90,"end":91,"id":13}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"mathematical_functions|NOUN","word":"mathematical functions","sense":"NOUN","meta":{"score":0.7720999718,"sense":"NOUN"},"_input_hash":698636212,"_task_hash":578343466,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"mathematical_functions|NOUN","start":0,"end":27,"id":0}]}
{"text":"We explore further properties of this unique scheme, stability and convergence are established.","_input_hash":-1864270311,"_task_hash":647865512,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"explore","start":3,"end":10,"id":1},{"text":"further","start":11,"end":18,"id":2},{"text":"properties","start":19,"end":29,"id":3},{"text":"of","start":30,"end":32,"id":4},{"text":"this","start":33,"end":37,"id":5},{"text":"unique","start":38,"end":44,"id":6},{"text":"scheme","start":45,"end":51,"id":7},{"text":",","start":51,"end":52,"id":8},{"text":"stability","start":53,"end":62,"id":9},{"text":"and","start":63,"end":66,"id":10},{"text":"convergence","start":67,"end":78,"id":11},{"text":"are","start":79,"end":82,"id":12},{"text":"established","start":83,"end":94,"id":13},{"text":".","start":94,"end":95,"id":14}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"Following the manifold regularization approach, Laplacian Support Vector Machines (LapSVMs) have shown the state of the art performance in semi--supervised classification.","_input_hash":1370397883,"_task_hash":1015134833,"tokens":[{"text":"Following","start":0,"end":9,"id":0},{"text":"the","start":10,"end":13,"id":1},{"text":"manifold","start":14,"end":22,"id":2},{"text":"regularization","start":23,"end":37,"id":3},{"text":"approach","start":38,"end":46,"id":4},{"text":",","start":46,"end":47,"id":5},{"text":"Laplacian","start":48,"end":57,"id":6},{"text":"Support","start":58,"end":65,"id":7},{"text":"Vector","start":66,"end":72,"id":8},{"text":"Machines","start":73,"end":81,"id":9},{"text":"(","start":82,"end":83,"id":10},{"text":"LapSVMs","start":83,"end":90,"id":11},{"text":")","start":90,"end":91,"id":12},{"text":"have","start":92,"end":96,"id":13},{"text":"shown","start":97,"end":102,"id":14},{"text":"the","start":103,"end":106,"id":15},{"text":"state","start":107,"end":112,"id":16},{"text":"of","start":113,"end":115,"id":17},{"text":"the","start":116,"end":119,"id":18},{"text":"art","start":120,"end":123,"id":19},{"text":"performance","start":124,"end":135,"id":20},{"text":"in","start":136,"end":138,"id":21},{"text":"semi","start":139,"end":143,"id":22},{"text":"--","start":143,"end":145,"id":23},{"text":"supervised","start":145,"end":155,"id":24},{"text":"classification","start":156,"end":170,"id":25},{"text":".","start":170,"end":171,"id":26}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":48,"end":81,"token_start":6,"token_end":9,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"ll","meta":{"score":0},"_input_hash":-877253534,"_task_hash":674024764,"_session_id":null,"_view_id":"text","answer":"reject","spans":[],"tokens":[{"text":"ll","start":0,"end":2,"id":0}]}
{"text":"In particular, we present a bound on the excess risk incurred by the method.","_input_hash":791849239,"_task_hash":277318544,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"particular","start":3,"end":13,"id":1},{"text":",","start":13,"end":14,"id":2},{"text":"we","start":15,"end":17,"id":3},{"text":"present","start":18,"end":25,"id":4},{"text":"a","start":26,"end":27,"id":5},{"text":"bound","start":28,"end":33,"id":6},{"text":"on","start":34,"end":36,"id":7},{"text":"the","start":37,"end":40,"id":8},{"text":"excess","start":41,"end":47,"id":9},{"text":"risk","start":48,"end":52,"id":10},{"text":"incurred","start":53,"end":61,"id":11},{"text":"by","start":62,"end":64,"id":12},{"text":"the","start":65,"end":68,"id":13},{"text":"method","start":69,"end":75,"id":14},{"text":".","start":75,"end":76,"id":15}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"The standard techniques include $K$-fold cross-validation ($K$-CV), Akaike information criterion (AIC), and Bayesian information criterion (BIC).","_input_hash":26445775,"_task_hash":402900288,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"standard","start":4,"end":12,"id":1},{"text":"techniques","start":13,"end":23,"id":2},{"text":"include","start":24,"end":31,"id":3},{"text":"$","start":32,"end":33,"id":4},{"text":"K$-fold","start":33,"end":40,"id":5},{"text":"cross","start":41,"end":46,"id":6},{"text":"-","start":46,"end":47,"id":7},{"text":"validation","start":47,"end":57,"id":8},{"text":"(","start":58,"end":59,"id":9},{"text":"$","start":59,"end":60,"id":10},{"text":"K$-CV","start":60,"end":65,"id":11},{"text":")","start":65,"end":66,"id":12},{"text":",","start":66,"end":67,"id":13},{"text":"Akaike","start":68,"end":74,"id":14},{"text":"information","start":75,"end":86,"id":15},{"text":"criterion","start":87,"end":96,"id":16},{"text":"(","start":97,"end":98,"id":17},{"text":"AIC","start":98,"end":101,"id":18},{"text":")","start":101,"end":102,"id":19},{"text":",","start":102,"end":103,"id":20},{"text":"and","start":104,"end":107,"id":21},{"text":"Bayesian","start":108,"end":116,"id":22},{"text":"information","start":117,"end":128,"id":23},{"text":"criterion","start":129,"end":138,"id":24},{"text":"(","start":139,"end":140,"id":25},{"text":"BIC","start":140,"end":143,"id":26},{"text":")","start":143,"end":144,"id":27},{"text":".","start":144,"end":145,"id":28}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"na","meta":{"score":0},"_input_hash":1015630174,"_task_hash":-2063517165,"_session_id":null,"_view_id":"text","answer":"reject","spans":[],"tokens":[{"text":"na","start":0,"end":2,"id":0}]}
{"text":"We prove that the criterion for Markov equivalence provided by Zhao et al. (","_input_hash":-235937920,"_task_hash":2126952125,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"prove","start":3,"end":8,"id":1},{"text":"that","start":9,"end":13,"id":2},{"text":"the","start":14,"end":17,"id":3},{"text":"criterion","start":18,"end":27,"id":4},{"text":"for","start":28,"end":31,"id":5},{"text":"Markov","start":32,"end":38,"id":6},{"text":"equivalence","start":39,"end":50,"id":7},{"text":"provided","start":51,"end":59,"id":8},{"text":"by","start":60,"end":62,"id":9},{"text":"Zhao","start":63,"end":67,"id":10},{"text":"et","start":68,"end":70,"id":11},{"text":"al","start":71,"end":73,"id":12},{"text":".","start":73,"end":74,"id":13},{"text":"(","start":75,"end":76,"id":14}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"We analyze the method in the high-dimensional setting, where the number of dimensions $p$ is allowed to grow with the number of observations $n$. The rate of convergence of the estimate is demonstrated to depend explicitly on the sparsity of the underlying graph.","_input_hash":418561057,"_task_hash":812657583,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"analyze","start":3,"end":10,"id":1},{"text":"the","start":11,"end":14,"id":2},{"text":"method","start":15,"end":21,"id":3},{"text":"in","start":22,"end":24,"id":4},{"text":"the","start":25,"end":28,"id":5},{"text":"high","start":29,"end":33,"id":6},{"text":"-","start":33,"end":34,"id":7},{"text":"dimensional","start":34,"end":45,"id":8},{"text":"setting","start":46,"end":53,"id":9},{"text":",","start":53,"end":54,"id":10},{"text":"where","start":55,"end":60,"id":11},{"text":"the","start":61,"end":64,"id":12},{"text":"number","start":65,"end":71,"id":13},{"text":"of","start":72,"end":74,"id":14},{"text":"dimensions","start":75,"end":85,"id":15},{"text":"$","start":86,"end":87,"id":16},{"text":"p$","start":87,"end":89,"id":17},{"text":"is","start":90,"end":92,"id":18},{"text":"allowed","start":93,"end":100,"id":19},{"text":"to","start":101,"end":103,"id":20},{"text":"grow","start":104,"end":108,"id":21},{"text":"with","start":109,"end":113,"id":22},{"text":"the","start":114,"end":117,"id":23},{"text":"number","start":118,"end":124,"id":24},{"text":"of","start":125,"end":127,"id":25},{"text":"observations","start":128,"end":140,"id":26},{"text":"$","start":141,"end":142,"id":27},{"text":"n$.","start":142,"end":145,"id":28},{"text":"The","start":146,"end":149,"id":29},{"text":"rate","start":150,"end":154,"id":30},{"text":"of","start":155,"end":157,"id":31},{"text":"convergence","start":158,"end":169,"id":32},{"text":"of","start":170,"end":172,"id":33},{"text":"the","start":173,"end":176,"id":34},{"text":"estimate","start":177,"end":185,"id":35},{"text":"is","start":186,"end":188,"id":36},{"text":"demonstrated","start":189,"end":201,"id":37},{"text":"to","start":202,"end":204,"id":38},{"text":"depend","start":205,"end":211,"id":39},{"text":"explicitly","start":212,"end":222,"id":40},{"text":"on","start":223,"end":225,"id":41},{"text":"the","start":226,"end":229,"id":42},{"text":"sparsity","start":230,"end":238,"id":43},{"text":"of","start":239,"end":241,"id":44},{"text":"the","start":242,"end":245,"id":45},{"text":"underlying","start":246,"end":256,"id":46},{"text":"graph","start":257,"end":262,"id":47},{"text":".","start":262,"end":263,"id":48}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"In this paper, we propose a method to find exogenous variables in a linear non-Gaussian causal model, which requires much smaller sample sizes than conventional methods and works even when p>>n.","_input_hash":-335248842,"_task_hash":1503881421,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"this","start":3,"end":7,"id":1},{"text":"paper","start":8,"end":13,"id":2},{"text":",","start":13,"end":14,"id":3},{"text":"we","start":15,"end":17,"id":4},{"text":"propose","start":18,"end":25,"id":5},{"text":"a","start":26,"end":27,"id":6},{"text":"method","start":28,"end":34,"id":7},{"text":"to","start":35,"end":37,"id":8},{"text":"find","start":38,"end":42,"id":9},{"text":"exogenous","start":43,"end":52,"id":10},{"text":"variables","start":53,"end":62,"id":11},{"text":"in","start":63,"end":65,"id":12},{"text":"a","start":66,"end":67,"id":13},{"text":"linear","start":68,"end":74,"id":14},{"text":"non","start":75,"end":78,"id":15},{"text":"-","start":78,"end":79,"id":16},{"text":"Gaussian","start":79,"end":87,"id":17},{"text":"causal","start":88,"end":94,"id":18},{"text":"model","start":95,"end":100,"id":19},{"text":",","start":100,"end":101,"id":20},{"text":"which","start":102,"end":107,"id":21},{"text":"requires","start":108,"end":116,"id":22},{"text":"much","start":117,"end":121,"id":23},{"text":"smaller","start":122,"end":129,"id":24},{"text":"sample","start":130,"end":136,"id":25},{"text":"sizes","start":137,"end":142,"id":26},{"text":"than","start":143,"end":147,"id":27},{"text":"conventional","start":148,"end":160,"id":28},{"text":"methods","start":161,"end":168,"id":29},{"text":"and","start":169,"end":172,"id":30},{"text":"works","start":173,"end":178,"id":31},{"text":"even","start":179,"end":183,"id":32},{"text":"when","start":184,"end":188,"id":33},{"text":"p>>n","start":189,"end":193,"id":34},{"text":".","start":193,"end":194,"id":35}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":26,"end":34,"token_start":6,"token_end":7,"label":"ALGO","answer":"reject"},{"start":88,"end":100,"token_start":18,"token_end":19,"label":"ALGO","answer":"reject"}],"answer":"reject"}
{"text":"neural_network","answer":"accept","_input_hash":-985022579,"_task_hash":1842934393,"spans":[],"tokens":[{"text":"neural_network","start":0,"end":14,"id":0}]}
{"text":"Further relaxations are also discussed.","_input_hash":1994490729,"_task_hash":1846884363,"tokens":[{"text":"Further","start":0,"end":7,"id":0},{"text":"relaxations","start":8,"end":19,"id":1},{"text":"are","start":20,"end":23,"id":2},{"text":"also","start":24,"end":28,"id":3},{"text":"discussed","start":29,"end":38,"id":4},{"text":".","start":38,"end":39,"id":5}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"For fast mixing graphs, we show that the cost functions introduced by Shi and Malik can be well approximated as the rate of loss of predictive information about the location of random walkers on the graph.","_input_hash":-1891153758,"_task_hash":1014870552,"tokens":[{"text":"For","start":0,"end":3,"id":0},{"text":"fast","start":4,"end":8,"id":1},{"text":"mixing","start":9,"end":15,"id":2},{"text":"graphs","start":16,"end":22,"id":3},{"text":",","start":22,"end":23,"id":4},{"text":"we","start":24,"end":26,"id":5},{"text":"show","start":27,"end":31,"id":6},{"text":"that","start":32,"end":36,"id":7},{"text":"the","start":37,"end":40,"id":8},{"text":"cost","start":41,"end":45,"id":9},{"text":"functions","start":46,"end":55,"id":10},{"text":"introduced","start":56,"end":66,"id":11},{"text":"by","start":67,"end":69,"id":12},{"text":"Shi","start":70,"end":73,"id":13},{"text":"and","start":74,"end":77,"id":14},{"text":"Malik","start":78,"end":83,"id":15},{"text":"can","start":84,"end":87,"id":16},{"text":"be","start":88,"end":90,"id":17},{"text":"well","start":91,"end":95,"id":18},{"text":"approximated","start":96,"end":108,"id":19},{"text":"as","start":109,"end":111,"id":20},{"text":"the","start":112,"end":115,"id":21},{"text":"rate","start":116,"end":120,"id":22},{"text":"of","start":121,"end":123,"id":23},{"text":"loss","start":124,"end":128,"id":24},{"text":"of","start":129,"end":131,"id":25},{"text":"predictive","start":132,"end":142,"id":26},{"text":"information","start":143,"end":154,"id":27},{"text":"about","start":155,"end":160,"id":28},{"text":"the","start":161,"end":164,"id":29},{"text":"location","start":165,"end":173,"id":30},{"text":"of","start":174,"end":176,"id":31},{"text":"random","start":177,"end":183,"id":32},{"text":"walkers","start":184,"end":191,"id":33},{"text":"on","start":192,"end":194,"id":34},{"text":"the","start":195,"end":198,"id":35},{"text":"graph","start":199,"end":204,"id":36},{"text":".","start":204,"end":205,"id":37}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"complex_behaviors|NOUN","word":"complex behaviors","sense":"NOUN","meta":{"score":0.766900003,"sense":"NOUN"},"_input_hash":-1114203009,"_task_hash":2044834824,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"complex_behaviors|NOUN","start":0,"end":22,"id":0}]}
{"text":"Then it is shown that the hypothesis set of any learning machine has to be a generalized reproducing set.","_input_hash":116611826,"_task_hash":-1781046768,"tokens":[{"text":"Then","start":0,"end":4,"id":0},{"text":"it","start":5,"end":7,"id":1},{"text":"is","start":8,"end":10,"id":2},{"text":"shown","start":11,"end":16,"id":3},{"text":"that","start":17,"end":21,"id":4},{"text":"the","start":22,"end":25,"id":5},{"text":"hypothesis","start":26,"end":36,"id":6},{"text":"set","start":37,"end":40,"id":7},{"text":"of","start":41,"end":43,"id":8},{"text":"any","start":44,"end":47,"id":9},{"text":"learning","start":48,"end":56,"id":10},{"text":"machine","start":57,"end":64,"id":11},{"text":"has","start":65,"end":68,"id":12},{"text":"to","start":69,"end":71,"id":13},{"text":"be","start":72,"end":74,"id":14},{"text":"a","start":75,"end":76,"id":15},{"text":"generalized","start":77,"end":88,"id":16},{"text":"reproducing","start":89,"end":100,"id":17},{"text":"set","start":101,"end":104,"id":18},{"text":".","start":104,"end":105,"id":19}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":48,"end":64,"token_start":10,"token_end":11,"label":"ALGO","answer":"reject"}],"answer":"reject"}
{"text":"artificial_brain|NOUN","word":"artificial brain","sense":"NOUN","meta":{"score":0.7709000111,"sense":"NOUN"},"_input_hash":922519583,"_task_hash":-710599281,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"artificial_brain|NOUN","start":0,"end":21,"id":0}]}
{"text":"We develop novel greedy and sequential Monte Carlo inferences which operate in a bottom-up agglomerative fashion.","_input_hash":1376566390,"_task_hash":-1945168562,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"develop","start":3,"end":10,"id":1},{"text":"novel","start":11,"end":16,"id":2},{"text":"greedy","start":17,"end":23,"id":3},{"text":"and","start":24,"end":27,"id":4},{"text":"sequential","start":28,"end":38,"id":5},{"text":"Monte","start":39,"end":44,"id":6},{"text":"Carlo","start":45,"end":50,"id":7},{"text":"inferences","start":51,"end":61,"id":8},{"text":"which","start":62,"end":67,"id":9},{"text":"operate","start":68,"end":75,"id":10},{"text":"in","start":76,"end":78,"id":11},{"text":"a","start":79,"end":80,"id":12},{"text":"bottom","start":81,"end":87,"id":13},{"text":"-","start":87,"end":88,"id":14},{"text":"up","start":88,"end":90,"id":15},{"text":"agglomerative","start":91,"end":104,"id":16},{"text":"fashion","start":105,"end":112,"id":17},{"text":".","start":112,"end":113,"id":18}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"This regularization not only encourages hidden units of many groups to be inactive given observed data but also makes hidden units within a group compete with each other for modeling observed data.","_input_hash":146966403,"_task_hash":-1646911325,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"regularization","start":5,"end":19,"id":1},{"text":"not","start":20,"end":23,"id":2},{"text":"only","start":24,"end":28,"id":3},{"text":"encourages","start":29,"end":39,"id":4},{"text":"hidden","start":40,"end":46,"id":5},{"text":"units","start":47,"end":52,"id":6},{"text":"of","start":53,"end":55,"id":7},{"text":"many","start":56,"end":60,"id":8},{"text":"groups","start":61,"end":67,"id":9},{"text":"to","start":68,"end":70,"id":10},{"text":"be","start":71,"end":73,"id":11},{"text":"inactive","start":74,"end":82,"id":12},{"text":"given","start":83,"end":88,"id":13},{"text":"observed","start":89,"end":97,"id":14},{"text":"data","start":98,"end":102,"id":15},{"text":"but","start":103,"end":106,"id":16},{"text":"also","start":107,"end":111,"id":17},{"text":"makes","start":112,"end":117,"id":18},{"text":"hidden","start":118,"end":124,"id":19},{"text":"units","start":125,"end":130,"id":20},{"text":"within","start":131,"end":137,"id":21},{"text":"a","start":138,"end":139,"id":22},{"text":"group","start":140,"end":145,"id":23},{"text":"compete","start":146,"end":153,"id":24},{"text":"with","start":154,"end":158,"id":25},{"text":"each","start":159,"end":163,"id":26},{"text":"other","start":164,"end":169,"id":27},{"text":"for","start":170,"end":173,"id":28},{"text":"modeling","start":174,"end":182,"id":29},{"text":"observed","start":183,"end":191,"id":30},{"text":"data","start":192,"end":196,"id":31},{"text":".","start":196,"end":197,"id":32}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"It corresponds to an ubiquitous situation in many applications such as information retrieval or gene ranking, when we have identified a set of data of interest sharing a particular property, and we wish to automatically retrieve additional data sharing the same property among a large and easily available pool of unlabeled data.","_input_hash":1701931546,"_task_hash":-732835768,"tokens":[{"text":"It","start":0,"end":2,"id":0},{"text":"corresponds","start":3,"end":14,"id":1},{"text":"to","start":15,"end":17,"id":2},{"text":"an","start":18,"end":20,"id":3},{"text":"ubiquitous","start":21,"end":31,"id":4},{"text":"situation","start":32,"end":41,"id":5},{"text":"in","start":42,"end":44,"id":6},{"text":"many","start":45,"end":49,"id":7},{"text":"applications","start":50,"end":62,"id":8},{"text":"such","start":63,"end":67,"id":9},{"text":"as","start":68,"end":70,"id":10},{"text":"information","start":71,"end":82,"id":11},{"text":"retrieval","start":83,"end":92,"id":12},{"text":"or","start":93,"end":95,"id":13},{"text":"gene","start":96,"end":100,"id":14},{"text":"ranking","start":101,"end":108,"id":15},{"text":",","start":108,"end":109,"id":16},{"text":"when","start":110,"end":114,"id":17},{"text":"we","start":115,"end":117,"id":18},{"text":"have","start":118,"end":122,"id":19},{"text":"identified","start":123,"end":133,"id":20},{"text":"a","start":134,"end":135,"id":21},{"text":"set","start":136,"end":139,"id":22},{"text":"of","start":140,"end":142,"id":23},{"text":"data","start":143,"end":147,"id":24},{"text":"of","start":148,"end":150,"id":25},{"text":"interest","start":151,"end":159,"id":26},{"text":"sharing","start":160,"end":167,"id":27},{"text":"a","start":168,"end":169,"id":28},{"text":"particular","start":170,"end":180,"id":29},{"text":"property","start":181,"end":189,"id":30},{"text":",","start":189,"end":190,"id":31},{"text":"and","start":191,"end":194,"id":32},{"text":"we","start":195,"end":197,"id":33},{"text":"wish","start":198,"end":202,"id":34},{"text":"to","start":203,"end":205,"id":35},{"text":"automatically","start":206,"end":219,"id":36},{"text":"retrieve","start":220,"end":228,"id":37},{"text":"additional","start":229,"end":239,"id":38},{"text":"data","start":240,"end":244,"id":39},{"text":"sharing","start":245,"end":252,"id":40},{"text":"the","start":253,"end":256,"id":41},{"text":"same","start":257,"end":261,"id":42},{"text":"property","start":262,"end":270,"id":43},{"text":"among","start":271,"end":276,"id":44},{"text":"a","start":277,"end":278,"id":45},{"text":"large","start":279,"end":284,"id":46},{"text":"and","start":285,"end":288,"id":47},{"text":"easily","start":289,"end":295,"id":48},{"text":"available","start":296,"end":305,"id":49},{"text":"pool","start":306,"end":310,"id":50},{"text":"of","start":311,"end":313,"id":51},{"text":"unlabeled","start":314,"end":323,"id":52},{"text":"data","start":324,"end":328,"id":53},{"text":".","start":328,"end":329,"id":54}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We introduce a multi-way latent variable model for this new task, by extending the generative model of Bayesian canonical correlation analysis (CCA) both to take multi-way covariate information into account as population priors, and by reducing the dimensionality by an integrated factor analysis that assumes the metabolites to come in correlated groups.","_input_hash":474078842,"_task_hash":-1737377232,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"introduce","start":3,"end":12,"id":1},{"text":"a","start":13,"end":14,"id":2},{"text":"multi","start":15,"end":20,"id":3},{"text":"-","start":20,"end":21,"id":4},{"text":"way","start":21,"end":24,"id":5},{"text":"latent","start":25,"end":31,"id":6},{"text":"variable","start":32,"end":40,"id":7},{"text":"model","start":41,"end":46,"id":8},{"text":"for","start":47,"end":50,"id":9},{"text":"this","start":51,"end":55,"id":10},{"text":"new","start":56,"end":59,"id":11},{"text":"task","start":60,"end":64,"id":12},{"text":",","start":64,"end":65,"id":13},{"text":"by","start":66,"end":68,"id":14},{"text":"extending","start":69,"end":78,"id":15},{"text":"the","start":79,"end":82,"id":16},{"text":"generative","start":83,"end":93,"id":17},{"text":"model","start":94,"end":99,"id":18},{"text":"of","start":100,"end":102,"id":19},{"text":"Bayesian","start":103,"end":111,"id":20},{"text":"canonical","start":112,"end":121,"id":21},{"text":"correlation","start":122,"end":133,"id":22},{"text":"analysis","start":134,"end":142,"id":23},{"text":"(","start":143,"end":144,"id":24},{"text":"CCA","start":144,"end":147,"id":25},{"text":")","start":147,"end":148,"id":26},{"text":"both","start":149,"end":153,"id":27},{"text":"to","start":154,"end":156,"id":28},{"text":"take","start":157,"end":161,"id":29},{"text":"multi","start":162,"end":167,"id":30},{"text":"-","start":167,"end":168,"id":31},{"text":"way","start":168,"end":171,"id":32},{"text":"covariate","start":172,"end":181,"id":33},{"text":"information","start":182,"end":193,"id":34},{"text":"into","start":194,"end":198,"id":35},{"text":"account","start":199,"end":206,"id":36},{"text":"as","start":207,"end":209,"id":37},{"text":"population","start":210,"end":220,"id":38},{"text":"priors","start":221,"end":227,"id":39},{"text":",","start":227,"end":228,"id":40},{"text":"and","start":229,"end":232,"id":41},{"text":"by","start":233,"end":235,"id":42},{"text":"reducing","start":236,"end":244,"id":43},{"text":"the","start":245,"end":248,"id":44},{"text":"dimensionality","start":249,"end":263,"id":45},{"text":"by","start":264,"end":266,"id":46},{"text":"an","start":267,"end":269,"id":47},{"text":"integrated","start":270,"end":280,"id":48},{"text":"factor","start":281,"end":287,"id":49},{"text":"analysis","start":288,"end":296,"id":50},{"text":"that","start":297,"end":301,"id":51},{"text":"assumes","start":302,"end":309,"id":52},{"text":"the","start":310,"end":313,"id":53},{"text":"metabolites","start":314,"end":325,"id":54},{"text":"to","start":326,"end":328,"id":55},{"text":"come","start":329,"end":333,"id":56},{"text":"in","start":334,"end":336,"id":57},{"text":"correlated","start":337,"end":347,"id":58},{"text":"groups","start":348,"end":354,"id":59},{"text":".","start":354,"end":355,"id":60}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":25,"end":46,"token_start":6,"token_end":8,"label":"ALGO","answer":"accept"},{"start":103,"end":133,"token_start":20,"token_end":22,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Namely, given a dictionary $F = \\{f_1, ..., f_M \\}$ of functions, we look for an optimal aggregation algorithm that writes $\\tilde f = \\sum_{j=1}^M \\theta_j f_j$ with as many zero coefficients $\\theta_j$ as possible.","_input_hash":-990215100,"_task_hash":818776701,"tokens":[{"text":"Namely","start":0,"end":6,"id":0},{"text":",","start":6,"end":7,"id":1},{"text":"given","start":8,"end":13,"id":2},{"text":"a","start":14,"end":15,"id":3},{"text":"dictionary","start":16,"end":26,"id":4},{"text":"$","start":27,"end":28,"id":5},{"text":"F","start":28,"end":29,"id":6},{"text":"=","start":30,"end":31,"id":7},{"text":"\\{f_1","start":32,"end":37,"id":8},{"text":",","start":37,"end":38,"id":9},{"text":"...","start":39,"end":42,"id":10},{"text":",","start":42,"end":43,"id":11},{"text":"f_M","start":44,"end":47,"id":12},{"text":"\\}$","start":48,"end":51,"id":13},{"text":"of","start":52,"end":54,"id":14},{"text":"functions","start":55,"end":64,"id":15},{"text":",","start":64,"end":65,"id":16},{"text":"we","start":66,"end":68,"id":17},{"text":"look","start":69,"end":73,"id":18},{"text":"for","start":74,"end":77,"id":19},{"text":"an","start":78,"end":80,"id":20},{"text":"optimal","start":81,"end":88,"id":21},{"text":"aggregation","start":89,"end":100,"id":22},{"text":"algorithm","start":101,"end":110,"id":23},{"text":"that","start":111,"end":115,"id":24},{"text":"writes","start":116,"end":122,"id":25},{"text":"$","start":123,"end":124,"id":26},{"text":"\\tilde","start":124,"end":130,"id":27},{"text":"f","start":131,"end":132,"id":28},{"text":"=","start":133,"end":134,"id":29},{"text":"\\sum_{j=1}^M","start":135,"end":147,"id":30},{"text":"\\theta_j","start":148,"end":156,"id":31},{"text":"f_j$","start":157,"end":161,"id":32},{"text":"with","start":162,"end":166,"id":33},{"text":"as","start":167,"end":169,"id":34},{"text":"many","start":170,"end":174,"id":35},{"text":"zero","start":175,"end":179,"id":36},{"text":"coefficients","start":180,"end":192,"id":37},{"text":"$","start":193,"end":194,"id":38},{"text":"\\theta_j$","start":194,"end":203,"id":39},{"text":"as","start":204,"end":206,"id":40},{"text":"possible","start":207,"end":215,"id":41},{"text":".","start":215,"end":216,"id":42}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"This paper examines from an experimental perspective random forests, the increasingly used statistical method for classification and regression problems introduced by Leo Breiman in 2001.","_input_hash":-308113262,"_task_hash":-1920693465,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"paper","start":5,"end":10,"id":1},{"text":"examines","start":11,"end":19,"id":2},{"text":"from","start":20,"end":24,"id":3},{"text":"an","start":25,"end":27,"id":4},{"text":"experimental","start":28,"end":40,"id":5},{"text":"perspective","start":41,"end":52,"id":6},{"text":"random","start":53,"end":59,"id":7},{"text":"forests","start":60,"end":67,"id":8},{"text":",","start":67,"end":68,"id":9},{"text":"the","start":69,"end":72,"id":10},{"text":"increasingly","start":73,"end":85,"id":11},{"text":"used","start":86,"end":90,"id":12},{"text":"statistical","start":91,"end":102,"id":13},{"text":"method","start":103,"end":109,"id":14},{"text":"for","start":110,"end":113,"id":15},{"text":"classification","start":114,"end":128,"id":16},{"text":"and","start":129,"end":132,"id":17},{"text":"regression","start":133,"end":143,"id":18},{"text":"problems","start":144,"end":152,"id":19},{"text":"introduced","start":153,"end":163,"id":20},{"text":"by","start":164,"end":166,"id":21},{"text":"Leo","start":167,"end":170,"id":22},{"text":"Breiman","start":171,"end":178,"id":23},{"text":"in","start":179,"end":181,"id":24},{"text":"2001","start":182,"end":186,"id":25},{"text":".","start":186,"end":187,"id":26}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":53,"end":67,"token_start":7,"token_end":8,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"The distance metric plays an important role in nearest neighbor (NN) classification.","_input_hash":533487841,"_task_hash":-1008111325,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"distance","start":4,"end":12,"id":1},{"text":"metric","start":13,"end":19,"id":2},{"text":"plays","start":20,"end":25,"id":3},{"text":"an","start":26,"end":28,"id":4},{"text":"important","start":29,"end":38,"id":5},{"text":"role","start":39,"end":43,"id":6},{"text":"in","start":44,"end":46,"id":7},{"text":"nearest","start":47,"end":54,"id":8},{"text":"neighbor","start":55,"end":63,"id":9},{"text":"(","start":64,"end":65,"id":10},{"text":"NN","start":65,"end":67,"id":11},{"text":")","start":67,"end":68,"id":12},{"text":"classification","start":69,"end":83,"id":13},{"text":".","start":83,"end":84,"id":14}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":47,"end":63,"token_start":8,"token_end":9,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Furthermore, we provide oracle results which yield asymptotic optimality of our estimator for high dimensional but sparse additive models.","_input_hash":529858503,"_task_hash":-1090201922,"tokens":[{"text":"Furthermore","start":0,"end":11,"id":0},{"text":",","start":11,"end":12,"id":1},{"text":"we","start":13,"end":15,"id":2},{"text":"provide","start":16,"end":23,"id":3},{"text":"oracle","start":24,"end":30,"id":4},{"text":"results","start":31,"end":38,"id":5},{"text":"which","start":39,"end":44,"id":6},{"text":"yield","start":45,"end":50,"id":7},{"text":"asymptotic","start":51,"end":61,"id":8},{"text":"optimality","start":62,"end":72,"id":9},{"text":"of","start":73,"end":75,"id":10},{"text":"our","start":76,"end":79,"id":11},{"text":"estimator","start":80,"end":89,"id":12},{"text":"for","start":90,"end":93,"id":13},{"text":"high","start":94,"end":98,"id":14},{"text":"dimensional","start":99,"end":110,"id":15},{"text":"but","start":111,"end":114,"id":16},{"text":"sparse","start":115,"end":121,"id":17},{"text":"additive","start":122,"end":130,"id":18},{"text":"models","start":131,"end":137,"id":19},{"text":".","start":137,"end":138,"id":20}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":122,"end":137,"token_start":18,"token_end":19,"label":"ALGO","answer":"reject"}],"answer":"reject"}
{"text":"gradient_descent|NOUN","word":"gradient descent","sense":"NOUN","meta":{"score":0.7688000202,"sense":"NOUN"},"_input_hash":1773996794,"_task_hash":744111126,"_session_id":null,"_view_id":"html","answer":"accept","spans":[],"tokens":[{"text":"gradient_descent|NOUN","start":0,"end":21,"id":0}]}
{"text":"We describe a new method for visualizing topics, the distributions over terms that are automatically extracted from large text corpora using latent variable models.","_input_hash":1236677145,"_task_hash":-1645557954,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"describe","start":3,"end":11,"id":1},{"text":"a","start":12,"end":13,"id":2},{"text":"new","start":14,"end":17,"id":3},{"text":"method","start":18,"end":24,"id":4},{"text":"for","start":25,"end":28,"id":5},{"text":"visualizing","start":29,"end":40,"id":6},{"text":"topics","start":41,"end":47,"id":7},{"text":",","start":47,"end":48,"id":8},{"text":"the","start":49,"end":52,"id":9},{"text":"distributions","start":53,"end":66,"id":10},{"text":"over","start":67,"end":71,"id":11},{"text":"terms","start":72,"end":77,"id":12},{"text":"that","start":78,"end":82,"id":13},{"text":"are","start":83,"end":86,"id":14},{"text":"automatically","start":87,"end":100,"id":15},{"text":"extracted","start":101,"end":110,"id":16},{"text":"from","start":111,"end":115,"id":17},{"text":"large","start":116,"end":121,"id":18},{"text":"text","start":122,"end":126,"id":19},{"text":"corpora","start":127,"end":134,"id":20},{"text":"using","start":135,"end":140,"id":21},{"text":"latent","start":141,"end":147,"id":22},{"text":"variable","start":148,"end":156,"id":23},{"text":"models","start":157,"end":163,"id":24},{"text":".","start":163,"end":164,"id":25}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":141,"end":163,"token_start":22,"token_end":24,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Given a corpus of documents, a posterior inference algorithm finds an approximation to a posterior distribution over trees, topics and allocations of words to levels of the tree.","_input_hash":-1107644007,"_task_hash":1722989748,"tokens":[{"text":"Given","start":0,"end":5,"id":0},{"text":"a","start":6,"end":7,"id":1},{"text":"corpus","start":8,"end":14,"id":2},{"text":"of","start":15,"end":17,"id":3},{"text":"documents","start":18,"end":27,"id":4},{"text":",","start":27,"end":28,"id":5},{"text":"a","start":29,"end":30,"id":6},{"text":"posterior","start":31,"end":40,"id":7},{"text":"inference","start":41,"end":50,"id":8},{"text":"algorithm","start":51,"end":60,"id":9},{"text":"finds","start":61,"end":66,"id":10},{"text":"an","start":67,"end":69,"id":11},{"text":"approximation","start":70,"end":83,"id":12},{"text":"to","start":84,"end":86,"id":13},{"text":"a","start":87,"end":88,"id":14},{"text":"posterior","start":89,"end":98,"id":15},{"text":"distribution","start":99,"end":111,"id":16},{"text":"over","start":112,"end":116,"id":17},{"text":"trees","start":117,"end":122,"id":18},{"text":",","start":122,"end":123,"id":19},{"text":"topics","start":124,"end":130,"id":20},{"text":"and","start":131,"end":134,"id":21},{"text":"allocations","start":135,"end":146,"id":22},{"text":"of","start":147,"end":149,"id":23},{"text":"words","start":150,"end":155,"id":24},{"text":"to","start":156,"end":158,"id":25},{"text":"levels","start":159,"end":165,"id":26},{"text":"of","start":166,"end":168,"id":27},{"text":"the","start":169,"end":172,"id":28},{"text":"tree","start":173,"end":177,"id":29},{"text":".","start":177,"end":178,"id":30}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"computations|NOUN","word":"computations","sense":"NOUN","meta":{"score":0.7953000069,"sense":"NOUN"},"_input_hash":-231334158,"_task_hash":381895043,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"computations|NOUN","start":0,"end":17,"id":0}]}
{"text":"The idea is to select causal hypotheses for which the conditional density of every variable, given its causes, becomes smooth.","_input_hash":1327662163,"_task_hash":2094304353,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"idea","start":4,"end":8,"id":1},{"text":"is","start":9,"end":11,"id":2},{"text":"to","start":12,"end":14,"id":3},{"text":"select","start":15,"end":21,"id":4},{"text":"causal","start":22,"end":28,"id":5},{"text":"hypotheses","start":29,"end":39,"id":6},{"text":"for","start":40,"end":43,"id":7},{"text":"which","start":44,"end":49,"id":8},{"text":"the","start":50,"end":53,"id":9},{"text":"conditional","start":54,"end":65,"id":10},{"text":"density","start":66,"end":73,"id":11},{"text":"of","start":74,"end":76,"id":12},{"text":"every","start":77,"end":82,"id":13},{"text":"variable","start":83,"end":91,"id":14},{"text":",","start":91,"end":92,"id":15},{"text":"given","start":93,"end":98,"id":16},{"text":"its","start":99,"end":102,"id":17},{"text":"causes","start":103,"end":109,"id":18},{"text":",","start":109,"end":110,"id":19},{"text":"becomes","start":111,"end":118,"id":20},{"text":"smooth","start":119,"end":125,"id":21},{"text":".","start":125,"end":126,"id":22}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"We present in this work a new family of kernels to compare positive measures on arbitrary spaces $\\Xcal$ endowed with a positive kernel $\\kappa$, which translates naturally into kernels between histograms or clouds of points.","_input_hash":1811341578,"_task_hash":1470975420,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"present","start":3,"end":10,"id":1},{"text":"in","start":11,"end":13,"id":2},{"text":"this","start":14,"end":18,"id":3},{"text":"work","start":19,"end":23,"id":4},{"text":"a","start":24,"end":25,"id":5},{"text":"new","start":26,"end":29,"id":6},{"text":"family","start":30,"end":36,"id":7},{"text":"of","start":37,"end":39,"id":8},{"text":"kernels","start":40,"end":47,"id":9},{"text":"to","start":48,"end":50,"id":10},{"text":"compare","start":51,"end":58,"id":11},{"text":"positive","start":59,"end":67,"id":12},{"text":"measures","start":68,"end":76,"id":13},{"text":"on","start":77,"end":79,"id":14},{"text":"arbitrary","start":80,"end":89,"id":15},{"text":"spaces","start":90,"end":96,"id":16},{"text":"$","start":97,"end":98,"id":17},{"text":"\\Xcal$","start":98,"end":104,"id":18},{"text":"endowed","start":105,"end":112,"id":19},{"text":"with","start":113,"end":117,"id":20},{"text":"a","start":118,"end":119,"id":21},{"text":"positive","start":120,"end":128,"id":22},{"text":"kernel","start":129,"end":135,"id":23},{"text":"$","start":136,"end":137,"id":24},{"text":"\\kappa$","start":137,"end":144,"id":25},{"text":",","start":144,"end":145,"id":26},{"text":"which","start":146,"end":151,"id":27},{"text":"translates","start":152,"end":162,"id":28},{"text":"naturally","start":163,"end":172,"id":29},{"text":"into","start":173,"end":177,"id":30},{"text":"kernels","start":178,"end":185,"id":31},{"text":"between","start":186,"end":193,"id":32},{"text":"histograms","start":194,"end":204,"id":33},{"text":"or","start":205,"end":207,"id":34},{"text":"clouds","start":208,"end":214,"id":35},{"text":"of","start":215,"end":217,"id":36},{"text":"points","start":218,"end":224,"id":37},{"text":".","start":224,"end":225,"id":38}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"Moreover, the computational complexity of the training algorithm is reduced from O(n^3) to O(n^2) using preconditioned conjugate gradient, where n is the combined number of labeled and unlabeled examples.","_input_hash":-83149934,"_task_hash":-578274512,"tokens":[{"text":"Moreover","start":0,"end":8,"id":0},{"text":",","start":8,"end":9,"id":1},{"text":"the","start":10,"end":13,"id":2},{"text":"computational","start":14,"end":27,"id":3},{"text":"complexity","start":28,"end":38,"id":4},{"text":"of","start":39,"end":41,"id":5},{"text":"the","start":42,"end":45,"id":6},{"text":"training","start":46,"end":54,"id":7},{"text":"algorithm","start":55,"end":64,"id":8},{"text":"is","start":65,"end":67,"id":9},{"text":"reduced","start":68,"end":75,"id":10},{"text":"from","start":76,"end":80,"id":11},{"text":"O(n^3","start":81,"end":86,"id":12},{"text":")","start":86,"end":87,"id":13},{"text":"to","start":88,"end":90,"id":14},{"text":"O(n^2","start":91,"end":96,"id":15},{"text":")","start":96,"end":97,"id":16},{"text":"using","start":98,"end":103,"id":17},{"text":"preconditioned","start":104,"end":118,"id":18},{"text":"conjugate","start":119,"end":128,"id":19},{"text":"gradient","start":129,"end":137,"id":20},{"text":",","start":137,"end":138,"id":21},{"text":"where","start":139,"end":144,"id":22},{"text":"n","start":145,"end":146,"id":23},{"text":"is","start":147,"end":149,"id":24},{"text":"the","start":150,"end":153,"id":25},{"text":"combined","start":154,"end":162,"id":26},{"text":"number","start":163,"end":169,"id":27},{"text":"of","start":170,"end":172,"id":28},{"text":"labeled","start":173,"end":180,"id":29},{"text":"and","start":181,"end":184,"id":30},{"text":"unlabeled","start":185,"end":194,"id":31},{"text":"examples","start":195,"end":203,"id":32},{"text":".","start":203,"end":204,"id":33}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"i.e. with high probability, all the true edges will be included in the selected model even when the graph size diverges with the sample size.","_input_hash":-1163250996,"_task_hash":-772001038,"tokens":[{"text":"i.e.","start":0,"end":4,"id":0},{"text":"with","start":5,"end":9,"id":1},{"text":"high","start":10,"end":14,"id":2},{"text":"probability","start":15,"end":26,"id":3},{"text":",","start":26,"end":27,"id":4},{"text":"all","start":28,"end":31,"id":5},{"text":"the","start":32,"end":35,"id":6},{"text":"true","start":36,"end":40,"id":7},{"text":"edges","start":41,"end":46,"id":8},{"text":"will","start":47,"end":51,"id":9},{"text":"be","start":52,"end":54,"id":10},{"text":"included","start":55,"end":63,"id":11},{"text":"in","start":64,"end":66,"id":12},{"text":"the","start":67,"end":70,"id":13},{"text":"selected","start":71,"end":79,"id":14},{"text":"model","start":80,"end":85,"id":15},{"text":"even","start":86,"end":90,"id":16},{"text":"when","start":91,"end":95,"id":17},{"text":"the","start":96,"end":99,"id":18},{"text":"graph","start":100,"end":105,"id":19},{"text":"size","start":106,"end":110,"id":20},{"text":"diverges","start":111,"end":119,"id":21},{"text":"with","start":120,"end":124,"id":22},{"text":"the","start":125,"end":128,"id":23},{"text":"sample","start":129,"end":135,"id":24},{"text":"size","start":136,"end":140,"id":25},{"text":".","start":140,"end":141,"id":26}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"We study graph estimation and density estimation in high dimensions, using a family of density estimators based on forest structured undirected graphical models.","_input_hash":2048189827,"_task_hash":397530579,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"study","start":3,"end":8,"id":1},{"text":"graph","start":9,"end":14,"id":2},{"text":"estimation","start":15,"end":25,"id":3},{"text":"and","start":26,"end":29,"id":4},{"text":"density","start":30,"end":37,"id":5},{"text":"estimation","start":38,"end":48,"id":6},{"text":"in","start":49,"end":51,"id":7},{"text":"high","start":52,"end":56,"id":8},{"text":"dimensions","start":57,"end":67,"id":9},{"text":",","start":67,"end":68,"id":10},{"text":"using","start":69,"end":74,"id":11},{"text":"a","start":75,"end":76,"id":12},{"text":"family","start":77,"end":83,"id":13},{"text":"of","start":84,"end":86,"id":14},{"text":"density","start":87,"end":94,"id":15},{"text":"estimators","start":95,"end":105,"id":16},{"text":"based","start":106,"end":111,"id":17},{"text":"on","start":112,"end":114,"id":18},{"text":"forest","start":115,"end":121,"id":19},{"text":"structured","start":122,"end":132,"id":20},{"text":"undirected","start":133,"end":143,"id":21},{"text":"graphical","start":144,"end":153,"id":22},{"text":"models","start":154,"end":160,"id":23},{"text":".","start":160,"end":161,"id":24}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"The PVM selects a relatively small number of representative points which can then be used for classification.","_input_hash":1942868972,"_task_hash":784115329,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"PVM","start":4,"end":7,"id":1},{"text":"selects","start":8,"end":15,"id":2},{"text":"a","start":16,"end":17,"id":3},{"text":"relatively","start":18,"end":28,"id":4},{"text":"small","start":29,"end":34,"id":5},{"text":"number","start":35,"end":41,"id":6},{"text":"of","start":42,"end":44,"id":7},{"text":"representative","start":45,"end":59,"id":8},{"text":"points","start":60,"end":66,"id":9},{"text":"which","start":67,"end":72,"id":10},{"text":"can","start":73,"end":76,"id":11},{"text":"then","start":77,"end":81,"id":12},{"text":"be","start":82,"end":84,"id":13},{"text":"used","start":85,"end":89,"id":14},{"text":"for","start":90,"end":93,"id":15},{"text":"classification","start":94,"end":108,"id":16},{"text":".","start":108,"end":109,"id":17}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Unfortunately, such decision rules are hardly accessible to humans and cannot easily be used to gain insights about the application domain.","_input_hash":-2093767764,"_task_hash":627266201,"tokens":[{"text":"Unfortunately","start":0,"end":13,"id":0},{"text":",","start":13,"end":14,"id":1},{"text":"such","start":15,"end":19,"id":2},{"text":"decision","start":20,"end":28,"id":3},{"text":"rules","start":29,"end":34,"id":4},{"text":"are","start":35,"end":38,"id":5},{"text":"hardly","start":39,"end":45,"id":6},{"text":"accessible","start":46,"end":56,"id":7},{"text":"to","start":57,"end":59,"id":8},{"text":"humans","start":60,"end":66,"id":9},{"text":"and","start":67,"end":70,"id":10},{"text":"can","start":71,"end":74,"id":11},{"text":"not","start":74,"end":77,"id":12},{"text":"easily","start":78,"end":84,"id":13},{"text":"be","start":85,"end":87,"id":14},{"text":"used","start":88,"end":92,"id":15},{"text":"to","start":93,"end":95,"id":16},{"text":"gain","start":96,"end":100,"id":17},{"text":"insights","start":101,"end":109,"id":18},{"text":"about","start":110,"end":115,"id":19},{"text":"the","start":116,"end":119,"id":20},{"text":"application","start":120,"end":131,"id":21},{"text":"domain","start":132,"end":138,"id":22},{"text":".","start":138,"end":139,"id":23}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"We empirically demonstrate the relevance of the method on simulated and real data, where it performs at least as well as existing methods while being faster.","_input_hash":-1043606083,"_task_hash":-841274767,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"empirically","start":3,"end":14,"id":1},{"text":"demonstrate","start":15,"end":26,"id":2},{"text":"the","start":27,"end":30,"id":3},{"text":"relevance","start":31,"end":40,"id":4},{"text":"of","start":41,"end":43,"id":5},{"text":"the","start":44,"end":47,"id":6},{"text":"method","start":48,"end":54,"id":7},{"text":"on","start":55,"end":57,"id":8},{"text":"simulated","start":58,"end":67,"id":9},{"text":"and","start":68,"end":71,"id":10},{"text":"real","start":72,"end":76,"id":11},{"text":"data","start":77,"end":81,"id":12},{"text":",","start":81,"end":82,"id":13},{"text":"where","start":83,"end":88,"id":14},{"text":"it","start":89,"end":91,"id":15},{"text":"performs","start":92,"end":100,"id":16},{"text":"at","start":101,"end":103,"id":17},{"text":"least","start":104,"end":109,"id":18},{"text":"as","start":110,"end":112,"id":19},{"text":"well","start":113,"end":117,"id":20},{"text":"as","start":118,"end":120,"id":21},{"text":"existing","start":121,"end":129,"id":22},{"text":"methods","start":130,"end":137,"id":23},{"text":"while","start":138,"end":143,"id":24},{"text":"being","start":144,"end":149,"id":25},{"text":"faster","start":150,"end":156,"id":26},{"text":".","start":156,"end":157,"id":27}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Moreover, this causes LLE to converge to a linear projection of the input, as opposed to its non-linear embedding goal.","_input_hash":1855610389,"_task_hash":1141540989,"tokens":[{"text":"Moreover","start":0,"end":8,"id":0},{"text":",","start":8,"end":9,"id":1},{"text":"this","start":10,"end":14,"id":2},{"text":"causes","start":15,"end":21,"id":3},{"text":"LLE","start":22,"end":25,"id":4},{"text":"to","start":26,"end":28,"id":5},{"text":"converge","start":29,"end":37,"id":6},{"text":"to","start":38,"end":40,"id":7},{"text":"a","start":41,"end":42,"id":8},{"text":"linear","start":43,"end":49,"id":9},{"text":"projection","start":50,"end":60,"id":10},{"text":"of","start":61,"end":63,"id":11},{"text":"the","start":64,"end":67,"id":12},{"text":"input","start":68,"end":73,"id":13},{"text":",","start":73,"end":74,"id":14},{"text":"as","start":75,"end":77,"id":15},{"text":"opposed","start":78,"end":85,"id":16},{"text":"to","start":86,"end":88,"id":17},{"text":"its","start":89,"end":92,"id":18},{"text":"non","start":93,"end":96,"id":19},{"text":"-","start":96,"end":97,"id":20},{"text":"linear","start":97,"end":103,"id":21},{"text":"embedding","start":104,"end":113,"id":22},{"text":"goal","start":114,"end":118,"id":23},{"text":".","start":118,"end":119,"id":24}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"In the general setting, we prove sanity-check bounds in the spirit of \\cite{KR99} \\textquotedblleft\\textit{bounds showing that the worst-case error of this estimate is not much worse that of training error estimate} \\textquotedblright .","_input_hash":-1841768082,"_task_hash":1562831002,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"the","start":3,"end":6,"id":1},{"text":"general","start":7,"end":14,"id":2},{"text":"setting","start":15,"end":22,"id":3},{"text":",","start":22,"end":23,"id":4},{"text":"we","start":24,"end":26,"id":5},{"text":"prove","start":27,"end":32,"id":6},{"text":"sanity","start":33,"end":39,"id":7},{"text":"-","start":39,"end":40,"id":8},{"text":"check","start":40,"end":45,"id":9},{"text":"bounds","start":46,"end":52,"id":10},{"text":"in","start":53,"end":55,"id":11},{"text":"the","start":56,"end":59,"id":12},{"text":"spirit","start":60,"end":66,"id":13},{"text":"of","start":67,"end":69,"id":14},{"text":"\\cite{KR99","start":70,"end":80,"id":15},{"text":"}","start":80,"end":81,"id":16},{"text":"\\textquotedblleft\\textit{bounds","start":82,"end":113,"id":17},{"text":"showing","start":114,"end":121,"id":18},{"text":"that","start":122,"end":126,"id":19},{"text":"the","start":127,"end":130,"id":20},{"text":"worst","start":131,"end":136,"id":21},{"text":"-","start":136,"end":137,"id":22},{"text":"case","start":137,"end":141,"id":23},{"text":"error","start":142,"end":147,"id":24},{"text":"of","start":148,"end":150,"id":25},{"text":"this","start":151,"end":155,"id":26},{"text":"estimate","start":156,"end":164,"id":27},{"text":"is","start":165,"end":167,"id":28},{"text":"not","start":168,"end":171,"id":29},{"text":"much","start":172,"end":176,"id":30},{"text":"worse","start":177,"end":182,"id":31},{"text":"that","start":183,"end":187,"id":32},{"text":"of","start":188,"end":190,"id":33},{"text":"training","start":191,"end":199,"id":34},{"text":"error","start":200,"end":205,"id":35},{"text":"estimate","start":206,"end":214,"id":36},{"text":"}","start":214,"end":215,"id":37},{"text":"\\textquotedblright","start":216,"end":234,"id":38},{"text":".","start":235,"end":236,"id":39}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"This polynomial rooting can be performed algebraically, and thus at low cost, at each iteration.","_input_hash":2146141936,"_task_hash":94118176,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"polynomial","start":5,"end":15,"id":1},{"text":"rooting","start":16,"end":23,"id":2},{"text":"can","start":24,"end":27,"id":3},{"text":"be","start":28,"end":30,"id":4},{"text":"performed","start":31,"end":40,"id":5},{"text":"algebraically","start":41,"end":54,"id":6},{"text":",","start":54,"end":55,"id":7},{"text":"and","start":56,"end":59,"id":8},{"text":"thus","start":60,"end":64,"id":9},{"text":"at","start":65,"end":67,"id":10},{"text":"low","start":68,"end":71,"id":11},{"text":"cost","start":72,"end":76,"id":12},{"text":",","start":76,"end":77,"id":13},{"text":"at","start":78,"end":80,"id":14},{"text":"each","start":81,"end":85,"id":15},{"text":"iteration","start":86,"end":95,"id":16},{"text":".","start":95,"end":96,"id":17}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We compute the probability distribution of the length of the current ``run,'' or time since the last changepoint, using a simple message-passing algorithm.","_input_hash":-2035327106,"_task_hash":-1898219252,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"compute","start":3,"end":10,"id":1},{"text":"the","start":11,"end":14,"id":2},{"text":"probability","start":15,"end":26,"id":3},{"text":"distribution","start":27,"end":39,"id":4},{"text":"of","start":40,"end":42,"id":5},{"text":"the","start":43,"end":46,"id":6},{"text":"length","start":47,"end":53,"id":7},{"text":"of","start":54,"end":56,"id":8},{"text":"the","start":57,"end":60,"id":9},{"text":"current","start":61,"end":68,"id":10},{"text":"`","start":69,"end":70,"id":11},{"text":"`","start":70,"end":71,"id":12},{"text":"run","start":71,"end":74,"id":13},{"text":",","start":74,"end":75,"id":14},{"text":"'","start":75,"end":76,"id":15},{"text":"'","start":76,"end":77,"id":16},{"text":"or","start":78,"end":80,"id":17},{"text":"time","start":81,"end":85,"id":18},{"text":"since","start":86,"end":91,"id":19},{"text":"the","start":92,"end":95,"id":20},{"text":"last","start":96,"end":100,"id":21},{"text":"changepoint","start":101,"end":112,"id":22},{"text":",","start":112,"end":113,"id":23},{"text":"using","start":114,"end":119,"id":24},{"text":"a","start":120,"end":121,"id":25},{"text":"simple","start":122,"end":128,"id":26},{"text":"message","start":129,"end":136,"id":27},{"text":"-","start":136,"end":137,"id":28},{"text":"passing","start":137,"end":144,"id":29},{"text":"algorithm","start":145,"end":154,"id":30},{"text":".","start":154,"end":155,"id":31}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":137,"end":154,"token_start":29,"token_end":30,"label":"ALGO","answer":"reject"}],"answer":"reject"}
{"text":"neocortex|NOUN","word":"neocortex","sense":"NOUN","meta":{"score":0.7925999761,"sense":"NOUN"},"_input_hash":903284907,"_task_hash":2071432346,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"neocortex|NOUN","start":0,"end":14,"id":0}]}
{"text":"dynamic_programming|NOUN","word":"dynamic programming","sense":"NOUN","meta":{"score":0.7954000235,"sense":"NOUN"},"_input_hash":1898988806,"_task_hash":-751709129,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"dynamic_programming|NOUN","start":0,"end":24,"id":0}]}
{"text":"mathematical_methods|NOUN","word":"mathematical methods","sense":"NOUN","meta":{"score":0.7922999859,"sense":"NOUN"},"_input_hash":-685186284,"_task_hash":767042026,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"mathematical_methods|NOUN","start":0,"end":25,"id":0}]}
{"text":"time_complexity|NOUN","word":"time complexity","sense":"NOUN","meta":{"score":0.7764999866,"sense":"NOUN"},"_input_hash":1861635269,"_task_hash":-1014444453,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"time_complexity|NOUN","start":0,"end":20,"id":0}]}
{"text":"A challenging problem in estimating high-dimensional graphical models is to choose the regularization parameter in a data-dependent way.","_input_hash":-429533200,"_task_hash":-17330569,"tokens":[{"text":"A","start":0,"end":1,"id":0},{"text":"challenging","start":2,"end":13,"id":1},{"text":"problem","start":14,"end":21,"id":2},{"text":"in","start":22,"end":24,"id":3},{"text":"estimating","start":25,"end":35,"id":4},{"text":"high","start":36,"end":40,"id":5},{"text":"-","start":40,"end":41,"id":6},{"text":"dimensional","start":41,"end":52,"id":7},{"text":"graphical","start":53,"end":62,"id":8},{"text":"models","start":63,"end":69,"id":9},{"text":"is","start":70,"end":72,"id":10},{"text":"to","start":73,"end":75,"id":11},{"text":"choose","start":76,"end":82,"id":12},{"text":"the","start":83,"end":86,"id":13},{"text":"regularization","start":87,"end":101,"id":14},{"text":"parameter","start":102,"end":111,"id":15},{"text":"in","start":112,"end":114,"id":16},{"text":"a","start":115,"end":116,"id":17},{"text":"data","start":117,"end":121,"id":18},{"text":"-","start":121,"end":122,"id":19},{"text":"dependent","start":122,"end":131,"id":20},{"text":"way","start":132,"end":135,"id":21},{"text":".","start":135,"end":136,"id":22}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"The Minimum Description Length (MDL) principle states that the optimal model for a given data set is that which compresses it best.","_input_hash":-1032037555,"_task_hash":83716215,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"Minimum","start":4,"end":11,"id":1},{"text":"Description","start":12,"end":23,"id":2},{"text":"Length","start":24,"end":30,"id":3},{"text":"(","start":31,"end":32,"id":4},{"text":"MDL","start":32,"end":35,"id":5},{"text":")","start":35,"end":36,"id":6},{"text":"principle","start":37,"end":46,"id":7},{"text":"states","start":47,"end":53,"id":8},{"text":"that","start":54,"end":58,"id":9},{"text":"the","start":59,"end":62,"id":10},{"text":"optimal","start":63,"end":70,"id":11},{"text":"model","start":71,"end":76,"id":12},{"text":"for","start":77,"end":80,"id":13},{"text":"a","start":81,"end":82,"id":14},{"text":"given","start":83,"end":88,"id":15},{"text":"data","start":89,"end":93,"id":16},{"text":"set","start":94,"end":97,"id":17},{"text":"is","start":98,"end":100,"id":18},{"text":"that","start":101,"end":105,"id":19},{"text":"which","start":106,"end":111,"id":20},{"text":"compresses","start":112,"end":122,"id":21},{"text":"it","start":123,"end":125,"id":22},{"text":"best","start":126,"end":130,"id":23},{"text":".","start":130,"end":131,"id":24}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"supervised_learning|NOUN","word":"supervised learning","sense":"NOUN","meta":{"score":0.7681999803,"sense":"NOUN"},"_input_hash":632551848,"_task_hash":-1680525600,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"supervised_learning|NOUN","start":0,"end":24,"id":0}]}
{"text":"This paper proposes an original approach to cluster multi-component data sets, including an estimation of the number of clusters.","_input_hash":-974865813,"_task_hash":1105404466,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"paper","start":5,"end":10,"id":1},{"text":"proposes","start":11,"end":19,"id":2},{"text":"an","start":20,"end":22,"id":3},{"text":"original","start":23,"end":31,"id":4},{"text":"approach","start":32,"end":40,"id":5},{"text":"to","start":41,"end":43,"id":6},{"text":"cluster","start":44,"end":51,"id":7},{"text":"multi","start":52,"end":57,"id":8},{"text":"-","start":57,"end":58,"id":9},{"text":"component","start":58,"end":67,"id":10},{"text":"data","start":68,"end":72,"id":11},{"text":"sets","start":73,"end":77,"id":12},{"text":",","start":77,"end":78,"id":13},{"text":"including","start":79,"end":88,"id":14},{"text":"an","start":89,"end":91,"id":15},{"text":"estimation","start":92,"end":102,"id":16},{"text":"of","start":103,"end":105,"id":17},{"text":"the","start":106,"end":109,"id":18},{"text":"number","start":110,"end":116,"id":19},{"text":"of","start":117,"end":119,"id":20},{"text":"clusters","start":120,"end":128,"id":21},{"text":".","start":128,"end":129,"id":22}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We also present two novel dimension-reduction techniques that attempt to minimize the suggested measure, and compare the results of these techniques to the results of existing algorithms.","_input_hash":-589856197,"_task_hash":789082106,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"also","start":3,"end":7,"id":1},{"text":"present","start":8,"end":15,"id":2},{"text":"two","start":16,"end":19,"id":3},{"text":"novel","start":20,"end":25,"id":4},{"text":"dimension","start":26,"end":35,"id":5},{"text":"-","start":35,"end":36,"id":6},{"text":"reduction","start":36,"end":45,"id":7},{"text":"techniques","start":46,"end":56,"id":8},{"text":"that","start":57,"end":61,"id":9},{"text":"attempt","start":62,"end":69,"id":10},{"text":"to","start":70,"end":72,"id":11},{"text":"minimize","start":73,"end":81,"id":12},{"text":"the","start":82,"end":85,"id":13},{"text":"suggested","start":86,"end":95,"id":14},{"text":"measure","start":96,"end":103,"id":15},{"text":",","start":103,"end":104,"id":16},{"text":"and","start":105,"end":108,"id":17},{"text":"compare","start":109,"end":116,"id":18},{"text":"the","start":117,"end":120,"id":19},{"text":"results","start":121,"end":128,"id":20},{"text":"of","start":129,"end":131,"id":21},{"text":"these","start":132,"end":137,"id":22},{"text":"techniques","start":138,"end":148,"id":23},{"text":"to","start":149,"end":151,"id":24},{"text":"the","start":152,"end":155,"id":25},{"text":"results","start":156,"end":163,"id":26},{"text":"of","start":164,"end":166,"id":27},{"text":"existing","start":167,"end":175,"id":28},{"text":"algorithms","start":176,"end":186,"id":29},{"text":".","start":186,"end":187,"id":30}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Some results are derived for evaluating the false positive rate of our cluster detection algorithm, with the help of approximations relevant in Euclidean spaces.","_input_hash":809253512,"_task_hash":2043502754,"tokens":[{"text":"Some","start":0,"end":4,"id":0},{"text":"results","start":5,"end":12,"id":1},{"text":"are","start":13,"end":16,"id":2},{"text":"derived","start":17,"end":24,"id":3},{"text":"for","start":25,"end":28,"id":4},{"text":"evaluating","start":29,"end":39,"id":5},{"text":"the","start":40,"end":43,"id":6},{"text":"false","start":44,"end":49,"id":7},{"text":"positive","start":50,"end":58,"id":8},{"text":"rate","start":59,"end":63,"id":9},{"text":"of","start":64,"end":66,"id":10},{"text":"our","start":67,"end":70,"id":11},{"text":"cluster","start":71,"end":78,"id":12},{"text":"detection","start":79,"end":88,"id":13},{"text":"algorithm","start":89,"end":98,"id":14},{"text":",","start":98,"end":99,"id":15},{"text":"with","start":100,"end":104,"id":16},{"text":"the","start":105,"end":108,"id":17},{"text":"help","start":109,"end":113,"id":18},{"text":"of","start":114,"end":116,"id":19},{"text":"approximations","start":117,"end":131,"id":20},{"text":"relevant","start":132,"end":140,"id":21},{"text":"in","start":141,"end":143,"id":22},{"text":"Euclidean","start":144,"end":153,"id":23},{"text":"spaces","start":154,"end":160,"id":24},{"text":".","start":160,"end":161,"id":25}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We are interested in the actual clustering, not only in the costs of the solution.","_input_hash":-137369651,"_task_hash":256525834,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"are","start":3,"end":6,"id":1},{"text":"interested","start":7,"end":17,"id":2},{"text":"in","start":18,"end":20,"id":3},{"text":"the","start":21,"end":24,"id":4},{"text":"actual","start":25,"end":31,"id":5},{"text":"clustering","start":32,"end":42,"id":6},{"text":",","start":42,"end":43,"id":7},{"text":"not","start":44,"end":47,"id":8},{"text":"only","start":48,"end":52,"id":9},{"text":"in","start":53,"end":55,"id":10},{"text":"the","start":56,"end":59,"id":11},{"text":"costs","start":60,"end":65,"id":12},{"text":"of","start":66,"end":68,"id":13},{"text":"the","start":69,"end":72,"id":14},{"text":"solution","start":73,"end":81,"id":15},{"text":".","start":81,"end":82,"id":16}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"re","meta":{"score":0},"_input_hash":-2018819473,"_task_hash":1617930389,"_session_id":null,"_view_id":"text","answer":"reject","spans":[],"tokens":[{"text":"re","start":0,"end":2,"id":0}]}
{"text":"  In this paper we mainly address the question how to design SVMs by choosing the reproducing kernel Hilbert space (RKHS) or its corresponding kernel to obtain consistent and statistically robust estimators in additive models.","_input_hash":1285429061,"_task_hash":1950645287,"tokens":[{"text":"  ","start":0,"end":2,"id":0},{"text":"In","start":2,"end":4,"id":1},{"text":"this","start":5,"end":9,"id":2},{"text":"paper","start":10,"end":15,"id":3},{"text":"we","start":16,"end":18,"id":4},{"text":"mainly","start":19,"end":25,"id":5},{"text":"address","start":26,"end":33,"id":6},{"text":"the","start":34,"end":37,"id":7},{"text":"question","start":38,"end":46,"id":8},{"text":"how","start":47,"end":50,"id":9},{"text":"to","start":51,"end":53,"id":10},{"text":"design","start":54,"end":60,"id":11},{"text":"SVMs","start":61,"end":65,"id":12},{"text":"by","start":66,"end":68,"id":13},{"text":"choosing","start":69,"end":77,"id":14},{"text":"the","start":78,"end":81,"id":15},{"text":"reproducing","start":82,"end":93,"id":16},{"text":"kernel","start":94,"end":100,"id":17},{"text":"Hilbert","start":101,"end":108,"id":18},{"text":"space","start":109,"end":114,"id":19},{"text":"(","start":115,"end":116,"id":20},{"text":"RKHS","start":116,"end":120,"id":21},{"text":")","start":120,"end":121,"id":22},{"text":"or","start":122,"end":124,"id":23},{"text":"its","start":125,"end":128,"id":24},{"text":"corresponding","start":129,"end":142,"id":25},{"text":"kernel","start":143,"end":149,"id":26},{"text":"to","start":150,"end":152,"id":27},{"text":"obtain","start":153,"end":159,"id":28},{"text":"consistent","start":160,"end":170,"id":29},{"text":"and","start":171,"end":174,"id":30},{"text":"statistically","start":175,"end":188,"id":31},{"text":"robust","start":189,"end":195,"id":32},{"text":"estimators","start":196,"end":206,"id":33},{"text":"in","start":207,"end":209,"id":34},{"text":"additive","start":210,"end":218,"id":35},{"text":"models","start":219,"end":225,"id":36},{"text":".","start":225,"end":226,"id":37}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":61,"end":65,"token_start":12,"token_end":12,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We show how it can be interpreted as a density model in the observed space.","_input_hash":-1130648324,"_task_hash":-449197602,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"show","start":3,"end":7,"id":1},{"text":"how","start":8,"end":11,"id":2},{"text":"it","start":12,"end":14,"id":3},{"text":"can","start":15,"end":18,"id":4},{"text":"be","start":19,"end":21,"id":5},{"text":"interpreted","start":22,"end":33,"id":6},{"text":"as","start":34,"end":36,"id":7},{"text":"a","start":37,"end":38,"id":8},{"text":"density","start":39,"end":46,"id":9},{"text":"model","start":47,"end":52,"id":10},{"text":"in","start":53,"end":55,"id":11},{"text":"the","start":56,"end":59,"id":12},{"text":"observed","start":60,"end":68,"id":13},{"text":"space","start":69,"end":74,"id":14},{"text":".","start":74,"end":75,"id":15}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We demonstrate the approach with applications.","_input_hash":-958605827,"_task_hash":-58402604,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"demonstrate","start":3,"end":14,"id":1},{"text":"the","start":15,"end":18,"id":2},{"text":"approach","start":19,"end":27,"id":3},{"text":"with","start":28,"end":32,"id":4},{"text":"applications","start":33,"end":45,"id":5},{"text":".","start":45,"end":46,"id":6}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"Finally, we suggest a simple iterative method that can be used to improve the output of existing algorithms.","_input_hash":-402573250,"_task_hash":-2136247451,"tokens":[{"text":"Finally","start":0,"end":7,"id":0},{"text":",","start":7,"end":8,"id":1},{"text":"we","start":9,"end":11,"id":2},{"text":"suggest","start":12,"end":19,"id":3},{"text":"a","start":20,"end":21,"id":4},{"text":"simple","start":22,"end":28,"id":5},{"text":"iterative","start":29,"end":38,"id":6},{"text":"method","start":39,"end":45,"id":7},{"text":"that","start":46,"end":50,"id":8},{"text":"can","start":51,"end":54,"id":9},{"text":"be","start":55,"end":57,"id":10},{"text":"used","start":58,"end":62,"id":11},{"text":"to","start":63,"end":65,"id":12},{"text":"improve","start":66,"end":73,"id":13},{"text":"the","start":74,"end":77,"id":14},{"text":"output","start":78,"end":84,"id":15},{"text":"of","start":85,"end":87,"id":16},{"text":"existing","start":88,"end":96,"id":17},{"text":"algorithms","start":97,"end":107,"id":18},{"text":".","start":107,"end":108,"id":19}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"matrices|NOUN","word":"matrices","sense":"NOUN","meta":{"score":0.777700007,"sense":"NOUN"},"_input_hash":1469316484,"_task_hash":1001135322,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"matrices|NOUN","start":0,"end":13,"id":0}]}
{"text":"data_processing|NOUN","word":"data processing","sense":"NOUN","meta":{"score":0.7868000269,"sense":"NOUN"},"_input_hash":2060747866,"_task_hash":-906526590,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"data_processing|NOUN","start":0,"end":20,"id":0}]}
{"text":"For graphs generated from a stochastic algorithm designed to model community structure, the optimal information theoretic partition and the optimal min-cut partition are shown to be the same with high probability.","_input_hash":1434805796,"_task_hash":829451928,"tokens":[{"text":"For","start":0,"end":3,"id":0},{"text":"graphs","start":4,"end":10,"id":1},{"text":"generated","start":11,"end":20,"id":2},{"text":"from","start":21,"end":25,"id":3},{"text":"a","start":26,"end":27,"id":4},{"text":"stochastic","start":28,"end":38,"id":5},{"text":"algorithm","start":39,"end":48,"id":6},{"text":"designed","start":49,"end":57,"id":7},{"text":"to","start":58,"end":60,"id":8},{"text":"model","start":61,"end":66,"id":9},{"text":"community","start":67,"end":76,"id":10},{"text":"structure","start":77,"end":86,"id":11},{"text":",","start":86,"end":87,"id":12},{"text":"the","start":88,"end":91,"id":13},{"text":"optimal","start":92,"end":99,"id":14},{"text":"information","start":100,"end":111,"id":15},{"text":"theoretic","start":112,"end":121,"id":16},{"text":"partition","start":122,"end":131,"id":17},{"text":"and","start":132,"end":135,"id":18},{"text":"the","start":136,"end":139,"id":19},{"text":"optimal","start":140,"end":147,"id":20},{"text":"min","start":148,"end":151,"id":21},{"text":"-","start":151,"end":152,"id":22},{"text":"cut","start":152,"end":155,"id":23},{"text":"partition","start":156,"end":165,"id":24},{"text":"are","start":166,"end":169,"id":25},{"text":"shown","start":170,"end":175,"id":26},{"text":"to","start":176,"end":178,"id":27},{"text":"be","start":179,"end":181,"id":28},{"text":"the","start":182,"end":185,"id":29},{"text":"same","start":186,"end":190,"id":30},{"text":"with","start":191,"end":195,"id":31},{"text":"high","start":196,"end":200,"id":32},{"text":"probability","start":201,"end":212,"id":33},{"text":".","start":212,"end":213,"id":34}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"As a demonstration of the benefits of the framework we derive an inference procedure for a state-space model over permutations.","_input_hash":-2037131638,"_task_hash":1203947134,"tokens":[{"text":"As","start":0,"end":2,"id":0},{"text":"a","start":3,"end":4,"id":1},{"text":"demonstration","start":5,"end":18,"id":2},{"text":"of","start":19,"end":21,"id":3},{"text":"the","start":22,"end":25,"id":4},{"text":"benefits","start":26,"end":34,"id":5},{"text":"of","start":35,"end":37,"id":6},{"text":"the","start":38,"end":41,"id":7},{"text":"framework","start":42,"end":51,"id":8},{"text":"we","start":52,"end":54,"id":9},{"text":"derive","start":55,"end":61,"id":10},{"text":"an","start":62,"end":64,"id":11},{"text":"inference","start":65,"end":74,"id":12},{"text":"procedure","start":75,"end":84,"id":13},{"text":"for","start":85,"end":88,"id":14},{"text":"a","start":89,"end":90,"id":15},{"text":"state","start":91,"end":96,"id":16},{"text":"-","start":96,"end":97,"id":17},{"text":"space","start":97,"end":102,"id":18},{"text":"model","start":103,"end":108,"id":19},{"text":"over","start":109,"end":113,"id":20},{"text":"permutations","start":114,"end":126,"id":21},{"text":".","start":126,"end":127,"id":22}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"computing|VERB","word":"computing","sense":"VERB","meta":{"score":0.7967000008,"sense":"VERB"},"_input_hash":103455134,"_task_hash":-770430485,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"computing|VERB","start":0,"end":14,"id":0}]}
{"text":"By using machine learning techniques, we allow clinicians to visualize data in a low dimension defined by a linear combination of all of the available markers, rather than just 2 at a time.","_input_hash":-1723825170,"_task_hash":1661651710,"tokens":[{"text":"By","start":0,"end":2,"id":0},{"text":"using","start":3,"end":8,"id":1},{"text":"machine","start":9,"end":16,"id":2},{"text":"learning","start":17,"end":25,"id":3},{"text":"techniques","start":26,"end":36,"id":4},{"text":",","start":36,"end":37,"id":5},{"text":"we","start":38,"end":40,"id":6},{"text":"allow","start":41,"end":46,"id":7},{"text":"clinicians","start":47,"end":57,"id":8},{"text":"to","start":58,"end":60,"id":9},{"text":"visualize","start":61,"end":70,"id":10},{"text":"data","start":71,"end":75,"id":11},{"text":"in","start":76,"end":78,"id":12},{"text":"a","start":79,"end":80,"id":13},{"text":"low","start":81,"end":84,"id":14},{"text":"dimension","start":85,"end":94,"id":15},{"text":"defined","start":95,"end":102,"id":16},{"text":"by","start":103,"end":105,"id":17},{"text":"a","start":106,"end":107,"id":18},{"text":"linear","start":108,"end":114,"id":19},{"text":"combination","start":115,"end":126,"id":20},{"text":"of","start":127,"end":129,"id":21},{"text":"all","start":130,"end":133,"id":22},{"text":"of","start":134,"end":136,"id":23},{"text":"the","start":137,"end":140,"id":24},{"text":"available","start":141,"end":150,"id":25},{"text":"markers","start":151,"end":158,"id":26},{"text":",","start":158,"end":159,"id":27},{"text":"rather","start":160,"end":166,"id":28},{"text":"than","start":167,"end":171,"id":29},{"text":"just","start":172,"end":176,"id":30},{"text":"2","start":177,"end":178,"id":31},{"text":"at","start":179,"end":181,"id":32},{"text":"a","start":182,"end":183,"id":33},{"text":"time","start":184,"end":188,"id":34},{"text":".","start":188,"end":189,"id":35}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":9,"end":36,"token_start":2,"token_end":4,"label":"ALGO","answer":"reject"}],"answer":"reject"}
{"text":"First, it is valid for an arbitrary finite number of outcomes.","_input_hash":252638691,"_task_hash":2115868365,"tokens":[{"text":"First","start":0,"end":5,"id":0},{"text":",","start":5,"end":6,"id":1},{"text":"it","start":7,"end":9,"id":2},{"text":"is","start":10,"end":12,"id":3},{"text":"valid","start":13,"end":18,"id":4},{"text":"for","start":19,"end":22,"id":5},{"text":"an","start":23,"end":25,"id":6},{"text":"arbitrary","start":26,"end":35,"id":7},{"text":"finite","start":36,"end":42,"id":8},{"text":"number","start":43,"end":49,"id":9},{"text":"of","start":50,"end":52,"id":10},{"text":"outcomes","start":53,"end":61,"id":11},{"text":".","start":61,"end":62,"id":12}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"Although the main focus of this paper is to present a general, theoretical framework for metric embedding in a NN setting, we demonstrate the performance of the proposed method on some benchmark datasets and show that it performs better than the Mahalanobis metric learning algorithm in terms of leave-one-out and generalization errors.","_input_hash":-1031192136,"_task_hash":-1963508146,"tokens":[{"text":"Although","start":0,"end":8,"id":0},{"text":"the","start":9,"end":12,"id":1},{"text":"main","start":13,"end":17,"id":2},{"text":"focus","start":18,"end":23,"id":3},{"text":"of","start":24,"end":26,"id":4},{"text":"this","start":27,"end":31,"id":5},{"text":"paper","start":32,"end":37,"id":6},{"text":"is","start":38,"end":40,"id":7},{"text":"to","start":41,"end":43,"id":8},{"text":"present","start":44,"end":51,"id":9},{"text":"a","start":52,"end":53,"id":10},{"text":"general","start":54,"end":61,"id":11},{"text":",","start":61,"end":62,"id":12},{"text":"theoretical","start":63,"end":74,"id":13},{"text":"framework","start":75,"end":84,"id":14},{"text":"for","start":85,"end":88,"id":15},{"text":"metric","start":89,"end":95,"id":16},{"text":"embedding","start":96,"end":105,"id":17},{"text":"in","start":106,"end":108,"id":18},{"text":"a","start":109,"end":110,"id":19},{"text":"NN","start":111,"end":113,"id":20},{"text":"setting","start":114,"end":121,"id":21},{"text":",","start":121,"end":122,"id":22},{"text":"we","start":123,"end":125,"id":23},{"text":"demonstrate","start":126,"end":137,"id":24},{"text":"the","start":138,"end":141,"id":25},{"text":"performance","start":142,"end":153,"id":26},{"text":"of","start":154,"end":156,"id":27},{"text":"the","start":157,"end":160,"id":28},{"text":"proposed","start":161,"end":169,"id":29},{"text":"method","start":170,"end":176,"id":30},{"text":"on","start":177,"end":179,"id":31},{"text":"some","start":180,"end":184,"id":32},{"text":"benchmark","start":185,"end":194,"id":33},{"text":"datasets","start":195,"end":203,"id":34},{"text":"and","start":204,"end":207,"id":35},{"text":"show","start":208,"end":212,"id":36},{"text":"that","start":213,"end":217,"id":37},{"text":"it","start":218,"end":220,"id":38},{"text":"performs","start":221,"end":229,"id":39},{"text":"better","start":230,"end":236,"id":40},{"text":"than","start":237,"end":241,"id":41},{"text":"the","start":242,"end":245,"id":42},{"text":"Mahalanobis","start":246,"end":257,"id":43},{"text":"metric","start":258,"end":264,"id":44},{"text":"learning","start":265,"end":273,"id":45},{"text":"algorithm","start":274,"end":283,"id":46},{"text":"in","start":284,"end":286,"id":47},{"text":"terms","start":287,"end":292,"id":48},{"text":"of","start":293,"end":295,"id":49},{"text":"leave","start":296,"end":301,"id":50},{"text":"-","start":301,"end":302,"id":51},{"text":"one","start":302,"end":305,"id":52},{"text":"-","start":305,"end":306,"id":53},{"text":"out","start":306,"end":309,"id":54},{"text":"and","start":310,"end":313,"id":55},{"text":"generalization","start":314,"end":328,"id":56},{"text":"errors","start":329,"end":335,"id":57},{"text":".","start":335,"end":336,"id":58}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We extend the previous work on this problem to adversarial scenarios, while also improving on their results in the iid setup.","_input_hash":-2014859766,"_task_hash":1841005240,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"extend","start":3,"end":9,"id":1},{"text":"the","start":10,"end":13,"id":2},{"text":"previous","start":14,"end":22,"id":3},{"text":"work","start":23,"end":27,"id":4},{"text":"on","start":28,"end":30,"id":5},{"text":"this","start":31,"end":35,"id":6},{"text":"problem","start":36,"end":43,"id":7},{"text":"to","start":44,"end":46,"id":8},{"text":"adversarial","start":47,"end":58,"id":9},{"text":"scenarios","start":59,"end":68,"id":10},{"text":",","start":68,"end":69,"id":11},{"text":"while","start":70,"end":75,"id":12},{"text":"also","start":76,"end":80,"id":13},{"text":"improving","start":81,"end":90,"id":14},{"text":"on","start":91,"end":93,"id":15},{"text":"their","start":94,"end":99,"id":16},{"text":"results","start":100,"end":107,"id":17},{"text":"in","start":108,"end":110,"id":18},{"text":"the","start":111,"end":114,"id":19},{"text":"iid","start":115,"end":118,"id":20},{"text":"setup","start":119,"end":124,"id":21},{"text":".","start":124,"end":125,"id":22}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"In this paper, we present a general formulation for supervised dictionary learning adapted to a wide variety of tasks, and present an efficient algorithm for solving the corresponding optimization problem.","_input_hash":1983508494,"_task_hash":-1332257413,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"this","start":3,"end":7,"id":1},{"text":"paper","start":8,"end":13,"id":2},{"text":",","start":13,"end":14,"id":3},{"text":"we","start":15,"end":17,"id":4},{"text":"present","start":18,"end":25,"id":5},{"text":"a","start":26,"end":27,"id":6},{"text":"general","start":28,"end":35,"id":7},{"text":"formulation","start":36,"end":47,"id":8},{"text":"for","start":48,"end":51,"id":9},{"text":"supervised","start":52,"end":62,"id":10},{"text":"dictionary","start":63,"end":73,"id":11},{"text":"learning","start":74,"end":82,"id":12},{"text":"adapted","start":83,"end":90,"id":13},{"text":"to","start":91,"end":93,"id":14},{"text":"a","start":94,"end":95,"id":15},{"text":"wide","start":96,"end":100,"id":16},{"text":"variety","start":101,"end":108,"id":17},{"text":"of","start":109,"end":111,"id":18},{"text":"tasks","start":112,"end":117,"id":19},{"text":",","start":117,"end":118,"id":20},{"text":"and","start":119,"end":122,"id":21},{"text":"present","start":123,"end":130,"id":22},{"text":"an","start":131,"end":133,"id":23},{"text":"efficient","start":134,"end":143,"id":24},{"text":"algorithm","start":144,"end":153,"id":25},{"text":"for","start":154,"end":157,"id":26},{"text":"solving","start":158,"end":165,"id":27},{"text":"the","start":166,"end":169,"id":28},{"text":"corresponding","start":170,"end":183,"id":29},{"text":"optimization","start":184,"end":196,"id":30},{"text":"problem","start":197,"end":204,"id":31},{"text":".","start":204,"end":205,"id":32}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":134,"end":153,"token_start":24,"token_end":25,"label":"ALGO","answer":"reject"},{"start":184,"end":204,"token_start":30,"token_end":31,"label":"ALGO","answer":"reject"}],"answer":"reject"}
{"text":"The update rule is derived by minimizing AdaBoost's loss when viewed in an incremental form.","_input_hash":1538992120,"_task_hash":362230522,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"update","start":4,"end":10,"id":1},{"text":"rule","start":11,"end":15,"id":2},{"text":"is","start":16,"end":18,"id":3},{"text":"derived","start":19,"end":26,"id":4},{"text":"by","start":27,"end":29,"id":5},{"text":"minimizing","start":30,"end":40,"id":6},{"text":"AdaBoost","start":41,"end":49,"id":7},{"text":"'s","start":49,"end":51,"id":8},{"text":"loss","start":52,"end":56,"id":9},{"text":"when","start":57,"end":61,"id":10},{"text":"viewed","start":62,"end":68,"id":11},{"text":"in","start":69,"end":71,"id":12},{"text":"an","start":72,"end":74,"id":13},{"text":"incremental","start":75,"end":86,"id":14},{"text":"form","start":87,"end":91,"id":15},{"text":".","start":91,"end":92,"id":16}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":41,"end":49,"token_start":7,"token_end":7,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Exogenous variables work as triggers that activate a causal chain in the model, and their identification leads to more efficient experimental designs and better understanding of the causal mechanism.","_input_hash":848702007,"_task_hash":1844109875,"tokens":[{"text":"Exogenous","start":0,"end":9,"id":0},{"text":"variables","start":10,"end":19,"id":1},{"text":"work","start":20,"end":24,"id":2},{"text":"as","start":25,"end":27,"id":3},{"text":"triggers","start":28,"end":36,"id":4},{"text":"that","start":37,"end":41,"id":5},{"text":"activate","start":42,"end":50,"id":6},{"text":"a","start":51,"end":52,"id":7},{"text":"causal","start":53,"end":59,"id":8},{"text":"chain","start":60,"end":65,"id":9},{"text":"in","start":66,"end":68,"id":10},{"text":"the","start":69,"end":72,"id":11},{"text":"model","start":73,"end":78,"id":12},{"text":",","start":78,"end":79,"id":13},{"text":"and","start":80,"end":83,"id":14},{"text":"their","start":84,"end":89,"id":15},{"text":"identification","start":90,"end":104,"id":16},{"text":"leads","start":105,"end":110,"id":17},{"text":"to","start":111,"end":113,"id":18},{"text":"more","start":114,"end":118,"id":19},{"text":"efficient","start":119,"end":128,"id":20},{"text":"experimental","start":129,"end":141,"id":21},{"text":"designs","start":142,"end":149,"id":22},{"text":"and","start":150,"end":153,"id":23},{"text":"better","start":154,"end":160,"id":24},{"text":"understanding","start":161,"end":174,"id":25},{"text":"of","start":175,"end":177,"id":26},{"text":"the","start":178,"end":181,"id":27},{"text":"causal","start":182,"end":188,"id":28},{"text":"mechanism","start":189,"end":198,"id":29},{"text":".","start":198,"end":199,"id":30}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":69,"end":78,"token_start":11,"token_end":12,"label":"ALGO","answer":"reject"}],"answer":"reject"}
{"text":"mathematical_models|NOUN","word":"mathematical models","sense":"NOUN","meta":{"score":0.7792000175,"sense":"NOUN"},"_input_hash":-108171284,"_task_hash":2050202363,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"mathematical_models|NOUN","start":0,"end":24,"id":0}]}
{"text":"all","meta":{"score":0},"_input_hash":1340842546,"_task_hash":-1880101108,"_session_id":null,"_view_id":"text","answer":"reject","spans":[],"tokens":[{"text":"all","start":0,"end":3,"id":0}]}
{"text":"On the other hand, if K>1 and p>1, then we show that both all underlying subspaces and the best l0 subspace cannot be recovered and even nearly recovered.","_input_hash":-1290926402,"_task_hash":196427689,"tokens":[{"text":"On","start":0,"end":2,"id":0},{"text":"the","start":3,"end":6,"id":1},{"text":"other","start":7,"end":12,"id":2},{"text":"hand","start":13,"end":17,"id":3},{"text":",","start":17,"end":18,"id":4},{"text":"if","start":19,"end":21,"id":5},{"text":"K>1","start":22,"end":25,"id":6},{"text":"and","start":26,"end":29,"id":7},{"text":"p>1","start":30,"end":33,"id":8},{"text":",","start":33,"end":34,"id":9},{"text":"then","start":35,"end":39,"id":10},{"text":"we","start":40,"end":42,"id":11},{"text":"show","start":43,"end":47,"id":12},{"text":"that","start":48,"end":52,"id":13},{"text":"both","start":53,"end":57,"id":14},{"text":"all","start":58,"end":61,"id":15},{"text":"underlying","start":62,"end":72,"id":16},{"text":"subspaces","start":73,"end":82,"id":17},{"text":"and","start":83,"end":86,"id":18},{"text":"the","start":87,"end":90,"id":19},{"text":"best","start":91,"end":95,"id":20},{"text":"l0","start":96,"end":98,"id":21},{"text":"subspace","start":99,"end":107,"id":22},{"text":"can","start":108,"end":111,"id":23},{"text":"not","start":111,"end":114,"id":24},{"text":"be","start":115,"end":117,"id":25},{"text":"recovered","start":118,"end":127,"id":26},{"text":"and","start":128,"end":131,"id":27},{"text":"even","start":132,"end":136,"id":28},{"text":"nearly","start":137,"end":143,"id":29},{"text":"recovered","start":144,"end":153,"id":30},{"text":".","start":153,"end":154,"id":31}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"Under mild conditions, we show that StARS is partially sparsistent in terms of graph estimation:","_input_hash":1336059987,"_task_hash":-717748478,"tokens":[{"text":"Under","start":0,"end":5,"id":0},{"text":"mild","start":6,"end":10,"id":1},{"text":"conditions","start":11,"end":21,"id":2},{"text":",","start":21,"end":22,"id":3},{"text":"we","start":23,"end":25,"id":4},{"text":"show","start":26,"end":30,"id":5},{"text":"that","start":31,"end":35,"id":6},{"text":"StARS","start":36,"end":41,"id":7},{"text":"is","start":42,"end":44,"id":8},{"text":"partially","start":45,"end":54,"id":9},{"text":"sparsistent","start":55,"end":66,"id":10},{"text":"in","start":67,"end":69,"id":11},{"text":"terms","start":70,"end":75,"id":12},{"text":"of","start":76,"end":78,"id":13},{"text":"graph","start":79,"end":84,"id":14},{"text":"estimation","start":85,"end":95,"id":15},{"text":":","start":95,"end":96,"id":16}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"The prior and conditional probabilities are expressed in terms of RKHS functions of an empirical sample:","_input_hash":-2062731311,"_task_hash":-1793643208,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"prior","start":4,"end":9,"id":1},{"text":"and","start":10,"end":13,"id":2},{"text":"conditional","start":14,"end":25,"id":3},{"text":"probabilities","start":26,"end":39,"id":4},{"text":"are","start":40,"end":43,"id":5},{"text":"expressed","start":44,"end":53,"id":6},{"text":"in","start":54,"end":56,"id":7},{"text":"terms","start":57,"end":62,"id":8},{"text":"of","start":63,"end":65,"id":9},{"text":"RKHS","start":66,"end":70,"id":10},{"text":"functions","start":71,"end":80,"id":11},{"text":"of","start":81,"end":83,"id":12},{"text":"an","start":84,"end":86,"id":13},{"text":"empirical","start":87,"end":96,"id":14},{"text":"sample","start":97,"end":103,"id":15},{"text":":","start":103,"end":104,"id":16}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"rather, we form kernel density estimates of the bivariate and univariate marginals, and apply Kruskal's algorithm to estimate the optimal forest on held out data.","_input_hash":-1117850548,"_task_hash":-708795722,"tokens":[{"text":"rather","start":0,"end":6,"id":0},{"text":",","start":6,"end":7,"id":1},{"text":"we","start":8,"end":10,"id":2},{"text":"form","start":11,"end":15,"id":3},{"text":"kernel","start":16,"end":22,"id":4},{"text":"density","start":23,"end":30,"id":5},{"text":"estimates","start":31,"end":40,"id":6},{"text":"of","start":41,"end":43,"id":7},{"text":"the","start":44,"end":47,"id":8},{"text":"bivariate","start":48,"end":57,"id":9},{"text":"and","start":58,"end":61,"id":10},{"text":"univariate","start":62,"end":72,"id":11},{"text":"marginals","start":73,"end":82,"id":12},{"text":",","start":82,"end":83,"id":13},{"text":"and","start":84,"end":87,"id":14},{"text":"apply","start":88,"end":93,"id":15},{"text":"Kruskal","start":94,"end":101,"id":16},{"text":"'s","start":101,"end":103,"id":17},{"text":"algorithm","start":104,"end":113,"id":18},{"text":"to","start":114,"end":116,"id":19},{"text":"estimate","start":117,"end":125,"id":20},{"text":"the","start":126,"end":129,"id":21},{"text":"optimal","start":130,"end":137,"id":22},{"text":"forest","start":138,"end":144,"id":23},{"text":"on","start":145,"end":147,"id":24},{"text":"held","start":148,"end":152,"id":25},{"text":"out","start":153,"end":156,"id":26},{"text":"data","start":157,"end":161,"id":27},{"text":".","start":161,"end":162,"id":28}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":94,"end":113,"token_start":16,"token_end":18,"label":"ALGO","answer":"accept"},{"start":138,"end":144,"token_start":23,"token_end":23,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"finite_state_machines|NOUN","word":"finite state machines","sense":"NOUN","meta":{"score":0.7749999762,"sense":"NOUN"},"_input_hash":2067080263,"_task_hash":-267329049,"_session_id":null,"_view_id":"html","answer":"accept","spans":[],"tokens":[{"text":"finite_state_machines|NOUN","start":0,"end":26,"id":0}]}
{"text":"We prove an oracle inequality on the excess risk of the resulting estimator relative to the risk of the best forest.","_input_hash":-1656166332,"_task_hash":-129100090,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"prove","start":3,"end":8,"id":1},{"text":"an","start":9,"end":11,"id":2},{"text":"oracle","start":12,"end":18,"id":3},{"text":"inequality","start":19,"end":29,"id":4},{"text":"on","start":30,"end":32,"id":5},{"text":"the","start":33,"end":36,"id":6},{"text":"excess","start":37,"end":43,"id":7},{"text":"risk","start":44,"end":48,"id":8},{"text":"of","start":49,"end":51,"id":9},{"text":"the","start":52,"end":55,"id":10},{"text":"resulting","start":56,"end":65,"id":11},{"text":"estimator","start":66,"end":75,"id":12},{"text":"relative","start":76,"end":84,"id":13},{"text":"to","start":85,"end":87,"id":14},{"text":"the","start":88,"end":91,"id":15},{"text":"risk","start":92,"end":96,"id":16},{"text":"of","start":97,"end":99,"id":17},{"text":"the","start":100,"end":103,"id":18},{"text":"best","start":104,"end":108,"id":19},{"text":"forest","start":109,"end":115,"id":20},{"text":".","start":115,"end":116,"id":21}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":109,"end":115,"token_start":20,"token_end":20,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We study the problem of learning a sparse linear regression vector under additional conditions on the structure of its sparsity pattern.","_input_hash":-1336290528,"_task_hash":-908265437,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"study","start":3,"end":8,"id":1},{"text":"the","start":9,"end":12,"id":2},{"text":"problem","start":13,"end":20,"id":3},{"text":"of","start":21,"end":23,"id":4},{"text":"learning","start":24,"end":32,"id":5},{"text":"a","start":33,"end":34,"id":6},{"text":"sparse","start":35,"end":41,"id":7},{"text":"linear","start":42,"end":48,"id":8},{"text":"regression","start":49,"end":59,"id":9},{"text":"vector","start":60,"end":66,"id":10},{"text":"under","start":67,"end":72,"id":11},{"text":"additional","start":73,"end":83,"id":12},{"text":"conditions","start":84,"end":94,"id":13},{"text":"on","start":95,"end":97,"id":14},{"text":"the","start":98,"end":101,"id":15},{"text":"structure","start":102,"end":111,"id":16},{"text":"of","start":112,"end":114,"id":17},{"text":"its","start":115,"end":118,"id":18},{"text":"sparsity","start":119,"end":127,"id":19},{"text":"pattern","start":128,"end":135,"id":20},{"text":".","start":135,"end":136,"id":21}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":35,"end":59,"token_start":7,"token_end":9,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"More precisely, we show that the proximal operator associated with this norm is computable exactly via a dual approach that can be viewed as the composition of elementary proximal operators.","_input_hash":-1252767780,"_task_hash":-2041237220,"tokens":[{"text":"More","start":0,"end":4,"id":0},{"text":"precisely","start":5,"end":14,"id":1},{"text":",","start":14,"end":15,"id":2},{"text":"we","start":16,"end":18,"id":3},{"text":"show","start":19,"end":23,"id":4},{"text":"that","start":24,"end":28,"id":5},{"text":"the","start":29,"end":32,"id":6},{"text":"proximal","start":33,"end":41,"id":7},{"text":"operator","start":42,"end":50,"id":8},{"text":"associated","start":51,"end":61,"id":9},{"text":"with","start":62,"end":66,"id":10},{"text":"this","start":67,"end":71,"id":11},{"text":"norm","start":72,"end":76,"id":12},{"text":"is","start":77,"end":79,"id":13},{"text":"computable","start":80,"end":90,"id":14},{"text":"exactly","start":91,"end":98,"id":15},{"text":"via","start":99,"end":102,"id":16},{"text":"a","start":103,"end":104,"id":17},{"text":"dual","start":105,"end":109,"id":18},{"text":"approach","start":110,"end":118,"id":19},{"text":"that","start":119,"end":123,"id":20},{"text":"can","start":124,"end":127,"id":21},{"text":"be","start":128,"end":130,"id":22},{"text":"viewed","start":131,"end":137,"id":23},{"text":"as","start":138,"end":140,"id":24},{"text":"the","start":141,"end":144,"id":25},{"text":"composition","start":145,"end":156,"id":26},{"text":"of","start":157,"end":159,"id":27},{"text":"elementary","start":160,"end":170,"id":28},{"text":"proximal","start":171,"end":179,"id":29},{"text":"operators","start":180,"end":189,"id":30},{"text":".","start":189,"end":190,"id":31}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"we use the fitted model to predict response values for new documents.","_input_hash":262643015,"_task_hash":440893708,"tokens":[{"text":"we","start":0,"end":2,"id":0},{"text":"use","start":3,"end":6,"id":1},{"text":"the","start":7,"end":10,"id":2},{"text":"fitted","start":11,"end":17,"id":3},{"text":"model","start":18,"end":23,"id":4},{"text":"to","start":24,"end":26,"id":5},{"text":"predict","start":27,"end":34,"id":6},{"text":"response","start":35,"end":43,"id":7},{"text":"values","start":44,"end":50,"id":8},{"text":"for","start":51,"end":54,"id":9},{"text":"new","start":55,"end":58,"id":10},{"text":"documents","start":59,"end":68,"id":11},{"text":".","start":68,"end":69,"id":12}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"logic_gates|NOUN","word":"logic gates","sense":"NOUN","meta":{"score":0.7624999881,"sense":"NOUN"},"_input_hash":-274935574,"_task_hash":-1027000730,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"logic_gates|NOUN","start":0,"end":16,"id":0}]}
{"text":"why","meta":{"score":0},"_input_hash":-1812285969,"_task_hash":2116629306,"_session_id":null,"_view_id":"text","answer":"reject","spans":[],"tokens":[{"text":"why","start":0,"end":3,"id":0}]}
{"text":"In this article, it is shown that with the proposed kernel function it is feasible to make the pointwise MSE of the density estimator converge at O(n^-2/3) regardless of the dimension of the vector space, provided that the probability density function at the point of interest meets certain conditions.","_input_hash":-521693369,"_task_hash":-1250994352,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"this","start":3,"end":7,"id":1},{"text":"article","start":8,"end":15,"id":2},{"text":",","start":15,"end":16,"id":3},{"text":"it","start":17,"end":19,"id":4},{"text":"is","start":20,"end":22,"id":5},{"text":"shown","start":23,"end":28,"id":6},{"text":"that","start":29,"end":33,"id":7},{"text":"with","start":34,"end":38,"id":8},{"text":"the","start":39,"end":42,"id":9},{"text":"proposed","start":43,"end":51,"id":10},{"text":"kernel","start":52,"end":58,"id":11},{"text":"function","start":59,"end":67,"id":12},{"text":"it","start":68,"end":70,"id":13},{"text":"is","start":71,"end":73,"id":14},{"text":"feasible","start":74,"end":82,"id":15},{"text":"to","start":83,"end":85,"id":16},{"text":"make","start":86,"end":90,"id":17},{"text":"the","start":91,"end":94,"id":18},{"text":"pointwise","start":95,"end":104,"id":19},{"text":"MSE","start":105,"end":108,"id":20},{"text":"of","start":109,"end":111,"id":21},{"text":"the","start":112,"end":115,"id":22},{"text":"density","start":116,"end":123,"id":23},{"text":"estimator","start":124,"end":133,"id":24},{"text":"converge","start":134,"end":142,"id":25},{"text":"at","start":143,"end":145,"id":26},{"text":"O(n^-2/3","start":146,"end":154,"id":27},{"text":")","start":154,"end":155,"id":28},{"text":"regardless","start":156,"end":166,"id":29},{"text":"of","start":167,"end":169,"id":30},{"text":"the","start":170,"end":173,"id":31},{"text":"dimension","start":174,"end":183,"id":32},{"text":"of","start":184,"end":186,"id":33},{"text":"the","start":187,"end":190,"id":34},{"text":"vector","start":191,"end":197,"id":35},{"text":"space","start":198,"end":203,"id":36},{"text":",","start":203,"end":204,"id":37},{"text":"provided","start":205,"end":213,"id":38},{"text":"that","start":214,"end":218,"id":39},{"text":"the","start":219,"end":222,"id":40},{"text":"probability","start":223,"end":234,"id":41},{"text":"density","start":235,"end":242,"id":42},{"text":"function","start":243,"end":251,"id":43},{"text":"at","start":252,"end":254,"id":44},{"text":"the","start":255,"end":258,"id":45},{"text":"point","start":259,"end":264,"id":46},{"text":"of","start":265,"end":267,"id":47},{"text":"interest","start":268,"end":276,"id":48},{"text":"meets","start":277,"end":282,"id":49},{"text":"certain","start":283,"end":290,"id":50},{"text":"conditions","start":291,"end":301,"id":51},{"text":".","start":301,"end":302,"id":52}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"We closely follow the formalism introduced by \\cite{DUD03} to cover a large variety of cross-validation procedures including leave-one-out cross-validation, $k$% -fold cross-validation, hold-out cross-validation (or split sample), and the leave-$\\upsilon$-out cross-validation.","_input_hash":-288228580,"_task_hash":-635896188,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"closely","start":3,"end":10,"id":1},{"text":"follow","start":11,"end":17,"id":2},{"text":"the","start":18,"end":21,"id":3},{"text":"formalism","start":22,"end":31,"id":4},{"text":"introduced","start":32,"end":42,"id":5},{"text":"by","start":43,"end":45,"id":6},{"text":"\\cite{DUD03","start":46,"end":57,"id":7},{"text":"}","start":57,"end":58,"id":8},{"text":"to","start":59,"end":61,"id":9},{"text":"cover","start":62,"end":67,"id":10},{"text":"a","start":68,"end":69,"id":11},{"text":"large","start":70,"end":75,"id":12},{"text":"variety","start":76,"end":83,"id":13},{"text":"of","start":84,"end":86,"id":14},{"text":"cross","start":87,"end":92,"id":15},{"text":"-","start":92,"end":93,"id":16},{"text":"validation","start":93,"end":103,"id":17},{"text":"procedures","start":104,"end":114,"id":18},{"text":"including","start":115,"end":124,"id":19},{"text":"leave","start":125,"end":130,"id":20},{"text":"-","start":130,"end":131,"id":21},{"text":"one","start":131,"end":134,"id":22},{"text":"-","start":134,"end":135,"id":23},{"text":"out","start":135,"end":138,"id":24},{"text":"cross","start":139,"end":144,"id":25},{"text":"-","start":144,"end":145,"id":26},{"text":"validation","start":145,"end":155,"id":27},{"text":",","start":155,"end":156,"id":28},{"text":"$","start":157,"end":158,"id":29},{"text":"k$%","start":158,"end":161,"id":30},{"text":"-fold","start":162,"end":167,"id":31},{"text":"cross","start":168,"end":173,"id":32},{"text":"-","start":173,"end":174,"id":33},{"text":"validation","start":174,"end":184,"id":34},{"text":",","start":184,"end":185,"id":35},{"text":"hold","start":186,"end":190,"id":36},{"text":"-","start":190,"end":191,"id":37},{"text":"out","start":191,"end":194,"id":38},{"text":"cross","start":195,"end":200,"id":39},{"text":"-","start":200,"end":201,"id":40},{"text":"validation","start":201,"end":211,"id":41},{"text":"(","start":212,"end":213,"id":42},{"text":"or","start":213,"end":215,"id":43},{"text":"split","start":216,"end":221,"id":44},{"text":"sample","start":222,"end":228,"id":45},{"text":")","start":228,"end":229,"id":46},{"text":",","start":229,"end":230,"id":47},{"text":"and","start":231,"end":234,"id":48},{"text":"the","start":235,"end":238,"id":49},{"text":"leave-$\\upsilon$-out","start":239,"end":259,"id":50},{"text":"cross","start":260,"end":265,"id":51},{"text":"-","start":265,"end":266,"id":52},{"text":"validation","start":266,"end":276,"id":53},{"text":".","start":276,"end":277,"id":54}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"We derive the optimal estimator of f-divergence in the sense of the asymptotic variance, and then investigate the relation between the proposed test procedure and the existing score test based on empirical likelihood estimator.","_input_hash":1058532734,"_task_hash":-1757208693,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"derive","start":3,"end":9,"id":1},{"text":"the","start":10,"end":13,"id":2},{"text":"optimal","start":14,"end":21,"id":3},{"text":"estimator","start":22,"end":31,"id":4},{"text":"of","start":32,"end":34,"id":5},{"text":"f","start":35,"end":36,"id":6},{"text":"-","start":36,"end":37,"id":7},{"text":"divergence","start":37,"end":47,"id":8},{"text":"in","start":48,"end":50,"id":9},{"text":"the","start":51,"end":54,"id":10},{"text":"sense","start":55,"end":60,"id":11},{"text":"of","start":61,"end":63,"id":12},{"text":"the","start":64,"end":67,"id":13},{"text":"asymptotic","start":68,"end":78,"id":14},{"text":"variance","start":79,"end":87,"id":15},{"text":",","start":87,"end":88,"id":16},{"text":"and","start":89,"end":92,"id":17},{"text":"then","start":93,"end":97,"id":18},{"text":"investigate","start":98,"end":109,"id":19},{"text":"the","start":110,"end":113,"id":20},{"text":"relation","start":114,"end":122,"id":21},{"text":"between","start":123,"end":130,"id":22},{"text":"the","start":131,"end":134,"id":23},{"text":"proposed","start":135,"end":143,"id":24},{"text":"test","start":144,"end":148,"id":25},{"text":"procedure","start":149,"end":158,"id":26},{"text":"and","start":159,"end":162,"id":27},{"text":"the","start":163,"end":166,"id":28},{"text":"existing","start":167,"end":175,"id":29},{"text":"score","start":176,"end":181,"id":30},{"text":"test","start":182,"end":186,"id":31},{"text":"based","start":187,"end":192,"id":32},{"text":"on","start":193,"end":195,"id":33},{"text":"empirical","start":196,"end":205,"id":34},{"text":"likelihood","start":206,"end":216,"id":35},{"text":"estimator","start":217,"end":226,"id":36},{"text":".","start":226,"end":227,"id":37}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"This family subsumes the $\\ell_1$ norm and is flexible enough to include different models of sparsity patterns, which are of practical and theoretical importance.","_input_hash":1032534764,"_task_hash":2000981939,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"family","start":5,"end":11,"id":1},{"text":"subsumes","start":12,"end":20,"id":2},{"text":"the","start":21,"end":24,"id":3},{"text":"$","start":25,"end":26,"id":4},{"text":"\\ell_1","start":26,"end":32,"id":5},{"text":"$","start":32,"end":33,"id":6},{"text":"norm","start":34,"end":38,"id":7},{"text":"and","start":39,"end":42,"id":8},{"text":"is","start":43,"end":45,"id":9},{"text":"flexible","start":46,"end":54,"id":10},{"text":"enough","start":55,"end":61,"id":11},{"text":"to","start":62,"end":64,"id":12},{"text":"include","start":65,"end":72,"id":13},{"text":"different","start":73,"end":82,"id":14},{"text":"models","start":83,"end":89,"id":15},{"text":"of","start":90,"end":92,"id":16},{"text":"sparsity","start":93,"end":101,"id":17},{"text":"patterns","start":102,"end":110,"id":18},{"text":",","start":110,"end":111,"id":19},{"text":"which","start":112,"end":117,"id":20},{"text":"are","start":118,"end":121,"id":21},{"text":"of","start":122,"end":124,"id":22},{"text":"practical","start":125,"end":134,"id":23},{"text":"and","start":135,"end":138,"id":24},{"text":"theoretical","start":139,"end":150,"id":25},{"text":"importance","start":151,"end":161,"id":26},{"text":".","start":161,"end":162,"id":27}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"These are defined as the internal states of a system that lead to the same decision, based on a user-provided utility or pay-off function.","_input_hash":-64737787,"_task_hash":-325842168,"tokens":[{"text":"These","start":0,"end":5,"id":0},{"text":"are","start":6,"end":9,"id":1},{"text":"defined","start":10,"end":17,"id":2},{"text":"as","start":18,"end":20,"id":3},{"text":"the","start":21,"end":24,"id":4},{"text":"internal","start":25,"end":33,"id":5},{"text":"states","start":34,"end":40,"id":6},{"text":"of","start":41,"end":43,"id":7},{"text":"a","start":44,"end":45,"id":8},{"text":"system","start":46,"end":52,"id":9},{"text":"that","start":53,"end":57,"id":10},{"text":"lead","start":58,"end":62,"id":11},{"text":"to","start":63,"end":65,"id":12},{"text":"the","start":66,"end":69,"id":13},{"text":"same","start":70,"end":74,"id":14},{"text":"decision","start":75,"end":83,"id":15},{"text":",","start":83,"end":84,"id":16},{"text":"based","start":85,"end":90,"id":17},{"text":"on","start":91,"end":93,"id":18},{"text":"a","start":94,"end":95,"id":19},{"text":"user","start":96,"end":100,"id":20},{"text":"-","start":100,"end":101,"id":21},{"text":"provided","start":101,"end":109,"id":22},{"text":"utility","start":110,"end":117,"id":23},{"text":"or","start":118,"end":120,"id":24},{"text":"pay","start":121,"end":124,"id":25},{"text":"-","start":124,"end":125,"id":26},{"text":"off","start":125,"end":128,"id":27},{"text":"function","start":129,"end":137,"id":28},{"text":".","start":137,"end":138,"id":29}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"Our proof captures the essence of existing proofs based on approachability (e.g., the proof by Foster, 1999 in case of binary outcomes) and highlights the intrinsic connection between approachability and calibration.","_input_hash":459132159,"_task_hash":-490034561,"tokens":[{"text":"Our","start":0,"end":3,"id":0},{"text":"proof","start":4,"end":9,"id":1},{"text":"captures","start":10,"end":18,"id":2},{"text":"the","start":19,"end":22,"id":3},{"text":"essence","start":23,"end":30,"id":4},{"text":"of","start":31,"end":33,"id":5},{"text":"existing","start":34,"end":42,"id":6},{"text":"proofs","start":43,"end":49,"id":7},{"text":"based","start":50,"end":55,"id":8},{"text":"on","start":56,"end":58,"id":9},{"text":"approachability","start":59,"end":74,"id":10},{"text":"(","start":75,"end":76,"id":11},{"text":"e.g.","start":76,"end":80,"id":12},{"text":",","start":80,"end":81,"id":13},{"text":"the","start":82,"end":85,"id":14},{"text":"proof","start":86,"end":91,"id":15},{"text":"by","start":92,"end":94,"id":16},{"text":"Foster","start":95,"end":101,"id":17},{"text":",","start":101,"end":102,"id":18},{"text":"1999","start":103,"end":107,"id":19},{"text":"in","start":108,"end":110,"id":20},{"text":"case","start":111,"end":115,"id":21},{"text":"of","start":116,"end":118,"id":22},{"text":"binary","start":119,"end":125,"id":23},{"text":"outcomes","start":126,"end":134,"id":24},{"text":")","start":134,"end":135,"id":25},{"text":"and","start":136,"end":139,"id":26},{"text":"highlights","start":140,"end":150,"id":27},{"text":"the","start":151,"end":154,"id":28},{"text":"intrinsic","start":155,"end":164,"id":29},{"text":"connection","start":165,"end":175,"id":30},{"text":"between","start":176,"end":183,"id":31},{"text":"approachability","start":184,"end":199,"id":32},{"text":"and","start":200,"end":203,"id":33},{"text":"calibration","start":204,"end":215,"id":34},{"text":".","start":215,"end":216,"id":35}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"Usually the Euclidean distance metric is assumed or a Mahalanobis distance metric is optimized to improve the NN performance.","_input_hash":184187675,"_task_hash":1971771159,"tokens":[{"text":"Usually","start":0,"end":7,"id":0},{"text":"the","start":8,"end":11,"id":1},{"text":"Euclidean","start":12,"end":21,"id":2},{"text":"distance","start":22,"end":30,"id":3},{"text":"metric","start":31,"end":37,"id":4},{"text":"is","start":38,"end":40,"id":5},{"text":"assumed","start":41,"end":48,"id":6},{"text":"or","start":49,"end":51,"id":7},{"text":"a","start":52,"end":53,"id":8},{"text":"Mahalanobis","start":54,"end":65,"id":9},{"text":"distance","start":66,"end":74,"id":10},{"text":"metric","start":75,"end":81,"id":11},{"text":"is","start":82,"end":84,"id":12},{"text":"optimized","start":85,"end":94,"id":13},{"text":"to","start":95,"end":97,"id":14},{"text":"improve","start":98,"end":105,"id":15},{"text":"the","start":106,"end":109,"id":16},{"text":"NN","start":110,"end":112,"id":17},{"text":"performance","start":113,"end":124,"id":18},{"text":".","start":124,"end":125,"id":19}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Density modeling is notoriously difficult for high dimensional data.","_input_hash":-410685504,"_task_hash":-239155619,"tokens":[{"text":"Density","start":0,"end":7,"id":0},{"text":"modeling","start":8,"end":16,"id":1},{"text":"is","start":17,"end":19,"id":2},{"text":"notoriously","start":20,"end":31,"id":3},{"text":"difficult","start":32,"end":41,"id":4},{"text":"for","start":42,"end":45,"id":5},{"text":"high","start":46,"end":50,"id":6},{"text":"dimensional","start":51,"end":62,"id":7},{"text":"data","start":63,"end":67,"id":8},{"text":".","start":67,"end":68,"id":9}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"The corresponding cluster centroids are then computed in order to initialize the generalized Lloyd's algorithm, also known as $K$-means, which allows to circumvent initialization problems.","_input_hash":-1615555195,"_task_hash":1936697132,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"corresponding","start":4,"end":17,"id":1},{"text":"cluster","start":18,"end":25,"id":2},{"text":"centroids","start":26,"end":35,"id":3},{"text":"are","start":36,"end":39,"id":4},{"text":"then","start":40,"end":44,"id":5},{"text":"computed","start":45,"end":53,"id":6},{"text":"in","start":54,"end":56,"id":7},{"text":"order","start":57,"end":62,"id":8},{"text":"to","start":63,"end":65,"id":9},{"text":"initialize","start":66,"end":76,"id":10},{"text":"the","start":77,"end":80,"id":11},{"text":"generalized","start":81,"end":92,"id":12},{"text":"Lloyd","start":93,"end":98,"id":13},{"text":"'s","start":98,"end":100,"id":14},{"text":"algorithm","start":101,"end":110,"id":15},{"text":",","start":110,"end":111,"id":16},{"text":"also","start":112,"end":116,"id":17},{"text":"known","start":117,"end":122,"id":18},{"text":"as","start":123,"end":125,"id":19},{"text":"$","start":126,"end":127,"id":20},{"text":"K$-means","start":127,"end":135,"id":21},{"text":",","start":135,"end":136,"id":22},{"text":"which","start":137,"end":142,"id":23},{"text":"allows","start":143,"end":149,"id":24},{"text":"to","start":150,"end":152,"id":25},{"text":"circumvent","start":153,"end":163,"id":26},{"text":"initialization","start":164,"end":178,"id":27},{"text":"problems","start":179,"end":187,"id":28},{"text":".","start":187,"end":188,"id":29}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":93,"end":110,"token_start":13,"token_end":15,"label":"ALGO","answer":"accept"},{"start":126,"end":135,"token_start":20,"token_end":21,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Though these methods work well for low-dimensional problems, they are not suitable in high dimensional settings.","_input_hash":-239673365,"_task_hash":-1537214801,"tokens":[{"text":"Though","start":0,"end":6,"id":0},{"text":"these","start":7,"end":12,"id":1},{"text":"methods","start":13,"end":20,"id":2},{"text":"work","start":21,"end":25,"id":3},{"text":"well","start":26,"end":30,"id":4},{"text":"for","start":31,"end":34,"id":5},{"text":"low","start":35,"end":38,"id":6},{"text":"-","start":38,"end":39,"id":7},{"text":"dimensional","start":39,"end":50,"id":8},{"text":"problems","start":51,"end":59,"id":9},{"text":",","start":59,"end":60,"id":10},{"text":"they","start":61,"end":65,"id":11},{"text":"are","start":66,"end":69,"id":12},{"text":"not","start":70,"end":73,"id":13},{"text":"suitable","start":74,"end":82,"id":14},{"text":"in","start":83,"end":85,"id":15},{"text":"high","start":86,"end":90,"id":16},{"text":"dimensional","start":91,"end":102,"id":17},{"text":"settings","start":103,"end":111,"id":18},{"text":".","start":111,"end":112,"id":19}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"During the last years a great part of the statistical research on SVMs has concentrated on the question how to design SVMs such that they are universally consistent and statistically robust for nonparametric classification or nonparametric regression purposes.","_input_hash":931169385,"_task_hash":-616955355,"tokens":[{"text":"During","start":0,"end":6,"id":0},{"text":"the","start":7,"end":10,"id":1},{"text":"last","start":11,"end":15,"id":2},{"text":"years","start":16,"end":21,"id":3},{"text":"a","start":22,"end":23,"id":4},{"text":"great","start":24,"end":29,"id":5},{"text":"part","start":30,"end":34,"id":6},{"text":"of","start":35,"end":37,"id":7},{"text":"the","start":38,"end":41,"id":8},{"text":"statistical","start":42,"end":53,"id":9},{"text":"research","start":54,"end":62,"id":10},{"text":"on","start":63,"end":65,"id":11},{"text":"SVMs","start":66,"end":70,"id":12},{"text":"has","start":71,"end":74,"id":13},{"text":"concentrated","start":75,"end":87,"id":14},{"text":"on","start":88,"end":90,"id":15},{"text":"the","start":91,"end":94,"id":16},{"text":"question","start":95,"end":103,"id":17},{"text":"how","start":104,"end":107,"id":18},{"text":"to","start":108,"end":110,"id":19},{"text":"design","start":111,"end":117,"id":20},{"text":"SVMs","start":118,"end":122,"id":21},{"text":"such","start":123,"end":127,"id":22},{"text":"that","start":128,"end":132,"id":23},{"text":"they","start":133,"end":137,"id":24},{"text":"are","start":138,"end":141,"id":25},{"text":"universally","start":142,"end":153,"id":26},{"text":"consistent","start":154,"end":164,"id":27},{"text":"and","start":165,"end":168,"id":28},{"text":"statistically","start":169,"end":182,"id":29},{"text":"robust","start":183,"end":189,"id":30},{"text":"for","start":190,"end":193,"id":31},{"text":"nonparametric","start":194,"end":207,"id":32},{"text":"classification","start":208,"end":222,"id":33},{"text":"or","start":223,"end":225,"id":34},{"text":"nonparametric","start":226,"end":239,"id":35},{"text":"regression","start":240,"end":250,"id":36},{"text":"purposes","start":251,"end":259,"id":37},{"text":".","start":259,"end":260,"id":38}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":66,"end":70,"token_start":12,"token_end":12,"label":"ALGO","answer":"accept"},{"start":118,"end":122,"token_start":21,"token_end":21,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"This modification also removes the need for regularization when the number of neighbors is larger than the dimension of the input.","_input_hash":-46676230,"_task_hash":-509110954,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"modification","start":5,"end":17,"id":1},{"text":"also","start":18,"end":22,"id":2},{"text":"removes","start":23,"end":30,"id":3},{"text":"the","start":31,"end":34,"id":4},{"text":"need","start":35,"end":39,"id":5},{"text":"for","start":40,"end":43,"id":6},{"text":"regularization","start":44,"end":58,"id":7},{"text":"when","start":59,"end":63,"id":8},{"text":"the","start":64,"end":67,"id":9},{"text":"number","start":68,"end":74,"id":10},{"text":"of","start":75,"end":77,"id":11},{"text":"neighbors","start":78,"end":87,"id":12},{"text":"is","start":88,"end":90,"id":13},{"text":"larger","start":91,"end":97,"id":14},{"text":"than","start":98,"end":102,"id":15},{"text":"the","start":103,"end":106,"id":16},{"text":"dimension","start":107,"end":116,"id":17},{"text":"of","start":117,"end":119,"id":18},{"text":"the","start":120,"end":123,"id":19},{"text":"input","start":124,"end":129,"id":20},{"text":".","start":129,"end":130,"id":21}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Using convex analysis on the set of posterior probability measures, we show how to get local measures of the complexity of the classification model involving the relative entropy of posterior distributions with respect to Gibbs posterior measures.","_input_hash":-1112714564,"_task_hash":-1706041221,"tokens":[{"text":"Using","start":0,"end":5,"id":0},{"text":"convex","start":6,"end":12,"id":1},{"text":"analysis","start":13,"end":21,"id":2},{"text":"on","start":22,"end":24,"id":3},{"text":"the","start":25,"end":28,"id":4},{"text":"set","start":29,"end":32,"id":5},{"text":"of","start":33,"end":35,"id":6},{"text":"posterior","start":36,"end":45,"id":7},{"text":"probability","start":46,"end":57,"id":8},{"text":"measures","start":58,"end":66,"id":9},{"text":",","start":66,"end":67,"id":10},{"text":"we","start":68,"end":70,"id":11},{"text":"show","start":71,"end":75,"id":12},{"text":"how","start":76,"end":79,"id":13},{"text":"to","start":80,"end":82,"id":14},{"text":"get","start":83,"end":86,"id":15},{"text":"local","start":87,"end":92,"id":16},{"text":"measures","start":93,"end":101,"id":17},{"text":"of","start":102,"end":104,"id":18},{"text":"the","start":105,"end":108,"id":19},{"text":"complexity","start":109,"end":119,"id":20},{"text":"of","start":120,"end":122,"id":21},{"text":"the","start":123,"end":126,"id":22},{"text":"classification","start":127,"end":141,"id":23},{"text":"model","start":142,"end":147,"id":24},{"text":"involving","start":148,"end":157,"id":25},{"text":"the","start":158,"end":161,"id":26},{"text":"relative","start":162,"end":170,"id":27},{"text":"entropy","start":171,"end":178,"id":28},{"text":"of","start":179,"end":181,"id":29},{"text":"posterior","start":182,"end":191,"id":30},{"text":"distributions","start":192,"end":205,"id":31},{"text":"with","start":206,"end":210,"id":32},{"text":"respect","start":211,"end":218,"id":33},{"text":"to","start":219,"end":221,"id":34},{"text":"Gibbs","start":222,"end":227,"id":35},{"text":"posterior","start":228,"end":237,"id":36},{"text":"measures","start":238,"end":246,"id":37},{"text":".","start":246,"end":247,"id":38}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"The issues associated with these difficulties relate to the broader structure of discrete exponential families.","_input_hash":-1951682925,"_task_hash":-212268159,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"issues","start":4,"end":10,"id":1},{"text":"associated","start":11,"end":21,"id":2},{"text":"with","start":22,"end":26,"id":3},{"text":"these","start":27,"end":32,"id":4},{"text":"difficulties","start":33,"end":45,"id":5},{"text":"relate","start":46,"end":52,"id":6},{"text":"to","start":53,"end":55,"id":7},{"text":"the","start":56,"end":59,"id":8},{"text":"broader","start":60,"end":67,"id":9},{"text":"structure","start":68,"end":77,"id":10},{"text":"of","start":78,"end":80,"id":11},{"text":"discrete","start":81,"end":89,"id":12},{"text":"exponential","start":90,"end":101,"id":13},{"text":"families","start":102,"end":110,"id":14},{"text":".","start":110,"end":111,"id":15}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"Through numerical studies, we illustrate the adequacy of the asymptotic theory for finite-sample inference.","_input_hash":-82152914,"_task_hash":-83550846,"tokens":[{"text":"Through","start":0,"end":7,"id":0},{"text":"numerical","start":8,"end":17,"id":1},{"text":"studies","start":18,"end":25,"id":2},{"text":",","start":25,"end":26,"id":3},{"text":"we","start":27,"end":29,"id":4},{"text":"illustrate","start":30,"end":40,"id":5},{"text":"the","start":41,"end":44,"id":6},{"text":"adequacy","start":45,"end":53,"id":7},{"text":"of","start":54,"end":56,"id":8},{"text":"the","start":57,"end":60,"id":9},{"text":"asymptotic","start":61,"end":71,"id":10},{"text":"theory","start":72,"end":78,"id":11},{"text":"for","start":79,"end":82,"id":12},{"text":"finite","start":83,"end":89,"id":13},{"text":"-","start":89,"end":90,"id":14},{"text":"sample","start":90,"end":96,"id":15},{"text":"inference","start":97,"end":106,"id":16},{"text":".","start":106,"end":107,"id":17}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"We propose a conceptually simple method, akin to bagging, to approach both inductive and transductive PU learning problems, by converting them into series of supervised binary classification problems discriminating the known positive examples from random subsamples of the unlabeled set.","_input_hash":574901897,"_task_hash":1990272058,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"propose","start":3,"end":10,"id":1},{"text":"a","start":11,"end":12,"id":2},{"text":"conceptually","start":13,"end":25,"id":3},{"text":"simple","start":26,"end":32,"id":4},{"text":"method","start":33,"end":39,"id":5},{"text":",","start":39,"end":40,"id":6},{"text":"akin","start":41,"end":45,"id":7},{"text":"to","start":46,"end":48,"id":8},{"text":"bagging","start":49,"end":56,"id":9},{"text":",","start":56,"end":57,"id":10},{"text":"to","start":58,"end":60,"id":11},{"text":"approach","start":61,"end":69,"id":12},{"text":"both","start":70,"end":74,"id":13},{"text":"inductive","start":75,"end":84,"id":14},{"text":"and","start":85,"end":88,"id":15},{"text":"transductive","start":89,"end":101,"id":16},{"text":"PU","start":102,"end":104,"id":17},{"text":"learning","start":105,"end":113,"id":18},{"text":"problems","start":114,"end":122,"id":19},{"text":",","start":122,"end":123,"id":20},{"text":"by","start":124,"end":126,"id":21},{"text":"converting","start":127,"end":137,"id":22},{"text":"them","start":138,"end":142,"id":23},{"text":"into","start":143,"end":147,"id":24},{"text":"series","start":148,"end":154,"id":25},{"text":"of","start":155,"end":157,"id":26},{"text":"supervised","start":158,"end":168,"id":27},{"text":"binary","start":169,"end":175,"id":28},{"text":"classification","start":176,"end":190,"id":29},{"text":"problems","start":191,"end":199,"id":30},{"text":"discriminating","start":200,"end":214,"id":31},{"text":"the","start":215,"end":218,"id":32},{"text":"known","start":219,"end":224,"id":33},{"text":"positive","start":225,"end":233,"id":34},{"text":"examples","start":234,"end":242,"id":35},{"text":"from","start":243,"end":247,"id":36},{"text":"random","start":248,"end":254,"id":37},{"text":"subsamples","start":255,"end":265,"id":38},{"text":"of","start":266,"end":268,"id":39},{"text":"the","start":269,"end":272,"id":40},{"text":"unlabeled","start":273,"end":282,"id":41},{"text":"set","start":283,"end":286,"id":42},{"text":".","start":286,"end":287,"id":43}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":49,"end":56,"token_start":9,"token_end":9,"label":"ALGO","answer":"accept"},{"start":169,"end":190,"token_start":28,"token_end":29,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"fitness_function|NOUN","word":"fitness function","sense":"NOUN","meta":{"score":0.8312000036,"sense":"NOUN"},"_input_hash":-1091655408,"_task_hash":42907997,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"fitness_function|NOUN","start":0,"end":21,"id":0}]}
{"text":"As opposed to other papers, we consider the actual k-means algorithm and do not ignore its property of getting stuck in local optima.","_input_hash":1791070084,"_task_hash":-361261615,"tokens":[{"text":"As","start":0,"end":2,"id":0},{"text":"opposed","start":3,"end":10,"id":1},{"text":"to","start":11,"end":13,"id":2},{"text":"other","start":14,"end":19,"id":3},{"text":"papers","start":20,"end":26,"id":4},{"text":",","start":26,"end":27,"id":5},{"text":"we","start":28,"end":30,"id":6},{"text":"consider","start":31,"end":39,"id":7},{"text":"the","start":40,"end":43,"id":8},{"text":"actual","start":44,"end":50,"id":9},{"text":"k","start":51,"end":52,"id":10},{"text":"-","start":52,"end":53,"id":11},{"text":"means","start":53,"end":58,"id":12},{"text":"algorithm","start":59,"end":68,"id":13},{"text":"and","start":69,"end":72,"id":14},{"text":"do","start":73,"end":75,"id":15},{"text":"not","start":76,"end":79,"id":16},{"text":"ignore","start":80,"end":86,"id":17},{"text":"its","start":87,"end":90,"id":18},{"text":"property","start":91,"end":99,"id":19},{"text":"of","start":100,"end":102,"id":20},{"text":"getting","start":103,"end":110,"id":21},{"text":"stuck","start":111,"end":116,"id":22},{"text":"in","start":117,"end":119,"id":23},{"text":"local","start":120,"end":125,"id":24},{"text":"optima","start":126,"end":132,"id":25},{"text":".","start":132,"end":133,"id":26}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":51,"end":68,"token_start":10,"token_end":13,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"computational_complexity|NOUN","word":"computational complexity","sense":"NOUN","meta":{"score":0.7725999951,"sense":"NOUN"},"_input_hash":-771373447,"_task_hash":-322907055,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"computational_complexity|NOUN","start":0,"end":29,"id":0}]}
{"text":"The fuzzy logic can be easily introduced into the system, even if learning agents are build from simple binary classification machine learning algorithms by calculating the percentage of agreeing agents.","_input_hash":-1774464216,"_task_hash":-1526623302,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"fuzzy","start":4,"end":9,"id":1},{"text":"logic","start":10,"end":15,"id":2},{"text":"can","start":16,"end":19,"id":3},{"text":"be","start":20,"end":22,"id":4},{"text":"easily","start":23,"end":29,"id":5},{"text":"introduced","start":30,"end":40,"id":6},{"text":"into","start":41,"end":45,"id":7},{"text":"the","start":46,"end":49,"id":8},{"text":"system","start":50,"end":56,"id":9},{"text":",","start":56,"end":57,"id":10},{"text":"even","start":58,"end":62,"id":11},{"text":"if","start":63,"end":65,"id":12},{"text":"learning","start":66,"end":74,"id":13},{"text":"agents","start":75,"end":81,"id":14},{"text":"are","start":82,"end":85,"id":15},{"text":"build","start":86,"end":91,"id":16},{"text":"from","start":92,"end":96,"id":17},{"text":"simple","start":97,"end":103,"id":18},{"text":"binary","start":104,"end":110,"id":19},{"text":"classification","start":111,"end":125,"id":20},{"text":"machine","start":126,"end":133,"id":21},{"text":"learning","start":134,"end":142,"id":22},{"text":"algorithms","start":143,"end":153,"id":23},{"text":"by","start":154,"end":156,"id":24},{"text":"calculating","start":157,"end":168,"id":25},{"text":"the","start":169,"end":172,"id":26},{"text":"percentage","start":173,"end":183,"id":27},{"text":"of","start":184,"end":186,"id":28},{"text":"agreeing","start":187,"end":195,"id":29},{"text":"agents","start":196,"end":202,"id":30},{"text":".","start":202,"end":203,"id":31}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":4,"end":15,"token_start":1,"token_end":2,"label":"ALGO","answer":"accept"},{"start":104,"end":125,"token_start":19,"token_end":20,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"biological_system|NOUN","word":"biological system","sense":"NOUN","meta":{"score":0.7764000297,"sense":"NOUN"},"_input_hash":676608932,"_task_hash":-210405964,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"biological_system|NOUN","start":0,"end":22,"id":0}]}
{"text":"The desirable properties of FIRM are investigated analytically and illustrated in simulations.","_input_hash":-1728202811,"_task_hash":1898366464,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"desirable","start":4,"end":13,"id":1},{"text":"properties","start":14,"end":24,"id":2},{"text":"of","start":25,"end":27,"id":3},{"text":"FIRM","start":28,"end":32,"id":4},{"text":"are","start":33,"end":36,"id":5},{"text":"investigated","start":37,"end":49,"id":6},{"text":"analytically","start":50,"end":62,"id":7},{"text":"and","start":63,"end":66,"id":8},{"text":"illustrated","start":67,"end":78,"id":9},{"text":"in","start":79,"end":81,"id":10},{"text":"simulations","start":82,"end":93,"id":11},{"text":".","start":93,"end":94,"id":12}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"This method embeds a dendrogram as a subtree into the Bruhat-Tits tree associated to the p-adic numbers, and goes back to Cornelissen et al. (","_input_hash":778461077,"_task_hash":-683209314,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"method","start":5,"end":11,"id":1},{"text":"embeds","start":12,"end":18,"id":2},{"text":"a","start":19,"end":20,"id":3},{"text":"dendrogram","start":21,"end":31,"id":4},{"text":"as","start":32,"end":34,"id":5},{"text":"a","start":35,"end":36,"id":6},{"text":"subtree","start":37,"end":44,"id":7},{"text":"into","start":45,"end":49,"id":8},{"text":"the","start":50,"end":53,"id":9},{"text":"Bruhat","start":54,"end":60,"id":10},{"text":"-","start":60,"end":61,"id":11},{"text":"Tits","start":61,"end":65,"id":12},{"text":"tree","start":66,"end":70,"id":13},{"text":"associated","start":71,"end":81,"id":14},{"text":"to","start":82,"end":84,"id":15},{"text":"the","start":85,"end":88,"id":16},{"text":"p","start":89,"end":90,"id":17},{"text":"-","start":90,"end":91,"id":18},{"text":"adic","start":91,"end":95,"id":19},{"text":"numbers","start":96,"end":103,"id":20},{"text":",","start":103,"end":104,"id":21},{"text":"and","start":105,"end":108,"id":22},{"text":"goes","start":109,"end":113,"id":23},{"text":"back","start":114,"end":118,"id":24},{"text":"to","start":119,"end":121,"id":25},{"text":"Cornelissen","start":122,"end":133,"id":26},{"text":"et","start":134,"end":136,"id":27},{"text":"al","start":137,"end":139,"id":28},{"text":".","start":139,"end":140,"id":29},{"text":"(","start":141,"end":142,"id":30}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"Dendrograms used in data analysis are ultrametric spaces, hence objects of nonarchimedean geometry.","_input_hash":-1804434462,"_task_hash":1288390984,"tokens":[{"text":"Dendrograms","start":0,"end":11,"id":0},{"text":"used","start":12,"end":16,"id":1},{"text":"in","start":17,"end":19,"id":2},{"text":"data","start":20,"end":24,"id":3},{"text":"analysis","start":25,"end":33,"id":4},{"text":"are","start":34,"end":37,"id":5},{"text":"ultrametric","start":38,"end":49,"id":6},{"text":"spaces","start":50,"end":56,"id":7},{"text":",","start":56,"end":57,"id":8},{"text":"hence","start":58,"end":63,"id":9},{"text":"objects","start":64,"end":71,"id":10},{"text":"of","start":72,"end":74,"id":11},{"text":"nonarchimedean","start":75,"end":89,"id":12},{"text":"geometry","start":90,"end":98,"id":13},{"text":".","start":98,"end":99,"id":14}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"The combination of sparsity and smoothness is crucial for mathematical theory as well as performance for finite-sample data.","_input_hash":161615895,"_task_hash":-1344135724,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"combination","start":4,"end":15,"id":1},{"text":"of","start":16,"end":18,"id":2},{"text":"sparsity","start":19,"end":27,"id":3},{"text":"and","start":28,"end":31,"id":4},{"text":"smoothness","start":32,"end":42,"id":5},{"text":"is","start":43,"end":45,"id":6},{"text":"crucial","start":46,"end":53,"id":7},{"text":"for","start":54,"end":57,"id":8},{"text":"mathematical","start":58,"end":70,"id":9},{"text":"theory","start":71,"end":77,"id":10},{"text":"as","start":78,"end":80,"id":11},{"text":"well","start":81,"end":85,"id":12},{"text":"as","start":86,"end":88,"id":13},{"text":"performance","start":89,"end":100,"id":14},{"text":"for","start":101,"end":104,"id":15},{"text":"finite","start":105,"end":111,"id":16},{"text":"-","start":111,"end":112,"id":17},{"text":"sample","start":112,"end":118,"id":18},{"text":"data","start":119,"end":123,"id":19},{"text":".","start":123,"end":124,"id":20}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"nothin","meta":{"score":0},"_input_hash":-1242699596,"_task_hash":-169358100,"_session_id":null,"_view_id":"text","answer":"reject","spans":[],"tokens":[{"text":"nothin","start":0,"end":6,"id":0}]}
{"text":"The method combines cutting plane optimization with red-black tree based approach to subgradient calculations, and has O(m*s+m*log(m)) time complexity, where m is the number of training examples, and s the average number of non-zero features per example.","_input_hash":1704006268,"_task_hash":2071263571,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"method","start":4,"end":10,"id":1},{"text":"combines","start":11,"end":19,"id":2},{"text":"cutting","start":20,"end":27,"id":3},{"text":"plane","start":28,"end":33,"id":4},{"text":"optimization","start":34,"end":46,"id":5},{"text":"with","start":47,"end":51,"id":6},{"text":"red","start":52,"end":55,"id":7},{"text":"-","start":55,"end":56,"id":8},{"text":"black","start":56,"end":61,"id":9},{"text":"tree","start":62,"end":66,"id":10},{"text":"based","start":67,"end":72,"id":11},{"text":"approach","start":73,"end":81,"id":12},{"text":"to","start":82,"end":84,"id":13},{"text":"subgradient","start":85,"end":96,"id":14},{"text":"calculations","start":97,"end":109,"id":15},{"text":",","start":109,"end":110,"id":16},{"text":"and","start":111,"end":114,"id":17},{"text":"has","start":115,"end":118,"id":18},{"text":"O(m*s+m*log(m","start":119,"end":132,"id":19},{"text":")","start":132,"end":133,"id":20},{"text":")","start":133,"end":134,"id":21},{"text":"time","start":135,"end":139,"id":22},{"text":"complexity","start":140,"end":150,"id":23},{"text":",","start":150,"end":151,"id":24},{"text":"where","start":152,"end":157,"id":25},{"text":"m","start":158,"end":159,"id":26},{"text":"is","start":160,"end":162,"id":27},{"text":"the","start":163,"end":166,"id":28},{"text":"number","start":167,"end":173,"id":29},{"text":"of","start":174,"end":176,"id":30},{"text":"training","start":177,"end":185,"id":31},{"text":"examples","start":186,"end":194,"id":32},{"text":",","start":194,"end":195,"id":33},{"text":"and","start":196,"end":199,"id":34},{"text":"s","start":200,"end":201,"id":35},{"text":"the","start":202,"end":205,"id":36},{"text":"average","start":206,"end":213,"id":37},{"text":"number","start":214,"end":220,"id":38},{"text":"of","start":221,"end":223,"id":39},{"text":"non","start":224,"end":227,"id":40},{"text":"-","start":227,"end":228,"id":41},{"text":"zero","start":228,"end":232,"id":42},{"text":"features","start":233,"end":241,"id":43},{"text":"per","start":242,"end":245,"id":44},{"text":"example","start":246,"end":253,"id":45},{"text":".","start":253,"end":254,"id":46}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"In addition, a slight modification of KNIFE yields an efficient algorithm for finding feature regularization paths, or the paths of each feature's weight.","_input_hash":1018985911,"_task_hash":-1010991773,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"addition","start":3,"end":11,"id":1},{"text":",","start":11,"end":12,"id":2},{"text":"a","start":13,"end":14,"id":3},{"text":"slight","start":15,"end":21,"id":4},{"text":"modification","start":22,"end":34,"id":5},{"text":"of","start":35,"end":37,"id":6},{"text":"KNIFE","start":38,"end":43,"id":7},{"text":"yields","start":44,"end":50,"id":8},{"text":"an","start":51,"end":53,"id":9},{"text":"efficient","start":54,"end":63,"id":10},{"text":"algorithm","start":64,"end":73,"id":11},{"text":"for","start":74,"end":77,"id":12},{"text":"finding","start":78,"end":85,"id":13},{"text":"feature","start":86,"end":93,"id":14},{"text":"regularization","start":94,"end":108,"id":15},{"text":"paths","start":109,"end":114,"id":16},{"text":",","start":114,"end":115,"id":17},{"text":"or","start":116,"end":118,"id":18},{"text":"the","start":119,"end":122,"id":19},{"text":"paths","start":123,"end":128,"id":20},{"text":"of","start":129,"end":131,"id":21},{"text":"each","start":132,"end":136,"id":22},{"text":"feature","start":137,"end":144,"id":23},{"text":"'s","start":144,"end":146,"id":24},{"text":"weight","start":147,"end":153,"id":25},{"text":".","start":153,"end":154,"id":26}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We present a new boosting algorithm, motivated by the large margins theory for boosting.","_input_hash":1649373943,"_task_hash":-1718613511,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"present","start":3,"end":10,"id":1},{"text":"a","start":11,"end":12,"id":2},{"text":"new","start":13,"end":16,"id":3},{"text":"boosting","start":17,"end":25,"id":4},{"text":"algorithm","start":26,"end":35,"id":5},{"text":",","start":35,"end":36,"id":6},{"text":"motivated","start":37,"end":46,"id":7},{"text":"by","start":47,"end":49,"id":8},{"text":"the","start":50,"end":53,"id":9},{"text":"large","start":54,"end":59,"id":10},{"text":"margins","start":60,"end":67,"id":11},{"text":"theory","start":68,"end":74,"id":12},{"text":"for","start":75,"end":78,"id":13},{"text":"boosting","start":79,"end":87,"id":14},{"text":".","start":87,"end":88,"id":15}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":17,"end":25,"token_start":4,"token_end":4,"label":"ALGO","answer":"accept"},{"start":79,"end":87,"token_start":14,"token_end":14,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"This \\emph{structured sparse PCA} is based on a structured regularization recently introduced by [1].","_input_hash":353680850,"_task_hash":443923090,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"\\emph{structured","start":5,"end":21,"id":1},{"text":"sparse","start":22,"end":28,"id":2},{"text":"PCA","start":29,"end":32,"id":3},{"text":"}","start":32,"end":33,"id":4},{"text":"is","start":34,"end":36,"id":5},{"text":"based","start":37,"end":42,"id":6},{"text":"on","start":43,"end":45,"id":7},{"text":"a","start":46,"end":47,"id":8},{"text":"structured","start":48,"end":58,"id":9},{"text":"regularization","start":59,"end":73,"id":10},{"text":"recently","start":74,"end":82,"id":11},{"text":"introduced","start":83,"end":93,"id":12},{"text":"by","start":94,"end":96,"id":13},{"text":"[","start":97,"end":98,"id":14},{"text":"1","start":98,"end":99,"id":15},{"text":"]","start":99,"end":100,"id":16},{"text":".","start":100,"end":101,"id":17}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":22,"end":32,"token_start":2,"token_end":3,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Then, we apply our optimization tools in the context of dictionary learning, where learned dictionary elements naturally organize in a prespecified arborescent structure, leading to a better performance in reconstruction of natural image patches.","_input_hash":1548222526,"_task_hash":-1757348982,"tokens":[{"text":"Then","start":0,"end":4,"id":0},{"text":",","start":4,"end":5,"id":1},{"text":"we","start":6,"end":8,"id":2},{"text":"apply","start":9,"end":14,"id":3},{"text":"our","start":15,"end":18,"id":4},{"text":"optimization","start":19,"end":31,"id":5},{"text":"tools","start":32,"end":37,"id":6},{"text":"in","start":38,"end":40,"id":7},{"text":"the","start":41,"end":44,"id":8},{"text":"context","start":45,"end":52,"id":9},{"text":"of","start":53,"end":55,"id":10},{"text":"dictionary","start":56,"end":66,"id":11},{"text":"learning","start":67,"end":75,"id":12},{"text":",","start":75,"end":76,"id":13},{"text":"where","start":77,"end":82,"id":14},{"text":"learned","start":83,"end":90,"id":15},{"text":"dictionary","start":91,"end":101,"id":16},{"text":"elements","start":102,"end":110,"id":17},{"text":"naturally","start":111,"end":120,"id":18},{"text":"organize","start":121,"end":129,"id":19},{"text":"in","start":130,"end":132,"id":20},{"text":"a","start":133,"end":134,"id":21},{"text":"prespecified","start":135,"end":147,"id":22},{"text":"arborescent","start":148,"end":159,"id":23},{"text":"structure","start":160,"end":169,"id":24},{"text":",","start":169,"end":170,"id":25},{"text":"leading","start":171,"end":178,"id":26},{"text":"to","start":179,"end":181,"id":27},{"text":"a","start":182,"end":183,"id":28},{"text":"better","start":184,"end":190,"id":29},{"text":"performance","start":191,"end":202,"id":30},{"text":"in","start":203,"end":205,"id":31},{"text":"reconstruction","start":206,"end":220,"id":32},{"text":"of","start":221,"end":223,"id":33},{"text":"natural","start":224,"end":231,"id":34},{"text":"image","start":232,"end":237,"id":35},{"text":"patches","start":238,"end":245,"id":36},{"text":".","start":245,"end":246,"id":37}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"abstractions|NOUN","word":"abstractions","sense":"NOUN","meta":{"score":0.7645000219,"sense":"NOUN"},"_input_hash":-19373359,"_task_hash":1548704561,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"abstractions|NOUN","start":0,"end":17,"id":0}]}
{"text":"But the main contribution of this paper is twofold:","_input_hash":839304675,"_task_hash":946156102,"tokens":[{"text":"But","start":0,"end":3,"id":0},{"text":"the","start":4,"end":7,"id":1},{"text":"main","start":8,"end":12,"id":2},{"text":"contribution","start":13,"end":25,"id":3},{"text":"of","start":26,"end":28,"id":4},{"text":"this","start":29,"end":33,"id":5},{"text":"paper","start":34,"end":39,"id":6},{"text":"is","start":40,"end":42,"id":7},{"text":"twofold","start":43,"end":50,"id":8},{"text":":","start":50,"end":51,"id":9}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We model the contribution of each source to all mixture channels in the time-frequency domain as a zero-mean Gaussian random variable whose covariance encodes the spatial characteristics of the source.","_input_hash":-1097143795,"_task_hash":-1929968510,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"model","start":3,"end":8,"id":1},{"text":"the","start":9,"end":12,"id":2},{"text":"contribution","start":13,"end":25,"id":3},{"text":"of","start":26,"end":28,"id":4},{"text":"each","start":29,"end":33,"id":5},{"text":"source","start":34,"end":40,"id":6},{"text":"to","start":41,"end":43,"id":7},{"text":"all","start":44,"end":47,"id":8},{"text":"mixture","start":48,"end":55,"id":9},{"text":"channels","start":56,"end":64,"id":10},{"text":"in","start":65,"end":67,"id":11},{"text":"the","start":68,"end":71,"id":12},{"text":"time","start":72,"end":76,"id":13},{"text":"-","start":76,"end":77,"id":14},{"text":"frequency","start":77,"end":86,"id":15},{"text":"domain","start":87,"end":93,"id":16},{"text":"as","start":94,"end":96,"id":17},{"text":"a","start":97,"end":98,"id":18},{"text":"zero","start":99,"end":103,"id":19},{"text":"-","start":103,"end":104,"id":20},{"text":"mean","start":104,"end":108,"id":21},{"text":"Gaussian","start":109,"end":117,"id":22},{"text":"random","start":118,"end":124,"id":23},{"text":"variable","start":125,"end":133,"id":24},{"text":"whose","start":134,"end":139,"id":25},{"text":"covariance","start":140,"end":150,"id":26},{"text":"encodes","start":151,"end":158,"id":27},{"text":"the","start":159,"end":162,"id":28},{"text":"spatial","start":163,"end":170,"id":29},{"text":"characteristics","start":171,"end":186,"id":30},{"text":"of","start":187,"end":189,"id":31},{"text":"the","start":190,"end":193,"id":32},{"text":"source","start":194,"end":200,"id":33},{"text":".","start":200,"end":201,"id":34}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"A conceptual framework for cluster analysis from the viewpoint of p-adic geometry is introduced by describing the space of all dendrograms for n datapoints and relating it to the moduli space of p-adic Riemannian spheres with punctures using a method recently applied by Murtagh (2004b).","_input_hash":-1260630185,"_task_hash":-739351078,"tokens":[{"text":"A","start":0,"end":1,"id":0},{"text":"conceptual","start":2,"end":12,"id":1},{"text":"framework","start":13,"end":22,"id":2},{"text":"for","start":23,"end":26,"id":3},{"text":"cluster","start":27,"end":34,"id":4},{"text":"analysis","start":35,"end":43,"id":5},{"text":"from","start":44,"end":48,"id":6},{"text":"the","start":49,"end":52,"id":7},{"text":"viewpoint","start":53,"end":62,"id":8},{"text":"of","start":63,"end":65,"id":9},{"text":"p","start":66,"end":67,"id":10},{"text":"-","start":67,"end":68,"id":11},{"text":"adic","start":68,"end":72,"id":12},{"text":"geometry","start":73,"end":81,"id":13},{"text":"is","start":82,"end":84,"id":14},{"text":"introduced","start":85,"end":95,"id":15},{"text":"by","start":96,"end":98,"id":16},{"text":"describing","start":99,"end":109,"id":17},{"text":"the","start":110,"end":113,"id":18},{"text":"space","start":114,"end":119,"id":19},{"text":"of","start":120,"end":122,"id":20},{"text":"all","start":123,"end":126,"id":21},{"text":"dendrograms","start":127,"end":138,"id":22},{"text":"for","start":139,"end":142,"id":23},{"text":"n","start":143,"end":144,"id":24},{"text":"datapoints","start":145,"end":155,"id":25},{"text":"and","start":156,"end":159,"id":26},{"text":"relating","start":160,"end":168,"id":27},{"text":"it","start":169,"end":171,"id":28},{"text":"to","start":172,"end":174,"id":29},{"text":"the","start":175,"end":178,"id":30},{"text":"moduli","start":179,"end":185,"id":31},{"text":"space","start":186,"end":191,"id":32},{"text":"of","start":192,"end":194,"id":33},{"text":"p","start":195,"end":196,"id":34},{"text":"-","start":196,"end":197,"id":35},{"text":"adic","start":197,"end":201,"id":36},{"text":"Riemannian","start":202,"end":212,"id":37},{"text":"spheres","start":213,"end":220,"id":38},{"text":"with","start":221,"end":225,"id":39},{"text":"punctures","start":226,"end":235,"id":40},{"text":"using","start":236,"end":241,"id":41},{"text":"a","start":242,"end":243,"id":42},{"text":"method","start":244,"end":250,"id":43},{"text":"recently","start":251,"end":259,"id":44},{"text":"applied","start":260,"end":267,"id":45},{"text":"by","start":268,"end":270,"id":46},{"text":"Murtagh","start":271,"end":278,"id":47},{"text":"(","start":279,"end":280,"id":48},{"text":"2004b","start":280,"end":285,"id":49},{"text":")","start":285,"end":286,"id":50},{"text":".","start":286,"end":287,"id":51}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"For graph estimation, we consider the problem of estimating forests with restricted tree sizes.","_input_hash":-2038417911,"_task_hash":1821268744,"tokens":[{"text":"For","start":0,"end":3,"id":0},{"text":"graph","start":4,"end":9,"id":1},{"text":"estimation","start":10,"end":20,"id":2},{"text":",","start":20,"end":21,"id":3},{"text":"we","start":22,"end":24,"id":4},{"text":"consider","start":25,"end":33,"id":5},{"text":"the","start":34,"end":37,"id":6},{"text":"problem","start":38,"end":45,"id":7},{"text":"of","start":46,"end":48,"id":8},{"text":"estimating","start":49,"end":59,"id":9},{"text":"forests","start":60,"end":67,"id":10},{"text":"with","start":68,"end":72,"id":11},{"text":"restricted","start":73,"end":83,"id":12},{"text":"tree","start":84,"end":88,"id":13},{"text":"sizes","start":89,"end":94,"id":14},{"text":".","start":94,"end":95,"id":15}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":60,"end":67,"token_start":10,"token_end":10,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"In the last few years, due to the growing ubiquity of unlabeled data, much effort has been spent by the machine learning community to develop better understanding and improve the quality of classifiers exploiting unlabeled data.","_input_hash":1675284532,"_task_hash":-2132619793,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"the","start":3,"end":6,"id":1},{"text":"last","start":7,"end":11,"id":2},{"text":"few","start":12,"end":15,"id":3},{"text":"years","start":16,"end":21,"id":4},{"text":",","start":21,"end":22,"id":5},{"text":"due","start":23,"end":26,"id":6},{"text":"to","start":27,"end":29,"id":7},{"text":"the","start":30,"end":33,"id":8},{"text":"growing","start":34,"end":41,"id":9},{"text":"ubiquity","start":42,"end":50,"id":10},{"text":"of","start":51,"end":53,"id":11},{"text":"unlabeled","start":54,"end":63,"id":12},{"text":"data","start":64,"end":68,"id":13},{"text":",","start":68,"end":69,"id":14},{"text":"much","start":70,"end":74,"id":15},{"text":"effort","start":75,"end":81,"id":16},{"text":"has","start":82,"end":85,"id":17},{"text":"been","start":86,"end":90,"id":18},{"text":"spent","start":91,"end":96,"id":19},{"text":"by","start":97,"end":99,"id":20},{"text":"the","start":100,"end":103,"id":21},{"text":"machine","start":104,"end":111,"id":22},{"text":"learning","start":112,"end":120,"id":23},{"text":"community","start":121,"end":130,"id":24},{"text":"to","start":131,"end":133,"id":25},{"text":"develop","start":134,"end":141,"id":26},{"text":"better","start":142,"end":148,"id":27},{"text":"understanding","start":149,"end":162,"id":28},{"text":"and","start":163,"end":166,"id":29},{"text":"improve","start":167,"end":174,"id":30},{"text":"the","start":175,"end":178,"id":31},{"text":"quality","start":179,"end":186,"id":32},{"text":"of","start":187,"end":189,"id":33},{"text":"classifiers","start":190,"end":201,"id":34},{"text":"exploiting","start":202,"end":212,"id":35},{"text":"unlabeled","start":213,"end":222,"id":36},{"text":"data","start":223,"end":227,"id":37},{"text":".","start":227,"end":228,"id":38}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Concerning design of kernel density estimators, one essential issue is how fast the pointwise mean square error (MSE) and/or the integrated mean square error (IMSE) diminish as the number of sampling instances increases.","_input_hash":2132921784,"_task_hash":233238422,"tokens":[{"text":"Concerning","start":0,"end":10,"id":0},{"text":"design","start":11,"end":17,"id":1},{"text":"of","start":18,"end":20,"id":2},{"text":"kernel","start":21,"end":27,"id":3},{"text":"density","start":28,"end":35,"id":4},{"text":"estimators","start":36,"end":46,"id":5},{"text":",","start":46,"end":47,"id":6},{"text":"one","start":48,"end":51,"id":7},{"text":"essential","start":52,"end":61,"id":8},{"text":"issue","start":62,"end":67,"id":9},{"text":"is","start":68,"end":70,"id":10},{"text":"how","start":71,"end":74,"id":11},{"text":"fast","start":75,"end":79,"id":12},{"text":"the","start":80,"end":83,"id":13},{"text":"pointwise","start":84,"end":93,"id":14},{"text":"mean","start":94,"end":98,"id":15},{"text":"square","start":99,"end":105,"id":16},{"text":"error","start":106,"end":111,"id":17},{"text":"(","start":112,"end":113,"id":18},{"text":"MSE","start":113,"end":116,"id":19},{"text":")","start":116,"end":117,"id":20},{"text":"and/or","start":118,"end":124,"id":21},{"text":"the","start":125,"end":128,"id":22},{"text":"integrated","start":129,"end":139,"id":23},{"text":"mean","start":140,"end":144,"id":24},{"text":"square","start":145,"end":151,"id":25},{"text":"error","start":152,"end":157,"id":26},{"text":"(","start":158,"end":159,"id":27},{"text":"IMSE","start":159,"end":163,"id":28},{"text":")","start":163,"end":164,"id":29},{"text":"diminish","start":165,"end":173,"id":30},{"text":"as","start":174,"end":176,"id":31},{"text":"the","start":177,"end":180,"id":32},{"text":"number","start":181,"end":187,"id":33},{"text":"of","start":188,"end":190,"id":34},{"text":"sampling","start":191,"end":199,"id":35},{"text":"instances","start":200,"end":209,"id":36},{"text":"increases","start":210,"end":219,"id":37},{"text":".","start":219,"end":220,"id":38}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"By using an iterative procedure of the dynamic logic process \"from vague-to-crisp,\" the new tracker overcomes combinatorial complexity of tracking in highly-cluttered scenarios and results in a significant improvement in signal-to-clutter ratio.","_input_hash":1253113625,"_task_hash":-339023907,"tokens":[{"text":"By","start":0,"end":2,"id":0},{"text":"using","start":3,"end":8,"id":1},{"text":"an","start":9,"end":11,"id":2},{"text":"iterative","start":12,"end":21,"id":3},{"text":"procedure","start":22,"end":31,"id":4},{"text":"of","start":32,"end":34,"id":5},{"text":"the","start":35,"end":38,"id":6},{"text":"dynamic","start":39,"end":46,"id":7},{"text":"logic","start":47,"end":52,"id":8},{"text":"process","start":53,"end":60,"id":9},{"text":"\"","start":61,"end":62,"id":10},{"text":"from","start":62,"end":66,"id":11},{"text":"vague","start":67,"end":72,"id":12},{"text":"-","start":72,"end":73,"id":13},{"text":"to","start":73,"end":75,"id":14},{"text":"-","start":75,"end":76,"id":15},{"text":"crisp","start":76,"end":81,"id":16},{"text":",","start":81,"end":82,"id":17},{"text":"\"","start":82,"end":83,"id":18},{"text":"the","start":84,"end":87,"id":19},{"text":"new","start":88,"end":91,"id":20},{"text":"tracker","start":92,"end":99,"id":21},{"text":"overcomes","start":100,"end":109,"id":22},{"text":"combinatorial","start":110,"end":123,"id":23},{"text":"complexity","start":124,"end":134,"id":24},{"text":"of","start":135,"end":137,"id":25},{"text":"tracking","start":138,"end":146,"id":26},{"text":"in","start":147,"end":149,"id":27},{"text":"highly","start":150,"end":156,"id":28},{"text":"-","start":156,"end":157,"id":29},{"text":"cluttered","start":157,"end":166,"id":30},{"text":"scenarios","start":167,"end":176,"id":31},{"text":"and","start":177,"end":180,"id":32},{"text":"results","start":181,"end":188,"id":33},{"text":"in","start":189,"end":191,"id":34},{"text":"a","start":192,"end":193,"id":35},{"text":"significant","start":194,"end":205,"id":36},{"text":"improvement","start":206,"end":217,"id":37},{"text":"in","start":218,"end":220,"id":38},{"text":"signal","start":221,"end":227,"id":39},{"text":"-","start":227,"end":228,"id":40},{"text":"to","start":228,"end":230,"id":41},{"text":"-","start":230,"end":231,"id":42},{"text":"clutter","start":231,"end":238,"id":43},{"text":"ratio","start":239,"end":244,"id":44},{"text":".","start":244,"end":245,"id":45}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"We show that Confusion Entropy, a recently introduced classifier performance measure for multi-class problems, has a strong (monotone) relation with the multi-class generalization of a classical metric, the Matthews Correlation Coefficient.","_input_hash":-1737030226,"_task_hash":-1011408899,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"show","start":3,"end":7,"id":1},{"text":"that","start":8,"end":12,"id":2},{"text":"Confusion","start":13,"end":22,"id":3},{"text":"Entropy","start":23,"end":30,"id":4},{"text":",","start":30,"end":31,"id":5},{"text":"a","start":32,"end":33,"id":6},{"text":"recently","start":34,"end":42,"id":7},{"text":"introduced","start":43,"end":53,"id":8},{"text":"classifier","start":54,"end":64,"id":9},{"text":"performance","start":65,"end":76,"id":10},{"text":"measure","start":77,"end":84,"id":11},{"text":"for","start":85,"end":88,"id":12},{"text":"multi","start":89,"end":94,"id":13},{"text":"-","start":94,"end":95,"id":14},{"text":"class","start":95,"end":100,"id":15},{"text":"problems","start":101,"end":109,"id":16},{"text":",","start":109,"end":110,"id":17},{"text":"has","start":111,"end":114,"id":18},{"text":"a","start":115,"end":116,"id":19},{"text":"strong","start":117,"end":123,"id":20},{"text":"(","start":124,"end":125,"id":21},{"text":"monotone","start":125,"end":133,"id":22},{"text":")","start":133,"end":134,"id":23},{"text":"relation","start":135,"end":143,"id":24},{"text":"with","start":144,"end":148,"id":25},{"text":"the","start":149,"end":152,"id":26},{"text":"multi","start":153,"end":158,"id":27},{"text":"-","start":158,"end":159,"id":28},{"text":"class","start":159,"end":164,"id":29},{"text":"generalization","start":165,"end":179,"id":30},{"text":"of","start":180,"end":182,"id":31},{"text":"a","start":183,"end":184,"id":32},{"text":"classical","start":185,"end":194,"id":33},{"text":"metric","start":195,"end":201,"id":34},{"text":",","start":201,"end":202,"id":35},{"text":"the","start":203,"end":206,"id":36},{"text":"Matthews","start":207,"end":215,"id":37},{"text":"Correlation","start":216,"end":227,"id":38},{"text":"Coefficient","start":228,"end":239,"id":39},{"text":".","start":239,"end":240,"id":40}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"We show that within this framework, one can prove a theorem analogous to one of J. Kleinberg, in which one obtains an existence and uniqueness theorem instead of a non-existence result.","_input_hash":-874321344,"_task_hash":-1692011917,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"show","start":3,"end":7,"id":1},{"text":"that","start":8,"end":12,"id":2},{"text":"within","start":13,"end":19,"id":3},{"text":"this","start":20,"end":24,"id":4},{"text":"framework","start":25,"end":34,"id":5},{"text":",","start":34,"end":35,"id":6},{"text":"one","start":36,"end":39,"id":7},{"text":"can","start":40,"end":43,"id":8},{"text":"prove","start":44,"end":49,"id":9},{"text":"a","start":50,"end":51,"id":10},{"text":"theorem","start":52,"end":59,"id":11},{"text":"analogous","start":60,"end":69,"id":12},{"text":"to","start":70,"end":72,"id":13},{"text":"one","start":73,"end":76,"id":14},{"text":"of","start":77,"end":79,"id":15},{"text":"J.","start":80,"end":82,"id":16},{"text":"Kleinberg","start":83,"end":92,"id":17},{"text":",","start":92,"end":93,"id":18},{"text":"in","start":94,"end":96,"id":19},{"text":"which","start":97,"end":102,"id":20},{"text":"one","start":103,"end":106,"id":21},{"text":"obtains","start":107,"end":114,"id":22},{"text":"an","start":115,"end":117,"id":23},{"text":"existence","start":118,"end":127,"id":24},{"text":"and","start":128,"end":131,"id":25},{"text":"uniqueness","start":132,"end":142,"id":26},{"text":"theorem","start":143,"end":150,"id":27},{"text":"instead","start":151,"end":158,"id":28},{"text":"of","start":159,"end":161,"id":29},{"text":"a","start":162,"end":163,"id":30},{"text":"non","start":164,"end":167,"id":31},{"text":"-","start":167,"end":168,"id":32},{"text":"existence","start":168,"end":177,"id":33},{"text":"result","start":178,"end":184,"id":34},{"text":".","start":184,"end":185,"id":35}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"are","meta":{"score":0},"_input_hash":2090809870,"_task_hash":-1815127427,"_session_id":null,"_view_id":"text","answer":"reject","spans":[],"tokens":[{"text":"are","start":0,"end":3,"id":0}]}
{"text":"The decisional states form a partition of the lower-level causal states that is defined according to the higher-level user's knowledge.","_input_hash":-90963145,"_task_hash":-1141820307,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"decisional","start":4,"end":14,"id":1},{"text":"states","start":15,"end":21,"id":2},{"text":"form","start":22,"end":26,"id":3},{"text":"a","start":27,"end":28,"id":4},{"text":"partition","start":29,"end":38,"id":5},{"text":"of","start":39,"end":41,"id":6},{"text":"the","start":42,"end":45,"id":7},{"text":"lower","start":46,"end":51,"id":8},{"text":"-","start":51,"end":52,"id":9},{"text":"level","start":52,"end":57,"id":10},{"text":"causal","start":58,"end":64,"id":11},{"text":"states","start":65,"end":71,"id":12},{"text":"that","start":72,"end":76,"id":13},{"text":"is","start":77,"end":79,"id":14},{"text":"defined","start":80,"end":87,"id":15},{"text":"according","start":88,"end":97,"id":16},{"text":"to","start":98,"end":100,"id":17},{"text":"the","start":101,"end":104,"id":18},{"text":"higher","start":105,"end":111,"id":19},{"text":"-","start":111,"end":112,"id":20},{"text":"level","start":112,"end":117,"id":21},{"text":"user","start":118,"end":122,"id":22},{"text":"'s","start":122,"end":124,"id":23},{"text":"knowledge","start":125,"end":134,"id":24},{"text":".","start":134,"end":135,"id":25}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"LSTM|ORG","word":"LSTM","sense":"ORG","meta":{"score":0.8119999766,"sense":"ORG"},"_input_hash":482522666,"_task_hash":543578026,"_session_id":null,"_view_id":"html","answer":"accept","spans":[],"tokens":[{"text":"LSTM|ORG","start":0,"end":8,"id":0}]}
{"text":"Our experimental evaluation carried out on real HTTP and exploit traces confirms the tightness of our theoretical bounds and practicality of our protection mechanisms.","_input_hash":1836513628,"_task_hash":188608810,"tokens":[{"text":"Our","start":0,"end":3,"id":0},{"text":"experimental","start":4,"end":16,"id":1},{"text":"evaluation","start":17,"end":27,"id":2},{"text":"carried","start":28,"end":35,"id":3},{"text":"out","start":36,"end":39,"id":4},{"text":"on","start":40,"end":42,"id":5},{"text":"real","start":43,"end":47,"id":6},{"text":"HTTP","start":48,"end":52,"id":7},{"text":"and","start":53,"end":56,"id":8},{"text":"exploit","start":57,"end":64,"id":9},{"text":"traces","start":65,"end":71,"id":10},{"text":"confirms","start":72,"end":80,"id":11},{"text":"the","start":81,"end":84,"id":12},{"text":"tightness","start":85,"end":94,"id":13},{"text":"of","start":95,"end":97,"id":14},{"text":"our","start":98,"end":101,"id":15},{"text":"theoretical","start":102,"end":113,"id":16},{"text":"bounds","start":114,"end":120,"id":17},{"text":"and","start":121,"end":124,"id":18},{"text":"practicality","start":125,"end":137,"id":19},{"text":"of","start":138,"end":140,"id":20},{"text":"our","start":141,"end":144,"id":21},{"text":"protection","start":145,"end":155,"id":22},{"text":"mechanisms","start":156,"end":166,"id":23},{"text":".","start":166,"end":167,"id":24}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"Within this framework we first discuss about the hypothesis set:","_input_hash":1905021306,"_task_hash":1925488981,"tokens":[{"text":"Within","start":0,"end":6,"id":0},{"text":"this","start":7,"end":11,"id":1},{"text":"framework","start":12,"end":21,"id":2},{"text":"we","start":22,"end":24,"id":3},{"text":"first","start":25,"end":30,"id":4},{"text":"discuss","start":31,"end":38,"id":5},{"text":"about","start":39,"end":44,"id":6},{"text":"the","start":45,"end":48,"id":7},{"text":"hypothesis","start":49,"end":59,"id":8},{"text":"set","start":60,"end":63,"id":9},{"text":":","start":63,"end":64,"id":10}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":49,"end":59,"token_start":8,"token_end":8,"label":"ALGO","answer":"reject"}],"answer":"reject"}
{"text":"goin","meta":{"score":0},"_input_hash":-2013390729,"_task_hash":504184629,"_session_id":null,"_view_id":"text","answer":"reject","spans":[],"tokens":[{"text":"goin","start":0,"end":4,"id":0}]}
{"text":"The most direct way to express arbitrary dependencies in datasets is to estimate the joint distribution and to apply afterwards the argmax-function to obtain the mode of the corresponding conditional distribution.","_input_hash":-1027595124,"_task_hash":179847809,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"most","start":4,"end":8,"id":1},{"text":"direct","start":9,"end":15,"id":2},{"text":"way","start":16,"end":19,"id":3},{"text":"to","start":20,"end":22,"id":4},{"text":"express","start":23,"end":30,"id":5},{"text":"arbitrary","start":31,"end":40,"id":6},{"text":"dependencies","start":41,"end":53,"id":7},{"text":"in","start":54,"end":56,"id":8},{"text":"datasets","start":57,"end":65,"id":9},{"text":"is","start":66,"end":68,"id":10},{"text":"to","start":69,"end":71,"id":11},{"text":"estimate","start":72,"end":80,"id":12},{"text":"the","start":81,"end":84,"id":13},{"text":"joint","start":85,"end":90,"id":14},{"text":"distribution","start":91,"end":103,"id":15},{"text":"and","start":104,"end":107,"id":16},{"text":"to","start":108,"end":110,"id":17},{"text":"apply","start":111,"end":116,"id":18},{"text":"afterwards","start":117,"end":127,"id":19},{"text":"the","start":128,"end":131,"id":20},{"text":"argmax","start":132,"end":138,"id":21},{"text":"-","start":138,"end":139,"id":22},{"text":"function","start":139,"end":147,"id":23},{"text":"to","start":148,"end":150,"id":24},{"text":"obtain","start":151,"end":157,"id":25},{"text":"the","start":158,"end":161,"id":26},{"text":"mode","start":162,"end":166,"id":27},{"text":"of","start":167,"end":169,"id":28},{"text":"the","start":170,"end":173,"id":29},{"text":"corresponding","start":174,"end":187,"id":30},{"text":"conditional","start":188,"end":199,"id":31},{"text":"distribution","start":200,"end":212,"id":32},{"text":".","start":212,"end":213,"id":33}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"Best previously known training algorithms achieve the same efficiency only for restricted special cases, whereas the proposed approach allows any real valued utility scores in the training data.","_input_hash":-1787796891,"_task_hash":-1934592570,"tokens":[{"text":"Best","start":0,"end":4,"id":0},{"text":"previously","start":5,"end":15,"id":1},{"text":"known","start":16,"end":21,"id":2},{"text":"training","start":22,"end":30,"id":3},{"text":"algorithms","start":31,"end":41,"id":4},{"text":"achieve","start":42,"end":49,"id":5},{"text":"the","start":50,"end":53,"id":6},{"text":"same","start":54,"end":58,"id":7},{"text":"efficiency","start":59,"end":69,"id":8},{"text":"only","start":70,"end":74,"id":9},{"text":"for","start":75,"end":78,"id":10},{"text":"restricted","start":79,"end":89,"id":11},{"text":"special","start":90,"end":97,"id":12},{"text":"cases","start":98,"end":103,"id":13},{"text":",","start":103,"end":104,"id":14},{"text":"whereas","start":105,"end":112,"id":15},{"text":"the","start":113,"end":116,"id":16},{"text":"proposed","start":117,"end":125,"id":17},{"text":"approach","start":126,"end":134,"id":18},{"text":"allows","start":135,"end":141,"id":19},{"text":"any","start":142,"end":145,"id":20},{"text":"real","start":146,"end":150,"id":21},{"text":"valued","start":151,"end":157,"id":22},{"text":"utility","start":158,"end":165,"id":23},{"text":"scores","start":166,"end":172,"id":24},{"text":"in","start":173,"end":175,"id":25},{"text":"the","start":176,"end":179,"id":26},{"text":"training","start":180,"end":188,"id":27},{"text":"data","start":189,"end":193,"id":28},{"text":".","start":193,"end":194,"id":29}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Holmes and Adams (2002) focused on the performance of BKNN in terms of misclassification error but did not assess its ability to quantify uncertainty.","_input_hash":819004482,"_task_hash":-365523871,"tokens":[{"text":"Holmes","start":0,"end":6,"id":0},{"text":"and","start":7,"end":10,"id":1},{"text":"Adams","start":11,"end":16,"id":2},{"text":"(","start":17,"end":18,"id":3},{"text":"2002","start":18,"end":22,"id":4},{"text":")","start":22,"end":23,"id":5},{"text":"focused","start":24,"end":31,"id":6},{"text":"on","start":32,"end":34,"id":7},{"text":"the","start":35,"end":38,"id":8},{"text":"performance","start":39,"end":50,"id":9},{"text":"of","start":51,"end":53,"id":10},{"text":"BKNN","start":54,"end":58,"id":11},{"text":"in","start":59,"end":61,"id":12},{"text":"terms","start":62,"end":67,"id":13},{"text":"of","start":68,"end":70,"id":14},{"text":"misclassification","start":71,"end":88,"id":15},{"text":"error","start":89,"end":94,"id":16},{"text":"but","start":95,"end":98,"id":17},{"text":"did","start":99,"end":102,"id":18},{"text":"not","start":103,"end":106,"id":19},{"text":"assess","start":107,"end":113,"id":20},{"text":"its","start":114,"end":117,"id":21},{"text":"ability","start":118,"end":125,"id":22},{"text":"to","start":126,"end":128,"id":23},{"text":"quantify","start":129,"end":137,"id":24},{"text":"uncertainty","start":138,"end":149,"id":25},{"text":".","start":149,"end":150,"id":26}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Online detection of changepoints is useful in modelling and prediction of time series in application areas such as finance, biometrics, and robotics.","_input_hash":-2147316778,"_task_hash":392928165,"tokens":[{"text":"Online","start":0,"end":6,"id":0},{"text":"detection","start":7,"end":16,"id":1},{"text":"of","start":17,"end":19,"id":2},{"text":"changepoints","start":20,"end":32,"id":3},{"text":"is","start":33,"end":35,"id":4},{"text":"useful","start":36,"end":42,"id":5},{"text":"in","start":43,"end":45,"id":6},{"text":"modelling","start":46,"end":55,"id":7},{"text":"and","start":56,"end":59,"id":8},{"text":"prediction","start":60,"end":70,"id":9},{"text":"of","start":71,"end":73,"id":10},{"text":"time","start":74,"end":78,"id":11},{"text":"series","start":79,"end":85,"id":12},{"text":"in","start":86,"end":88,"id":13},{"text":"application","start":89,"end":100,"id":14},{"text":"areas","start":101,"end":106,"id":15},{"text":"such","start":107,"end":111,"id":16},{"text":"as","start":112,"end":114,"id":17},{"text":"finance","start":115,"end":122,"id":18},{"text":",","start":122,"end":123,"id":19},{"text":"biometrics","start":124,"end":134,"id":20},{"text":",","start":134,"end":135,"id":21},{"text":"and","start":136,"end":139,"id":22},{"text":"robotics","start":140,"end":148,"id":23},{"text":".","start":148,"end":149,"id":24}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"it has two merits.","_input_hash":926998138,"_task_hash":350605449,"tokens":[{"text":"it","start":0,"end":2,"id":0},{"text":"has","start":3,"end":6,"id":1},{"text":"two","start":7,"end":10,"id":2},{"text":"merits","start":11,"end":17,"id":3},{"text":".","start":17,"end":18,"id":4}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"The first one is to find important variables for interpretation and the second one is more restrictive and try to design a good prediction model.","_input_hash":1826510268,"_task_hash":-748678460,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"first","start":4,"end":9,"id":1},{"text":"one","start":10,"end":13,"id":2},{"text":"is","start":14,"end":16,"id":3},{"text":"to","start":17,"end":19,"id":4},{"text":"find","start":20,"end":24,"id":5},{"text":"important","start":25,"end":34,"id":6},{"text":"variables","start":35,"end":44,"id":7},{"text":"for","start":45,"end":48,"id":8},{"text":"interpretation","start":49,"end":63,"id":9},{"text":"and","start":64,"end":67,"id":10},{"text":"the","start":68,"end":71,"id":11},{"text":"second","start":72,"end":78,"id":12},{"text":"one","start":79,"end":82,"id":13},{"text":"is","start":83,"end":85,"id":14},{"text":"more","start":86,"end":90,"id":15},{"text":"restrictive","start":91,"end":102,"id":16},{"text":"and","start":103,"end":106,"id":17},{"text":"try","start":107,"end":110,"id":18},{"text":"to","start":111,"end":113,"id":19},{"text":"design","start":114,"end":120,"id":20},{"text":"a","start":121,"end":122,"id":21},{"text":"good","start":123,"end":127,"id":22},{"text":"prediction","start":128,"end":138,"id":23},{"text":"model","start":139,"end":144,"id":24},{"text":".","start":144,"end":145,"id":25}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We provide yet another proof of the existence of calibrated forecasters;","_input_hash":-683543219,"_task_hash":762053067,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"provide","start":3,"end":10,"id":1},{"text":"yet","start":11,"end":14,"id":2},{"text":"another","start":15,"end":22,"id":3},{"text":"proof","start":23,"end":28,"id":4},{"text":"of","start":29,"end":31,"id":5},{"text":"the","start":32,"end":35,"id":6},{"text":"existence","start":36,"end":45,"id":7},{"text":"of","start":46,"end":48,"id":8},{"text":"calibrated","start":49,"end":59,"id":9},{"text":"forecasters","start":60,"end":71,"id":10},{"text":";","start":71,"end":72,"id":11}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"We provide polynomial time projections between the continuous hypersphere representation and the $n!$-element permutation space.","_input_hash":-1651165132,"_task_hash":944683808,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"provide","start":3,"end":10,"id":1},{"text":"polynomial","start":11,"end":21,"id":2},{"text":"time","start":22,"end":26,"id":3},{"text":"projections","start":27,"end":38,"id":4},{"text":"between","start":39,"end":46,"id":5},{"text":"the","start":47,"end":50,"id":6},{"text":"continuous","start":51,"end":61,"id":7},{"text":"hypersphere","start":62,"end":73,"id":8},{"text":"representation","start":74,"end":88,"id":9},{"text":"and","start":89,"end":92,"id":10},{"text":"the","start":93,"end":96,"id":11},{"text":"$","start":97,"end":98,"id":12},{"text":"n!$-element","start":98,"end":109,"id":13},{"text":"permutation","start":110,"end":121,"id":14},{"text":"space","start":122,"end":127,"id":15},{"text":".","start":127,"end":128,"id":16}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"A simple and computationally efficient scheme for tree-structured vector quantization is presented.","_input_hash":240911377,"_task_hash":689423020,"tokens":[{"text":"A","start":0,"end":1,"id":0},{"text":"simple","start":2,"end":8,"id":1},{"text":"and","start":9,"end":12,"id":2},{"text":"computationally","start":13,"end":28,"id":3},{"text":"efficient","start":29,"end":38,"id":4},{"text":"scheme","start":39,"end":45,"id":5},{"text":"for","start":46,"end":49,"id":6},{"text":"tree","start":50,"end":54,"id":7},{"text":"-","start":54,"end":55,"id":8},{"text":"structured","start":55,"end":65,"id":9},{"text":"vector","start":66,"end":72,"id":10},{"text":"quantization","start":73,"end":85,"id":11},{"text":"is","start":86,"end":88,"id":12},{"text":"presented","start":89,"end":98,"id":13},{"text":".","start":98,"end":99,"id":14}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"reinforcement_learning|NOUN","word":"reinforcement learning","sense":"NOUN","meta":{"score":0.7753999829,"sense":"NOUN"},"_input_hash":309528292,"_task_hash":917873712,"_session_id":null,"_view_id":"html","answer":"accept","spans":[],"tokens":[{"text":"reinforcement_learning|NOUN","start":0,"end":27,"id":0}]}
{"text":"Being among the easiest ways to find meaningful structure from discrete data, Latent Dirichlet Allocation (LDA) and related component models have been applied widely.","_input_hash":-96609974,"_task_hash":348892886,"tokens":[{"text":"Being","start":0,"end":5,"id":0},{"text":"among","start":6,"end":11,"id":1},{"text":"the","start":12,"end":15,"id":2},{"text":"easiest","start":16,"end":23,"id":3},{"text":"ways","start":24,"end":28,"id":4},{"text":"to","start":29,"end":31,"id":5},{"text":"find","start":32,"end":36,"id":6},{"text":"meaningful","start":37,"end":47,"id":7},{"text":"structure","start":48,"end":57,"id":8},{"text":"from","start":58,"end":62,"id":9},{"text":"discrete","start":63,"end":71,"id":10},{"text":"data","start":72,"end":76,"id":11},{"text":",","start":76,"end":77,"id":12},{"text":"Latent","start":78,"end":84,"id":13},{"text":"Dirichlet","start":85,"end":94,"id":14},{"text":"Allocation","start":95,"end":105,"id":15},{"text":"(","start":106,"end":107,"id":16},{"text":"LDA","start":107,"end":110,"id":17},{"text":")","start":110,"end":111,"id":18},{"text":"and","start":112,"end":115,"id":19},{"text":"related","start":116,"end":123,"id":20},{"text":"component","start":124,"end":133,"id":21},{"text":"models","start":134,"end":140,"id":22},{"text":"have","start":141,"end":145,"id":23},{"text":"been","start":146,"end":150,"id":24},{"text":"applied","start":151,"end":158,"id":25},{"text":"widely","start":159,"end":165,"id":26},{"text":".","start":165,"end":166,"id":27}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":78,"end":105,"token_start":13,"token_end":15,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We propose two algorithms for approximating its solution.","_input_hash":-1643745972,"_task_hash":-1754255784,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"propose","start":3,"end":10,"id":1},{"text":"two","start":11,"end":14,"id":2},{"text":"algorithms","start":15,"end":25,"id":3},{"text":"for","start":26,"end":29,"id":4},{"text":"approximating","start":30,"end":43,"id":5},{"text":"its","start":44,"end":47,"id":6},{"text":"solution","start":48,"end":56,"id":7},{"text":".","start":56,"end":57,"id":8}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"In the proposed test procedure, the f-divergence between two probability densities is estimated using a density-ratio estimator.","_input_hash":-934151411,"_task_hash":-410209735,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"the","start":3,"end":6,"id":1},{"text":"proposed","start":7,"end":15,"id":2},{"text":"test","start":16,"end":20,"id":3},{"text":"procedure","start":21,"end":30,"id":4},{"text":",","start":30,"end":31,"id":5},{"text":"the","start":32,"end":35,"id":6},{"text":"f","start":36,"end":37,"id":7},{"text":"-","start":37,"end":38,"id":8},{"text":"divergence","start":38,"end":48,"id":9},{"text":"between","start":49,"end":56,"id":10},{"text":"two","start":57,"end":60,"id":11},{"text":"probability","start":61,"end":72,"id":12},{"text":"densities","start":73,"end":82,"id":13},{"text":"is","start":83,"end":85,"id":14},{"text":"estimated","start":86,"end":95,"id":15},{"text":"using","start":96,"end":101,"id":16},{"text":"a","start":102,"end":103,"id":17},{"text":"density","start":104,"end":111,"id":18},{"text":"-","start":111,"end":112,"id":19},{"text":"ratio","start":112,"end":117,"id":20},{"text":"estimator","start":118,"end":127,"id":21},{"text":".","start":127,"end":128,"id":22}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"A visualization tool of this kind is useful in signal processing and machine learning whenever learning/adaptation algorithms insist on high-dimensional parameter manifolds.","_input_hash":-475828779,"_task_hash":1723077357,"tokens":[{"text":"A","start":0,"end":1,"id":0},{"text":"visualization","start":2,"end":15,"id":1},{"text":"tool","start":16,"end":20,"id":2},{"text":"of","start":21,"end":23,"id":3},{"text":"this","start":24,"end":28,"id":4},{"text":"kind","start":29,"end":33,"id":5},{"text":"is","start":34,"end":36,"id":6},{"text":"useful","start":37,"end":43,"id":7},{"text":"in","start":44,"end":46,"id":8},{"text":"signal","start":47,"end":53,"id":9},{"text":"processing","start":54,"end":64,"id":10},{"text":"and","start":65,"end":68,"id":11},{"text":"machine","start":69,"end":76,"id":12},{"text":"learning","start":77,"end":85,"id":13},{"text":"whenever","start":86,"end":94,"id":14},{"text":"learning","start":95,"end":103,"id":15},{"text":"/","start":103,"end":104,"id":16},{"text":"adaptation","start":104,"end":114,"id":17},{"text":"algorithms","start":115,"end":125,"id":18},{"text":"insist","start":126,"end":132,"id":19},{"text":"on","start":133,"end":135,"id":20},{"text":"high","start":136,"end":140,"id":21},{"text":"-","start":140,"end":141,"id":22},{"text":"dimensional","start":141,"end":152,"id":23},{"text":"parameter","start":153,"end":162,"id":24},{"text":"manifolds","start":163,"end":172,"id":25},{"text":".","start":172,"end":173,"id":26}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"type_theory|NOUN","word":"type theory","sense":"NOUN","meta":{"score":0.7825000286,"sense":"NOUN"},"_input_hash":2002456453,"_task_hash":-1816820699,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"type_theory|NOUN","start":0,"end":16,"id":0}]}
{"text":"We show experimentally the superiority of our algorithms over others, and demonstrate our approach in document clustering and phylolinguistics.","_input_hash":540279485,"_task_hash":606675593,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"show","start":3,"end":7,"id":1},{"text":"experimentally","start":8,"end":22,"id":2},{"text":"the","start":23,"end":26,"id":3},{"text":"superiority","start":27,"end":38,"id":4},{"text":"of","start":39,"end":41,"id":5},{"text":"our","start":42,"end":45,"id":6},{"text":"algorithms","start":46,"end":56,"id":7},{"text":"over","start":57,"end":61,"id":8},{"text":"others","start":62,"end":68,"id":9},{"text":",","start":68,"end":69,"id":10},{"text":"and","start":70,"end":73,"id":11},{"text":"demonstrate","start":74,"end":85,"id":12},{"text":"our","start":86,"end":89,"id":13},{"text":"approach","start":90,"end":98,"id":14},{"text":"in","start":99,"end":101,"id":15},{"text":"document","start":102,"end":110,"id":16},{"text":"clustering","start":111,"end":121,"id":17},{"text":"and","start":122,"end":125,"id":18},{"text":"phylolinguistics","start":126,"end":142,"id":19},{"text":".","start":142,"end":143,"id":20}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"The difficulty of dealing with these distributions is caused by the size of their domain, which is factorial in the number of considered entities ($n!$).","_input_hash":-102247442,"_task_hash":422470984,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"difficulty","start":4,"end":14,"id":1},{"text":"of","start":15,"end":17,"id":2},{"text":"dealing","start":18,"end":25,"id":3},{"text":"with","start":26,"end":30,"id":4},{"text":"these","start":31,"end":36,"id":5},{"text":"distributions","start":37,"end":50,"id":6},{"text":"is","start":51,"end":53,"id":7},{"text":"caused","start":54,"end":60,"id":8},{"text":"by","start":61,"end":63,"id":9},{"text":"the","start":64,"end":67,"id":10},{"text":"size","start":68,"end":72,"id":11},{"text":"of","start":73,"end":75,"id":12},{"text":"their","start":76,"end":81,"id":13},{"text":"domain","start":82,"end":88,"id":14},{"text":",","start":88,"end":89,"id":15},{"text":"which","start":90,"end":95,"id":16},{"text":"is","start":96,"end":98,"id":17},{"text":"factorial","start":99,"end":108,"id":18},{"text":"in","start":109,"end":111,"id":19},{"text":"the","start":112,"end":115,"id":20},{"text":"number","start":116,"end":122,"id":21},{"text":"of","start":123,"end":125,"id":22},{"text":"considered","start":126,"end":136,"id":23},{"text":"entities","start":137,"end":145,"id":24},{"text":"(","start":146,"end":147,"id":25},{"text":"$","start":147,"end":148,"id":26},{"text":"n!$","start":148,"end":151,"id":27},{"text":")","start":151,"end":152,"id":28},{"text":".","start":152,"end":153,"id":29}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"Some applications of this method in the multi/hyper-spectral imagery domain to a satellite view of Paris and to an image of the Mars planet are also presented.","_input_hash":2095742317,"_task_hash":530776238,"tokens":[{"text":"Some","start":0,"end":4,"id":0},{"text":"applications","start":5,"end":17,"id":1},{"text":"of","start":18,"end":20,"id":2},{"text":"this","start":21,"end":25,"id":3},{"text":"method","start":26,"end":32,"id":4},{"text":"in","start":33,"end":35,"id":5},{"text":"the","start":36,"end":39,"id":6},{"text":"multi","start":40,"end":45,"id":7},{"text":"/","start":45,"end":46,"id":8},{"text":"hyper","start":46,"end":51,"id":9},{"text":"-","start":51,"end":52,"id":10},{"text":"spectral","start":52,"end":60,"id":11},{"text":"imagery","start":61,"end":68,"id":12},{"text":"domain","start":69,"end":75,"id":13},{"text":"to","start":76,"end":78,"id":14},{"text":"a","start":79,"end":80,"id":15},{"text":"satellite","start":81,"end":90,"id":16},{"text":"view","start":91,"end":95,"id":17},{"text":"of","start":96,"end":98,"id":18},{"text":"Paris","start":99,"end":104,"id":19},{"text":"and","start":105,"end":108,"id":20},{"text":"to","start":109,"end":111,"id":21},{"text":"an","start":112,"end":114,"id":22},{"text":"image","start":115,"end":120,"id":23},{"text":"of","start":121,"end":123,"id":24},{"text":"the","start":124,"end":127,"id":25},{"text":"Mars","start":128,"end":132,"id":26},{"text":"planet","start":133,"end":139,"id":27},{"text":"are","start":140,"end":143,"id":28},{"text":"also","start":144,"end":148,"id":29},{"text":"presented","start":149,"end":158,"id":30},{"text":".","start":158,"end":159,"id":31}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"A computer program is available that implements the proposed shrinkage estimator.","_input_hash":-701394453,"_task_hash":-179084809,"tokens":[{"text":"A","start":0,"end":1,"id":0},{"text":"computer","start":2,"end":10,"id":1},{"text":"program","start":11,"end":18,"id":2},{"text":"is","start":19,"end":21,"id":3},{"text":"available","start":22,"end":31,"id":4},{"text":"that","start":32,"end":36,"id":5},{"text":"implements","start":37,"end":47,"id":6},{"text":"the","start":48,"end":51,"id":7},{"text":"proposed","start":52,"end":60,"id":8},{"text":"shrinkage","start":61,"end":70,"id":9},{"text":"estimator","start":71,"end":80,"id":10},{"text":".","start":80,"end":81,"id":11}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"For signals such as natural images that admit such sparse representations, it is now well established that these models are well suited to restoration tasks.","_input_hash":1848857112,"_task_hash":-748868030,"tokens":[{"text":"For","start":0,"end":3,"id":0},{"text":"signals","start":4,"end":11,"id":1},{"text":"such","start":12,"end":16,"id":2},{"text":"as","start":17,"end":19,"id":3},{"text":"natural","start":20,"end":27,"id":4},{"text":"images","start":28,"end":34,"id":5},{"text":"that","start":35,"end":39,"id":6},{"text":"admit","start":40,"end":45,"id":7},{"text":"such","start":46,"end":50,"id":8},{"text":"sparse","start":51,"end":57,"id":9},{"text":"representations","start":58,"end":73,"id":10},{"text":",","start":73,"end":74,"id":11},{"text":"it","start":75,"end":77,"id":12},{"text":"is","start":78,"end":80,"id":13},{"text":"now","start":81,"end":84,"id":14},{"text":"well","start":85,"end":89,"id":15},{"text":"established","start":90,"end":101,"id":16},{"text":"that","start":102,"end":106,"id":17},{"text":"these","start":107,"end":112,"id":18},{"text":"models","start":113,"end":119,"id":19},{"text":"are","start":120,"end":123,"id":20},{"text":"well","start":124,"end":128,"id":21},{"text":"suited","start":129,"end":135,"id":22},{"text":"to","start":136,"end":138,"id":23},{"text":"restoration","start":139,"end":150,"id":24},{"text":"tasks","start":151,"end":156,"id":25},{"text":".","start":156,"end":157,"id":26}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"Due to its simplicity, training LapSVM in the primal can be the starting point for additional enhancements of the original LapSVM formulation, such as those for dealing with large datasets.","_input_hash":-1273538646,"_task_hash":-623617392,"tokens":[{"text":"Due","start":0,"end":3,"id":0},{"text":"to","start":4,"end":6,"id":1},{"text":"its","start":7,"end":10,"id":2},{"text":"simplicity","start":11,"end":21,"id":3},{"text":",","start":21,"end":22,"id":4},{"text":"training","start":23,"end":31,"id":5},{"text":"LapSVM","start":32,"end":38,"id":6},{"text":"in","start":39,"end":41,"id":7},{"text":"the","start":42,"end":45,"id":8},{"text":"primal","start":46,"end":52,"id":9},{"text":"can","start":53,"end":56,"id":10},{"text":"be","start":57,"end":59,"id":11},{"text":"the","start":60,"end":63,"id":12},{"text":"starting","start":64,"end":72,"id":13},{"text":"point","start":73,"end":78,"id":14},{"text":"for","start":79,"end":82,"id":15},{"text":"additional","start":83,"end":93,"id":16},{"text":"enhancements","start":94,"end":106,"id":17},{"text":"of","start":107,"end":109,"id":18},{"text":"the","start":110,"end":113,"id":19},{"text":"original","start":114,"end":122,"id":20},{"text":"LapSVM","start":123,"end":129,"id":21},{"text":"formulation","start":130,"end":141,"id":22},{"text":",","start":141,"end":142,"id":23},{"text":"such","start":143,"end":147,"id":24},{"text":"as","start":148,"end":150,"id":25},{"text":"those","start":151,"end":156,"id":26},{"text":"for","start":157,"end":160,"id":27},{"text":"dealing","start":161,"end":168,"id":28},{"text":"with","start":169,"end":173,"id":29},{"text":"large","start":174,"end":179,"id":30},{"text":"datasets","start":180,"end":188,"id":31},{"text":".","start":188,"end":189,"id":32}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Therefore, thanks to a general \"representer theorem\", the solution of the learning problem is still a linear combination of a kernel.","_input_hash":-196811180,"_task_hash":-1969279708,"tokens":[{"text":"Therefore","start":0,"end":9,"id":0},{"text":",","start":9,"end":10,"id":1},{"text":"thanks","start":11,"end":17,"id":2},{"text":"to","start":18,"end":20,"id":3},{"text":"a","start":21,"end":22,"id":4},{"text":"general","start":23,"end":30,"id":5},{"text":"\"","start":31,"end":32,"id":6},{"text":"representer","start":32,"end":43,"id":7},{"text":"theorem","start":44,"end":51,"id":8},{"text":"\"","start":51,"end":52,"id":9},{"text":",","start":52,"end":53,"id":10},{"text":"the","start":54,"end":57,"id":11},{"text":"solution","start":58,"end":66,"id":12},{"text":"of","start":67,"end":69,"id":13},{"text":"the","start":70,"end":73,"id":14},{"text":"learning","start":74,"end":82,"id":15},{"text":"problem","start":83,"end":90,"id":16},{"text":"is","start":91,"end":93,"id":17},{"text":"still","start":94,"end":99,"id":18},{"text":"a","start":100,"end":101,"id":19},{"text":"linear","start":102,"end":108,"id":20},{"text":"combination","start":109,"end":120,"id":21},{"text":"of","start":121,"end":123,"id":22},{"text":"a","start":124,"end":125,"id":23},{"text":"kernel","start":126,"end":132,"id":24},{"text":".","start":132,"end":133,"id":25}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":31,"end":52,"token_start":6,"token_end":9,"label":"ALGO","answer":"reject"},{"start":102,"end":120,"token_start":20,"token_end":21,"label":"ALGO","answer":"reject"}],"answer":"reject"}
{"text":"persistence and functoriality.","_input_hash":690850410,"_task_hash":2015507077,"tokens":[{"text":"persistence","start":0,"end":11,"id":0},{"text":"and","start":12,"end":15,"id":1},{"text":"functoriality","start":16,"end":29,"id":2},{"text":".","start":29,"end":30,"id":3}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"From the construction of a minimal spanning tree with Prim's algorithm, and the assumption that the vertices are approximately distributed according to a Poisson distribution, the number of clusters is estimated by thresholding the Prim's trajectory.","_input_hash":-1335032842,"_task_hash":630487856,"tokens":[{"text":"From","start":0,"end":4,"id":0},{"text":"the","start":5,"end":8,"id":1},{"text":"construction","start":9,"end":21,"id":2},{"text":"of","start":22,"end":24,"id":3},{"text":"a","start":25,"end":26,"id":4},{"text":"minimal","start":27,"end":34,"id":5},{"text":"spanning","start":35,"end":43,"id":6},{"text":"tree","start":44,"end":48,"id":7},{"text":"with","start":49,"end":53,"id":8},{"text":"Prim","start":54,"end":58,"id":9},{"text":"'s","start":58,"end":60,"id":10},{"text":"algorithm","start":61,"end":70,"id":11},{"text":",","start":70,"end":71,"id":12},{"text":"and","start":72,"end":75,"id":13},{"text":"the","start":76,"end":79,"id":14},{"text":"assumption","start":80,"end":90,"id":15},{"text":"that","start":91,"end":95,"id":16},{"text":"the","start":96,"end":99,"id":17},{"text":"vertices","start":100,"end":108,"id":18},{"text":"are","start":109,"end":112,"id":19},{"text":"approximately","start":113,"end":126,"id":20},{"text":"distributed","start":127,"end":138,"id":21},{"text":"according","start":139,"end":148,"id":22},{"text":"to","start":149,"end":151,"id":23},{"text":"a","start":152,"end":153,"id":24},{"text":"Poisson","start":154,"end":161,"id":25},{"text":"distribution","start":162,"end":174,"id":26},{"text":",","start":174,"end":175,"id":27},{"text":"the","start":176,"end":179,"id":28},{"text":"number","start":180,"end":186,"id":29},{"text":"of","start":187,"end":189,"id":30},{"text":"clusters","start":190,"end":198,"id":31},{"text":"is","start":199,"end":201,"id":32},{"text":"estimated","start":202,"end":211,"id":33},{"text":"by","start":212,"end":214,"id":34},{"text":"thresholding","start":215,"end":227,"id":35},{"text":"the","start":228,"end":231,"id":36},{"text":"Prim","start":232,"end":236,"id":37},{"text":"'s","start":236,"end":238,"id":38},{"text":"trajectory","start":239,"end":249,"id":39},{"text":".","start":249,"end":250,"id":40}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":54,"end":70,"token_start":9,"token_end":11,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We introduce a new nearest-prototype classifier, the prototype vector machine (PVM).","_input_hash":-69021237,"_task_hash":-2058255007,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"introduce","start":3,"end":12,"id":1},{"text":"a","start":13,"end":14,"id":2},{"text":"new","start":15,"end":18,"id":3},{"text":"nearest","start":19,"end":26,"id":4},{"text":"-","start":26,"end":27,"id":5},{"text":"prototype","start":27,"end":36,"id":6},{"text":"classifier","start":37,"end":47,"id":7},{"text":",","start":47,"end":48,"id":8},{"text":"the","start":49,"end":52,"id":9},{"text":"prototype","start":53,"end":62,"id":10},{"text":"vector","start":63,"end":69,"id":11},{"text":"machine","start":70,"end":77,"id":12},{"text":"(","start":78,"end":79,"id":13},{"text":"PVM","start":79,"end":82,"id":14},{"text":")","start":82,"end":83,"id":15},{"text":".","start":83,"end":84,"id":16}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":53,"end":77,"token_start":10,"token_end":12,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"The implications are that certain moduli spaces known in algebraic geometry are $p$-adic parameter spaces of (families of) dendrograms, and stochastic classification can also be handled within this framework.","_input_hash":-1428625400,"_task_hash":-98704122,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"implications","start":4,"end":16,"id":1},{"text":"are","start":17,"end":20,"id":2},{"text":"that","start":21,"end":25,"id":3},{"text":"certain","start":26,"end":33,"id":4},{"text":"moduli","start":34,"end":40,"id":5},{"text":"spaces","start":41,"end":47,"id":6},{"text":"known","start":48,"end":53,"id":7},{"text":"in","start":54,"end":56,"id":8},{"text":"algebraic","start":57,"end":66,"id":9},{"text":"geometry","start":67,"end":75,"id":10},{"text":"are","start":76,"end":79,"id":11},{"text":"$","start":80,"end":81,"id":12},{"text":"p$-adic","start":81,"end":88,"id":13},{"text":"parameter","start":89,"end":98,"id":14},{"text":"spaces","start":99,"end":105,"id":15},{"text":"of","start":106,"end":108,"id":16},{"text":"(","start":109,"end":110,"id":17},{"text":"families","start":110,"end":118,"id":18},{"text":"of","start":119,"end":121,"id":19},{"text":")","start":121,"end":122,"id":20},{"text":"dendrograms","start":123,"end":134,"id":21},{"text":",","start":134,"end":135,"id":22},{"text":"and","start":136,"end":139,"id":23},{"text":"stochastic","start":140,"end":150,"id":24},{"text":"classification","start":151,"end":165,"id":25},{"text":"can","start":166,"end":169,"id":26},{"text":"also","start":170,"end":174,"id":27},{"text":"be","start":175,"end":177,"id":28},{"text":"handled","start":178,"end":185,"id":29},{"text":"within","start":186,"end":192,"id":30},{"text":"this","start":193,"end":197,"id":31},{"text":"framework","start":198,"end":207,"id":32},{"text":".","start":207,"end":208,"id":33}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"The abstract learning space, where all agents are located, is constructed here using a fully connected model that couples all agents with random strength values.","_input_hash":810703736,"_task_hash":-178055020,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"abstract","start":4,"end":12,"id":1},{"text":"learning","start":13,"end":21,"id":2},{"text":"space","start":22,"end":27,"id":3},{"text":",","start":27,"end":28,"id":4},{"text":"where","start":29,"end":34,"id":5},{"text":"all","start":35,"end":38,"id":6},{"text":"agents","start":39,"end":45,"id":7},{"text":"are","start":46,"end":49,"id":8},{"text":"located","start":50,"end":57,"id":9},{"text":",","start":57,"end":58,"id":10},{"text":"is","start":59,"end":61,"id":11},{"text":"constructed","start":62,"end":73,"id":12},{"text":"here","start":74,"end":78,"id":13},{"text":"using","start":79,"end":84,"id":14},{"text":"a","start":85,"end":86,"id":15},{"text":"fully","start":87,"end":92,"id":16},{"text":"connected","start":93,"end":102,"id":17},{"text":"model","start":103,"end":108,"id":18},{"text":"that","start":109,"end":113,"id":19},{"text":"couples","start":114,"end":121,"id":20},{"text":"all","start":122,"end":125,"id":21},{"text":"agents","start":126,"end":132,"id":22},{"text":"with","start":133,"end":137,"id":23},{"text":"random","start":138,"end":144,"id":24},{"text":"strength","start":145,"end":153,"id":25},{"text":"values","start":154,"end":160,"id":26},{"text":".","start":160,"end":161,"id":27}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Our analysis addresses the following security-related issues:","_input_hash":-355262591,"_task_hash":-236809872,"tokens":[{"text":"Our","start":0,"end":3,"id":0},{"text":"analysis","start":4,"end":12,"id":1},{"text":"addresses","start":13,"end":22,"id":2},{"text":"the","start":23,"end":26,"id":3},{"text":"following","start":27,"end":36,"id":4},{"text":"security","start":37,"end":45,"id":5},{"text":"-","start":45,"end":46,"id":6},{"text":"related","start":46,"end":53,"id":7},{"text":"issues","start":54,"end":60,"id":8},{"text":":","start":60,"end":61,"id":9}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"where","meta":{"score":0},"_input_hash":-843605128,"_task_hash":-1226513431,"_session_id":null,"_view_id":"text","answer":"reject","spans":[],"tokens":[{"text":"where","start":0,"end":5,"id":0}]}
{"text":"We consider an extension of this framework where the atoms are further assumed to be embedded in a tree.","_input_hash":1553095004,"_task_hash":-1492449366,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"consider","start":3,"end":11,"id":1},{"text":"an","start":12,"end":14,"id":2},{"text":"extension","start":15,"end":24,"id":3},{"text":"of","start":25,"end":27,"id":4},{"text":"this","start":28,"end":32,"id":5},{"text":"framework","start":33,"end":42,"id":6},{"text":"where","start":43,"end":48,"id":7},{"text":"the","start":49,"end":52,"id":8},{"text":"atoms","start":53,"end":58,"id":9},{"text":"are","start":59,"end":62,"id":10},{"text":"further","start":63,"end":70,"id":11},{"text":"assumed","start":71,"end":78,"id":12},{"text":"to","start":79,"end":81,"id":13},{"text":"be","start":82,"end":84,"id":14},{"text":"embedded","start":85,"end":93,"id":15},{"text":"in","start":94,"end":96,"id":16},{"text":"a","start":97,"end":98,"id":17},{"text":"tree","start":99,"end":103,"id":18},{"text":".","start":103,"end":104,"id":19}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"graph_theory|NOUN","word":"graph theory","sense":"NOUN","meta":{"score":0.7795000076,"sense":"NOUN"},"_input_hash":-516121810,"_task_hash":618213229,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"graph_theory|NOUN","start":0,"end":17,"id":0}]}
{"text":"is","meta":{"score":0},"_input_hash":1475404023,"_task_hash":306015886,"_session_id":null,"_view_id":"text","answer":"reject","spans":[],"tokens":[{"text":"is","start":0,"end":2,"id":0}]}
{"text":"The kernel density estimation based approaches are of interest due to the low time complexity of either O(n) or O(n*log(n)) for constructing a classifier, where n is the number of sampling instances.","_input_hash":-1025131444,"_task_hash":-2069335017,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"kernel","start":4,"end":10,"id":1},{"text":"density","start":11,"end":18,"id":2},{"text":"estimation","start":19,"end":29,"id":3},{"text":"based","start":30,"end":35,"id":4},{"text":"approaches","start":36,"end":46,"id":5},{"text":"are","start":47,"end":50,"id":6},{"text":"of","start":51,"end":53,"id":7},{"text":"interest","start":54,"end":62,"id":8},{"text":"due","start":63,"end":66,"id":9},{"text":"to","start":67,"end":69,"id":10},{"text":"the","start":70,"end":73,"id":11},{"text":"low","start":74,"end":77,"id":12},{"text":"time","start":78,"end":82,"id":13},{"text":"complexity","start":83,"end":93,"id":14},{"text":"of","start":94,"end":96,"id":15},{"text":"either","start":97,"end":103,"id":16},{"text":"O(n","start":104,"end":107,"id":17},{"text":")","start":107,"end":108,"id":18},{"text":"or","start":109,"end":111,"id":19},{"text":"O(n*log(n","start":112,"end":121,"id":20},{"text":")","start":121,"end":122,"id":21},{"text":")","start":122,"end":123,"id":22},{"text":"for","start":124,"end":127,"id":23},{"text":"constructing","start":128,"end":140,"id":24},{"text":"a","start":141,"end":142,"id":25},{"text":"classifier","start":143,"end":153,"id":26},{"text":",","start":153,"end":154,"id":27},{"text":"where","start":155,"end":160,"id":28},{"text":"n","start":161,"end":162,"id":29},{"text":"is","start":163,"end":165,"id":30},{"text":"the","start":166,"end":169,"id":31},{"text":"number","start":170,"end":176,"id":32},{"text":"of","start":177,"end":179,"id":33},{"text":"sampling","start":180,"end":188,"id":34},{"text":"instances","start":189,"end":198,"id":35},{"text":".","start":198,"end":199,"id":36}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"nt","meta":{"score":0},"_input_hash":364139271,"_task_hash":-230280815,"_session_id":null,"_view_id":"text","answer":"reject","spans":[],"tokens":[{"text":"nt","start":0,"end":2,"id":0}]}
{"text":"Sparse coding consists in representing signals as sparse linear combinations of atoms selected from a dictionary.","_input_hash":689515512,"_task_hash":-533399019,"tokens":[{"text":"Sparse","start":0,"end":6,"id":0},{"text":"coding","start":7,"end":13,"id":1},{"text":"consists","start":14,"end":22,"id":2},{"text":"in","start":23,"end":25,"id":3},{"text":"representing","start":26,"end":38,"id":4},{"text":"signals","start":39,"end":46,"id":5},{"text":"as","start":47,"end":49,"id":6},{"text":"sparse","start":50,"end":56,"id":7},{"text":"linear","start":57,"end":63,"id":8},{"text":"combinations","start":64,"end":76,"id":9},{"text":"of","start":77,"end":79,"id":10},{"text":"atoms","start":80,"end":85,"id":11},{"text":"selected","start":86,"end":94,"id":12},{"text":"from","start":95,"end":99,"id":13},{"text":"a","start":100,"end":101,"id":14},{"text":"dictionary","start":102,"end":112,"id":15},{"text":".","start":112,"end":113,"id":16}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"A simulated example of this algorithm is proposed on a dictionary obtained using LARS, for the problem of selection of the regularization parameter of the LASSO.","_input_hash":-339762701,"_task_hash":1109614066,"tokens":[{"text":"A","start":0,"end":1,"id":0},{"text":"simulated","start":2,"end":11,"id":1},{"text":"example","start":12,"end":19,"id":2},{"text":"of","start":20,"end":22,"id":3},{"text":"this","start":23,"end":27,"id":4},{"text":"algorithm","start":28,"end":37,"id":5},{"text":"is","start":38,"end":40,"id":6},{"text":"proposed","start":41,"end":49,"id":7},{"text":"on","start":50,"end":52,"id":8},{"text":"a","start":53,"end":54,"id":9},{"text":"dictionary","start":55,"end":65,"id":10},{"text":"obtained","start":66,"end":74,"id":11},{"text":"using","start":75,"end":80,"id":12},{"text":"LARS","start":81,"end":85,"id":13},{"text":",","start":85,"end":86,"id":14},{"text":"for","start":87,"end":90,"id":15},{"text":"the","start":91,"end":94,"id":16},{"text":"problem","start":95,"end":102,"id":17},{"text":"of","start":103,"end":105,"id":18},{"text":"selection","start":106,"end":115,"id":19},{"text":"of","start":116,"end":118,"id":20},{"text":"the","start":119,"end":122,"id":21},{"text":"regularization","start":123,"end":137,"id":22},{"text":"parameter","start":138,"end":147,"id":23},{"text":"of","start":148,"end":150,"id":24},{"text":"the","start":151,"end":154,"id":25},{"text":"LASSO","start":155,"end":160,"id":26},{"text":".","start":160,"end":161,"id":27}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":28,"end":37,"token_start":5,"token_end":5,"label":"ALGO","answer":"reject"}],"answer":"reject"}
{"text":"The proposed sparse group RBMs are applied to three tasks:","_input_hash":1504587923,"_task_hash":343018655,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"proposed","start":4,"end":12,"id":1},{"text":"sparse","start":13,"end":19,"id":2},{"text":"group","start":20,"end":25,"id":3},{"text":"RBMs","start":26,"end":30,"id":4},{"text":"are","start":31,"end":34,"id":5},{"text":"applied","start":35,"end":42,"id":6},{"text":"to","start":43,"end":45,"id":7},{"text":"three","start":46,"end":51,"id":8},{"text":"tasks","start":52,"end":57,"id":9},{"text":":","start":57,"end":58,"id":10}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"This enables us to prove that it is reasonable to select the number of clusters based on stability scores.","_input_hash":775854868,"_task_hash":-1528433280,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"enables","start":5,"end":12,"id":1},{"text":"us","start":13,"end":15,"id":2},{"text":"to","start":16,"end":18,"id":3},{"text":"prove","start":19,"end":24,"id":4},{"text":"that","start":25,"end":29,"id":5},{"text":"it","start":30,"end":32,"id":6},{"text":"is","start":33,"end":35,"id":7},{"text":"reasonable","start":36,"end":46,"id":8},{"text":"to","start":47,"end":49,"id":9},{"text":"select","start":50,"end":56,"id":10},{"text":"the","start":57,"end":60,"id":11},{"text":"number","start":61,"end":67,"id":12},{"text":"of","start":68,"end":70,"id":13},{"text":"clusters","start":71,"end":79,"id":14},{"text":"based","start":80,"end":85,"id":15},{"text":"on","start":86,"end":88,"id":16},{"text":"stability","start":89,"end":98,"id":17},{"text":"scores","start":99,"end":105,"id":18},{"text":".","start":105,"end":106,"id":19}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We introduce supervised latent Dirichlet allocation (sLDA), a statistical model of labelled documents.","_input_hash":1571434723,"_task_hash":940774689,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"introduce","start":3,"end":12,"id":1},{"text":"supervised","start":13,"end":23,"id":2},{"text":"latent","start":24,"end":30,"id":3},{"text":"Dirichlet","start":31,"end":40,"id":4},{"text":"allocation","start":41,"end":51,"id":5},{"text":"(","start":52,"end":53,"id":6},{"text":"sLDA","start":53,"end":57,"id":7},{"text":")","start":57,"end":58,"id":8},{"text":",","start":58,"end":59,"id":9},{"text":"a","start":60,"end":61,"id":10},{"text":"statistical","start":62,"end":73,"id":11},{"text":"model","start":74,"end":79,"id":12},{"text":"of","start":80,"end":82,"id":13},{"text":"labelled","start":83,"end":91,"id":14},{"text":"documents","start":92,"end":101,"id":15},{"text":".","start":101,"end":102,"id":16}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":24,"end":51,"token_start":3,"token_end":5,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Additionally, we apply the PVM to a protein classification problem in which a kernel-based distance is used.","_input_hash":173881543,"_task_hash":-1230398961,"tokens":[{"text":"Additionally","start":0,"end":12,"id":0},{"text":",","start":12,"end":13,"id":1},{"text":"we","start":14,"end":16,"id":2},{"text":"apply","start":17,"end":22,"id":3},{"text":"the","start":23,"end":26,"id":4},{"text":"PVM","start":27,"end":30,"id":5},{"text":"to","start":31,"end":33,"id":6},{"text":"a","start":34,"end":35,"id":7},{"text":"protein","start":36,"end":43,"id":8},{"text":"classification","start":44,"end":58,"id":9},{"text":"problem","start":59,"end":66,"id":10},{"text":"in","start":67,"end":69,"id":11},{"text":"which","start":70,"end":75,"id":12},{"text":"a","start":76,"end":77,"id":13},{"text":"kernel","start":78,"end":84,"id":14},{"text":"-","start":84,"end":85,"id":15},{"text":"based","start":85,"end":90,"id":16},{"text":"distance","start":91,"end":99,"id":17},{"text":"is","start":100,"end":102,"id":18},{"text":"used","start":103,"end":107,"id":19},{"text":".","start":107,"end":108,"id":20}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"mathematical_operations|NOUN","word":"mathematical operations","sense":"NOUN","meta":{"score":0.7605000138,"sense":"NOUN"},"_input_hash":706991560,"_task_hash":917819397,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"mathematical_operations|NOUN","start":0,"end":28,"id":0}]}
{"text":"MATLAB|ORG","word":"MATLAB","sense":"ORG","meta":{"score":0.7875999808,"sense":"ORG"},"_input_hash":58671369,"_task_hash":264404537,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"MATLAB|ORG","start":0,"end":10,"id":0}]}
{"text":"We show how to extend systematically all the results obtained in the inductive setting to transductive learning, and use this to improve Vapnik's generalization bounds, extending them to the case when the sample is made of independent non-identically distributed pairs of patterns and labels.","_input_hash":-979179916,"_task_hash":399736153,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"show","start":3,"end":7,"id":1},{"text":"how","start":8,"end":11,"id":2},{"text":"to","start":12,"end":14,"id":3},{"text":"extend","start":15,"end":21,"id":4},{"text":"systematically","start":22,"end":36,"id":5},{"text":"all","start":37,"end":40,"id":6},{"text":"the","start":41,"end":44,"id":7},{"text":"results","start":45,"end":52,"id":8},{"text":"obtained","start":53,"end":61,"id":9},{"text":"in","start":62,"end":64,"id":10},{"text":"the","start":65,"end":68,"id":11},{"text":"inductive","start":69,"end":78,"id":12},{"text":"setting","start":79,"end":86,"id":13},{"text":"to","start":87,"end":89,"id":14},{"text":"transductive","start":90,"end":102,"id":15},{"text":"learning","start":103,"end":111,"id":16},{"text":",","start":111,"end":112,"id":17},{"text":"and","start":113,"end":116,"id":18},{"text":"use","start":117,"end":120,"id":19},{"text":"this","start":121,"end":125,"id":20},{"text":"to","start":126,"end":128,"id":21},{"text":"improve","start":129,"end":136,"id":22},{"text":"Vapnik","start":137,"end":143,"id":23},{"text":"'s","start":143,"end":145,"id":24},{"text":"generalization","start":146,"end":160,"id":25},{"text":"bounds","start":161,"end":167,"id":26},{"text":",","start":167,"end":168,"id":27},{"text":"extending","start":169,"end":178,"id":28},{"text":"them","start":179,"end":183,"id":29},{"text":"to","start":184,"end":186,"id":30},{"text":"the","start":187,"end":190,"id":31},{"text":"case","start":191,"end":195,"id":32},{"text":"when","start":196,"end":200,"id":33},{"text":"the","start":201,"end":204,"id":34},{"text":"sample","start":205,"end":211,"id":35},{"text":"is","start":212,"end":214,"id":36},{"text":"made","start":215,"end":219,"id":37},{"text":"of","start":220,"end":222,"id":38},{"text":"independent","start":223,"end":234,"id":39},{"text":"non","start":235,"end":238,"id":40},{"text":"-","start":238,"end":239,"id":41},{"text":"identically","start":239,"end":250,"id":42},{"text":"distributed","start":251,"end":262,"id":43},{"text":"pairs","start":263,"end":268,"id":44},{"text":"of","start":269,"end":271,"id":45},{"text":"patterns","start":272,"end":280,"id":46},{"text":"and","start":281,"end":284,"id":47},{"text":"labels","start":285,"end":291,"id":48},{"text":".","start":291,"end":292,"id":49}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"This model exemplifies a recent trend in statistical machine learning--the use of Bayesian nonparametric methods to infer distributions on flexible data structures.","_input_hash":1038966363,"_task_hash":1930955695,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"model","start":5,"end":10,"id":1},{"text":"exemplifies","start":11,"end":22,"id":2},{"text":"a","start":23,"end":24,"id":3},{"text":"recent","start":25,"end":31,"id":4},{"text":"trend","start":32,"end":37,"id":5},{"text":"in","start":38,"end":40,"id":6},{"text":"statistical","start":41,"end":52,"id":7},{"text":"machine","start":53,"end":60,"id":8},{"text":"learning","start":61,"end":69,"id":9},{"text":"--","start":69,"end":71,"id":10},{"text":"the","start":71,"end":74,"id":11},{"text":"use","start":75,"end":78,"id":12},{"text":"of","start":79,"end":81,"id":13},{"text":"Bayesian","start":82,"end":90,"id":14},{"text":"nonparametric","start":91,"end":104,"id":15},{"text":"methods","start":105,"end":112,"id":16},{"text":"to","start":113,"end":115,"id":17},{"text":"infer","start":116,"end":121,"id":18},{"text":"distributions","start":122,"end":135,"id":19},{"text":"on","start":136,"end":138,"id":20},{"text":"flexible","start":139,"end":147,"id":21},{"text":"data","start":148,"end":152,"id":22},{"text":"structures","start":153,"end":163,"id":23},{"text":".","start":163,"end":164,"id":24}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We consider the problem of binary classification where one can, for a particular cost, choose not to classify an observation.","_input_hash":-1660335345,"_task_hash":-2061475922,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"consider","start":3,"end":11,"id":1},{"text":"the","start":12,"end":15,"id":2},{"text":"problem","start":16,"end":23,"id":3},{"text":"of","start":24,"end":26,"id":4},{"text":"binary","start":27,"end":33,"id":5},{"text":"classification","start":34,"end":48,"id":6},{"text":"where","start":49,"end":54,"id":7},{"text":"one","start":55,"end":58,"id":8},{"text":"can","start":59,"end":62,"id":9},{"text":",","start":62,"end":63,"id":10},{"text":"for","start":64,"end":67,"id":11},{"text":"a","start":68,"end":69,"id":12},{"text":"particular","start":70,"end":80,"id":13},{"text":"cost","start":81,"end":85,"id":14},{"text":",","start":85,"end":86,"id":15},{"text":"choose","start":87,"end":93,"id":16},{"text":"not","start":94,"end":97,"id":17},{"text":"to","start":98,"end":100,"id":18},{"text":"classify","start":101,"end":109,"id":19},{"text":"an","start":110,"end":112,"id":20},{"text":"observation","start":113,"end":124,"id":21},{"text":".","start":124,"end":125,"id":22}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":27,"end":48,"token_start":5,"token_end":6,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Deflation-based implementations, such as the popular one-unit FastICA algorithm and its variants, extract the independent components one after another.","_input_hash":-170851408,"_task_hash":-1224616534,"tokens":[{"text":"Deflation","start":0,"end":9,"id":0},{"text":"-","start":9,"end":10,"id":1},{"text":"based","start":10,"end":15,"id":2},{"text":"implementations","start":16,"end":31,"id":3},{"text":",","start":31,"end":32,"id":4},{"text":"such","start":33,"end":37,"id":5},{"text":"as","start":38,"end":40,"id":6},{"text":"the","start":41,"end":44,"id":7},{"text":"popular","start":45,"end":52,"id":8},{"text":"one","start":53,"end":56,"id":9},{"text":"-","start":56,"end":57,"id":10},{"text":"unit","start":57,"end":61,"id":11},{"text":"FastICA","start":62,"end":69,"id":12},{"text":"algorithm","start":70,"end":79,"id":13},{"text":"and","start":80,"end":83,"id":14},{"text":"its","start":84,"end":87,"id":15},{"text":"variants","start":88,"end":96,"id":16},{"text":",","start":96,"end":97,"id":17},{"text":"extract","start":98,"end":105,"id":18},{"text":"the","start":106,"end":109,"id":19},{"text":"independent","start":110,"end":121,"id":20},{"text":"components","start":122,"end":132,"id":21},{"text":"one","start":133,"end":136,"id":22},{"text":"after","start":137,"end":142,"id":23},{"text":"another","start":143,"end":150,"id":24},{"text":".","start":150,"end":151,"id":25}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":62,"end":69,"token_start":12,"token_end":12,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We present surrogate regret bounds for arbitrary surrogate losses in the context of binary classification with label-dependent costs.","_input_hash":-1479826433,"_task_hash":1126243502,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"present","start":3,"end":10,"id":1},{"text":"surrogate","start":11,"end":20,"id":2},{"text":"regret","start":21,"end":27,"id":3},{"text":"bounds","start":28,"end":34,"id":4},{"text":"for","start":35,"end":38,"id":5},{"text":"arbitrary","start":39,"end":48,"id":6},{"text":"surrogate","start":49,"end":58,"id":7},{"text":"losses","start":59,"end":65,"id":8},{"text":"in","start":66,"end":68,"id":9},{"text":"the","start":69,"end":72,"id":10},{"text":"context","start":73,"end":80,"id":11},{"text":"of","start":81,"end":83,"id":12},{"text":"binary","start":84,"end":90,"id":13},{"text":"classification","start":91,"end":105,"id":14},{"text":"with","start":106,"end":110,"id":15},{"text":"label","start":111,"end":116,"id":16},{"text":"-","start":116,"end":117,"id":17},{"text":"dependent","start":117,"end":126,"id":18},{"text":"costs","start":127,"end":132,"id":19},{"text":".","start":132,"end":133,"id":20}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"The same approach has also been used for learning features from data for other purposes, e.g., image classification, but tuning the dictionary in a supervised way for these tasks has proven to be more difficult.","_input_hash":1224610203,"_task_hash":-855153702,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"same","start":4,"end":8,"id":1},{"text":"approach","start":9,"end":17,"id":2},{"text":"has","start":18,"end":21,"id":3},{"text":"also","start":22,"end":26,"id":4},{"text":"been","start":27,"end":31,"id":5},{"text":"used","start":32,"end":36,"id":6},{"text":"for","start":37,"end":40,"id":7},{"text":"learning","start":41,"end":49,"id":8},{"text":"features","start":50,"end":58,"id":9},{"text":"from","start":59,"end":63,"id":10},{"text":"data","start":64,"end":68,"id":11},{"text":"for","start":69,"end":72,"id":12},{"text":"other","start":73,"end":78,"id":13},{"text":"purposes","start":79,"end":87,"id":14},{"text":",","start":87,"end":88,"id":15},{"text":"e.g.","start":89,"end":93,"id":16},{"text":",","start":93,"end":94,"id":17},{"text":"image","start":95,"end":100,"id":18},{"text":"classification","start":101,"end":115,"id":19},{"text":",","start":115,"end":116,"id":20},{"text":"but","start":117,"end":120,"id":21},{"text":"tuning","start":121,"end":127,"id":22},{"text":"the","start":128,"end":131,"id":23},{"text":"dictionary","start":132,"end":142,"id":24},{"text":"in","start":143,"end":145,"id":25},{"text":"a","start":146,"end":147,"id":26},{"text":"supervised","start":148,"end":158,"id":27},{"text":"way","start":159,"end":162,"id":28},{"text":"for","start":163,"end":166,"id":29},{"text":"these","start":167,"end":172,"id":30},{"text":"tasks","start":173,"end":178,"id":31},{"text":"has","start":179,"end":182,"id":32},{"text":"proven","start":183,"end":189,"id":33},{"text":"to","start":190,"end":192,"id":34},{"text":"be","start":193,"end":195,"id":35},{"text":"more","start":196,"end":200,"id":36},{"text":"difficult","start":201,"end":210,"id":37},{"text":".","start":210,"end":211,"id":38}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"The use of these informational divergences together with the proposed method leads to better results, compared to other clustering methods for the problem of astrophysical data processing.","_input_hash":-1974866019,"_task_hash":368956603,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"use","start":4,"end":7,"id":1},{"text":"of","start":8,"end":10,"id":2},{"text":"these","start":11,"end":16,"id":3},{"text":"informational","start":17,"end":30,"id":4},{"text":"divergences","start":31,"end":42,"id":5},{"text":"together","start":43,"end":51,"id":6},{"text":"with","start":52,"end":56,"id":7},{"text":"the","start":57,"end":60,"id":8},{"text":"proposed","start":61,"end":69,"id":9},{"text":"method","start":70,"end":76,"id":10},{"text":"leads","start":77,"end":82,"id":11},{"text":"to","start":83,"end":85,"id":12},{"text":"better","start":86,"end":92,"id":13},{"text":"results","start":93,"end":100,"id":14},{"text":",","start":100,"end":101,"id":15},{"text":"compared","start":102,"end":110,"id":16},{"text":"to","start":111,"end":113,"id":17},{"text":"other","start":114,"end":119,"id":18},{"text":"clustering","start":120,"end":130,"id":19},{"text":"methods","start":131,"end":138,"id":20},{"text":"for","start":139,"end":142,"id":21},{"text":"the","start":143,"end":146,"id":22},{"text":"problem","start":147,"end":154,"id":23},{"text":"of","start":155,"end":157,"id":24},{"text":"astrophysical","start":158,"end":171,"id":25},{"text":"data","start":172,"end":176,"id":26},{"text":"processing","start":177,"end":187,"id":27},{"text":".","start":187,"end":188,"id":28}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"simulation_program|NOUN","word":"simulation program","sense":"NOUN","meta":{"score":0.761500001,"sense":"NOUN"},"_input_hash":-2126182961,"_task_hash":946239638,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"simulation_program|NOUN","start":0,"end":23,"id":0}]}
{"text":"Recently, the Gaussian Process Latent Variable Model (GPLVM) has successfully been used to find low dimensional manifolds in a variety of complex data.","_input_hash":572638255,"_task_hash":-347468811,"tokens":[{"text":"Recently","start":0,"end":8,"id":0},{"text":",","start":8,"end":9,"id":1},{"text":"the","start":10,"end":13,"id":2},{"text":"Gaussian","start":14,"end":22,"id":3},{"text":"Process","start":23,"end":30,"id":4},{"text":"Latent","start":31,"end":37,"id":5},{"text":"Variable","start":38,"end":46,"id":6},{"text":"Model","start":47,"end":52,"id":7},{"text":"(","start":53,"end":54,"id":8},{"text":"GPLVM","start":54,"end":59,"id":9},{"text":")","start":59,"end":60,"id":10},{"text":"has","start":61,"end":64,"id":11},{"text":"successfully","start":65,"end":77,"id":12},{"text":"been","start":78,"end":82,"id":13},{"text":"used","start":83,"end":87,"id":14},{"text":"to","start":88,"end":90,"id":15},{"text":"find","start":91,"end":95,"id":16},{"text":"low","start":96,"end":99,"id":17},{"text":"dimensional","start":100,"end":111,"id":18},{"text":"manifolds","start":112,"end":121,"id":19},{"text":"in","start":122,"end":124,"id":20},{"text":"a","start":125,"end":126,"id":21},{"text":"variety","start":127,"end":134,"id":22},{"text":"of","start":135,"end":137,"id":23},{"text":"complex","start":138,"end":145,"id":24},{"text":"data","start":146,"end":150,"id":25},{"text":".","start":150,"end":151,"id":26}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":14,"end":52,"token_start":3,"token_end":7,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"ought","meta":{"score":0},"_input_hash":1959856811,"_task_hash":1815087780,"_session_id":null,"_view_id":"text","answer":"reject","spans":[],"tokens":[{"text":"ought","start":0,"end":5,"id":0}]}
{"text":"single_neuron|NOUN","word":"single neuron","sense":"NOUN","meta":{"score":0.7791000009,"sense":"NOUN"},"_input_hash":-1009445147,"_task_hash":-405101400,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"single_neuron|NOUN","start":0,"end":18,"id":0}]}
{"text":"should","meta":{"score":0},"_input_hash":325201159,"_task_hash":1270433523,"_session_id":null,"_view_id":"text","answer":"reject","spans":[],"tokens":[{"text":"should","start":0,"end":6,"id":0}]}
{"text":"We present an extension of sparse PCA, or sparse dictionary learning, where the sparsity patterns of all dictionary elements are structured and constrained to belong to a prespecified set of shapes.","_input_hash":405834443,"_task_hash":1956727009,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"present","start":3,"end":10,"id":1},{"text":"an","start":11,"end":13,"id":2},{"text":"extension","start":14,"end":23,"id":3},{"text":"of","start":24,"end":26,"id":4},{"text":"sparse","start":27,"end":33,"id":5},{"text":"PCA","start":34,"end":37,"id":6},{"text":",","start":37,"end":38,"id":7},{"text":"or","start":39,"end":41,"id":8},{"text":"sparse","start":42,"end":48,"id":9},{"text":"dictionary","start":49,"end":59,"id":10},{"text":"learning","start":60,"end":68,"id":11},{"text":",","start":68,"end":69,"id":12},{"text":"where","start":70,"end":75,"id":13},{"text":"the","start":76,"end":79,"id":14},{"text":"sparsity","start":80,"end":88,"id":15},{"text":"patterns","start":89,"end":97,"id":16},{"text":"of","start":98,"end":100,"id":17},{"text":"all","start":101,"end":104,"id":18},{"text":"dictionary","start":105,"end":115,"id":19},{"text":"elements","start":116,"end":124,"id":20},{"text":"are","start":125,"end":128,"id":21},{"text":"structured","start":129,"end":139,"id":22},{"text":"and","start":140,"end":143,"id":23},{"text":"constrained","start":144,"end":155,"id":24},{"text":"to","start":156,"end":158,"id":25},{"text":"belong","start":159,"end":165,"id":26},{"text":"to","start":166,"end":168,"id":27},{"text":"a","start":169,"end":170,"id":28},{"text":"prespecified","start":171,"end":183,"id":29},{"text":"set","start":184,"end":187,"id":30},{"text":"of","start":188,"end":190,"id":31},{"text":"shapes","start":191,"end":197,"id":32},{"text":".","start":197,"end":198,"id":33}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":27,"end":37,"token_start":5,"token_end":6,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"This problem is relevant in machine learning, statistics and signal processing.","_input_hash":-381626183,"_task_hash":-1360300335,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"problem","start":5,"end":12,"id":1},{"text":"is","start":13,"end":15,"id":2},{"text":"relevant","start":16,"end":24,"id":3},{"text":"in","start":25,"end":27,"id":4},{"text":"machine","start":28,"end":35,"id":5},{"text":"learning","start":36,"end":44,"id":6},{"text":",","start":44,"end":45,"id":7},{"text":"statistics","start":46,"end":56,"id":8},{"text":"and","start":57,"end":60,"id":9},{"text":"signal","start":61,"end":67,"id":10},{"text":"processing","start":68,"end":78,"id":11},{"text":".","start":78,"end":79,"id":12}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"At the end, we calculate the topology of the hidden part of a dendrogram.","_input_hash":1928286706,"_task_hash":1441321189,"tokens":[{"text":"At","start":0,"end":2,"id":0},{"text":"the","start":3,"end":6,"id":1},{"text":"end","start":7,"end":10,"id":2},{"text":",","start":10,"end":11,"id":3},{"text":"we","start":12,"end":14,"id":4},{"text":"calculate","start":15,"end":24,"id":5},{"text":"the","start":25,"end":28,"id":6},{"text":"topology","start":29,"end":37,"id":7},{"text":"of","start":38,"end":40,"id":8},{"text":"the","start":41,"end":44,"id":9},{"text":"hidden","start":45,"end":51,"id":10},{"text":"part","start":52,"end":56,"id":11},{"text":"of","start":57,"end":59,"id":12},{"text":"a","start":60,"end":61,"id":13},{"text":"dendrogram","start":62,"end":72,"id":14},{"text":".","start":72,"end":73,"id":15}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"We show here that it is possible for SFA to detect a component which is even slower than the driving force itself (e.g. the envelope of a modulated sine wave).","_input_hash":-1116922908,"_task_hash":1782611902,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"show","start":3,"end":7,"id":1},{"text":"here","start":8,"end":12,"id":2},{"text":"that","start":13,"end":17,"id":3},{"text":"it","start":18,"end":20,"id":4},{"text":"is","start":21,"end":23,"id":5},{"text":"possible","start":24,"end":32,"id":6},{"text":"for","start":33,"end":36,"id":7},{"text":"SFA","start":37,"end":40,"id":8},{"text":"to","start":41,"end":43,"id":9},{"text":"detect","start":44,"end":50,"id":10},{"text":"a","start":51,"end":52,"id":11},{"text":"component","start":53,"end":62,"id":12},{"text":"which","start":63,"end":68,"id":13},{"text":"is","start":69,"end":71,"id":14},{"text":"even","start":72,"end":76,"id":15},{"text":"slower","start":77,"end":83,"id":16},{"text":"than","start":84,"end":88,"id":17},{"text":"the","start":89,"end":92,"id":18},{"text":"driving","start":93,"end":100,"id":19},{"text":"force","start":101,"end":106,"id":20},{"text":"itself","start":107,"end":113,"id":21},{"text":"(","start":114,"end":115,"id":22},{"text":"e.g.","start":115,"end":119,"id":23},{"text":"the","start":120,"end":123,"id":24},{"text":"envelope","start":124,"end":132,"id":25},{"text":"of","start":133,"end":135,"id":26},{"text":"a","start":136,"end":137,"id":27},{"text":"modulated","start":138,"end":147,"id":28},{"text":"sine","start":148,"end":152,"id":29},{"text":"wave","start":153,"end":157,"id":30},{"text":")","start":157,"end":158,"id":31},{"text":".","start":158,"end":159,"id":32}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"We describe and study an alternative selection scheme based on relative bounds between estimators, and present a two step localization technique which can handle the selection of a parametric model from a family of those.","_input_hash":890575210,"_task_hash":-548643541,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"describe","start":3,"end":11,"id":1},{"text":"and","start":12,"end":15,"id":2},{"text":"study","start":16,"end":21,"id":3},{"text":"an","start":22,"end":24,"id":4},{"text":"alternative","start":25,"end":36,"id":5},{"text":"selection","start":37,"end":46,"id":6},{"text":"scheme","start":47,"end":53,"id":7},{"text":"based","start":54,"end":59,"id":8},{"text":"on","start":60,"end":62,"id":9},{"text":"relative","start":63,"end":71,"id":10},{"text":"bounds","start":72,"end":78,"id":11},{"text":"between","start":79,"end":86,"id":12},{"text":"estimators","start":87,"end":97,"id":13},{"text":",","start":97,"end":98,"id":14},{"text":"and","start":99,"end":102,"id":15},{"text":"present","start":103,"end":110,"id":16},{"text":"a","start":111,"end":112,"id":17},{"text":"two","start":113,"end":116,"id":18},{"text":"step","start":117,"end":121,"id":19},{"text":"localization","start":122,"end":134,"id":20},{"text":"technique","start":135,"end":144,"id":21},{"text":"which","start":145,"end":150,"id":22},{"text":"can","start":151,"end":154,"id":23},{"text":"handle","start":155,"end":161,"id":24},{"text":"the","start":162,"end":165,"id":25},{"text":"selection","start":166,"end":175,"id":26},{"text":"of","start":176,"end":178,"id":27},{"text":"a","start":179,"end":180,"id":28},{"text":"parametric","start":181,"end":191,"id":29},{"text":"model","start":192,"end":197,"id":30},{"text":"from","start":198,"end":202,"id":31},{"text":"a","start":203,"end":204,"id":32},{"text":"family","start":205,"end":211,"id":33},{"text":"of","start":212,"end":214,"id":34},{"text":"those","start":215,"end":220,"id":35},{"text":".","start":220,"end":221,"id":36}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Finally we review briefly the construction of Support Vector Machines and show how to derive generalization bounds for them, measuring the complexity either through the number of support vectors or through the value of the transductive or inductive margin.","_input_hash":1340001408,"_task_hash":1257872603,"tokens":[{"text":"Finally","start":0,"end":7,"id":0},{"text":"we","start":8,"end":10,"id":1},{"text":"review","start":11,"end":17,"id":2},{"text":"briefly","start":18,"end":25,"id":3},{"text":"the","start":26,"end":29,"id":4},{"text":"construction","start":30,"end":42,"id":5},{"text":"of","start":43,"end":45,"id":6},{"text":"Support","start":46,"end":53,"id":7},{"text":"Vector","start":54,"end":60,"id":8},{"text":"Machines","start":61,"end":69,"id":9},{"text":"and","start":70,"end":73,"id":10},{"text":"show","start":74,"end":78,"id":11},{"text":"how","start":79,"end":82,"id":12},{"text":"to","start":83,"end":85,"id":13},{"text":"derive","start":86,"end":92,"id":14},{"text":"generalization","start":93,"end":107,"id":15},{"text":"bounds","start":108,"end":114,"id":16},{"text":"for","start":115,"end":118,"id":17},{"text":"them","start":119,"end":123,"id":18},{"text":",","start":123,"end":124,"id":19},{"text":"measuring","start":125,"end":134,"id":20},{"text":"the","start":135,"end":138,"id":21},{"text":"complexity","start":139,"end":149,"id":22},{"text":"either","start":150,"end":156,"id":23},{"text":"through","start":157,"end":164,"id":24},{"text":"the","start":165,"end":168,"id":25},{"text":"number","start":169,"end":175,"id":26},{"text":"of","start":176,"end":178,"id":27},{"text":"support","start":179,"end":186,"id":28},{"text":"vectors","start":187,"end":194,"id":29},{"text":"or","start":195,"end":197,"id":30},{"text":"through","start":198,"end":205,"id":31},{"text":"the","start":206,"end":209,"id":32},{"text":"value","start":210,"end":215,"id":33},{"text":"of","start":216,"end":218,"id":34},{"text":"the","start":219,"end":222,"id":35},{"text":"transductive","start":223,"end":235,"id":36},{"text":"or","start":236,"end":238,"id":37},{"text":"inductive","start":239,"end":248,"id":38},{"text":"margin","start":249,"end":255,"id":39},{"text":".","start":255,"end":256,"id":40}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":46,"end":69,"token_start":7,"token_end":9,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Support vector machines (SVMs) are special kernel based methods and belong to the most successful learning methods since more than a decade.","_input_hash":-1389063560,"_task_hash":1964778525,"tokens":[{"text":"Support","start":0,"end":7,"id":0},{"text":"vector","start":8,"end":14,"id":1},{"text":"machines","start":15,"end":23,"id":2},{"text":"(","start":24,"end":25,"id":3},{"text":"SVMs","start":25,"end":29,"id":4},{"text":")","start":29,"end":30,"id":5},{"text":"are","start":31,"end":34,"id":6},{"text":"special","start":35,"end":42,"id":7},{"text":"kernel","start":43,"end":49,"id":8},{"text":"based","start":50,"end":55,"id":9},{"text":"methods","start":56,"end":63,"id":10},{"text":"and","start":64,"end":67,"id":11},{"text":"belong","start":68,"end":74,"id":12},{"text":"to","start":75,"end":77,"id":13},{"text":"the","start":78,"end":81,"id":14},{"text":"most","start":82,"end":86,"id":15},{"text":"successful","start":87,"end":97,"id":16},{"text":"learning","start":98,"end":106,"id":17},{"text":"methods","start":107,"end":114,"id":18},{"text":"since","start":115,"end":120,"id":19},{"text":"more","start":121,"end":125,"id":20},{"text":"than","start":126,"end":130,"id":21},{"text":"a","start":131,"end":132,"id":22},{"text":"decade","start":133,"end":139,"id":23},{"text":".","start":139,"end":140,"id":24}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":0,"end":23,"token_start":0,"token_end":2,"label":"ALGO","answer":"accept"},{"start":25,"end":29,"token_start":4,"token_end":4,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"statistical_models|NOUN","word":"statistical models","sense":"NOUN","meta":{"score":0.7725999951,"sense":"NOUN"},"_input_hash":1606212968,"_task_hash":1938092438,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"statistical_models|NOUN","start":0,"end":23,"id":0}]}
{"text":"Application examples are given for hidden model reconstruction, cellular automata filtering, and edge detection in images.","_input_hash":-23107903,"_task_hash":-1548598598,"tokens":[{"text":"Application","start":0,"end":11,"id":0},{"text":"examples","start":12,"end":20,"id":1},{"text":"are","start":21,"end":24,"id":2},{"text":"given","start":25,"end":30,"id":3},{"text":"for","start":31,"end":34,"id":4},{"text":"hidden","start":35,"end":41,"id":5},{"text":"model","start":42,"end":47,"id":6},{"text":"reconstruction","start":48,"end":62,"id":7},{"text":",","start":62,"end":63,"id":8},{"text":"cellular","start":64,"end":72,"id":9},{"text":"automata","start":73,"end":81,"id":10},{"text":"filtering","start":82,"end":91,"id":11},{"text":",","start":91,"end":92,"id":12},{"text":"and","start":93,"end":96,"id":13},{"text":"edge","start":97,"end":101,"id":14},{"text":"detection","start":102,"end":111,"id":15},{"text":"in","start":112,"end":114,"id":16},{"text":"images","start":115,"end":121,"id":17},{"text":".","start":121,"end":122,"id":18}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"We test sLDA on two real-world problems:","_input_hash":-333578088,"_task_hash":-520742590,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"test","start":3,"end":7,"id":1},{"text":"sLDA","start":8,"end":12,"id":2},{"text":"on","start":13,"end":15,"id":3},{"text":"two","start":16,"end":19,"id":4},{"text":"real","start":20,"end":24,"id":5},{"text":"-","start":24,"end":25,"id":6},{"text":"world","start":25,"end":30,"id":7},{"text":"problems","start":31,"end":39,"id":8},{"text":":","start":39,"end":40,"id":9}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"linear_regression|NOUN","word":"linear regression","sense":"NOUN","meta":{"score":0.7767000198,"sense":"NOUN"},"_input_hash":-1116490961,"_task_hash":-1941828833,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"linear_regression|NOUN","start":0,"end":22,"id":0}]}
{"text":"somethin","meta":{"score":0},"_input_hash":363249160,"_task_hash":212115723,"_session_id":null,"_view_id":"text","answer":"reject","spans":[],"tokens":[{"text":"somethin","start":0,"end":8,"id":0}]}
{"text":"We present experiments with artificial data and real-world gene expression data to evaluate the method.","_input_hash":-875733824,"_task_hash":-869719196,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"present","start":3,"end":10,"id":1},{"text":"experiments","start":11,"end":22,"id":2},{"text":"with","start":23,"end":27,"id":3},{"text":"artificial","start":28,"end":38,"id":4},{"text":"data","start":39,"end":43,"id":5},{"text":"and","start":44,"end":47,"id":6},{"text":"real","start":48,"end":52,"id":7},{"text":"-","start":52,"end":53,"id":8},{"text":"world","start":53,"end":58,"id":9},{"text":"gene","start":59,"end":63,"id":10},{"text":"expression","start":64,"end":74,"id":11},{"text":"data","start":75,"end":79,"id":12},{"text":"to","start":80,"end":82,"id":13},{"text":"evaluate","start":83,"end":91,"id":14},{"text":"the","start":92,"end":95,"id":15},{"text":"method","start":96,"end":102,"id":16},{"text":".","start":102,"end":103,"id":17}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":92,"end":102,"token_start":15,"token_end":16,"label":"ALGO","answer":"reject"}],"answer":"reject"}
{"text":"We empirically analyze the properties of the DP-GLM and why it provides better results than existing Dirichlet process mixture regression models.","_input_hash":1933387715,"_task_hash":-68566916,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"empirically","start":3,"end":14,"id":1},{"text":"analyze","start":15,"end":22,"id":2},{"text":"the","start":23,"end":26,"id":3},{"text":"properties","start":27,"end":37,"id":4},{"text":"of","start":38,"end":40,"id":5},{"text":"the","start":41,"end":44,"id":6},{"text":"DP","start":45,"end":47,"id":7},{"text":"-","start":47,"end":48,"id":8},{"text":"GLM","start":48,"end":51,"id":9},{"text":"and","start":52,"end":55,"id":10},{"text":"why","start":56,"end":59,"id":11},{"text":"it","start":60,"end":62,"id":12},{"text":"provides","start":63,"end":71,"id":13},{"text":"better","start":72,"end":78,"id":14},{"text":"results","start":79,"end":86,"id":15},{"text":"than","start":87,"end":91,"id":16},{"text":"existing","start":92,"end":100,"id":17},{"text":"Dirichlet","start":101,"end":110,"id":18},{"text":"process","start":111,"end":118,"id":19},{"text":"mixture","start":119,"end":126,"id":20},{"text":"regression","start":127,"end":137,"id":21},{"text":"models","start":138,"end":144,"id":22},{"text":".","start":144,"end":145,"id":23}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":101,"end":144,"token_start":18,"token_end":22,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We use the CIBP prior with the nonlinear Gaussian belief network so each unit can additionally vary its behavior between discrete and continuous representations.","_input_hash":-1633248066,"_task_hash":73295502,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"use","start":3,"end":6,"id":1},{"text":"the","start":7,"end":10,"id":2},{"text":"CIBP","start":11,"end":15,"id":3},{"text":"prior","start":16,"end":21,"id":4},{"text":"with","start":22,"end":26,"id":5},{"text":"the","start":27,"end":30,"id":6},{"text":"nonlinear","start":31,"end":40,"id":7},{"text":"Gaussian","start":41,"end":49,"id":8},{"text":"belief","start":50,"end":56,"id":9},{"text":"network","start":57,"end":64,"id":10},{"text":"so","start":65,"end":67,"id":11},{"text":"each","start":68,"end":72,"id":12},{"text":"unit","start":73,"end":77,"id":13},{"text":"can","start":78,"end":81,"id":14},{"text":"additionally","start":82,"end":94,"id":15},{"text":"vary","start":95,"end":99,"id":16},{"text":"its","start":100,"end":103,"id":17},{"text":"behavior","start":104,"end":112,"id":18},{"text":"between","start":113,"end":120,"id":19},{"text":"discrete","start":121,"end":129,"id":20},{"text":"and","start":130,"end":133,"id":21},{"text":"continuous","start":134,"end":144,"id":22},{"text":"representations","start":145,"end":160,"id":23},{"text":".","start":160,"end":161,"id":24}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Experiments with two practical tasks, face recognition and the study of the dynamics of a protein complex, demonstrate the benefits of the proposed structured approach over unstructured approaches.","_input_hash":2096971661,"_task_hash":1456351042,"tokens":[{"text":"Experiments","start":0,"end":11,"id":0},{"text":"with","start":12,"end":16,"id":1},{"text":"two","start":17,"end":20,"id":2},{"text":"practical","start":21,"end":30,"id":3},{"text":"tasks","start":31,"end":36,"id":4},{"text":",","start":36,"end":37,"id":5},{"text":"face","start":38,"end":42,"id":6},{"text":"recognition","start":43,"end":54,"id":7},{"text":"and","start":55,"end":58,"id":8},{"text":"the","start":59,"end":62,"id":9},{"text":"study","start":63,"end":68,"id":10},{"text":"of","start":69,"end":71,"id":11},{"text":"the","start":72,"end":75,"id":12},{"text":"dynamics","start":76,"end":84,"id":13},{"text":"of","start":85,"end":87,"id":14},{"text":"a","start":88,"end":89,"id":15},{"text":"protein","start":90,"end":97,"id":16},{"text":"complex","start":98,"end":105,"id":17},{"text":",","start":105,"end":106,"id":18},{"text":"demonstrate","start":107,"end":118,"id":19},{"text":"the","start":119,"end":122,"id":20},{"text":"benefits","start":123,"end":131,"id":21},{"text":"of","start":132,"end":134,"id":22},{"text":"the","start":135,"end":138,"id":23},{"text":"proposed","start":139,"end":147,"id":24},{"text":"structured","start":148,"end":158,"id":25},{"text":"approach","start":159,"end":167,"id":26},{"text":"over","start":168,"end":172,"id":27},{"text":"unstructured","start":173,"end":185,"id":28},{"text":"approaches","start":186,"end":196,"id":29},{"text":".","start":196,"end":197,"id":30}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"SVM|ORG","word":"SVM","sense":"ORG","meta":{"score":0.7735999823,"sense":"ORG"},"_input_hash":926515567,"_task_hash":1652806291,"_session_id":null,"_view_id":"html","answer":"accept","spans":[],"tokens":[{"text":"SVM|ORG","start":0,"end":7,"id":0}]}
{"text":"mathematical_modeling|NOUN","word":"mathematical modeling","sense":"NOUN","meta":{"score":0.7849000096,"sense":"NOUN"},"_input_hash":-871533782,"_task_hash":-31663915,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"mathematical_modeling|NOUN","start":0,"end":26,"id":0}]}
{"text":"complexity_theory|NOUN","word":"complexity theory","sense":"NOUN","meta":{"score":0.7652000189,"sense":"NOUN"},"_input_hash":1786667802,"_task_hash":1671588818,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"complexity_theory|NOUN","start":0,"end":22,"id":0}]}
{"text":"comparison_process|NOUN","word":"comparison process","sense":"NOUN","meta":{"score":0.7673000097,"sense":"NOUN"},"_input_hash":-906823434,"_task_hash":-935300860,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"comparison_process|NOUN","start":0,"end":23,"id":0}]}
{"text":"quantum_computer|NOUN","word":"quantum computer","sense":"NOUN","meta":{"score":0.7753000259,"sense":"NOUN"},"_input_hash":721128169,"_task_hash":-619621124,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"quantum_computer|NOUN","start":0,"end":21,"id":0}]}
{"text":"signal_processing|NOUN","word":"signal processing","sense":"NOUN","meta":{"score":0.793900013,"sense":"NOUN"},"_input_hash":2041971023,"_task_hash":520564469,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"signal_processing|NOUN","start":0,"end":22,"id":0}]}
{"text":"predictive_models|NOUN","word":"predictive models","sense":"NOUN","meta":{"score":0.7767999768,"sense":"NOUN"},"_input_hash":183603227,"_task_hash":1378661207,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"predictive_models|NOUN","start":0,"end":22,"id":0}]}
{"text":"matrix_math|NOUN","word":"matrix math","sense":"NOUN","meta":{"score":0.7785000205,"sense":"NOUN"},"_input_hash":2125274819,"_task_hash":812095251,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"matrix_math|NOUN","start":0,"end":16,"id":0}]}
{"text":"first, we consider fixed hierarchical dictionaries of wavelets to denoise natural images.","_input_hash":2020002736,"_task_hash":1235321103,"tokens":[{"text":"first","start":0,"end":5,"id":0},{"text":",","start":5,"end":6,"id":1},{"text":"we","start":7,"end":9,"id":2},{"text":"consider","start":10,"end":18,"id":3},{"text":"fixed","start":19,"end":24,"id":4},{"text":"hierarchical","start":25,"end":37,"id":5},{"text":"dictionaries","start":38,"end":50,"id":6},{"text":"of","start":51,"end":53,"id":7},{"text":"wavelets","start":54,"end":62,"id":8},{"text":"to","start":63,"end":65,"id":9},{"text":"denoise","start":66,"end":73,"id":10},{"text":"natural","start":74,"end":81,"id":11},{"text":"images","start":82,"end":88,"id":12},{"text":".","start":88,"end":89,"id":13}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"image_recognition|NOUN","word":"image recognition","sense":"NOUN","meta":{"score":0.7644000053,"sense":"NOUN"},"_input_hash":1670936266,"_task_hash":-1516639578,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"image_recognition|NOUN","start":0,"end":22,"id":0}]}
{"text":"Our bounds show that whereas a poisoning attack can be effectively staged in the unconstrained case, it can be made arbitrarily difficult (a strict upper bound on the attacker's gain) if external constraints are properly used.","_input_hash":-693715575,"_task_hash":-2075658,"tokens":[{"text":"Our","start":0,"end":3,"id":0},{"text":"bounds","start":4,"end":10,"id":1},{"text":"show","start":11,"end":15,"id":2},{"text":"that","start":16,"end":20,"id":3},{"text":"whereas","start":21,"end":28,"id":4},{"text":"a","start":29,"end":30,"id":5},{"text":"poisoning","start":31,"end":40,"id":6},{"text":"attack","start":41,"end":47,"id":7},{"text":"can","start":48,"end":51,"id":8},{"text":"be","start":52,"end":54,"id":9},{"text":"effectively","start":55,"end":66,"id":10},{"text":"staged","start":67,"end":73,"id":11},{"text":"in","start":74,"end":76,"id":12},{"text":"the","start":77,"end":80,"id":13},{"text":"unconstrained","start":81,"end":94,"id":14},{"text":"case","start":95,"end":99,"id":15},{"text":",","start":99,"end":100,"id":16},{"text":"it","start":101,"end":103,"id":17},{"text":"can","start":104,"end":107,"id":18},{"text":"be","start":108,"end":110,"id":19},{"text":"made","start":111,"end":115,"id":20},{"text":"arbitrarily","start":116,"end":127,"id":21},{"text":"difficult","start":128,"end":137,"id":22},{"text":"(","start":138,"end":139,"id":23},{"text":"a","start":139,"end":140,"id":24},{"text":"strict","start":141,"end":147,"id":25},{"text":"upper","start":148,"end":153,"id":26},{"text":"bound","start":154,"end":159,"id":27},{"text":"on","start":160,"end":162,"id":28},{"text":"the","start":163,"end":166,"id":29},{"text":"attacker","start":167,"end":175,"id":30},{"text":"'s","start":175,"end":177,"id":31},{"text":"gain","start":178,"end":182,"id":32},{"text":")","start":182,"end":183,"id":33},{"text":"if","start":184,"end":186,"id":34},{"text":"external","start":187,"end":195,"id":35},{"text":"constraints","start":196,"end":207,"id":36},{"text":"are","start":208,"end":211,"id":37},{"text":"properly","start":212,"end":220,"id":38},{"text":"used","start":221,"end":225,"id":39},{"text":".","start":225,"end":226,"id":40}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"The model accommodates a variety of response types.","_input_hash":-768690754,"_task_hash":435747910,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"model","start":4,"end":9,"id":1},{"text":"accommodates","start":10,"end":22,"id":2},{"text":"a","start":23,"end":24,"id":3},{"text":"variety","start":25,"end":32,"id":4},{"text":"of","start":33,"end":35,"id":5},{"text":"response","start":36,"end":44,"id":6},{"text":"types","start":45,"end":50,"id":7},{"text":".","start":50,"end":51,"id":8}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Many inference problems involving questions of optimality ask for the maximum or the minimum of a finite set of unknown quantities.","_input_hash":-1698370089,"_task_hash":-1547592385,"tokens":[{"text":"Many","start":0,"end":4,"id":0},{"text":"inference","start":5,"end":14,"id":1},{"text":"problems","start":15,"end":23,"id":2},{"text":"involving","start":24,"end":33,"id":3},{"text":"questions","start":34,"end":43,"id":4},{"text":"of","start":44,"end":46,"id":5},{"text":"optimality","start":47,"end":57,"id":6},{"text":"ask","start":58,"end":61,"id":7},{"text":"for","start":62,"end":65,"id":8},{"text":"the","start":66,"end":69,"id":9},{"text":"maximum","start":70,"end":77,"id":10},{"text":"or","start":78,"end":80,"id":11},{"text":"the","start":81,"end":84,"id":12},{"text":"minimum","start":85,"end":92,"id":13},{"text":"of","start":93,"end":95,"id":14},{"text":"a","start":96,"end":97,"id":15},{"text":"finite","start":98,"end":104,"id":16},{"text":"set","start":105,"end":108,"id":17},{"text":"of","start":109,"end":111,"id":18},{"text":"unknown","start":112,"end":119,"id":19},{"text":"quantities","start":120,"end":130,"id":20},{"text":".","start":130,"end":131,"id":21}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"These features are demonstrated by a comparative numerical analysis on synthetic data.","_input_hash":451875220,"_task_hash":-1027452501,"tokens":[{"text":"These","start":0,"end":5,"id":0},{"text":"features","start":6,"end":14,"id":1},{"text":"are","start":15,"end":18,"id":2},{"text":"demonstrated","start":19,"end":31,"id":3},{"text":"by","start":32,"end":34,"id":4},{"text":"a","start":35,"end":36,"id":5},{"text":"comparative","start":37,"end":48,"id":6},{"text":"numerical","start":49,"end":58,"id":7},{"text":"analysis","start":59,"end":67,"id":8},{"text":"on","start":68,"end":70,"id":9},{"text":"synthetic","start":71,"end":80,"id":10},{"text":"data","start":81,"end":85,"id":11},{"text":".","start":85,"end":86,"id":12}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"In this paper, we present StARS:","_input_hash":-930151000,"_task_hash":-1277374912,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"this","start":3,"end":7,"id":1},{"text":"paper","start":8,"end":13,"id":2},{"text":",","start":13,"end":14,"id":3},{"text":"we","start":15,"end":17,"id":4},{"text":"present","start":18,"end":25,"id":5},{"text":"StARS","start":26,"end":31,"id":6},{"text":":","start":31,"end":32,"id":7}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"neural_connections|NOUN","word":"neural connections","sense":"NOUN","meta":{"score":0.7734000087,"sense":"NOUN"},"_input_hash":-1124815141,"_task_hash":-1146691329,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"neural_connections|NOUN","start":0,"end":23,"id":0}]}
{"text":"strong_AI|NOUN","word":"strong AI","sense":"NOUN","meta":{"score":0.7667999864,"sense":"NOUN"},"_input_hash":277391246,"_task_hash":-1948459791,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"strong_AI|NOUN","start":0,"end":14,"id":0}]}
{"text":"Missing data is treated as Missing At Random mechanism by implementing maximum likelihood algorithm.","_input_hash":-445726575,"_task_hash":-1163333423,"tokens":[{"text":"Missing","start":0,"end":7,"id":0},{"text":"data","start":8,"end":12,"id":1},{"text":"is","start":13,"end":15,"id":2},{"text":"treated","start":16,"end":23,"id":3},{"text":"as","start":24,"end":26,"id":4},{"text":"Missing","start":27,"end":34,"id":5},{"text":"At","start":35,"end":37,"id":6},{"text":"Random","start":38,"end":44,"id":7},{"text":"mechanism","start":45,"end":54,"id":8},{"text":"by","start":55,"end":57,"id":9},{"text":"implementing","start":58,"end":70,"id":10},{"text":"maximum","start":71,"end":78,"id":11},{"text":"likelihood","start":79,"end":89,"id":12},{"text":"algorithm","start":90,"end":99,"id":13},{"text":".","start":99,"end":100,"id":14}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"what","meta":{"score":0},"_input_hash":-591943896,"_task_hash":991272394,"_session_id":null,"_view_id":"text","answer":"reject","spans":[],"tokens":[{"text":"what","start":0,"end":4,"id":0}]}
{"text":"to provide some insights about the behavior of the variable importance index based on random forests and in addition, to propose to investigate two classical issues of variable selection.","_input_hash":1956359793,"_task_hash":1819099401,"tokens":[{"text":"to","start":0,"end":2,"id":0},{"text":"provide","start":3,"end":10,"id":1},{"text":"some","start":11,"end":15,"id":2},{"text":"insights","start":16,"end":24,"id":3},{"text":"about","start":25,"end":30,"id":4},{"text":"the","start":31,"end":34,"id":5},{"text":"behavior","start":35,"end":43,"id":6},{"text":"of","start":44,"end":46,"id":7},{"text":"the","start":47,"end":50,"id":8},{"text":"variable","start":51,"end":59,"id":9},{"text":"importance","start":60,"end":70,"id":10},{"text":"index","start":71,"end":76,"id":11},{"text":"based","start":77,"end":82,"id":12},{"text":"on","start":83,"end":85,"id":13},{"text":"random","start":86,"end":92,"id":14},{"text":"forests","start":93,"end":100,"id":15},{"text":"and","start":101,"end":104,"id":16},{"text":"in","start":105,"end":107,"id":17},{"text":"addition","start":108,"end":116,"id":18},{"text":",","start":116,"end":117,"id":19},{"text":"to","start":118,"end":120,"id":20},{"text":"propose","start":121,"end":128,"id":21},{"text":"to","start":129,"end":131,"id":22},{"text":"investigate","start":132,"end":143,"id":23},{"text":"two","start":144,"end":147,"id":24},{"text":"classical","start":148,"end":157,"id":25},{"text":"issues","start":158,"end":164,"id":26},{"text":"of","start":165,"end":167,"id":27},{"text":"variable","start":168,"end":176,"id":28},{"text":"selection","start":177,"end":186,"id":29},{"text":".","start":186,"end":187,"id":30}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":86,"end":100,"token_start":14,"token_end":15,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Asymptotic null distributions under null hypothesis are derived, and consistency against fixed and local alternatives is assessed.","_input_hash":1669956228,"_task_hash":967005207,"tokens":[{"text":"Asymptotic","start":0,"end":10,"id":0},{"text":"null","start":11,"end":15,"id":1},{"text":"distributions","start":16,"end":29,"id":2},{"text":"under","start":30,"end":35,"id":3},{"text":"null","start":36,"end":40,"id":4},{"text":"hypothesis","start":41,"end":51,"id":5},{"text":"are","start":52,"end":55,"id":6},{"text":"derived","start":56,"end":63,"id":7},{"text":",","start":63,"end":64,"id":8},{"text":"and","start":65,"end":68,"id":9},{"text":"consistency","start":69,"end":80,"id":10},{"text":"against","start":81,"end":88,"id":11},{"text":"fixed","start":89,"end":94,"id":12},{"text":"and","start":95,"end":98,"id":13},{"text":"local","start":99,"end":104,"id":14},{"text":"alternatives","start":105,"end":117,"id":15},{"text":"is","start":118,"end":120,"id":16},{"text":"assessed","start":121,"end":129,"id":17},{"text":".","start":129,"end":130,"id":18}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"Independent component analysis (ICA) aims at decomposing an observed random vector into statistically independent variables.","_input_hash":-1584514654,"_task_hash":-366951719,"tokens":[{"text":"Independent","start":0,"end":11,"id":0},{"text":"component","start":12,"end":21,"id":1},{"text":"analysis","start":22,"end":30,"id":2},{"text":"(","start":31,"end":32,"id":3},{"text":"ICA","start":32,"end":35,"id":4},{"text":")","start":35,"end":36,"id":5},{"text":"aims","start":37,"end":41,"id":6},{"text":"at","start":42,"end":44,"id":7},{"text":"decomposing","start":45,"end":56,"id":8},{"text":"an","start":57,"end":59,"id":9},{"text":"observed","start":60,"end":68,"id":10},{"text":"random","start":69,"end":75,"id":11},{"text":"vector","start":76,"end":82,"id":12},{"text":"into","start":83,"end":87,"id":13},{"text":"statistically","start":88,"end":101,"id":14},{"text":"independent","start":102,"end":113,"id":15},{"text":"variables","start":114,"end":123,"id":16},{"text":".","start":123,"end":124,"id":17}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":0,"end":30,"token_start":0,"token_end":2,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"In this paper we formulate in general terms an approach to prove strong consistency of the Empirical Risk Minimisation inductive principle applied to the prototype or distance based clustering.","_input_hash":-214180296,"_task_hash":-607486716,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"this","start":3,"end":7,"id":1},{"text":"paper","start":8,"end":13,"id":2},{"text":"we","start":14,"end":16,"id":3},{"text":"formulate","start":17,"end":26,"id":4},{"text":"in","start":27,"end":29,"id":5},{"text":"general","start":30,"end":37,"id":6},{"text":"terms","start":38,"end":43,"id":7},{"text":"an","start":44,"end":46,"id":8},{"text":"approach","start":47,"end":55,"id":9},{"text":"to","start":56,"end":58,"id":10},{"text":"prove","start":59,"end":64,"id":11},{"text":"strong","start":65,"end":71,"id":12},{"text":"consistency","start":72,"end":83,"id":13},{"text":"of","start":84,"end":86,"id":14},{"text":"the","start":87,"end":90,"id":15},{"text":"Empirical","start":91,"end":100,"id":16},{"text":"Risk","start":101,"end":105,"id":17},{"text":"Minimisation","start":106,"end":118,"id":18},{"text":"inductive","start":119,"end":128,"id":19},{"text":"principle","start":129,"end":138,"id":20},{"text":"applied","start":139,"end":146,"id":21},{"text":"to","start":147,"end":149,"id":22},{"text":"the","start":150,"end":153,"id":23},{"text":"prototype","start":154,"end":163,"id":24},{"text":"or","start":164,"end":166,"id":25},{"text":"distance","start":167,"end":175,"id":26},{"text":"based","start":176,"end":181,"id":27},{"text":"clustering","start":182,"end":192,"id":28},{"text":".","start":192,"end":193,"id":29}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":91,"end":100,"token_start":16,"token_end":16,"label":"ALGO","answer":"reject"},{"start":106,"end":118,"token_start":18,"token_end":18,"label":"ALGO","answer":"reject"}],"answer":"reject"}
{"text":"boolean_logic|NOUN","word":"boolean logic","sense":"NOUN","meta":{"score":0.7964000106,"sense":"NOUN"},"_input_hash":-1135731581,"_task_hash":775918564,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"boolean_logic|NOUN","start":0,"end":18,"id":0}]}
{"text":"The main point under consideration is the nature of the hypothesis set when no prior information is available but data.","_input_hash":-1902021527,"_task_hash":118208701,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"main","start":4,"end":8,"id":1},{"text":"point","start":9,"end":14,"id":2},{"text":"under","start":15,"end":20,"id":3},{"text":"consideration","start":21,"end":34,"id":4},{"text":"is","start":35,"end":37,"id":5},{"text":"the","start":38,"end":41,"id":6},{"text":"nature","start":42,"end":48,"id":7},{"text":"of","start":49,"end":51,"id":8},{"text":"the","start":52,"end":55,"id":9},{"text":"hypothesis","start":56,"end":66,"id":10},{"text":"set","start":67,"end":70,"id":11},{"text":"when","start":71,"end":75,"id":12},{"text":"no","start":76,"end":78,"id":13},{"text":"prior","start":79,"end":84,"id":14},{"text":"information","start":85,"end":96,"id":15},{"text":"is","start":97,"end":99,"id":16},{"text":"available","start":100,"end":109,"id":17},{"text":"but","start":110,"end":113,"id":18},{"text":"data","start":114,"end":118,"id":19},{"text":".","start":118,"end":119,"id":20}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"With only a few nearest neighbors (or most similar images) to vote, the test set error rate on MNIST database could reach about 1.5%-2.0%, which is very close to many advanced models.","_input_hash":-2131117482,"_task_hash":1899270129,"tokens":[{"text":"With","start":0,"end":4,"id":0},{"text":"only","start":5,"end":9,"id":1},{"text":"a","start":10,"end":11,"id":2},{"text":"few","start":12,"end":15,"id":3},{"text":"nearest","start":16,"end":23,"id":4},{"text":"neighbors","start":24,"end":33,"id":5},{"text":"(","start":34,"end":35,"id":6},{"text":"or","start":35,"end":37,"id":7},{"text":"most","start":38,"end":42,"id":8},{"text":"similar","start":43,"end":50,"id":9},{"text":"images","start":51,"end":57,"id":10},{"text":")","start":57,"end":58,"id":11},{"text":"to","start":59,"end":61,"id":12},{"text":"vote","start":62,"end":66,"id":13},{"text":",","start":66,"end":67,"id":14},{"text":"the","start":68,"end":71,"id":15},{"text":"test","start":72,"end":76,"id":16},{"text":"set","start":77,"end":80,"id":17},{"text":"error","start":81,"end":86,"id":18},{"text":"rate","start":87,"end":91,"id":19},{"text":"on","start":92,"end":94,"id":20},{"text":"MNIST","start":95,"end":100,"id":21},{"text":"database","start":101,"end":109,"id":22},{"text":"could","start":110,"end":115,"id":23},{"text":"reach","start":116,"end":121,"id":24},{"text":"about","start":122,"end":127,"id":25},{"text":"1.5%-2.0","start":128,"end":136,"id":26},{"text":"%","start":136,"end":137,"id":27},{"text":",","start":137,"end":138,"id":28},{"text":"which","start":139,"end":144,"id":29},{"text":"is","start":145,"end":147,"id":30},{"text":"very","start":148,"end":152,"id":31},{"text":"close","start":153,"end":158,"id":32},{"text":"to","start":159,"end":161,"id":33},{"text":"many","start":162,"end":166,"id":34},{"text":"advanced","start":167,"end":175,"id":35},{"text":"models","start":176,"end":182,"id":36},{"text":".","start":182,"end":183,"id":37}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"This article proposes a method for finding global maxima if the joint distribution is modeled by a kernel density estimation.","_input_hash":232411880,"_task_hash":1743787454,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"article","start":5,"end":12,"id":1},{"text":"proposes","start":13,"end":21,"id":2},{"text":"a","start":22,"end":23,"id":3},{"text":"method","start":24,"end":30,"id":4},{"text":"for","start":31,"end":34,"id":5},{"text":"finding","start":35,"end":42,"id":6},{"text":"global","start":43,"end":49,"id":7},{"text":"maxima","start":50,"end":56,"id":8},{"text":"if","start":57,"end":59,"id":9},{"text":"the","start":60,"end":63,"id":10},{"text":"joint","start":64,"end":69,"id":11},{"text":"distribution","start":70,"end":82,"id":12},{"text":"is","start":83,"end":85,"id":13},{"text":"modeled","start":86,"end":93,"id":14},{"text":"by","start":94,"end":96,"id":15},{"text":"a","start":97,"end":98,"id":16},{"text":"kernel","start":99,"end":105,"id":17},{"text":"density","start":106,"end":113,"id":18},{"text":"estimation","start":114,"end":124,"id":19},{"text":".","start":124,"end":125,"id":20}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":22,"end":30,"token_start":3,"token_end":4,"label":"ALGO","answer":"reject"}],"answer":"reject"}
{"text":"This allows the algorithm to quickly compute approximate solutions with roughly the same classification accuracy as the optimal ones, considerably reducing the training time.","_input_hash":2017069756,"_task_hash":1674284664,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"allows","start":5,"end":11,"id":1},{"text":"the","start":12,"end":15,"id":2},{"text":"algorithm","start":16,"end":25,"id":3},{"text":"to","start":26,"end":28,"id":4},{"text":"quickly","start":29,"end":36,"id":5},{"text":"compute","start":37,"end":44,"id":6},{"text":"approximate","start":45,"end":56,"id":7},{"text":"solutions","start":57,"end":66,"id":8},{"text":"with","start":67,"end":71,"id":9},{"text":"roughly","start":72,"end":79,"id":10},{"text":"the","start":80,"end":83,"id":11},{"text":"same","start":84,"end":88,"id":12},{"text":"classification","start":89,"end":103,"id":13},{"text":"accuracy","start":104,"end":112,"id":14},{"text":"as","start":113,"end":115,"id":15},{"text":"the","start":116,"end":119,"id":16},{"text":"optimal","start":120,"end":127,"id":17},{"text":"ones","start":128,"end":132,"id":18},{"text":",","start":132,"end":133,"id":19},{"text":"considerably","start":134,"end":146,"id":20},{"text":"reducing","start":147,"end":155,"id":21},{"text":"the","start":156,"end":159,"id":22},{"text":"training","start":160,"end":168,"id":23},{"text":"time","start":169,"end":173,"id":24},{"text":".","start":173,"end":174,"id":25}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Experimental results over reverberant synthetic mixtures and live recordings of speech data show the effectiveness of the proposed approach.","_input_hash":-1693532875,"_task_hash":1109455853,"tokens":[{"text":"Experimental","start":0,"end":12,"id":0},{"text":"results","start":13,"end":20,"id":1},{"text":"over","start":21,"end":25,"id":2},{"text":"reverberant","start":26,"end":37,"id":3},{"text":"synthetic","start":38,"end":47,"id":4},{"text":"mixtures","start":48,"end":56,"id":5},{"text":"and","start":57,"end":60,"id":6},{"text":"live","start":61,"end":65,"id":7},{"text":"recordings","start":66,"end":76,"id":8},{"text":"of","start":77,"end":79,"id":9},{"text":"speech","start":80,"end":86,"id":10},{"text":"data","start":87,"end":91,"id":11},{"text":"show","start":92,"end":96,"id":12},{"text":"the","start":97,"end":100,"id":13},{"text":"effectiveness","start":101,"end":114,"id":14},{"text":"of","start":115,"end":117,"id":15},{"text":"the","start":118,"end":121,"id":16},{"text":"proposed","start":122,"end":130,"id":17},{"text":"approach","start":131,"end":139,"id":18},{"text":".","start":139,"end":140,"id":19}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"pattern_recognition|NOUN","word":"pattern recognition","sense":"NOUN","meta":{"score":0.7792000175,"sense":"NOUN"},"_input_hash":778820464,"_task_hash":-746637609,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"pattern_recognition|NOUN","start":0,"end":24,"id":0}]}
{"text":"RobustICA's capabilities in processing real-world data involving noncircular complex strongly super-Gaussian sources are illustrated by the biomedical problem of atrial activity (AA) extraction in atrial fibrillation (AF) electrocardiograms (ECGs), where it outperforms an alternative ICA-based technique.","_input_hash":237247266,"_task_hash":257349884,"tokens":[{"text":"RobustICA","start":0,"end":9,"id":0},{"text":"'s","start":9,"end":11,"id":1},{"text":"capabilities","start":12,"end":24,"id":2},{"text":"in","start":25,"end":27,"id":3},{"text":"processing","start":28,"end":38,"id":4},{"text":"real","start":39,"end":43,"id":5},{"text":"-","start":43,"end":44,"id":6},{"text":"world","start":44,"end":49,"id":7},{"text":"data","start":50,"end":54,"id":8},{"text":"involving","start":55,"end":64,"id":9},{"text":"noncircular","start":65,"end":76,"id":10},{"text":"complex","start":77,"end":84,"id":11},{"text":"strongly","start":85,"end":93,"id":12},{"text":"super","start":94,"end":99,"id":13},{"text":"-","start":99,"end":100,"id":14},{"text":"Gaussian","start":100,"end":108,"id":15},{"text":"sources","start":109,"end":116,"id":16},{"text":"are","start":117,"end":120,"id":17},{"text":"illustrated","start":121,"end":132,"id":18},{"text":"by","start":133,"end":135,"id":19},{"text":"the","start":136,"end":139,"id":20},{"text":"biomedical","start":140,"end":150,"id":21},{"text":"problem","start":151,"end":158,"id":22},{"text":"of","start":159,"end":161,"id":23},{"text":"atrial","start":162,"end":168,"id":24},{"text":"activity","start":169,"end":177,"id":25},{"text":"(","start":178,"end":179,"id":26},{"text":"AA","start":179,"end":181,"id":27},{"text":")","start":181,"end":182,"id":28},{"text":"extraction","start":183,"end":193,"id":29},{"text":"in","start":194,"end":196,"id":30},{"text":"atrial","start":197,"end":203,"id":31},{"text":"fibrillation","start":204,"end":216,"id":32},{"text":"(","start":217,"end":218,"id":33},{"text":"AF","start":218,"end":220,"id":34},{"text":")","start":220,"end":221,"id":35},{"text":"electrocardiograms","start":222,"end":240,"id":36},{"text":"(","start":241,"end":242,"id":37},{"text":"ECGs","start":242,"end":246,"id":38},{"text":")","start":246,"end":247,"id":39},{"text":",","start":247,"end":248,"id":40},{"text":"where","start":249,"end":254,"id":41},{"text":"it","start":255,"end":257,"id":42},{"text":"outperforms","start":258,"end":269,"id":43},{"text":"an","start":270,"end":272,"id":44},{"text":"alternative","start":273,"end":284,"id":45},{"text":"ICA","start":285,"end":288,"id":46},{"text":"-","start":288,"end":289,"id":47},{"text":"based","start":289,"end":294,"id":48},{"text":"technique","start":295,"end":304,"id":49},{"text":".","start":304,"end":305,"id":50}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":0,"end":9,"token_start":0,"token_end":0,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"lookup_tables|NOUN","word":"lookup tables","sense":"NOUN","meta":{"score":0.7833999991,"sense":"NOUN"},"_input_hash":74539311,"_task_hash":1996377615,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"lookup_tables|NOUN","start":0,"end":18,"id":0}]}
{"text":"This paper reviews the functional aspects of statistical learning theory.","_input_hash":-164428646,"_task_hash":-1393359911,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"paper","start":5,"end":10,"id":1},{"text":"reviews","start":11,"end":18,"id":2},{"text":"the","start":19,"end":22,"id":3},{"text":"functional","start":23,"end":33,"id":4},{"text":"aspects","start":34,"end":41,"id":5},{"text":"of","start":42,"end":44,"id":6},{"text":"statistical","start":45,"end":56,"id":7},{"text":"learning","start":57,"end":65,"id":8},{"text":"theory","start":66,"end":72,"id":9},{"text":".","start":72,"end":73,"id":10}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":45,"end":65,"token_start":7,"token_end":8,"label":"ALGO","answer":"reject"}],"answer":"reject"}
{"text":"The target field of application are CFD problems, where objective function are extremely expensive to evaluate, but the theory can be also used in other fields.","_input_hash":-863819772,"_task_hash":536865355,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"target","start":4,"end":10,"id":1},{"text":"field","start":11,"end":16,"id":2},{"text":"of","start":17,"end":19,"id":3},{"text":"application","start":20,"end":31,"id":4},{"text":"are","start":32,"end":35,"id":5},{"text":"CFD","start":36,"end":39,"id":6},{"text":"problems","start":40,"end":48,"id":7},{"text":",","start":48,"end":49,"id":8},{"text":"where","start":50,"end":55,"id":9},{"text":"objective","start":56,"end":65,"id":10},{"text":"function","start":66,"end":74,"id":11},{"text":"are","start":75,"end":78,"id":12},{"text":"extremely","start":79,"end":88,"id":13},{"text":"expensive","start":89,"end":98,"id":14},{"text":"to","start":99,"end":101,"id":15},{"text":"evaluate","start":102,"end":110,"id":16},{"text":",","start":110,"end":111,"id":17},{"text":"but","start":112,"end":115,"id":18},{"text":"the","start":116,"end":119,"id":19},{"text":"theory","start":120,"end":126,"id":20},{"text":"can","start":127,"end":130,"id":21},{"text":"be","start":131,"end":133,"id":22},{"text":"also","start":134,"end":138,"id":23},{"text":"used","start":139,"end":143,"id":24},{"text":"in","start":144,"end":146,"id":25},{"text":"other","start":147,"end":152,"id":26},{"text":"fields","start":153,"end":159,"id":27},{"text":".","start":159,"end":160,"id":28}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"In order to demonstrate the usefulness of divergences in our problem, the method with informational divergence as similarity measure is compared with the same method using classical metrics.","_input_hash":-1786147655,"_task_hash":-639912228,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"order","start":3,"end":8,"id":1},{"text":"to","start":9,"end":11,"id":2},{"text":"demonstrate","start":12,"end":23,"id":3},{"text":"the","start":24,"end":27,"id":4},{"text":"usefulness","start":28,"end":38,"id":5},{"text":"of","start":39,"end":41,"id":6},{"text":"divergences","start":42,"end":53,"id":7},{"text":"in","start":54,"end":56,"id":8},{"text":"our","start":57,"end":60,"id":9},{"text":"problem","start":61,"end":68,"id":10},{"text":",","start":68,"end":69,"id":11},{"text":"the","start":70,"end":73,"id":12},{"text":"method","start":74,"end":80,"id":13},{"text":"with","start":81,"end":85,"id":14},{"text":"informational","start":86,"end":99,"id":15},{"text":"divergence","start":100,"end":110,"id":16},{"text":"as","start":111,"end":113,"id":17},{"text":"similarity","start":114,"end":124,"id":18},{"text":"measure","start":125,"end":132,"id":19},{"text":"is","start":133,"end":135,"id":20},{"text":"compared","start":136,"end":144,"id":21},{"text":"with","start":145,"end":149,"id":22},{"text":"the","start":150,"end":153,"id":23},{"text":"same","start":154,"end":158,"id":24},{"text":"method","start":159,"end":165,"id":25},{"text":"using","start":166,"end":171,"id":26},{"text":"classical","start":172,"end":181,"id":27},{"text":"metrics","start":182,"end":189,"id":28},{"text":".","start":189,"end":190,"id":29}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"parallel_processing|NOUN","word":"parallel processing","sense":"NOUN","meta":{"score":0.7839999795,"sense":"NOUN"},"_input_hash":1736652507,"_task_hash":2055166339,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"parallel_processing|NOUN","start":0,"end":24,"id":0}]}
{"text":"We propose an extension of the concept of Expected Improvement criterion commonly used in Kriging based optimization.","_input_hash":1073976132,"_task_hash":-813542588,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"propose","start":3,"end":10,"id":1},{"text":"an","start":11,"end":13,"id":2},{"text":"extension","start":14,"end":23,"id":3},{"text":"of","start":24,"end":26,"id":4},{"text":"the","start":27,"end":30,"id":5},{"text":"concept","start":31,"end":38,"id":6},{"text":"of","start":39,"end":41,"id":7},{"text":"Expected","start":42,"end":50,"id":8},{"text":"Improvement","start":51,"end":62,"id":9},{"text":"criterion","start":63,"end":72,"id":10},{"text":"commonly","start":73,"end":81,"id":11},{"text":"used","start":82,"end":86,"id":12},{"text":"in","start":87,"end":89,"id":13},{"text":"Kriging","start":90,"end":97,"id":14},{"text":"based","start":98,"end":103,"id":15},{"text":"optimization","start":104,"end":116,"id":16},{"text":".","start":116,"end":117,"id":17}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"In a complex systems perspective, the decisional states are thus the \"emerging\" patterns corresponding to the utility function.","_input_hash":1018471510,"_task_hash":1819015503,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"a","start":3,"end":4,"id":1},{"text":"complex","start":5,"end":12,"id":2},{"text":"systems","start":13,"end":20,"id":3},{"text":"perspective","start":21,"end":32,"id":4},{"text":",","start":32,"end":33,"id":5},{"text":"the","start":34,"end":37,"id":6},{"text":"decisional","start":38,"end":48,"id":7},{"text":"states","start":49,"end":55,"id":8},{"text":"are","start":56,"end":59,"id":9},{"text":"thus","start":60,"end":64,"id":10},{"text":"the","start":65,"end":68,"id":11},{"text":"\"","start":69,"end":70,"id":12},{"text":"emerging","start":70,"end":78,"id":13},{"text":"\"","start":78,"end":79,"id":14},{"text":"patterns","start":80,"end":88,"id":15},{"text":"corresponding","start":89,"end":102,"id":16},{"text":"to","start":103,"end":105,"id":17},{"text":"the","start":106,"end":109,"id":18},{"text":"utility","start":110,"end":117,"id":19},{"text":"function","start":118,"end":126,"id":20},{"text":".","start":126,"end":127,"id":21}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"when","meta":{"score":0},"_input_hash":872985765,"_task_hash":-1852860873,"_session_id":null,"_view_id":"text","answer":"reject","spans":[],"tokens":[{"text":"when","start":0,"end":4,"id":0}]}
{"text":"It is however unclear if these heuristics can be derived from a more general principle facilitating generalization to new problem settings.","_input_hash":401064236,"_task_hash":1982098538,"tokens":[{"text":"It","start":0,"end":2,"id":0},{"text":"is","start":3,"end":5,"id":1},{"text":"however","start":6,"end":13,"id":2},{"text":"unclear","start":14,"end":21,"id":3},{"text":"if","start":22,"end":24,"id":4},{"text":"these","start":25,"end":30,"id":5},{"text":"heuristics","start":31,"end":41,"id":6},{"text":"can","start":42,"end":45,"id":7},{"text":"be","start":46,"end":48,"id":8},{"text":"derived","start":49,"end":56,"id":9},{"text":"from","start":57,"end":61,"id":10},{"text":"a","start":62,"end":63,"id":11},{"text":"more","start":64,"end":68,"id":12},{"text":"general","start":69,"end":76,"id":13},{"text":"principle","start":77,"end":86,"id":14},{"text":"facilitating","start":87,"end":99,"id":15},{"text":"generalization","start":100,"end":114,"id":16},{"text":"to","start":115,"end":117,"id":17},{"text":"new","start":118,"end":121,"id":18},{"text":"problem","start":122,"end":129,"id":19},{"text":"settings","start":130,"end":138,"id":20},{"text":".","start":138,"end":139,"id":21}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"we use the least amount of regularization that simultaneously makes a graph sparse and replicable under random sampling.","_input_hash":-1982708874,"_task_hash":751722846,"tokens":[{"text":"we","start":0,"end":2,"id":0},{"text":"use","start":3,"end":6,"id":1},{"text":"the","start":7,"end":10,"id":2},{"text":"least","start":11,"end":16,"id":3},{"text":"amount","start":17,"end":23,"id":4},{"text":"of","start":24,"end":26,"id":5},{"text":"regularization","start":27,"end":41,"id":6},{"text":"that","start":42,"end":46,"id":7},{"text":"simultaneously","start":47,"end":61,"id":8},{"text":"makes","start":62,"end":67,"id":9},{"text":"a","start":68,"end":69,"id":10},{"text":"graph","start":70,"end":75,"id":11},{"text":"sparse","start":76,"end":82,"id":12},{"text":"and","start":83,"end":86,"id":13},{"text":"replicable","start":87,"end":97,"id":14},{"text":"under","start":98,"end":103,"id":15},{"text":"random","start":104,"end":110,"id":16},{"text":"sampling","start":111,"end":119,"id":17},{"text":".","start":119,"end":120,"id":18}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"nuff","meta":{"score":0},"_input_hash":-490535719,"_task_hash":-396862726,"_session_id":null,"_view_id":"text","answer":"reject","spans":[],"tokens":[{"text":"nuff","start":0,"end":4,"id":0}]}
{"text":"Since learning is typically very slow in Boltzmann machines, there is a need to restrict connections within hidden layers.","_input_hash":1867951622,"_task_hash":344600058,"tokens":[{"text":"Since","start":0,"end":5,"id":0},{"text":"learning","start":6,"end":14,"id":1},{"text":"is","start":15,"end":17,"id":2},{"text":"typically","start":18,"end":27,"id":3},{"text":"very","start":28,"end":32,"id":4},{"text":"slow","start":33,"end":37,"id":5},{"text":"in","start":38,"end":40,"id":6},{"text":"Boltzmann","start":41,"end":50,"id":7},{"text":"machines","start":51,"end":59,"id":8},{"text":",","start":59,"end":60,"id":9},{"text":"there","start":61,"end":66,"id":10},{"text":"is","start":67,"end":69,"id":11},{"text":"a","start":70,"end":71,"id":12},{"text":"need","start":72,"end":76,"id":13},{"text":"to","start":77,"end":79,"id":14},{"text":"restrict","start":80,"end":88,"id":15},{"text":"connections","start":89,"end":100,"id":16},{"text":"within","start":101,"end":107,"id":17},{"text":"hidden","start":108,"end":114,"id":18},{"text":"layers","start":115,"end":121,"id":19},{"text":".","start":121,"end":122,"id":20}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":41,"end":59,"token_start":7,"token_end":8,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"matrix_multiplication|NOUN","word":"matrix multiplication","sense":"NOUN","meta":{"score":0.7892000079,"sense":"NOUN"},"_input_hash":1737348762,"_task_hash":110373905,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"matrix_multiplication|NOUN","start":0,"end":26,"id":0}]}
{"text":"In this example, the PVM outperforms the highly successful 1-NN with tangent distance, and does so retaining fewer than half of the data points.","_input_hash":702980345,"_task_hash":-432468476,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"this","start":3,"end":7,"id":1},{"text":"example","start":8,"end":15,"id":2},{"text":",","start":15,"end":16,"id":3},{"text":"the","start":17,"end":20,"id":4},{"text":"PVM","start":21,"end":24,"id":5},{"text":"outperforms","start":25,"end":36,"id":6},{"text":"the","start":37,"end":40,"id":7},{"text":"highly","start":41,"end":47,"id":8},{"text":"successful","start":48,"end":58,"id":9},{"text":"1-NN","start":59,"end":63,"id":10},{"text":"with","start":64,"end":68,"id":11},{"text":"tangent","start":69,"end":76,"id":12},{"text":"distance","start":77,"end":85,"id":13},{"text":",","start":85,"end":86,"id":14},{"text":"and","start":87,"end":90,"id":15},{"text":"does","start":91,"end":95,"id":16},{"text":"so","start":96,"end":98,"id":17},{"text":"retaining","start":99,"end":108,"id":18},{"text":"fewer","start":109,"end":114,"id":19},{"text":"than","start":115,"end":119,"id":20},{"text":"half","start":120,"end":124,"id":21},{"text":"of","start":125,"end":127,"id":22},{"text":"the","start":128,"end":131,"id":23},{"text":"data","start":132,"end":136,"id":24},{"text":"points","start":137,"end":143,"id":25},{"text":".","start":143,"end":144,"id":26}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Selecting important features in non-linear or kernel spaces is a difficult challenge in both classification and regression problems.","_input_hash":-365126464,"_task_hash":-231898137,"tokens":[{"text":"Selecting","start":0,"end":9,"id":0},{"text":"important","start":10,"end":19,"id":1},{"text":"features","start":20,"end":28,"id":2},{"text":"in","start":29,"end":31,"id":3},{"text":"non","start":32,"end":35,"id":4},{"text":"-","start":35,"end":36,"id":5},{"text":"linear","start":36,"end":42,"id":6},{"text":"or","start":43,"end":45,"id":7},{"text":"kernel","start":46,"end":52,"id":8},{"text":"spaces","start":53,"end":59,"id":9},{"text":"is","start":60,"end":62,"id":10},{"text":"a","start":63,"end":64,"id":11},{"text":"difficult","start":65,"end":74,"id":12},{"text":"challenge","start":75,"end":84,"id":13},{"text":"in","start":85,"end":87,"id":14},{"text":"both","start":88,"end":92,"id":15},{"text":"classification","start":93,"end":107,"id":16},{"text":"and","start":108,"end":111,"id":17},{"text":"regression","start":112,"end":122,"id":18},{"text":"problems","start":123,"end":131,"id":19},{"text":".","start":131,"end":132,"id":20}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"However, modern datasets including gene expression data need high-dimensional causal modeling in challenging situations with orders of magnitude more variables than observations (p>>n).","_input_hash":402902850,"_task_hash":-2104628399,"tokens":[{"text":"However","start":0,"end":7,"id":0},{"text":",","start":7,"end":8,"id":1},{"text":"modern","start":9,"end":15,"id":2},{"text":"datasets","start":16,"end":24,"id":3},{"text":"including","start":25,"end":34,"id":4},{"text":"gene","start":35,"end":39,"id":5},{"text":"expression","start":40,"end":50,"id":6},{"text":"data","start":51,"end":55,"id":7},{"text":"need","start":56,"end":60,"id":8},{"text":"high","start":61,"end":65,"id":9},{"text":"-","start":65,"end":66,"id":10},{"text":"dimensional","start":66,"end":77,"id":11},{"text":"causal","start":78,"end":84,"id":12},{"text":"modeling","start":85,"end":93,"id":13},{"text":"in","start":94,"end":96,"id":14},{"text":"challenging","start":97,"end":108,"id":15},{"text":"situations","start":109,"end":119,"id":16},{"text":"with","start":120,"end":124,"id":17},{"text":"orders","start":125,"end":131,"id":18},{"text":"of","start":132,"end":134,"id":19},{"text":"magnitude","start":135,"end":144,"id":20},{"text":"more","start":145,"end":149,"id":21},{"text":"variables","start":150,"end":159,"id":22},{"text":"than","start":160,"end":164,"id":23},{"text":"observations","start":165,"end":177,"id":24},{"text":"(","start":178,"end":179,"id":25},{"text":"p>>n","start":179,"end":183,"id":26},{"text":")","start":183,"end":184,"id":27},{"text":".","start":184,"end":185,"id":28}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":78,"end":93,"token_start":12,"token_end":13,"label":"ALGO","answer":"reject"}],"answer":"reject"}
{"text":"We carefully study the effects of different number of neighbors and weight schemes and report the results.","_input_hash":-307230380,"_task_hash":-1976115500,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"carefully","start":3,"end":12,"id":1},{"text":"study","start":13,"end":18,"id":2},{"text":"the","start":19,"end":22,"id":3},{"text":"effects","start":23,"end":30,"id":4},{"text":"of","start":31,"end":33,"id":5},{"text":"different","start":34,"end":43,"id":6},{"text":"number","start":44,"end":50,"id":7},{"text":"of","start":51,"end":53,"id":8},{"text":"neighbors","start":54,"end":63,"id":9},{"text":"and","start":64,"end":67,"id":10},{"text":"weight","start":68,"end":74,"id":11},{"text":"schemes","start":75,"end":82,"id":12},{"text":"and","start":83,"end":86,"id":13},{"text":"report","start":87,"end":93,"id":14},{"text":"the","start":94,"end":97,"id":15},{"text":"results","start":98,"end":105,"id":16},{"text":".","start":105,"end":106,"id":17}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"real_world_applications|NOUN","word":"real world applications","sense":"NOUN","meta":{"score":0.7928000093,"sense":"NOUN"},"_input_hash":-749044901,"_task_hash":-480192863,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"real_world_applications|NOUN","start":0,"end":28,"id":0}]}
{"text":"lambda_calculus|NOUN","word":"lambda calculus","sense":"NOUN","meta":{"score":0.7853999734,"sense":"NOUN"},"_input_hash":-1392574520,"_task_hash":-241209345,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"lambda_calculus|NOUN","start":0,"end":20,"id":0}]}
{"text":"The strategy involves a ranking of explanatory variables using the random forests score of importance and a stepwise ascending variable introduction strategy.","_input_hash":-1863389377,"_task_hash":874675346,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"strategy","start":4,"end":12,"id":1},{"text":"involves","start":13,"end":21,"id":2},{"text":"a","start":22,"end":23,"id":3},{"text":"ranking","start":24,"end":31,"id":4},{"text":"of","start":32,"end":34,"id":5},{"text":"explanatory","start":35,"end":46,"id":6},{"text":"variables","start":47,"end":56,"id":7},{"text":"using","start":57,"end":62,"id":8},{"text":"the","start":63,"end":66,"id":9},{"text":"random","start":67,"end":73,"id":10},{"text":"forests","start":74,"end":81,"id":11},{"text":"score","start":82,"end":87,"id":12},{"text":"of","start":88,"end":90,"id":13},{"text":"importance","start":91,"end":101,"id":14},{"text":"and","start":102,"end":105,"id":15},{"text":"a","start":106,"end":107,"id":16},{"text":"stepwise","start":108,"end":116,"id":17},{"text":"ascending","start":117,"end":126,"id":18},{"text":"variable","start":127,"end":135,"id":19},{"text":"introduction","start":136,"end":148,"id":20},{"text":"strategy","start":149,"end":157,"id":21},{"text":".","start":157,"end":158,"id":22}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":67,"end":81,"token_start":10,"token_end":11,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"In the astrophysics application, we also compare the method with the spectral clustering algorithms.","_input_hash":309712950,"_task_hash":-1588404325,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"the","start":3,"end":6,"id":1},{"text":"astrophysics","start":7,"end":19,"id":2},{"text":"application","start":20,"end":31,"id":3},{"text":",","start":31,"end":32,"id":4},{"text":"we","start":33,"end":35,"id":5},{"text":"also","start":36,"end":40,"id":6},{"text":"compare","start":41,"end":48,"id":7},{"text":"the","start":49,"end":52,"id":8},{"text":"method","start":53,"end":59,"id":9},{"text":"with","start":60,"end":64,"id":10},{"text":"the","start":65,"end":68,"id":11},{"text":"spectral","start":69,"end":77,"id":12},{"text":"clustering","start":78,"end":88,"id":13},{"text":"algorithms","start":89,"end":99,"id":14},{"text":".","start":99,"end":100,"id":15}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":69,"end":99,"token_start":12,"token_end":14,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Also, we propose clustering regularization restricting creation of additional clusters which are not significant or are not essentially different comparing with existing clusters.","_input_hash":-1797782799,"_task_hash":25283339,"tokens":[{"text":"Also","start":0,"end":4,"id":0},{"text":",","start":4,"end":5,"id":1},{"text":"we","start":6,"end":8,"id":2},{"text":"propose","start":9,"end":16,"id":3},{"text":"clustering","start":17,"end":27,"id":4},{"text":"regularization","start":28,"end":42,"id":5},{"text":"restricting","start":43,"end":54,"id":6},{"text":"creation","start":55,"end":63,"id":7},{"text":"of","start":64,"end":66,"id":8},{"text":"additional","start":67,"end":77,"id":9},{"text":"clusters","start":78,"end":86,"id":10},{"text":"which","start":87,"end":92,"id":11},{"text":"are","start":93,"end":96,"id":12},{"text":"not","start":97,"end":100,"id":13},{"text":"significant","start":101,"end":112,"id":14},{"text":"or","start":113,"end":115,"id":15},{"text":"are","start":116,"end":119,"id":16},{"text":"not","start":120,"end":123,"id":17},{"text":"essentially","start":124,"end":135,"id":18},{"text":"different","start":136,"end":145,"id":19},{"text":"comparing","start":146,"end":155,"id":20},{"text":"with","start":156,"end":160,"id":21},{"text":"existing","start":161,"end":169,"id":22},{"text":"clusters","start":170,"end":178,"id":23},{"text":".","start":178,"end":179,"id":24}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":17,"end":27,"token_start":4,"token_end":4,"label":"ALGO","answer":"reject"},{"start":78,"end":86,"token_start":10,"token_end":10,"label":"ALGO","answer":"reject"}],"answer":"reject"}
{"text":"We propose a novel algorithm for KPLS which not only computes (a) the fit, but also (b) its approximate degrees of freedom and (c) error bars in quadratic runtime.","_input_hash":-1985083073,"_task_hash":-376071550,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"propose","start":3,"end":10,"id":1},{"text":"a","start":11,"end":12,"id":2},{"text":"novel","start":13,"end":18,"id":3},{"text":"algorithm","start":19,"end":28,"id":4},{"text":"for","start":29,"end":32,"id":5},{"text":"KPLS","start":33,"end":37,"id":6},{"text":"which","start":38,"end":43,"id":7},{"text":"not","start":44,"end":47,"id":8},{"text":"only","start":48,"end":52,"id":9},{"text":"computes","start":53,"end":61,"id":10},{"text":"(","start":62,"end":63,"id":11},{"text":"a","start":63,"end":64,"id":12},{"text":")","start":64,"end":65,"id":13},{"text":"the","start":66,"end":69,"id":14},{"text":"fit","start":70,"end":73,"id":15},{"text":",","start":73,"end":74,"id":16},{"text":"but","start":75,"end":78,"id":17},{"text":"also","start":79,"end":83,"id":18},{"text":"(","start":84,"end":85,"id":19},{"text":"b","start":85,"end":86,"id":20},{"text":")","start":86,"end":87,"id":21},{"text":"its","start":88,"end":91,"id":22},{"text":"approximate","start":92,"end":103,"id":23},{"text":"degrees","start":104,"end":111,"id":24},{"text":"of","start":112,"end":114,"id":25},{"text":"freedom","start":115,"end":122,"id":26},{"text":"and","start":123,"end":126,"id":27},{"text":"(","start":127,"end":128,"id":28},{"text":"c","start":128,"end":129,"id":29},{"text":")","start":129,"end":130,"id":30},{"text":"error","start":131,"end":136,"id":31},{"text":"bars","start":137,"end":141,"id":32},{"text":"in","start":142,"end":144,"id":33},{"text":"quadratic","start":145,"end":154,"id":34},{"text":"runtime","start":155,"end":162,"id":35},{"text":".","start":162,"end":163,"id":36}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":13,"end":28,"token_start":3,"token_end":4,"label":"ALGO","answer":"reject"}],"answer":"reject"}
{"text":"computation|NOUN","word":"computation","sense":"NOUN","meta":{"score":0.815500021,"sense":"NOUN"},"_input_hash":-726706238,"_task_hash":1945593807,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"computation|NOUN","start":0,"end":16,"id":0}]}
{"text":"neural_nets|NOUN","word":"neural nets","sense":"NOUN","meta":{"score":0.807099998,"sense":"NOUN"},"_input_hash":-118475804,"_task_hash":1902131787,"_session_id":null,"_view_id":"html","answer":"accept","spans":[],"tokens":[{"text":"neural_nets|NOUN","start":0,"end":16,"id":0}]}
{"text":"The \"distance\" here is actually a kernel defining the similarity between two images.","_input_hash":-897828985,"_task_hash":-101705564,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"\"","start":4,"end":5,"id":1},{"text":"distance","start":5,"end":13,"id":2},{"text":"\"","start":13,"end":14,"id":3},{"text":"here","start":15,"end":19,"id":4},{"text":"is","start":20,"end":22,"id":5},{"text":"actually","start":23,"end":31,"id":6},{"text":"a","start":32,"end":33,"id":7},{"text":"kernel","start":34,"end":40,"id":8},{"text":"defining","start":41,"end":49,"id":9},{"text":"the","start":50,"end":53,"id":10},{"text":"similarity","start":54,"end":64,"id":11},{"text":"between","start":65,"end":72,"id":12},{"text":"two","start":73,"end":76,"id":13},{"text":"images","start":77,"end":83,"id":14},{"text":".","start":83,"end":84,"id":15}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Moreover, we present a convergent optimization algorithm for solving regularized least squares with these penalty functions.","_input_hash":1839975856,"_task_hash":-1690669992,"tokens":[{"text":"Moreover","start":0,"end":8,"id":0},{"text":",","start":8,"end":9,"id":1},{"text":"we","start":10,"end":12,"id":2},{"text":"present","start":13,"end":20,"id":3},{"text":"a","start":21,"end":22,"id":4},{"text":"convergent","start":23,"end":33,"id":5},{"text":"optimization","start":34,"end":46,"id":6},{"text":"algorithm","start":47,"end":56,"id":7},{"text":"for","start":57,"end":60,"id":8},{"text":"solving","start":61,"end":68,"id":9},{"text":"regularized","start":69,"end":80,"id":10},{"text":"least","start":81,"end":86,"id":11},{"text":"squares","start":87,"end":94,"id":12},{"text":"with","start":95,"end":99,"id":13},{"text":"these","start":100,"end":105,"id":14},{"text":"penalty","start":106,"end":113,"id":15},{"text":"functions","start":114,"end":123,"id":16},{"text":".","start":123,"end":124,"id":17}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"datasets|NOUN","word":"datasets","sense":"NOUN","meta":{"score":0.7763000131,"sense":"NOUN"},"_input_hash":-1935260918,"_task_hash":1373356229,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"datasets|NOUN","start":0,"end":13,"id":0}]}
{"text":"Our optimization technique efficiently incorporates the cutting-plane algorithm in order to obtain a tighter outer bound on the marginal polytope, which results in improvement of both parameter estimates and approximation to marginals.","_input_hash":150698891,"_task_hash":-2015493502,"tokens":[{"text":"Our","start":0,"end":3,"id":0},{"text":"optimization","start":4,"end":16,"id":1},{"text":"technique","start":17,"end":26,"id":2},{"text":"efficiently","start":27,"end":38,"id":3},{"text":"incorporates","start":39,"end":51,"id":4},{"text":"the","start":52,"end":55,"id":5},{"text":"cutting","start":56,"end":63,"id":6},{"text":"-","start":63,"end":64,"id":7},{"text":"plane","start":64,"end":69,"id":8},{"text":"algorithm","start":70,"end":79,"id":9},{"text":"in","start":80,"end":82,"id":10},{"text":"order","start":83,"end":88,"id":11},{"text":"to","start":89,"end":91,"id":12},{"text":"obtain","start":92,"end":98,"id":13},{"text":"a","start":99,"end":100,"id":14},{"text":"tighter","start":101,"end":108,"id":15},{"text":"outer","start":109,"end":114,"id":16},{"text":"bound","start":115,"end":120,"id":17},{"text":"on","start":121,"end":123,"id":18},{"text":"the","start":124,"end":127,"id":19},{"text":"marginal","start":128,"end":136,"id":20},{"text":"polytope","start":137,"end":145,"id":21},{"text":",","start":145,"end":146,"id":22},{"text":"which","start":147,"end":152,"id":23},{"text":"results","start":153,"end":160,"id":24},{"text":"in","start":161,"end":163,"id":25},{"text":"improvement","start":164,"end":175,"id":26},{"text":"of","start":176,"end":178,"id":27},{"text":"both","start":179,"end":183,"id":28},{"text":"parameter","start":184,"end":193,"id":29},{"text":"estimates","start":194,"end":203,"id":30},{"text":"and","start":204,"end":207,"id":31},{"text":"approximation","start":208,"end":221,"id":32},{"text":"to","start":222,"end":224,"id":33},{"text":"marginals","start":225,"end":234,"id":34},{"text":".","start":234,"end":235,"id":35}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":4,"end":26,"token_start":1,"token_end":2,"label":"ALGO","answer":"reject"}],"answer":"reject"}
{"text":"We evaluate DP-GLM on several data sets, comparing it to modern methods of nonparametric regression like CART, Bayesian trees and Gaussian processes.","_input_hash":-2094321264,"_task_hash":1374933634,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"evaluate","start":3,"end":11,"id":1},{"text":"DP","start":12,"end":14,"id":2},{"text":"-","start":14,"end":15,"id":3},{"text":"GLM","start":15,"end":18,"id":4},{"text":"on","start":19,"end":21,"id":5},{"text":"several","start":22,"end":29,"id":6},{"text":"data","start":30,"end":34,"id":7},{"text":"sets","start":35,"end":39,"id":8},{"text":",","start":39,"end":40,"id":9},{"text":"comparing","start":41,"end":50,"id":10},{"text":"it","start":51,"end":53,"id":11},{"text":"to","start":54,"end":56,"id":12},{"text":"modern","start":57,"end":63,"id":13},{"text":"methods","start":64,"end":71,"id":14},{"text":"of","start":72,"end":74,"id":15},{"text":"nonparametric","start":75,"end":88,"id":16},{"text":"regression","start":89,"end":99,"id":17},{"text":"like","start":100,"end":104,"id":18},{"text":"CART","start":105,"end":109,"id":19},{"text":",","start":109,"end":110,"id":20},{"text":"Bayesian","start":111,"end":119,"id":21},{"text":"trees","start":120,"end":125,"id":22},{"text":"and","start":126,"end":129,"id":23},{"text":"Gaussian","start":130,"end":138,"id":24},{"text":"processes","start":139,"end":148,"id":25},{"text":".","start":148,"end":149,"id":26}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":105,"end":109,"token_start":19,"token_end":19,"label":"ALGO","answer":"accept"},{"start":111,"end":125,"token_start":21,"token_end":22,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"y","meta":{"score":0},"_input_hash":2016919357,"_task_hash":1849090405,"_session_id":null,"_view_id":"text","answer":"reject","spans":[],"tokens":[{"text":"y","start":0,"end":1,"id":0}]}
{"text":"basic_building_blocks|NOUN","word":"basic building blocks","sense":"NOUN","meta":{"score":0.7598999739,"sense":"NOUN"},"_input_hash":1187527807,"_task_hash":-1634212821,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"basic_building_blocks|NOUN","start":0,"end":26,"id":0}]}
{"text":"We also give examples for when those conditions hold, including models for compactly supported continuous distributions and a model with continuous covariates and categorical response.","_input_hash":-98749212,"_task_hash":1798949911,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"also","start":3,"end":7,"id":1},{"text":"give","start":8,"end":12,"id":2},{"text":"examples","start":13,"end":21,"id":3},{"text":"for","start":22,"end":25,"id":4},{"text":"when","start":26,"end":30,"id":5},{"text":"those","start":31,"end":36,"id":6},{"text":"conditions","start":37,"end":47,"id":7},{"text":"hold","start":48,"end":52,"id":8},{"text":",","start":52,"end":53,"id":9},{"text":"including","start":54,"end":63,"id":10},{"text":"models","start":64,"end":70,"id":11},{"text":"for","start":71,"end":74,"id":12},{"text":"compactly","start":75,"end":84,"id":13},{"text":"supported","start":85,"end":94,"id":14},{"text":"continuous","start":95,"end":105,"id":15},{"text":"distributions","start":106,"end":119,"id":16},{"text":"and","start":120,"end":123,"id":17},{"text":"a","start":124,"end":125,"id":18},{"text":"model","start":126,"end":131,"id":19},{"text":"with","start":132,"end":136,"id":20},{"text":"continuous","start":137,"end":147,"id":21},{"text":"covariates","start":148,"end":158,"id":22},{"text":"and","start":159,"end":162,"id":23},{"text":"categorical","start":163,"end":174,"id":24},{"text":"response","start":175,"end":183,"id":25},{"text":".","start":183,"end":184,"id":26}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Slow feature analysis (SFA) is a method for extracting slowly varying driving forces from quickly varying nonstationary time series.","_input_hash":-1287211239,"_task_hash":1153447535,"tokens":[{"text":"Slow","start":0,"end":4,"id":0},{"text":"feature","start":5,"end":12,"id":1},{"text":"analysis","start":13,"end":21,"id":2},{"text":"(","start":22,"end":23,"id":3},{"text":"SFA","start":23,"end":26,"id":4},{"text":")","start":26,"end":27,"id":5},{"text":"is","start":28,"end":30,"id":6},{"text":"a","start":31,"end":32,"id":7},{"text":"method","start":33,"end":39,"id":8},{"text":"for","start":40,"end":43,"id":9},{"text":"extracting","start":44,"end":54,"id":10},{"text":"slowly","start":55,"end":61,"id":11},{"text":"varying","start":62,"end":69,"id":12},{"text":"driving","start":70,"end":77,"id":13},{"text":"forces","start":78,"end":84,"id":14},{"text":"from","start":85,"end":89,"id":15},{"text":"quickly","start":90,"end":97,"id":16},{"text":"varying","start":98,"end":105,"id":17},{"text":"nonstationary","start":106,"end":119,"id":18},{"text":"time","start":120,"end":124,"id":19},{"text":"series","start":125,"end":131,"id":20},{"text":".","start":131,"end":132,"id":21}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"complex_systems|NOUN","word":"complex systems","sense":"NOUN","meta":{"score":0.7547000051,"sense":"NOUN"},"_input_hash":542924411,"_task_hash":-735365160,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"complex_systems|NOUN","start":0,"end":20,"id":0}]}
{"text":"The method has a clear interpretation:","_input_hash":558527787,"_task_hash":697620002,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"method","start":4,"end":10,"id":1},{"text":"has","start":11,"end":14,"id":2},{"text":"a","start":15,"end":16,"id":3},{"text":"clear","start":17,"end":22,"id":4},{"text":"interpretation","start":23,"end":37,"id":5},{"text":":","start":37,"end":38,"id":6}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"complex_algorithms|NOUN","word":"complex algorithms","sense":"NOUN","meta":{"score":0.7595999837,"sense":"NOUN"},"_input_hash":452914458,"_task_hash":-1260359252,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"complex_algorithms|NOUN","start":0,"end":23,"id":0}]}
{"text":"It is shown that it depends on circumstances like the embedding dimension, the time series predictability, or the base frequency, whether the driving force itself or a slower subcomponent is detected.","_input_hash":-1705359368,"_task_hash":760229032,"tokens":[{"text":"It","start":0,"end":2,"id":0},{"text":"is","start":3,"end":5,"id":1},{"text":"shown","start":6,"end":11,"id":2},{"text":"that","start":12,"end":16,"id":3},{"text":"it","start":17,"end":19,"id":4},{"text":"depends","start":20,"end":27,"id":5},{"text":"on","start":28,"end":30,"id":6},{"text":"circumstances","start":31,"end":44,"id":7},{"text":"like","start":45,"end":49,"id":8},{"text":"the","start":50,"end":53,"id":9},{"text":"embedding","start":54,"end":63,"id":10},{"text":"dimension","start":64,"end":73,"id":11},{"text":",","start":73,"end":74,"id":12},{"text":"the","start":75,"end":78,"id":13},{"text":"time","start":79,"end":83,"id":14},{"text":"series","start":84,"end":90,"id":15},{"text":"predictability","start":91,"end":105,"id":16},{"text":",","start":105,"end":106,"id":17},{"text":"or","start":107,"end":109,"id":18},{"text":"the","start":110,"end":113,"id":19},{"text":"base","start":114,"end":118,"id":20},{"text":"frequency","start":119,"end":128,"id":21},{"text":",","start":128,"end":129,"id":22},{"text":"whether","start":130,"end":137,"id":23},{"text":"the","start":138,"end":141,"id":24},{"text":"driving","start":142,"end":149,"id":25},{"text":"force","start":150,"end":155,"id":26},{"text":"itself","start":156,"end":162,"id":27},{"text":"or","start":163,"end":165,"id":28},{"text":"a","start":166,"end":167,"id":29},{"text":"slower","start":168,"end":174,"id":30},{"text":"subcomponent","start":175,"end":187,"id":31},{"text":"is","start":188,"end":190,"id":32},{"text":"detected","start":191,"end":199,"id":33},{"text":".","start":199,"end":200,"id":34}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"The additional reduction in dimension when compared to bounds found in the literature, is at least $13\\%$, and, in some cases, up to $30\\%$ additional reduction is achieved.","_input_hash":-2144446175,"_task_hash":-232732974,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"additional","start":4,"end":14,"id":1},{"text":"reduction","start":15,"end":24,"id":2},{"text":"in","start":25,"end":27,"id":3},{"text":"dimension","start":28,"end":37,"id":4},{"text":"when","start":38,"end":42,"id":5},{"text":"compared","start":43,"end":51,"id":6},{"text":"to","start":52,"end":54,"id":7},{"text":"bounds","start":55,"end":61,"id":8},{"text":"found","start":62,"end":67,"id":9},{"text":"in","start":68,"end":70,"id":10},{"text":"the","start":71,"end":74,"id":11},{"text":"literature","start":75,"end":85,"id":12},{"text":",","start":85,"end":86,"id":13},{"text":"is","start":87,"end":89,"id":14},{"text":"at","start":90,"end":92,"id":15},{"text":"least","start":93,"end":98,"id":16},{"text":"$","start":99,"end":100,"id":17},{"text":"13\\%$","start":100,"end":105,"id":18},{"text":",","start":105,"end":106,"id":19},{"text":"and","start":107,"end":110,"id":20},{"text":",","start":110,"end":111,"id":21},{"text":"in","start":112,"end":114,"id":22},{"text":"some","start":115,"end":119,"id":23},{"text":"cases","start":120,"end":125,"id":24},{"text":",","start":125,"end":126,"id":25},{"text":"up","start":127,"end":129,"id":26},{"text":"to","start":130,"end":132,"id":27},{"text":"$","start":133,"end":134,"id":28},{"text":"30\\%$","start":134,"end":139,"id":29},{"text":"additional","start":140,"end":150,"id":30},{"text":"reduction","start":151,"end":160,"id":31},{"text":"is","start":161,"end":163,"id":32},{"text":"achieved","start":164,"end":172,"id":33},{"text":".","start":172,"end":173,"id":34}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"In this paper, we study the problem of embedding arbitrary metric spaces into a Euclidean space with the goal to improve the accuracy of the NN classifier.","_input_hash":1951559990,"_task_hash":89431343,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"this","start":3,"end":7,"id":1},{"text":"paper","start":8,"end":13,"id":2},{"text":",","start":13,"end":14,"id":3},{"text":"we","start":15,"end":17,"id":4},{"text":"study","start":18,"end":23,"id":5},{"text":"the","start":24,"end":27,"id":6},{"text":"problem","start":28,"end":35,"id":7},{"text":"of","start":36,"end":38,"id":8},{"text":"embedding","start":39,"end":48,"id":9},{"text":"arbitrary","start":49,"end":58,"id":10},{"text":"metric","start":59,"end":65,"id":11},{"text":"spaces","start":66,"end":72,"id":12},{"text":"into","start":73,"end":77,"id":13},{"text":"a","start":78,"end":79,"id":14},{"text":"Euclidean","start":80,"end":89,"id":15},{"text":"space","start":90,"end":95,"id":16},{"text":"with","start":96,"end":100,"id":17},{"text":"the","start":101,"end":104,"id":18},{"text":"goal","start":105,"end":109,"id":19},{"text":"to","start":110,"end":112,"id":20},{"text":"improve","start":113,"end":120,"id":21},{"text":"the","start":121,"end":124,"id":22},{"text":"accuracy","start":125,"end":133,"id":23},{"text":"of","start":134,"end":136,"id":24},{"text":"the","start":137,"end":140,"id":25},{"text":"NN","start":141,"end":143,"id":26},{"text":"classifier","start":144,"end":154,"id":27},{"text":".","start":154,"end":155,"id":28}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We study both the simultaneous recovery of all K underlying subspaces and the recovery of the best l0 subspace (i.e., with largest number of points) by minimizing the lp-averaged distances of data points from d-dimensional subspaces of the D-dimensional space.","_input_hash":-709730746,"_task_hash":1942975112,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"study","start":3,"end":8,"id":1},{"text":"both","start":9,"end":13,"id":2},{"text":"the","start":14,"end":17,"id":3},{"text":"simultaneous","start":18,"end":30,"id":4},{"text":"recovery","start":31,"end":39,"id":5},{"text":"of","start":40,"end":42,"id":6},{"text":"all","start":43,"end":46,"id":7},{"text":"K","start":47,"end":48,"id":8},{"text":"underlying","start":49,"end":59,"id":9},{"text":"subspaces","start":60,"end":69,"id":10},{"text":"and","start":70,"end":73,"id":11},{"text":"the","start":74,"end":77,"id":12},{"text":"recovery","start":78,"end":86,"id":13},{"text":"of","start":87,"end":89,"id":14},{"text":"the","start":90,"end":93,"id":15},{"text":"best","start":94,"end":98,"id":16},{"text":"l0","start":99,"end":101,"id":17},{"text":"subspace","start":102,"end":110,"id":18},{"text":"(","start":111,"end":112,"id":19},{"text":"i.e.","start":112,"end":116,"id":20},{"text":",","start":116,"end":117,"id":21},{"text":"with","start":118,"end":122,"id":22},{"text":"largest","start":123,"end":130,"id":23},{"text":"number","start":131,"end":137,"id":24},{"text":"of","start":138,"end":140,"id":25},{"text":"points","start":141,"end":147,"id":26},{"text":")","start":147,"end":148,"id":27},{"text":"by","start":149,"end":151,"id":28},{"text":"minimizing","start":152,"end":162,"id":29},{"text":"the","start":163,"end":166,"id":30},{"text":"lp","start":167,"end":169,"id":31},{"text":"-","start":169,"end":170,"id":32},{"text":"averaged","start":170,"end":178,"id":33},{"text":"distances","start":179,"end":188,"id":34},{"text":"of","start":189,"end":191,"id":35},{"text":"data","start":192,"end":196,"id":36},{"text":"points","start":197,"end":203,"id":37},{"text":"from","start":204,"end":208,"id":38},{"text":"d","start":209,"end":210,"id":39},{"text":"-","start":210,"end":211,"id":40},{"text":"dimensional","start":211,"end":222,"id":41},{"text":"subspaces","start":223,"end":232,"id":42},{"text":"of","start":233,"end":235,"id":43},{"text":"the","start":236,"end":239,"id":44},{"text":"D","start":240,"end":241,"id":45},{"text":"-","start":241,"end":242,"id":46},{"text":"dimensional","start":242,"end":253,"id":47},{"text":"space","start":254,"end":259,"id":48},{"text":".","start":259,"end":260,"id":49}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"Nonetheless, further novel functions are being proposed in literature.","_input_hash":-542452039,"_task_hash":-276762358,"tokens":[{"text":"Nonetheless","start":0,"end":11,"id":0},{"text":",","start":11,"end":12,"id":1},{"text":"further","start":13,"end":20,"id":2},{"text":"novel","start":21,"end":26,"id":3},{"text":"functions","start":27,"end":36,"id":4},{"text":"are","start":37,"end":40,"id":5},{"text":"being","start":41,"end":46,"id":6},{"text":"proposed","start":47,"end":55,"id":7},{"text":"in","start":56,"end":58,"id":8},{"text":"literature","start":59,"end":69,"id":9},{"text":".","start":69,"end":70,"id":10}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"These methods successfully drive the optimization of sampling trajectories for real-world magnetic resonance imaging through Bayesian experimental design, which has not been attempted before.","_input_hash":-1437027727,"_task_hash":884314657,"tokens":[{"text":"These","start":0,"end":5,"id":0},{"text":"methods","start":6,"end":13,"id":1},{"text":"successfully","start":14,"end":26,"id":2},{"text":"drive","start":27,"end":32,"id":3},{"text":"the","start":33,"end":36,"id":4},{"text":"optimization","start":37,"end":49,"id":5},{"text":"of","start":50,"end":52,"id":6},{"text":"sampling","start":53,"end":61,"id":7},{"text":"trajectories","start":62,"end":74,"id":8},{"text":"for","start":75,"end":78,"id":9},{"text":"real","start":79,"end":83,"id":10},{"text":"-","start":83,"end":84,"id":11},{"text":"world","start":84,"end":89,"id":12},{"text":"magnetic","start":90,"end":98,"id":13},{"text":"resonance","start":99,"end":108,"id":14},{"text":"imaging","start":109,"end":116,"id":15},{"text":"through","start":117,"end":124,"id":16},{"text":"Bayesian","start":125,"end":133,"id":17},{"text":"experimental","start":134,"end":146,"id":18},{"text":"design","start":147,"end":153,"id":19},{"text":",","start":153,"end":154,"id":20},{"text":"which","start":155,"end":160,"id":21},{"text":"has","start":161,"end":164,"id":22},{"text":"not","start":165,"end":168,"id":23},{"text":"been","start":169,"end":173,"id":24},{"text":"attempted","start":174,"end":183,"id":25},{"text":"before","start":184,"end":190,"id":26},{"text":".","start":190,"end":191,"id":27}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"ai","meta":{"score":0},"_input_hash":-1543822807,"_task_hash":1237340548,"_session_id":null,"_view_id":"text","answer":"reject","spans":[],"tokens":[{"text":"ai","start":0,"end":2,"id":0}]}
{"text":"nuthin","meta":{"score":0},"_input_hash":-8249646,"_task_hash":961687283,"_session_id":null,"_view_id":"text","answer":"reject","spans":[],"tokens":[{"text":"nuthin","start":0,"end":6,"id":0}]}
{"text":"However, a fast online approximation is possible.","_input_hash":-1349805728,"_task_hash":2002389883,"tokens":[{"text":"However","start":0,"end":7,"id":0},{"text":",","start":7,"end":8,"id":1},{"text":"a","start":9,"end":10,"id":2},{"text":"fast","start":11,"end":15,"id":3},{"text":"online","start":16,"end":22,"id":4},{"text":"approximation","start":23,"end":36,"id":5},{"text":"is","start":37,"end":39,"id":6},{"text":"possible","start":40,"end":48,"id":7},{"text":".","start":48,"end":49,"id":8}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"The combinatorial problem of selecting the nonzero components of this vector can be \"relaxed\" by regularizing the squared error with a convex penalty function like the $\\ell_1$ norm.","_input_hash":1149404712,"_task_hash":433008329,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"combinatorial","start":4,"end":17,"id":1},{"text":"problem","start":18,"end":25,"id":2},{"text":"of","start":26,"end":28,"id":3},{"text":"selecting","start":29,"end":38,"id":4},{"text":"the","start":39,"end":42,"id":5},{"text":"nonzero","start":43,"end":50,"id":6},{"text":"components","start":51,"end":61,"id":7},{"text":"of","start":62,"end":64,"id":8},{"text":"this","start":65,"end":69,"id":9},{"text":"vector","start":70,"end":76,"id":10},{"text":"can","start":77,"end":80,"id":11},{"text":"be","start":81,"end":83,"id":12},{"text":"\"","start":84,"end":85,"id":13},{"text":"relaxed","start":85,"end":92,"id":14},{"text":"\"","start":92,"end":93,"id":15},{"text":"by","start":94,"end":96,"id":16},{"text":"regularizing","start":97,"end":109,"id":17},{"text":"the","start":110,"end":113,"id":18},{"text":"squared","start":114,"end":121,"id":19},{"text":"error","start":122,"end":127,"id":20},{"text":"with","start":128,"end":132,"id":21},{"text":"a","start":133,"end":134,"id":22},{"text":"convex","start":135,"end":141,"id":23},{"text":"penalty","start":142,"end":149,"id":24},{"text":"function","start":150,"end":158,"id":25},{"text":"like","start":159,"end":163,"id":26},{"text":"the","start":164,"end":167,"id":27},{"text":"$","start":168,"end":169,"id":28},{"text":"\\ell_1","start":169,"end":175,"id":29},{"text":"$","start":175,"end":176,"id":30},{"text":"norm","start":177,"end":181,"id":31},{"text":".","start":181,"end":182,"id":32}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"biological_brain|NOUN","word":"biological brain","sense":"NOUN","meta":{"score":0.7725999951,"sense":"NOUN"},"_input_hash":739171017,"_task_hash":729194067,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"biological_brain|NOUN","start":0,"end":21,"id":0}]}
{"text":"We study the problem of allocating stocks to dark pools.","_input_hash":8280324,"_task_hash":-1197115459,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"study","start":3,"end":8,"id":1},{"text":"the","start":9,"end":12,"id":2},{"text":"problem","start":13,"end":20,"id":3},{"text":"of","start":21,"end":23,"id":4},{"text":"allocating","start":24,"end":34,"id":5},{"text":"stocks","start":35,"end":41,"id":6},{"text":"to","start":42,"end":44,"id":7},{"text":"dark","start":45,"end":49,"id":8},{"text":"pools","start":50,"end":55,"id":9},{"text":".","start":55,"end":56,"id":10}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"However, the resulting states of hidden units exhibit statistical dependencies.","_input_hash":256103734,"_task_hash":358219503,"tokens":[{"text":"However","start":0,"end":7,"id":0},{"text":",","start":7,"end":8,"id":1},{"text":"the","start":9,"end":12,"id":2},{"text":"resulting","start":13,"end":22,"id":3},{"text":"states","start":23,"end":29,"id":4},{"text":"of","start":30,"end":32,"id":5},{"text":"hidden","start":33,"end":39,"id":6},{"text":"units","start":40,"end":45,"id":7},{"text":"exhibit","start":46,"end":53,"id":8},{"text":"statistical","start":54,"end":65,"id":9},{"text":"dependencies","start":66,"end":78,"id":10},{"text":".","start":78,"end":79,"id":11}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Min-cut clustering, based on minimizing one of two heuristic cost-functions proposed by Shi and Malik, has spawned tremendous research, both analytic and algorithmic, in the graph partitioning and image segmentation communities over the last decade.","_input_hash":-2126837843,"_task_hash":-888286132,"tokens":[{"text":"Min","start":0,"end":3,"id":0},{"text":"-","start":3,"end":4,"id":1},{"text":"cut","start":4,"end":7,"id":2},{"text":"clustering","start":8,"end":18,"id":3},{"text":",","start":18,"end":19,"id":4},{"text":"based","start":20,"end":25,"id":5},{"text":"on","start":26,"end":28,"id":6},{"text":"minimizing","start":29,"end":39,"id":7},{"text":"one","start":40,"end":43,"id":8},{"text":"of","start":44,"end":46,"id":9},{"text":"two","start":47,"end":50,"id":10},{"text":"heuristic","start":51,"end":60,"id":11},{"text":"cost","start":61,"end":65,"id":12},{"text":"-","start":65,"end":66,"id":13},{"text":"functions","start":66,"end":75,"id":14},{"text":"proposed","start":76,"end":84,"id":15},{"text":"by","start":85,"end":87,"id":16},{"text":"Shi","start":88,"end":91,"id":17},{"text":"and","start":92,"end":95,"id":18},{"text":"Malik","start":96,"end":101,"id":19},{"text":",","start":101,"end":102,"id":20},{"text":"has","start":103,"end":106,"id":21},{"text":"spawned","start":107,"end":114,"id":22},{"text":"tremendous","start":115,"end":125,"id":23},{"text":"research","start":126,"end":134,"id":24},{"text":",","start":134,"end":135,"id":25},{"text":"both","start":136,"end":140,"id":26},{"text":"analytic","start":141,"end":149,"id":27},{"text":"and","start":150,"end":153,"id":28},{"text":"algorithmic","start":154,"end":165,"id":29},{"text":",","start":165,"end":166,"id":30},{"text":"in","start":167,"end":169,"id":31},{"text":"the","start":170,"end":173,"id":32},{"text":"graph","start":174,"end":179,"id":33},{"text":"partitioning","start":180,"end":192,"id":34},{"text":"and","start":193,"end":196,"id":35},{"text":"image","start":197,"end":202,"id":36},{"text":"segmentation","start":203,"end":215,"id":37},{"text":"communities","start":216,"end":227,"id":38},{"text":"over","start":228,"end":232,"id":39},{"text":"the","start":233,"end":236,"id":40},{"text":"last","start":237,"end":241,"id":41},{"text":"decade","start":242,"end":248,"id":42},{"text":".","start":248,"end":249,"id":43}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":0,"end":18,"token_start":0,"token_end":3,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Some experiments show advantages and shortcomings of the resulting regression method in comparison to the standard Nadaraya-Watson regression technique, which approximates the optimum by the expectation value.","_input_hash":730716699,"_task_hash":407675369,"tokens":[{"text":"Some","start":0,"end":4,"id":0},{"text":"experiments","start":5,"end":16,"id":1},{"text":"show","start":17,"end":21,"id":2},{"text":"advantages","start":22,"end":32,"id":3},{"text":"and","start":33,"end":36,"id":4},{"text":"shortcomings","start":37,"end":49,"id":5},{"text":"of","start":50,"end":52,"id":6},{"text":"the","start":53,"end":56,"id":7},{"text":"resulting","start":57,"end":66,"id":8},{"text":"regression","start":67,"end":77,"id":9},{"text":"method","start":78,"end":84,"id":10},{"text":"in","start":85,"end":87,"id":11},{"text":"comparison","start":88,"end":98,"id":12},{"text":"to","start":99,"end":101,"id":13},{"text":"the","start":102,"end":105,"id":14},{"text":"standard","start":106,"end":114,"id":15},{"text":"Nadaraya","start":115,"end":123,"id":16},{"text":"-","start":123,"end":124,"id":17},{"text":"Watson","start":124,"end":130,"id":18},{"text":"regression","start":131,"end":141,"id":19},{"text":"technique","start":142,"end":151,"id":20},{"text":",","start":151,"end":152,"id":21},{"text":"which","start":153,"end":158,"id":22},{"text":"approximates","start":159,"end":171,"id":23},{"text":"the","start":172,"end":175,"id":24},{"text":"optimum","start":176,"end":183,"id":25},{"text":"by","start":184,"end":186,"id":26},{"text":"the","start":187,"end":190,"id":27},{"text":"expectation","start":191,"end":202,"id":28},{"text":"value","start":203,"end":208,"id":29},{"text":".","start":208,"end":209,"id":30}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":57,"end":77,"token_start":8,"token_end":9,"label":"ALGO","answer":"reject"}],"answer":"reject"}
{"text":"various_algorithms|NOUN","word":"various algorithms","sense":"NOUN","meta":{"score":0.7899000049,"sense":"NOUN"},"_input_hash":995624589,"_task_hash":-3630006,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"various_algorithms|NOUN","start":0,"end":23,"id":0}]}
{"text":"Our implementation is highly modular so that the algorithm may be applied to a variety of types of data.","_input_hash":-1056800728,"_task_hash":1810515734,"tokens":[{"text":"Our","start":0,"end":3,"id":0},{"text":"implementation","start":4,"end":18,"id":1},{"text":"is","start":19,"end":21,"id":2},{"text":"highly","start":22,"end":28,"id":3},{"text":"modular","start":29,"end":36,"id":4},{"text":"so","start":37,"end":39,"id":5},{"text":"that","start":40,"end":44,"id":6},{"text":"the","start":45,"end":48,"id":7},{"text":"algorithm","start":49,"end":58,"id":8},{"text":"may","start":59,"end":62,"id":9},{"text":"be","start":63,"end":65,"id":10},{"text":"applied","start":66,"end":73,"id":11},{"text":"to","start":74,"end":76,"id":12},{"text":"a","start":77,"end":78,"id":13},{"text":"variety","start":79,"end":86,"id":14},{"text":"of","start":87,"end":89,"id":15},{"text":"types","start":90,"end":95,"id":16},{"text":"of","start":96,"end":98,"id":17},{"text":"data","start":99,"end":103,"id":18},{"text":".","start":103,"end":104,"id":19}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":45,"end":58,"token_start":7,"token_end":8,"label":"ALGO","answer":"reject"}],"answer":"reject"}
{"text":"We present a new online boosting algorithm for adapting the weights of a boosted classifier, which yields a closer approximation to Freund and Schapire's AdaBoost algorithm than previous online boosting algorithms.","_input_hash":199535872,"_task_hash":-1707277270,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"present","start":3,"end":10,"id":1},{"text":"a","start":11,"end":12,"id":2},{"text":"new","start":13,"end":16,"id":3},{"text":"online","start":17,"end":23,"id":4},{"text":"boosting","start":24,"end":32,"id":5},{"text":"algorithm","start":33,"end":42,"id":6},{"text":"for","start":43,"end":46,"id":7},{"text":"adapting","start":47,"end":55,"id":8},{"text":"the","start":56,"end":59,"id":9},{"text":"weights","start":60,"end":67,"id":10},{"text":"of","start":68,"end":70,"id":11},{"text":"a","start":71,"end":72,"id":12},{"text":"boosted","start":73,"end":80,"id":13},{"text":"classifier","start":81,"end":91,"id":14},{"text":",","start":91,"end":92,"id":15},{"text":"which","start":93,"end":98,"id":16},{"text":"yields","start":99,"end":105,"id":17},{"text":"a","start":106,"end":107,"id":18},{"text":"closer","start":108,"end":114,"id":19},{"text":"approximation","start":115,"end":128,"id":20},{"text":"to","start":129,"end":131,"id":21},{"text":"Freund","start":132,"end":138,"id":22},{"text":"and","start":139,"end":142,"id":23},{"text":"Schapire","start":143,"end":151,"id":24},{"text":"'s","start":151,"end":153,"id":25},{"text":"AdaBoost","start":154,"end":162,"id":26},{"text":"algorithm","start":163,"end":172,"id":27},{"text":"than","start":173,"end":177,"id":28},{"text":"previous","start":178,"end":186,"id":29},{"text":"online","start":187,"end":193,"id":30},{"text":"boosting","start":194,"end":202,"id":31},{"text":"algorithms","start":203,"end":213,"id":32},{"text":".","start":213,"end":214,"id":33}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":17,"end":32,"token_start":4,"token_end":5,"label":"ALGO","answer":"accept"},{"start":154,"end":162,"token_start":26,"token_end":26,"label":"ALGO","answer":"accept"},{"start":187,"end":202,"token_start":30,"token_end":31,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We propose a solution by appealing to the framework of regularization in a reproducing kernel Hilbert space and prove a representer-like theorem for NN classification.","_input_hash":88466947,"_task_hash":1176373988,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"propose","start":3,"end":10,"id":1},{"text":"a","start":11,"end":12,"id":2},{"text":"solution","start":13,"end":21,"id":3},{"text":"by","start":22,"end":24,"id":4},{"text":"appealing","start":25,"end":34,"id":5},{"text":"to","start":35,"end":37,"id":6},{"text":"the","start":38,"end":41,"id":7},{"text":"framework","start":42,"end":51,"id":8},{"text":"of","start":52,"end":54,"id":9},{"text":"regularization","start":55,"end":69,"id":10},{"text":"in","start":70,"end":72,"id":11},{"text":"a","start":73,"end":74,"id":12},{"text":"reproducing","start":75,"end":86,"id":13},{"text":"kernel","start":87,"end":93,"id":14},{"text":"Hilbert","start":94,"end":101,"id":15},{"text":"space","start":102,"end":107,"id":16},{"text":"and","start":108,"end":111,"id":17},{"text":"prove","start":112,"end":117,"id":18},{"text":"a","start":118,"end":119,"id":19},{"text":"representer","start":120,"end":131,"id":20},{"text":"-","start":131,"end":132,"id":21},{"text":"like","start":132,"end":136,"id":22},{"text":"theorem","start":137,"end":144,"id":23},{"text":"for","start":145,"end":148,"id":24},{"text":"NN","start":149,"end":151,"id":25},{"text":"classification","start":152,"end":166,"id":26},{"text":".","start":166,"end":167,"id":27}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"do","meta":{"score":0},"_input_hash":1823551004,"_task_hash":-917790804,"_session_id":null,"_view_id":"text","answer":"reject","spans":[],"tokens":[{"text":"do","start":0,"end":2,"id":0}]}
{"text":"In this paper, we present a family of convex penalty functions, which encode prior knowledge on the structure of the vector formed by the absolute values of the regression coefficients.","_input_hash":2074119255,"_task_hash":-394132537,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"this","start":3,"end":7,"id":1},{"text":"paper","start":8,"end":13,"id":2},{"text":",","start":13,"end":14,"id":3},{"text":"we","start":15,"end":17,"id":4},{"text":"present","start":18,"end":25,"id":5},{"text":"a","start":26,"end":27,"id":6},{"text":"family","start":28,"end":34,"id":7},{"text":"of","start":35,"end":37,"id":8},{"text":"convex","start":38,"end":44,"id":9},{"text":"penalty","start":45,"end":52,"id":10},{"text":"functions","start":53,"end":62,"id":11},{"text":",","start":62,"end":63,"id":12},{"text":"which","start":64,"end":69,"id":13},{"text":"encode","start":70,"end":76,"id":14},{"text":"prior","start":77,"end":82,"id":15},{"text":"knowledge","start":83,"end":92,"id":16},{"text":"on","start":93,"end":95,"id":17},{"text":"the","start":96,"end":99,"id":18},{"text":"structure","start":100,"end":109,"id":19},{"text":"of","start":110,"end":112,"id":20},{"text":"the","start":113,"end":116,"id":21},{"text":"vector","start":117,"end":123,"id":22},{"text":"formed","start":124,"end":130,"id":23},{"text":"by","start":131,"end":133,"id":24},{"text":"the","start":134,"end":137,"id":25},{"text":"absolute","start":138,"end":146,"id":26},{"text":"values","start":147,"end":153,"id":27},{"text":"of","start":154,"end":156,"id":28},{"text":"the","start":157,"end":160,"id":29},{"text":"regression","start":161,"end":171,"id":30},{"text":"coefficients","start":172,"end":184,"id":31},{"text":".","start":184,"end":185,"id":32}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"The algorithm is robust to local extrema and shows a very high convergence speed in terms of the computational cost required to reach a given source extraction quality, particularly for short data records.","_input_hash":1984769510,"_task_hash":-1720541801,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"algorithm","start":4,"end":13,"id":1},{"text":"is","start":14,"end":16,"id":2},{"text":"robust","start":17,"end":23,"id":3},{"text":"to","start":24,"end":26,"id":4},{"text":"local","start":27,"end":32,"id":5},{"text":"extrema","start":33,"end":40,"id":6},{"text":"and","start":41,"end":44,"id":7},{"text":"shows","start":45,"end":50,"id":8},{"text":"a","start":51,"end":52,"id":9},{"text":"very","start":53,"end":57,"id":10},{"text":"high","start":58,"end":62,"id":11},{"text":"convergence","start":63,"end":74,"id":12},{"text":"speed","start":75,"end":80,"id":13},{"text":"in","start":81,"end":83,"id":14},{"text":"terms","start":84,"end":89,"id":15},{"text":"of","start":90,"end":92,"id":16},{"text":"the","start":93,"end":96,"id":17},{"text":"computational","start":97,"end":110,"id":18},{"text":"cost","start":111,"end":115,"id":19},{"text":"required","start":116,"end":124,"id":20},{"text":"to","start":125,"end":127,"id":21},{"text":"reach","start":128,"end":133,"id":22},{"text":"a","start":134,"end":135,"id":23},{"text":"given","start":136,"end":141,"id":24},{"text":"source","start":142,"end":148,"id":25},{"text":"extraction","start":149,"end":159,"id":26},{"text":"quality","start":160,"end":167,"id":27},{"text":",","start":167,"end":168,"id":28},{"text":"particularly","start":169,"end":181,"id":29},{"text":"for","start":182,"end":185,"id":30},{"text":"short","start":186,"end":191,"id":31},{"text":"data","start":192,"end":196,"id":32},{"text":"records","start":197,"end":204,"id":33},{"text":".","start":204,"end":205,"id":34}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Under a latent function interpretation of the convolution transform we establish dependencies between output variables.","_input_hash":26082937,"_task_hash":1240780086,"tokens":[{"text":"Under","start":0,"end":5,"id":0},{"text":"a","start":6,"end":7,"id":1},{"text":"latent","start":8,"end":14,"id":2},{"text":"function","start":15,"end":23,"id":3},{"text":"interpretation","start":24,"end":38,"id":4},{"text":"of","start":39,"end":41,"id":5},{"text":"the","start":42,"end":45,"id":6},{"text":"convolution","start":46,"end":57,"id":7},{"text":"transform","start":58,"end":67,"id":8},{"text":"we","start":68,"end":70,"id":9},{"text":"establish","start":71,"end":80,"id":10},{"text":"dependencies","start":81,"end":93,"id":11},{"text":"between","start":94,"end":101,"id":12},{"text":"output","start":102,"end":108,"id":13},{"text":"variables","start":109,"end":118,"id":14},{"text":".","start":118,"end":119,"id":15}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"The absence of prewhitening improves asymptotic performance.","_input_hash":-1575921533,"_task_hash":-838305515,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"absence","start":4,"end":11,"id":1},{"text":"of","start":12,"end":14,"id":2},{"text":"prewhitening","start":15,"end":27,"id":3},{"text":"improves","start":28,"end":36,"id":4},{"text":"asymptotic","start":37,"end":47,"id":5},{"text":"performance","start":48,"end":59,"id":6},{"text":".","start":59,"end":60,"id":7}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"This example highlights the strengths of the PVM in yielding a low-error, highly interpretable model.","_input_hash":5530812,"_task_hash":-503068660,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"example","start":5,"end":12,"id":1},{"text":"highlights","start":13,"end":23,"id":2},{"text":"the","start":24,"end":27,"id":3},{"text":"strengths","start":28,"end":37,"id":4},{"text":"of","start":38,"end":40,"id":5},{"text":"the","start":41,"end":44,"id":6},{"text":"PVM","start":45,"end":48,"id":7},{"text":"in","start":49,"end":51,"id":8},{"text":"yielding","start":52,"end":60,"id":9},{"text":"a","start":61,"end":62,"id":10},{"text":"low","start":63,"end":66,"id":11},{"text":"-","start":66,"end":67,"id":12},{"text":"error","start":67,"end":72,"id":13},{"text":",","start":72,"end":73,"id":14},{"text":"highly","start":74,"end":80,"id":15},{"text":"interpretable","start":81,"end":94,"id":16},{"text":"model","start":95,"end":100,"id":17},{"text":".","start":100,"end":101,"id":18}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Many problems of low-level computer vision and image processing, such as denoising, deconvolution, tomographic reconstruction or super-resolution, can be addressed by maximizing the posterior distribution of a sparse linear model (SLM).","_input_hash":1701962311,"_task_hash":-537241462,"tokens":[{"text":"Many","start":0,"end":4,"id":0},{"text":"problems","start":5,"end":13,"id":1},{"text":"of","start":14,"end":16,"id":2},{"text":"low","start":17,"end":20,"id":3},{"text":"-","start":20,"end":21,"id":4},{"text":"level","start":21,"end":26,"id":5},{"text":"computer","start":27,"end":35,"id":6},{"text":"vision","start":36,"end":42,"id":7},{"text":"and","start":43,"end":46,"id":8},{"text":"image","start":47,"end":52,"id":9},{"text":"processing","start":53,"end":63,"id":10},{"text":",","start":63,"end":64,"id":11},{"text":"such","start":65,"end":69,"id":12},{"text":"as","start":70,"end":72,"id":13},{"text":"denoising","start":73,"end":82,"id":14},{"text":",","start":82,"end":83,"id":15},{"text":"deconvolution","start":84,"end":97,"id":16},{"text":",","start":97,"end":98,"id":17},{"text":"tomographic","start":99,"end":110,"id":18},{"text":"reconstruction","start":111,"end":125,"id":19},{"text":"or","start":126,"end":128,"id":20},{"text":"super","start":129,"end":134,"id":21},{"text":"-","start":134,"end":135,"id":22},{"text":"resolution","start":135,"end":145,"id":23},{"text":",","start":145,"end":146,"id":24},{"text":"can","start":147,"end":150,"id":25},{"text":"be","start":151,"end":153,"id":26},{"text":"addressed","start":154,"end":163,"id":27},{"text":"by","start":164,"end":166,"id":28},{"text":"maximizing","start":167,"end":177,"id":29},{"text":"the","start":178,"end":181,"id":30},{"text":"posterior","start":182,"end":191,"id":31},{"text":"distribution","start":192,"end":204,"id":32},{"text":"of","start":205,"end":207,"id":33},{"text":"a","start":208,"end":209,"id":34},{"text":"sparse","start":210,"end":216,"id":35},{"text":"linear","start":217,"end":223,"id":36},{"text":"model","start":224,"end":229,"id":37},{"text":"(","start":230,"end":231,"id":38},{"text":"SLM","start":231,"end":234,"id":39},{"text":")","start":234,"end":235,"id":40},{"text":".","start":235,"end":236,"id":41}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":210,"end":229,"token_start":35,"token_end":37,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Using the moment generating function technique, we further provide a lower bound for $k$ using pairwise $L_2$ distances in the space of points to be projected and pairwise $L_1$ distances in the space of the projected points.","_input_hash":823121753,"_task_hash":-1897836433,"tokens":[{"text":"Using","start":0,"end":5,"id":0},{"text":"the","start":6,"end":9,"id":1},{"text":"moment","start":10,"end":16,"id":2},{"text":"generating","start":17,"end":27,"id":3},{"text":"function","start":28,"end":36,"id":4},{"text":"technique","start":37,"end":46,"id":5},{"text":",","start":46,"end":47,"id":6},{"text":"we","start":48,"end":50,"id":7},{"text":"further","start":51,"end":58,"id":8},{"text":"provide","start":59,"end":66,"id":9},{"text":"a","start":67,"end":68,"id":10},{"text":"lower","start":69,"end":74,"id":11},{"text":"bound","start":75,"end":80,"id":12},{"text":"for","start":81,"end":84,"id":13},{"text":"$","start":85,"end":86,"id":14},{"text":"k$","start":86,"end":88,"id":15},{"text":"using","start":89,"end":94,"id":16},{"text":"pairwise","start":95,"end":103,"id":17},{"text":"$","start":104,"end":105,"id":18},{"text":"L_2","start":105,"end":108,"id":19},{"text":"$","start":108,"end":109,"id":20},{"text":"distances","start":110,"end":119,"id":21},{"text":"in","start":120,"end":122,"id":22},{"text":"the","start":123,"end":126,"id":23},{"text":"space","start":127,"end":132,"id":24},{"text":"of","start":133,"end":135,"id":25},{"text":"points","start":136,"end":142,"id":26},{"text":"to","start":143,"end":145,"id":27},{"text":"be","start":146,"end":148,"id":28},{"text":"projected","start":149,"end":158,"id":29},{"text":"and","start":159,"end":162,"id":30},{"text":"pairwise","start":163,"end":171,"id":31},{"text":"$","start":172,"end":173,"id":32},{"text":"L_1","start":173,"end":176,"id":33},{"text":"$","start":176,"end":177,"id":34},{"text":"distances","start":178,"end":187,"id":35},{"text":"in","start":188,"end":190,"id":36},{"text":"the","start":191,"end":194,"id":37},{"text":"space","start":195,"end":200,"id":38},{"text":"of","start":201,"end":203,"id":39},{"text":"the","start":204,"end":207,"id":40},{"text":"projected","start":208,"end":217,"id":41},{"text":"points","start":218,"end":224,"id":42},{"text":".","start":224,"end":225,"id":43}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"no explicit parametric model is needed for these quantities.","_input_hash":-113288791,"_task_hash":-1766393390,"tokens":[{"text":"no","start":0,"end":2,"id":0},{"text":"explicit","start":3,"end":11,"id":1},{"text":"parametric","start":12,"end":22,"id":2},{"text":"model","start":23,"end":28,"id":3},{"text":"is","start":29,"end":31,"id":4},{"text":"needed","start":32,"end":38,"id":5},{"text":"for","start":39,"end":42,"id":6},{"text":"these","start":43,"end":48,"id":7},{"text":"quantities","start":49,"end":59,"id":8},{"text":".","start":59,"end":60,"id":9}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"learning_algorithms|NOUN","word":"learning algorithms","sense":"NOUN","meta":{"score":0.7509999871,"sense":"NOUN"},"_input_hash":-313068494,"_task_hash":939381177,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"learning_algorithms|NOUN","start":0,"end":24,"id":0}]}
{"text":"Specifically, we present an application to information retrieval in which documents are modeled as paths down a random tree, and the preferential attachment dynamics of the nCRP leads to clustering of documents according to sharing of topics at multiple levels of abstraction.","_input_hash":1951118404,"_task_hash":1289881453,"tokens":[{"text":"Specifically","start":0,"end":12,"id":0},{"text":",","start":12,"end":13,"id":1},{"text":"we","start":14,"end":16,"id":2},{"text":"present","start":17,"end":24,"id":3},{"text":"an","start":25,"end":27,"id":4},{"text":"application","start":28,"end":39,"id":5},{"text":"to","start":40,"end":42,"id":6},{"text":"information","start":43,"end":54,"id":7},{"text":"retrieval","start":55,"end":64,"id":8},{"text":"in","start":65,"end":67,"id":9},{"text":"which","start":68,"end":73,"id":10},{"text":"documents","start":74,"end":83,"id":11},{"text":"are","start":84,"end":87,"id":12},{"text":"modeled","start":88,"end":95,"id":13},{"text":"as","start":96,"end":98,"id":14},{"text":"paths","start":99,"end":104,"id":15},{"text":"down","start":105,"end":109,"id":16},{"text":"a","start":110,"end":111,"id":17},{"text":"random","start":112,"end":118,"id":18},{"text":"tree","start":119,"end":123,"id":19},{"text":",","start":123,"end":124,"id":20},{"text":"and","start":125,"end":128,"id":21},{"text":"the","start":129,"end":132,"id":22},{"text":"preferential","start":133,"end":145,"id":23},{"text":"attachment","start":146,"end":156,"id":24},{"text":"dynamics","start":157,"end":165,"id":25},{"text":"of","start":166,"end":168,"id":26},{"text":"the","start":169,"end":172,"id":27},{"text":"nCRP","start":173,"end":177,"id":28},{"text":"leads","start":178,"end":183,"id":29},{"text":"to","start":184,"end":186,"id":30},{"text":"clustering","start":187,"end":197,"id":31},{"text":"of","start":198,"end":200,"id":32},{"text":"documents","start":201,"end":210,"id":33},{"text":"according","start":211,"end":220,"id":34},{"text":"to","start":221,"end":223,"id":35},{"text":"sharing","start":224,"end":231,"id":36},{"text":"of","start":232,"end":234,"id":37},{"text":"topics","start":235,"end":241,"id":38},{"text":"at","start":242,"end":244,"id":39},{"text":"multiple","start":245,"end":253,"id":40},{"text":"levels","start":254,"end":260,"id":41},{"text":"of","start":261,"end":263,"id":42},{"text":"abstraction","start":264,"end":275,"id":43},{"text":".","start":275,"end":276,"id":44}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We provide Markov chain Monte Carlo algorithms for inference in these belief networks and explore the structures learned on several image data sets.","_input_hash":-495814547,"_task_hash":-94937361,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"provide","start":3,"end":10,"id":1},{"text":"Markov","start":11,"end":17,"id":2},{"text":"chain","start":18,"end":23,"id":3},{"text":"Monte","start":24,"end":29,"id":4},{"text":"Carlo","start":30,"end":35,"id":5},{"text":"algorithms","start":36,"end":46,"id":6},{"text":"for","start":47,"end":50,"id":7},{"text":"inference","start":51,"end":60,"id":8},{"text":"in","start":61,"end":63,"id":9},{"text":"these","start":64,"end":69,"id":10},{"text":"belief","start":70,"end":76,"id":11},{"text":"networks","start":77,"end":85,"id":12},{"text":"and","start":86,"end":89,"id":13},{"text":"explore","start":90,"end":97,"id":14},{"text":"the","start":98,"end":101,"id":15},{"text":"structures","start":102,"end":112,"id":16},{"text":"learned","start":113,"end":120,"id":17},{"text":"on","start":121,"end":123,"id":18},{"text":"several","start":124,"end":131,"id":19},{"text":"image","start":132,"end":137,"id":20},{"text":"data","start":138,"end":142,"id":21},{"text":"sets","start":143,"end":147,"id":22},{"text":".","start":147,"end":148,"id":23}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":11,"end":35,"token_start":2,"token_end":5,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"bounded and unbounded percentage of traffic, and bounded false positive rate.","_input_hash":1838088007,"_task_hash":615499688,"tokens":[{"text":"bounded","start":0,"end":7,"id":0},{"text":"and","start":8,"end":11,"id":1},{"text":"unbounded","start":12,"end":21,"id":2},{"text":"percentage","start":22,"end":32,"id":3},{"text":"of","start":33,"end":35,"id":4},{"text":"traffic","start":36,"end":43,"id":5},{"text":",","start":43,"end":44,"id":6},{"text":"and","start":45,"end":48,"id":7},{"text":"bounded","start":49,"end":56,"id":8},{"text":"false","start":57,"end":62,"id":9},{"text":"positive","start":63,"end":71,"id":10},{"text":"rate","start":72,"end":76,"id":11},{"text":".","start":76,"end":77,"id":12}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"computer_program|NOUN","word":"computer program","sense":"NOUN","meta":{"score":0.7972999811,"sense":"NOUN"},"_input_hash":1317113466,"_task_hash":-699533892,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"computer_program|NOUN","start":0,"end":21,"id":0}]}
{"text":"would","meta":{"score":0},"_input_hash":811903774,"_task_hash":1240730338,"_session_id":null,"_view_id":"text","answer":"reject","spans":[],"tokens":[{"text":"would","start":0,"end":5,"id":0}]}
{"text":"The uneven hinge, squared error, exponential, and sigmoid losses are then treated in detail.","_input_hash":1449106070,"_task_hash":-176172246,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"uneven","start":4,"end":10,"id":1},{"text":"hinge","start":11,"end":16,"id":2},{"text":",","start":16,"end":17,"id":3},{"text":"squared","start":18,"end":25,"id":4},{"text":"error","start":26,"end":31,"id":5},{"text":",","start":31,"end":32,"id":6},{"text":"exponential","start":33,"end":44,"id":7},{"text":",","start":44,"end":45,"id":8},{"text":"and","start":46,"end":49,"id":9},{"text":"sigmoid","start":50,"end":57,"id":10},{"text":"losses","start":58,"end":64,"id":11},{"text":"are","start":65,"end":68,"id":12},{"text":"then","start":69,"end":73,"id":13},{"text":"treated","start":74,"end":81,"id":14},{"text":"in","start":82,"end":84,"id":15},{"text":"detail","start":85,"end":91,"id":16},{"text":".","start":91,"end":92,"id":17}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"2005) may involve a set of features of a graph that is exponential in the number of vertices.","_input_hash":181811664,"_task_hash":1250550150,"tokens":[{"text":"2005","start":0,"end":4,"id":0},{"text":")","start":4,"end":5,"id":1},{"text":"may","start":6,"end":9,"id":2},{"text":"involve","start":10,"end":17,"id":3},{"text":"a","start":18,"end":19,"id":4},{"text":"set","start":20,"end":23,"id":5},{"text":"of","start":24,"end":26,"id":6},{"text":"features","start":27,"end":35,"id":7},{"text":"of","start":36,"end":38,"id":8},{"text":"a","start":39,"end":40,"id":9},{"text":"graph","start":41,"end":46,"id":10},{"text":"that","start":47,"end":51,"id":11},{"text":"is","start":52,"end":54,"id":12},{"text":"exponential","start":55,"end":66,"id":13},{"text":"in","start":67,"end":69,"id":14},{"text":"the","start":70,"end":73,"id":15},{"text":"number","start":74,"end":80,"id":16},{"text":"of","start":81,"end":83,"id":17},{"text":"vertices","start":84,"end":92,"id":18},{"text":".","start":92,"end":93,"id":19}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"Flow cytometry is often used to characterize the malignant cells in leukemia and lymphoma patients, traced to the level of the individual cell.","_input_hash":-186932865,"_task_hash":2014982531,"tokens":[{"text":"Flow","start":0,"end":4,"id":0},{"text":"cytometry","start":5,"end":14,"id":1},{"text":"is","start":15,"end":17,"id":2},{"text":"often","start":18,"end":23,"id":3},{"text":"used","start":24,"end":28,"id":4},{"text":"to","start":29,"end":31,"id":5},{"text":"characterize","start":32,"end":44,"id":6},{"text":"the","start":45,"end":48,"id":7},{"text":"malignant","start":49,"end":58,"id":8},{"text":"cells","start":59,"end":64,"id":9},{"text":"in","start":65,"end":67,"id":10},{"text":"leukemia","start":68,"end":76,"id":11},{"text":"and","start":77,"end":80,"id":12},{"text":"lymphoma","start":81,"end":89,"id":13},{"text":"patients","start":90,"end":98,"id":14},{"text":",","start":98,"end":99,"id":15},{"text":"traced","start":100,"end":106,"id":16},{"text":"to","start":107,"end":109,"id":17},{"text":"the","start":110,"end":113,"id":18},{"text":"level","start":114,"end":119,"id":19},{"text":"of","start":120,"end":122,"id":20},{"text":"the","start":123,"end":126,"id":21},{"text":"individual","start":127,"end":137,"id":22},{"text":"cell","start":138,"end":142,"id":23},{"text":".","start":142,"end":143,"id":24}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"Both constraint-based and score-based algorithms are implemented, and can use the functionality provided by the snow package to improve their performance via parallel computing.","_input_hash":769924704,"_task_hash":1929571693,"tokens":[{"text":"Both","start":0,"end":4,"id":0},{"text":"constraint","start":5,"end":15,"id":1},{"text":"-","start":15,"end":16,"id":2},{"text":"based","start":16,"end":21,"id":3},{"text":"and","start":22,"end":25,"id":4},{"text":"score","start":26,"end":31,"id":5},{"text":"-","start":31,"end":32,"id":6},{"text":"based","start":32,"end":37,"id":7},{"text":"algorithms","start":38,"end":48,"id":8},{"text":"are","start":49,"end":52,"id":9},{"text":"implemented","start":53,"end":64,"id":10},{"text":",","start":64,"end":65,"id":11},{"text":"and","start":66,"end":69,"id":12},{"text":"can","start":70,"end":73,"id":13},{"text":"use","start":74,"end":77,"id":14},{"text":"the","start":78,"end":81,"id":15},{"text":"functionality","start":82,"end":95,"id":16},{"text":"provided","start":96,"end":104,"id":17},{"text":"by","start":105,"end":107,"id":18},{"text":"the","start":108,"end":111,"id":19},{"text":"snow","start":112,"end":116,"id":20},{"text":"package","start":117,"end":124,"id":21},{"text":"to","start":125,"end":127,"id":22},{"text":"improve","start":128,"end":135,"id":23},{"text":"their","start":136,"end":141,"id":24},{"text":"performance","start":142,"end":153,"id":25},{"text":"via","start":154,"end":157,"id":26},{"text":"parallel","start":158,"end":166,"id":27},{"text":"computing","start":167,"end":176,"id":28},{"text":".","start":176,"end":177,"id":29}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"The framework provides a way to use continuous directional probability densities and the methods developed thereof for establishing densities over permutations.","_input_hash":-1286580586,"_task_hash":1132838742,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"framework","start":4,"end":13,"id":1},{"text":"provides","start":14,"end":22,"id":2},{"text":"a","start":23,"end":24,"id":3},{"text":"way","start":25,"end":28,"id":4},{"text":"to","start":29,"end":31,"id":5},{"text":"use","start":32,"end":35,"id":6},{"text":"continuous","start":36,"end":46,"id":7},{"text":"directional","start":47,"end":58,"id":8},{"text":"probability","start":59,"end":70,"id":9},{"text":"densities","start":71,"end":80,"id":10},{"text":"and","start":81,"end":84,"id":11},{"text":"the","start":85,"end":88,"id":12},{"text":"methods","start":89,"end":96,"id":13},{"text":"developed","start":97,"end":106,"id":14},{"text":"thereof","start":107,"end":114,"id":15},{"text":"for","start":115,"end":118,"id":16},{"text":"establishing","start":119,"end":131,"id":17},{"text":"densities","start":132,"end":141,"id":18},{"text":"over","start":142,"end":146,"id":19},{"text":"permutations","start":147,"end":159,"id":20},{"text":".","start":159,"end":160,"id":21}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"The penalty selects functional groups of nodes in the trees.","_input_hash":-1197160411,"_task_hash":-168394650,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"penalty","start":4,"end":11,"id":1},{"text":"selects","start":12,"end":19,"id":2},{"text":"functional","start":20,"end":30,"id":3},{"text":"groups","start":31,"end":37,"id":4},{"text":"of","start":38,"end":40,"id":5},{"text":"nodes","start":41,"end":46,"id":6},{"text":"in","start":47,"end":49,"id":7},{"text":"the","start":50,"end":53,"id":8},{"text":"trees","start":54,"end":59,"id":9},{"text":".","start":59,"end":60,"id":10}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Finally, we discuss the potential application of our results to sample compression schemes.","_input_hash":551131111,"_task_hash":-674286629,"tokens":[{"text":"Finally","start":0,"end":7,"id":0},{"text":",","start":7,"end":8,"id":1},{"text":"we","start":9,"end":11,"id":2},{"text":"discuss","start":12,"end":19,"id":3},{"text":"the","start":20,"end":23,"id":4},{"text":"potential","start":24,"end":33,"id":5},{"text":"application","start":34,"end":45,"id":6},{"text":"of","start":46,"end":48,"id":7},{"text":"our","start":49,"end":52,"id":8},{"text":"results","start":53,"end":60,"id":9},{"text":"to","start":61,"end":63,"id":10},{"text":"sample","start":64,"end":70,"id":11},{"text":"compression","start":71,"end":82,"id":12},{"text":"schemes","start":83,"end":90,"id":13},{"text":".","start":90,"end":91,"id":14}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"We show some experimental results, which confirm the theory.","_input_hash":1008824299,"_task_hash":-1317059563,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"show","start":3,"end":7,"id":1},{"text":"some","start":8,"end":12,"id":2},{"text":"experimental","start":13,"end":25,"id":3},{"text":"results","start":26,"end":33,"id":4},{"text":",","start":33,"end":34,"id":5},{"text":"which","start":35,"end":40,"id":6},{"text":"confirm","start":41,"end":48,"id":7},{"text":"the","start":49,"end":52,"id":8},{"text":"theory","start":53,"end":59,"id":9},{"text":".","start":59,"end":60,"id":10}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"may","meta":{"score":0},"_input_hash":516114863,"_task_hash":1053903320,"_session_id":null,"_view_id":"text","answer":"reject","spans":[],"tokens":[{"text":"may","start":0,"end":3,"id":0}]}
{"text":"Using the Kullback-Leibler divergence as a measure of generalisation error we draw learning curves in simplified situations.","_input_hash":1258501792,"_task_hash":-828622494,"tokens":[{"text":"Using","start":0,"end":5,"id":0},{"text":"the","start":6,"end":9,"id":1},{"text":"Kullback","start":10,"end":18,"id":2},{"text":"-","start":18,"end":19,"id":3},{"text":"Leibler","start":19,"end":26,"id":4},{"text":"divergence","start":27,"end":37,"id":5},{"text":"as","start":38,"end":40,"id":6},{"text":"a","start":41,"end":42,"id":7},{"text":"measure","start":43,"end":50,"id":8},{"text":"of","start":51,"end":53,"id":9},{"text":"generalisation","start":54,"end":68,"id":10},{"text":"error","start":69,"end":74,"id":11},{"text":"we","start":75,"end":77,"id":12},{"text":"draw","start":78,"end":82,"id":13},{"text":"learning","start":83,"end":91,"id":14},{"text":"curves","start":92,"end":98,"id":15},{"text":"in","start":99,"end":101,"id":16},{"text":"simplified","start":102,"end":112,"id":17},{"text":"situations","start":113,"end":123,"id":18},{"text":".","start":123,"end":124,"id":19}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We discuss its relevance to maximum likelihood estimation, both from a theoretical and computational standpoint.","_input_hash":-1688515034,"_task_hash":-1378774903,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"discuss","start":3,"end":10,"id":1},{"text":"its","start":11,"end":14,"id":2},{"text":"relevance","start":15,"end":24,"id":3},{"text":"to","start":25,"end":27,"id":4},{"text":"maximum","start":28,"end":35,"id":5},{"text":"likelihood","start":36,"end":46,"id":6},{"text":"estimation","start":47,"end":57,"id":7},{"text":",","start":57,"end":58,"id":8},{"text":"both","start":59,"end":63,"id":9},{"text":"from","start":64,"end":68,"id":10},{"text":"a","start":69,"end":70,"id":11},{"text":"theoretical","start":71,"end":82,"id":12},{"text":"and","start":83,"end":86,"id":13},{"text":"computational","start":87,"end":100,"id":14},{"text":"standpoint","start":101,"end":111,"id":15},{"text":".","start":111,"end":112,"id":16}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"Tree ensembles, on the other hand, lack usually a formal way of variable selection and are difficult to visualize.","_input_hash":-2024684727,"_task_hash":410674121,"tokens":[{"text":"Tree","start":0,"end":4,"id":0},{"text":"ensembles","start":5,"end":14,"id":1},{"text":",","start":14,"end":15,"id":2},{"text":"on","start":16,"end":18,"id":3},{"text":"the","start":19,"end":22,"id":4},{"text":"other","start":23,"end":28,"id":5},{"text":"hand","start":29,"end":33,"id":6},{"text":",","start":33,"end":34,"id":7},{"text":"lack","start":35,"end":39,"id":8},{"text":"usually","start":40,"end":47,"id":9},{"text":"a","start":48,"end":49,"id":10},{"text":"formal","start":50,"end":56,"id":11},{"text":"way","start":57,"end":60,"id":12},{"text":"of","start":61,"end":63,"id":13},{"text":"variable","start":64,"end":72,"id":14},{"text":"selection","start":73,"end":82,"id":15},{"text":"and","start":83,"end":86,"id":16},{"text":"are","start":87,"end":90,"id":17},{"text":"difficult","start":91,"end":100,"id":18},{"text":"to","start":101,"end":103,"id":19},{"text":"visualize","start":104,"end":113,"id":20},{"text":".","start":113,"end":114,"id":21}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":0,"end":14,"token_start":0,"token_end":1,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"This problem, often referred to as \\emph{PU learning}, differs from the standard supervised classification problem by the lack of negative examples in the training set.","_input_hash":-1905433351,"_task_hash":1673544025,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"problem","start":5,"end":12,"id":1},{"text":",","start":12,"end":13,"id":2},{"text":"often","start":14,"end":19,"id":3},{"text":"referred","start":20,"end":28,"id":4},{"text":"to","start":29,"end":31,"id":5},{"text":"as","start":32,"end":34,"id":6},{"text":"\\emph{PU","start":35,"end":43,"id":7},{"text":"learning","start":44,"end":52,"id":8},{"text":"}","start":52,"end":53,"id":9},{"text":",","start":53,"end":54,"id":10},{"text":"differs","start":55,"end":62,"id":11},{"text":"from","start":63,"end":67,"id":12},{"text":"the","start":68,"end":71,"id":13},{"text":"standard","start":72,"end":80,"id":14},{"text":"supervised","start":81,"end":91,"id":15},{"text":"classification","start":92,"end":106,"id":16},{"text":"problem","start":107,"end":114,"id":17},{"text":"by","start":115,"end":117,"id":18},{"text":"the","start":118,"end":121,"id":19},{"text":"lack","start":122,"end":126,"id":20},{"text":"of","start":127,"end":129,"id":21},{"text":"negative","start":130,"end":138,"id":22},{"text":"examples","start":139,"end":147,"id":23},{"text":"in","start":148,"end":150,"id":24},{"text":"the","start":151,"end":154,"id":25},{"text":"training","start":155,"end":163,"id":26},{"text":"set","start":164,"end":167,"id":27},{"text":".","start":167,"end":168,"id":28}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"have","meta":{"score":0},"_input_hash":-416252709,"_task_hash":-1628194564,"_session_id":null,"_view_id":"text","answer":"reject","spans":[],"tokens":[{"text":"have","start":0,"end":4,"id":0}]}
{"text":"We refer to our method as Information Preserving Component Analysis (IPCA).","_input_hash":-455619717,"_task_hash":2112919666,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"refer","start":3,"end":8,"id":1},{"text":"to","start":9,"end":11,"id":2},{"text":"our","start":12,"end":15,"id":3},{"text":"method","start":16,"end":22,"id":4},{"text":"as","start":23,"end":25,"id":5},{"text":"Information","start":26,"end":37,"id":6},{"text":"Preserving","start":38,"end":48,"id":7},{"text":"Component","start":49,"end":58,"id":8},{"text":"Analysis","start":59,"end":67,"id":9},{"text":"(","start":68,"end":69,"id":10},{"text":"IPCA","start":69,"end":73,"id":11},{"text":")","start":73,"end":74,"id":12},{"text":".","start":74,"end":75,"id":13}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"information_processing|NOUN","word":"information processing","sense":"NOUN","meta":{"score":0.7926999927,"sense":"NOUN"},"_input_hash":-2066270993,"_task_hash":730692271,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"information_processing|NOUN","start":0,"end":27,"id":0}]}
{"text":"RNNs|NOUN","word":"RNNs","sense":"NOUN","meta":{"score":0.7761999965,"sense":"NOUN"},"_input_hash":709991632,"_task_hash":-1866294595,"_session_id":null,"_view_id":"html","answer":"accept","spans":[],"tokens":[{"text":"RNNs|NOUN","start":0,"end":9,"id":0}]}
{"text":"In the currently popular field of network modeling, relatively little work has taken uncertainty of data seriously in the Bayesian sense, and component models have been introduced to the field only recently, by treating each node as a bag of out-going links.","_input_hash":-2045461051,"_task_hash":-317357642,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"the","start":3,"end":6,"id":1},{"text":"currently","start":7,"end":16,"id":2},{"text":"popular","start":17,"end":24,"id":3},{"text":"field","start":25,"end":30,"id":4},{"text":"of","start":31,"end":33,"id":5},{"text":"network","start":34,"end":41,"id":6},{"text":"modeling","start":42,"end":50,"id":7},{"text":",","start":50,"end":51,"id":8},{"text":"relatively","start":52,"end":62,"id":9},{"text":"little","start":63,"end":69,"id":10},{"text":"work","start":70,"end":74,"id":11},{"text":"has","start":75,"end":78,"id":12},{"text":"taken","start":79,"end":84,"id":13},{"text":"uncertainty","start":85,"end":96,"id":14},{"text":"of","start":97,"end":99,"id":15},{"text":"data","start":100,"end":104,"id":16},{"text":"seriously","start":105,"end":114,"id":17},{"text":"in","start":115,"end":117,"id":18},{"text":"the","start":118,"end":121,"id":19},{"text":"Bayesian","start":122,"end":130,"id":20},{"text":"sense","start":131,"end":136,"id":21},{"text":",","start":136,"end":137,"id":22},{"text":"and","start":138,"end":141,"id":23},{"text":"component","start":142,"end":151,"id":24},{"text":"models","start":152,"end":158,"id":25},{"text":"have","start":159,"end":163,"id":26},{"text":"been","start":164,"end":168,"id":27},{"text":"introduced","start":169,"end":179,"id":28},{"text":"to","start":180,"end":182,"id":29},{"text":"the","start":183,"end":186,"id":30},{"text":"field","start":187,"end":192,"id":31},{"text":"only","start":193,"end":197,"id":32},{"text":"recently","start":198,"end":206,"id":33},{"text":",","start":206,"end":207,"id":34},{"text":"by","start":208,"end":210,"id":35},{"text":"treating","start":211,"end":219,"id":36},{"text":"each","start":220,"end":224,"id":37},{"text":"node","start":225,"end":229,"id":38},{"text":"as","start":230,"end":232,"id":39},{"text":"a","start":233,"end":234,"id":40},{"text":"bag","start":235,"end":238,"id":41},{"text":"of","start":239,"end":241,"id":42},{"text":"out","start":242,"end":245,"id":43},{"text":"-","start":245,"end":246,"id":44},{"text":"going","start":246,"end":251,"id":45},{"text":"links","start":252,"end":257,"id":46},{"text":".","start":257,"end":258,"id":47}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We describe a method for inferring linear causal relations among multi-dimensional variables.","_input_hash":254992690,"_task_hash":-1840526543,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"describe","start":3,"end":11,"id":1},{"text":"a","start":12,"end":13,"id":2},{"text":"method","start":14,"end":20,"id":3},{"text":"for","start":21,"end":24,"id":4},{"text":"inferring","start":25,"end":34,"id":5},{"text":"linear","start":35,"end":41,"id":6},{"text":"causal","start":42,"end":48,"id":7},{"text":"relations","start":49,"end":58,"id":8},{"text":"among","start":59,"end":64,"id":9},{"text":"multi","start":65,"end":70,"id":10},{"text":"-","start":70,"end":71,"id":11},{"text":"dimensional","start":71,"end":82,"id":12},{"text":"variables","start":83,"end":92,"id":13},{"text":".","start":92,"end":93,"id":14}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"The meta-learning is the process of combining outcomes of individual learning procedures in order to determine the final decision with higher accuracy than any single learning method.","_input_hash":-172294891,"_task_hash":707659187,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"meta","start":4,"end":8,"id":1},{"text":"-","start":8,"end":9,"id":2},{"text":"learning","start":9,"end":17,"id":3},{"text":"is","start":18,"end":20,"id":4},{"text":"the","start":21,"end":24,"id":5},{"text":"process","start":25,"end":32,"id":6},{"text":"of","start":33,"end":35,"id":7},{"text":"combining","start":36,"end":45,"id":8},{"text":"outcomes","start":46,"end":54,"id":9},{"text":"of","start":55,"end":57,"id":10},{"text":"individual","start":58,"end":68,"id":11},{"text":"learning","start":69,"end":77,"id":12},{"text":"procedures","start":78,"end":88,"id":13},{"text":"in","start":89,"end":91,"id":14},{"text":"order","start":92,"end":97,"id":15},{"text":"to","start":98,"end":100,"id":16},{"text":"determine","start":101,"end":110,"id":17},{"text":"the","start":111,"end":114,"id":18},{"text":"final","start":115,"end":120,"id":19},{"text":"decision","start":121,"end":129,"id":20},{"text":"with","start":130,"end":134,"id":21},{"text":"higher","start":135,"end":141,"id":22},{"text":"accuracy","start":142,"end":150,"id":23},{"text":"than","start":151,"end":155,"id":24},{"text":"any","start":156,"end":159,"id":25},{"text":"single","start":160,"end":166,"id":26},{"text":"learning","start":167,"end":175,"id":27},{"text":"method","start":176,"end":182,"id":28},{"text":".","start":182,"end":183,"id":29}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We discuss here the mean-field theory for a cellular automata model of meta-learning.","_input_hash":1230272902,"_task_hash":-783573503,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"discuss","start":3,"end":10,"id":1},{"text":"here","start":11,"end":15,"id":2},{"text":"the","start":16,"end":19,"id":3},{"text":"mean","start":20,"end":24,"id":4},{"text":"-","start":24,"end":25,"id":5},{"text":"field","start":25,"end":30,"id":6},{"text":"theory","start":31,"end":37,"id":7},{"text":"for","start":38,"end":41,"id":8},{"text":"a","start":42,"end":43,"id":9},{"text":"cellular","start":44,"end":52,"id":10},{"text":"automata","start":53,"end":61,"id":11},{"text":"model","start":62,"end":67,"id":12},{"text":"of","start":68,"end":70,"id":13},{"text":"meta","start":71,"end":75,"id":14},{"text":"-","start":75,"end":76,"id":15},{"text":"learning","start":76,"end":84,"id":16},{"text":".","start":84,"end":85,"id":17}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":44,"end":67,"token_start":10,"token_end":12,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"genetic_algorithm|NOUN","word":"genetic algorithm","sense":"NOUN","meta":{"score":0.7763000131,"sense":"NOUN"},"_input_hash":383830497,"_task_hash":966175593,"_session_id":null,"_view_id":"html","answer":"accept","spans":[],"tokens":[{"text":"genetic_algorithm|NOUN","start":0,"end":22,"id":0}]}
{"text":"We provide self-contained proof of a theorem relating probabilistic coherence of forecasts to their non-domination by rival forecasts with respect to any proper scoring rule.","_input_hash":419135333,"_task_hash":954605435,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"provide","start":3,"end":10,"id":1},{"text":"self","start":11,"end":15,"id":2},{"text":"-","start":15,"end":16,"id":3},{"text":"contained","start":16,"end":25,"id":4},{"text":"proof","start":26,"end":31,"id":5},{"text":"of","start":32,"end":34,"id":6},{"text":"a","start":35,"end":36,"id":7},{"text":"theorem","start":37,"end":44,"id":8},{"text":"relating","start":45,"end":53,"id":9},{"text":"probabilistic","start":54,"end":67,"id":10},{"text":"coherence","start":68,"end":77,"id":11},{"text":"of","start":78,"end":80,"id":12},{"text":"forecasts","start":81,"end":90,"id":13},{"text":"to","start":91,"end":93,"id":14},{"text":"their","start":94,"end":99,"id":15},{"text":"non","start":100,"end":103,"id":16},{"text":"-","start":103,"end":104,"id":17},{"text":"domination","start":104,"end":114,"id":18},{"text":"by","start":115,"end":117,"id":19},{"text":"rival","start":118,"end":123,"id":20},{"text":"forecasts","start":124,"end":133,"id":21},{"text":"with","start":134,"end":138,"id":22},{"text":"respect","start":139,"end":146,"id":23},{"text":"to","start":147,"end":149,"id":24},{"text":"any","start":150,"end":153,"id":25},{"text":"proper","start":154,"end":160,"id":26},{"text":"scoring","start":161,"end":168,"id":27},{"text":"rule","start":169,"end":173,"id":28},{"text":".","start":173,"end":174,"id":29}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"control_theory|NOUN","word":"control theory","sense":"NOUN","meta":{"score":0.7921000123,"sense":"NOUN"},"_input_hash":-696801847,"_task_hash":-1170324205,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"control_theory|NOUN","start":0,"end":19,"id":0}]}
{"text":"basic_algorithms|NOUN","word":"basic algorithms","sense":"NOUN","meta":{"score":0.8037999868,"sense":"NOUN"},"_input_hash":-1243754149,"_task_hash":-216566479,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"basic_algorithms|NOUN","start":0,"end":21,"id":0}]}
{"text":"We present the nested Chinese restaurant process (nCRP), a stochastic process which assigns probability distributions to infinitely-deep, infinitely-branching trees.","_input_hash":1329355179,"_task_hash":470386021,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"present","start":3,"end":10,"id":1},{"text":"the","start":11,"end":14,"id":2},{"text":"nested","start":15,"end":21,"id":3},{"text":"Chinese","start":22,"end":29,"id":4},{"text":"restaurant","start":30,"end":40,"id":5},{"text":"process","start":41,"end":48,"id":6},{"text":"(","start":49,"end":50,"id":7},{"text":"nCRP","start":50,"end":54,"id":8},{"text":")","start":54,"end":55,"id":9},{"text":",","start":55,"end":56,"id":10},{"text":"a","start":57,"end":58,"id":11},{"text":"stochastic","start":59,"end":69,"id":12},{"text":"process","start":70,"end":77,"id":13},{"text":"which","start":78,"end":83,"id":14},{"text":"assigns","start":84,"end":91,"id":15},{"text":"probability","start":92,"end":103,"id":16},{"text":"distributions","start":104,"end":117,"id":17},{"text":"to","start":118,"end":120,"id":18},{"text":"infinitely","start":121,"end":131,"id":19},{"text":"-","start":131,"end":132,"id":20},{"text":"deep","start":132,"end":136,"id":21},{"text":",","start":136,"end":137,"id":22},{"text":"infinitely","start":138,"end":148,"id":23},{"text":"-","start":148,"end":149,"id":24},{"text":"branching","start":149,"end":158,"id":25},{"text":"trees","start":159,"end":164,"id":26},{"text":".","start":164,"end":165,"id":27}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":22,"end":48,"token_start":4,"token_end":6,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"human_brains|NOUN","word":"human brains","sense":"NOUN","meta":{"score":0.8119999766,"sense":"NOUN"},"_input_hash":1651332467,"_task_hash":301556162,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"human_brains|NOUN","start":0,"end":17,"id":0}]}
{"text":"havin","meta":{"score":0},"_input_hash":1485247698,"_task_hash":-623552263,"_session_id":null,"_view_id":"text","answer":"reject","spans":[],"tokens":[{"text":"havin","start":0,"end":5,"id":0}]}
{"text":"Specifically, we develop a James-Stein-type shrinkage estimator, resulting in a procedure that is highly efficient statistically as well as computationally.","_input_hash":-358073182,"_task_hash":1281824272,"tokens":[{"text":"Specifically","start":0,"end":12,"id":0},{"text":",","start":12,"end":13,"id":1},{"text":"we","start":14,"end":16,"id":2},{"text":"develop","start":17,"end":24,"id":3},{"text":"a","start":25,"end":26,"id":4},{"text":"James","start":27,"end":32,"id":5},{"text":"-","start":32,"end":33,"id":6},{"text":"Stein","start":33,"end":38,"id":7},{"text":"-","start":38,"end":39,"id":8},{"text":"type","start":39,"end":43,"id":9},{"text":"shrinkage","start":44,"end":53,"id":10},{"text":"estimator","start":54,"end":63,"id":11},{"text":",","start":63,"end":64,"id":12},{"text":"resulting","start":65,"end":74,"id":13},{"text":"in","start":75,"end":77,"id":14},{"text":"a","start":78,"end":79,"id":15},{"text":"procedure","start":80,"end":89,"id":16},{"text":"that","start":90,"end":94,"id":17},{"text":"is","start":95,"end":97,"id":18},{"text":"highly","start":98,"end":104,"id":19},{"text":"efficient","start":105,"end":114,"id":20},{"text":"statistically","start":115,"end":128,"id":21},{"text":"as","start":129,"end":131,"id":22},{"text":"well","start":132,"end":136,"id":23},{"text":"as","start":137,"end":139,"id":24},{"text":"computationally","start":140,"end":155,"id":25},{"text":".","start":155,"end":156,"id":26}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"We extend it for more complex Kriging models, e.g. models using derivatives.","_input_hash":-910314375,"_task_hash":350022591,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"extend","start":3,"end":9,"id":1},{"text":"it","start":10,"end":12,"id":2},{"text":"for","start":13,"end":16,"id":3},{"text":"more","start":17,"end":21,"id":4},{"text":"complex","start":22,"end":29,"id":5},{"text":"Kriging","start":30,"end":37,"id":6},{"text":"models","start":38,"end":44,"id":7},{"text":",","start":44,"end":45,"id":8},{"text":"e.g.","start":46,"end":50,"id":9},{"text":"models","start":51,"end":57,"id":10},{"text":"using","start":58,"end":63,"id":11},{"text":"derivatives","start":64,"end":75,"id":12},{"text":".","start":75,"end":76,"id":13}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":30,"end":44,"token_start":6,"token_end":7,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"m","meta":{"score":0},"_input_hash":1369651553,"_task_hash":-96761666,"_session_id":null,"_view_id":"text","answer":"reject","spans":[],"tokens":[{"text":"m","start":0,"end":1,"id":0}]}
{"text":"However, the necessity of obtaining sensitivity measures as degrees of freedom for model selection or confidence intervals for more detailed analysis requires cubic runtime, and thus constitutes a computational bottleneck in real-world data analysis.","_input_hash":-487395949,"_task_hash":807936817,"tokens":[{"text":"However","start":0,"end":7,"id":0},{"text":",","start":7,"end":8,"id":1},{"text":"the","start":9,"end":12,"id":2},{"text":"necessity","start":13,"end":22,"id":3},{"text":"of","start":23,"end":25,"id":4},{"text":"obtaining","start":26,"end":35,"id":5},{"text":"sensitivity","start":36,"end":47,"id":6},{"text":"measures","start":48,"end":56,"id":7},{"text":"as","start":57,"end":59,"id":8},{"text":"degrees","start":60,"end":67,"id":9},{"text":"of","start":68,"end":70,"id":10},{"text":"freedom","start":71,"end":78,"id":11},{"text":"for","start":79,"end":82,"id":12},{"text":"model","start":83,"end":88,"id":13},{"text":"selection","start":89,"end":98,"id":14},{"text":"or","start":99,"end":101,"id":15},{"text":"confidence","start":102,"end":112,"id":16},{"text":"intervals","start":113,"end":122,"id":17},{"text":"for","start":123,"end":126,"id":18},{"text":"more","start":127,"end":131,"id":19},{"text":"detailed","start":132,"end":140,"id":20},{"text":"analysis","start":141,"end":149,"id":21},{"text":"requires","start":150,"end":158,"id":22},{"text":"cubic","start":159,"end":164,"id":23},{"text":"runtime","start":165,"end":172,"id":24},{"text":",","start":172,"end":173,"id":25},{"text":"and","start":174,"end":177,"id":26},{"text":"thus","start":178,"end":182,"id":27},{"text":"constitutes","start":183,"end":194,"id":28},{"text":"a","start":195,"end":196,"id":29},{"text":"computational","start":197,"end":210,"id":30},{"text":"bottleneck","start":211,"end":221,"id":31},{"text":"in","start":222,"end":224,"id":32},{"text":"real","start":225,"end":229,"id":33},{"text":"-","start":229,"end":230,"id":34},{"text":"world","start":230,"end":235,"id":35},{"text":"data","start":236,"end":240,"id":36},{"text":"analysis","start":241,"end":249,"id":37},{"text":".","start":249,"end":250,"id":38}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"Based on these principles an original theory is developed generalizing the notion of reproduction kernel Hilbert space to non hilbertian sets.","_input_hash":-680166752,"_task_hash":1444501464,"tokens":[{"text":"Based","start":0,"end":5,"id":0},{"text":"on","start":6,"end":8,"id":1},{"text":"these","start":9,"end":14,"id":2},{"text":"principles","start":15,"end":25,"id":3},{"text":"an","start":26,"end":28,"id":4},{"text":"original","start":29,"end":37,"id":5},{"text":"theory","start":38,"end":44,"id":6},{"text":"is","start":45,"end":47,"id":7},{"text":"developed","start":48,"end":57,"id":8},{"text":"generalizing","start":58,"end":70,"id":9},{"text":"the","start":71,"end":74,"id":10},{"text":"notion","start":75,"end":81,"id":11},{"text":"of","start":82,"end":84,"id":12},{"text":"reproduction","start":85,"end":97,"id":13},{"text":"kernel","start":98,"end":104,"id":14},{"text":"Hilbert","start":105,"end":112,"id":15},{"text":"space","start":113,"end":118,"id":16},{"text":"to","start":119,"end":121,"id":17},{"text":"non","start":122,"end":125,"id":18},{"text":"hilbertian","start":126,"end":136,"id":19},{"text":"sets","start":137,"end":141,"id":20},{"text":".","start":141,"end":142,"id":21}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":122,"end":141,"token_start":18,"token_end":20,"label":"ALGO","answer":"reject"}],"answer":"reject"}
{"text":"We propose a new sparsity-smoothness penalty for high-dimensional generalized additive models.","_input_hash":1320253224,"_task_hash":1994955183,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"propose","start":3,"end":10,"id":1},{"text":"a","start":11,"end":12,"id":2},{"text":"new","start":13,"end":16,"id":3},{"text":"sparsity","start":17,"end":25,"id":4},{"text":"-","start":25,"end":26,"id":5},{"text":"smoothness","start":26,"end":36,"id":6},{"text":"penalty","start":37,"end":44,"id":7},{"text":"for","start":45,"end":48,"id":8},{"text":"high","start":49,"end":53,"id":9},{"text":"-","start":53,"end":54,"id":10},{"text":"dimensional","start":54,"end":65,"id":11},{"text":"generalized","start":66,"end":77,"id":12},{"text":"additive","start":78,"end":86,"id":13},{"text":"models","start":87,"end":93,"id":14},{"text":".","start":93,"end":94,"id":15}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":78,"end":93,"token_start":13,"token_end":14,"label":"ALGO","answer":"reject"}],"answer":"reject"}
{"text":"A brief discussion about learning and symmetry breaking based on our results is also presented.","_input_hash":-1615577558,"_task_hash":102416772,"tokens":[{"text":"A","start":0,"end":1,"id":0},{"text":"brief","start":2,"end":7,"id":1},{"text":"discussion","start":8,"end":18,"id":2},{"text":"about","start":19,"end":24,"id":3},{"text":"learning","start":25,"end":33,"id":4},{"text":"and","start":34,"end":37,"id":5},{"text":"symmetry","start":38,"end":46,"id":6},{"text":"breaking","start":47,"end":55,"id":7},{"text":"based","start":56,"end":61,"id":8},{"text":"on","start":62,"end":64,"id":9},{"text":"our","start":65,"end":68,"id":10},{"text":"results","start":69,"end":76,"id":11},{"text":"is","start":77,"end":79,"id":12},{"text":"also","start":80,"end":84,"id":13},{"text":"presented","start":85,"end":94,"id":14},{"text":".","start":94,"end":95,"id":15}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Deep belief networks are a powerful way to model complex probability distributions.","_input_hash":1007910041,"_task_hash":2127470898,"tokens":[{"text":"Deep","start":0,"end":4,"id":0},{"text":"belief","start":5,"end":11,"id":1},{"text":"networks","start":12,"end":20,"id":2},{"text":"are","start":21,"end":24,"id":3},{"text":"a","start":25,"end":26,"id":4},{"text":"powerful","start":27,"end":35,"id":5},{"text":"way","start":36,"end":39,"id":6},{"text":"to","start":40,"end":42,"id":7},{"text":"model","start":43,"end":48,"id":8},{"text":"complex","start":49,"end":56,"id":9},{"text":"probability","start":57,"end":68,"id":10},{"text":"distributions","start":69,"end":82,"id":11},{"text":".","start":82,"end":83,"id":12}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"decision_tree","answer":"accept","_input_hash":586524735,"_task_hash":-693560190,"spans":[],"tokens":[{"text":"decision_tree","start":0,"end":13,"id":0}]}
{"text":"The method works for both stochastic and deterministic causal relations, provided that the dimensionality is sufficiently high (in some experiments, 5 was enough).","_input_hash":-325916622,"_task_hash":-1102930818,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"method","start":4,"end":10,"id":1},{"text":"works","start":11,"end":16,"id":2},{"text":"for","start":17,"end":20,"id":3},{"text":"both","start":21,"end":25,"id":4},{"text":"stochastic","start":26,"end":36,"id":5},{"text":"and","start":37,"end":40,"id":6},{"text":"deterministic","start":41,"end":54,"id":7},{"text":"causal","start":55,"end":61,"id":8},{"text":"relations","start":62,"end":71,"id":9},{"text":",","start":71,"end":72,"id":10},{"text":"provided","start":73,"end":81,"id":11},{"text":"that","start":82,"end":86,"id":12},{"text":"the","start":87,"end":90,"id":13},{"text":"dimensionality","start":91,"end":105,"id":14},{"text":"is","start":106,"end":108,"id":15},{"text":"sufficiently","start":109,"end":121,"id":16},{"text":"high","start":122,"end":126,"id":17},{"text":"(","start":127,"end":128,"id":18},{"text":"in","start":128,"end":130,"id":19},{"text":"some","start":131,"end":135,"id":20},{"text":"experiments","start":136,"end":147,"id":21},{"text":",","start":147,"end":148,"id":22},{"text":"5","start":149,"end":150,"id":23},{"text":"was","start":151,"end":154,"id":24},{"text":"enough","start":155,"end":161,"id":25},{"text":")","start":161,"end":162,"id":26},{"text":".","start":162,"end":163,"id":27}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"algorithms|NOUN","word":"algorithms","sense":"NOUN","meta":{"score":0.7537999749,"sense":"NOUN"},"_input_hash":1812822831,"_task_hash":-1922090826,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"algorithms|NOUN","start":0,"end":15,"id":0}]}
{"text":"We prove theoretically that this straightforward and computationally simple modification of LLE reduces LLE's sensitivity to noise.","_input_hash":1337426058,"_task_hash":-2017154891,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"prove","start":3,"end":8,"id":1},{"text":"theoretically","start":9,"end":22,"id":2},{"text":"that","start":23,"end":27,"id":3},{"text":"this","start":28,"end":32,"id":4},{"text":"straightforward","start":33,"end":48,"id":5},{"text":"and","start":49,"end":52,"id":6},{"text":"computationally","start":53,"end":68,"id":7},{"text":"simple","start":69,"end":75,"id":8},{"text":"modification","start":76,"end":88,"id":9},{"text":"of","start":89,"end":91,"id":10},{"text":"LLE","start":92,"end":95,"id":11},{"text":"reduces","start":96,"end":103,"id":12},{"text":"LLE","start":104,"end":107,"id":13},{"text":"'s","start":107,"end":109,"id":14},{"text":"sensitivity","start":110,"end":121,"id":15},{"text":"to","start":122,"end":124,"id":16},{"text":"noise","start":125,"end":130,"id":17},{"text":".","start":130,"end":131,"id":18}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Compared with the usual visualization, which simply lists the most probable topical terms, the multi-word expressions provide a better intuitive impression for what a topic is \"about.\"","_input_hash":-2034570161,"_task_hash":-885326438,"tokens":[{"text":"Compared","start":0,"end":8,"id":0},{"text":"with","start":9,"end":13,"id":1},{"text":"the","start":14,"end":17,"id":2},{"text":"usual","start":18,"end":23,"id":3},{"text":"visualization","start":24,"end":37,"id":4},{"text":",","start":37,"end":38,"id":5},{"text":"which","start":39,"end":44,"id":6},{"text":"simply","start":45,"end":51,"id":7},{"text":"lists","start":52,"end":57,"id":8},{"text":"the","start":58,"end":61,"id":9},{"text":"most","start":62,"end":66,"id":10},{"text":"probable","start":67,"end":75,"id":11},{"text":"topical","start":76,"end":83,"id":12},{"text":"terms","start":84,"end":89,"id":13},{"text":",","start":89,"end":90,"id":14},{"text":"the","start":91,"end":94,"id":15},{"text":"multi","start":95,"end":100,"id":16},{"text":"-","start":100,"end":101,"id":17},{"text":"word","start":101,"end":105,"id":18},{"text":"expressions","start":106,"end":117,"id":19},{"text":"provide","start":118,"end":125,"id":20},{"text":"a","start":126,"end":127,"id":21},{"text":"better","start":128,"end":134,"id":22},{"text":"intuitive","start":135,"end":144,"id":23},{"text":"impression","start":145,"end":155,"id":24},{"text":"for","start":156,"end":159,"id":25},{"text":"what","start":160,"end":164,"id":26},{"text":"a","start":165,"end":166,"id":27},{"text":"topic","start":167,"end":172,"id":28},{"text":"is","start":173,"end":175,"id":29},{"text":"\"","start":176,"end":177,"id":30},{"text":"about","start":177,"end":182,"id":31},{"text":".","start":182,"end":183,"id":32},{"text":"\"","start":183,"end":184,"id":33}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"First we consider the closure of $k$-dimensional exponential families of distribution with discrete base measure and polyhedral convex support $\\mathrm{P}$. We show that the normal fan of $\\mathrm{P}$ is a geometric object that plays a fundamental role in deriving the statistical and geometric properties of the corresponding extended exponential families.","_input_hash":1504062085,"_task_hash":1397639665,"tokens":[{"text":"First","start":0,"end":5,"id":0},{"text":"we","start":6,"end":8,"id":1},{"text":"consider","start":9,"end":17,"id":2},{"text":"the","start":18,"end":21,"id":3},{"text":"closure","start":22,"end":29,"id":4},{"text":"of","start":30,"end":32,"id":5},{"text":"$","start":33,"end":34,"id":6},{"text":"k$-dimensional","start":34,"end":48,"id":7},{"text":"exponential","start":49,"end":60,"id":8},{"text":"families","start":61,"end":69,"id":9},{"text":"of","start":70,"end":72,"id":10},{"text":"distribution","start":73,"end":85,"id":11},{"text":"with","start":86,"end":90,"id":12},{"text":"discrete","start":91,"end":99,"id":13},{"text":"base","start":100,"end":104,"id":14},{"text":"measure","start":105,"end":112,"id":15},{"text":"and","start":113,"end":116,"id":16},{"text":"polyhedral","start":117,"end":127,"id":17},{"text":"convex","start":128,"end":134,"id":18},{"text":"support","start":135,"end":142,"id":19},{"text":"$","start":143,"end":144,"id":20},{"text":"\\mathrm{P}$.","start":144,"end":156,"id":21},{"text":"We","start":157,"end":159,"id":22},{"text":"show","start":160,"end":164,"id":23},{"text":"that","start":165,"end":169,"id":24},{"text":"the","start":170,"end":173,"id":25},{"text":"normal","start":174,"end":180,"id":26},{"text":"fan","start":181,"end":184,"id":27},{"text":"of","start":185,"end":187,"id":28},{"text":"$","start":188,"end":189,"id":29},{"text":"\\mathrm{P}$","start":189,"end":200,"id":30},{"text":"is","start":201,"end":203,"id":31},{"text":"a","start":204,"end":205,"id":32},{"text":"geometric","start":206,"end":215,"id":33},{"text":"object","start":216,"end":222,"id":34},{"text":"that","start":223,"end":227,"id":35},{"text":"plays","start":228,"end":233,"id":36},{"text":"a","start":234,"end":235,"id":37},{"text":"fundamental","start":236,"end":247,"id":38},{"text":"role","start":248,"end":252,"id":39},{"text":"in","start":253,"end":255,"id":40},{"text":"deriving","start":256,"end":264,"id":41},{"text":"the","start":265,"end":268,"id":42},{"text":"statistical","start":269,"end":280,"id":43},{"text":"and","start":281,"end":284,"id":44},{"text":"geometric","start":285,"end":294,"id":45},{"text":"properties","start":295,"end":305,"id":46},{"text":"of","start":306,"end":308,"id":47},{"text":"the","start":309,"end":312,"id":48},{"text":"corresponding","start":313,"end":326,"id":49},{"text":"extended","start":327,"end":335,"id":50},{"text":"exponential","start":336,"end":347,"id":51},{"text":"families","start":348,"end":356,"id":52},{"text":".","start":356,"end":357,"id":53}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"For smooth loss functions, it is shown that the difference between the estimator, i.e.\\ the empirical SVM, and the theoretical SVM is asymptotically normal with rate $\\sqrt{n}$. That is, the standardized difference converges weakly to a Gaussian process in the reproducing kernel Hilbert space.","_input_hash":187094571,"_task_hash":780487471,"tokens":[{"text":"For","start":0,"end":3,"id":0},{"text":"smooth","start":4,"end":10,"id":1},{"text":"loss","start":11,"end":15,"id":2},{"text":"functions","start":16,"end":25,"id":3},{"text":",","start":25,"end":26,"id":4},{"text":"it","start":27,"end":29,"id":5},{"text":"is","start":30,"end":32,"id":6},{"text":"shown","start":33,"end":38,"id":7},{"text":"that","start":39,"end":43,"id":8},{"text":"the","start":44,"end":47,"id":9},{"text":"difference","start":48,"end":58,"id":10},{"text":"between","start":59,"end":66,"id":11},{"text":"the","start":67,"end":70,"id":12},{"text":"estimator","start":71,"end":80,"id":13},{"text":",","start":80,"end":81,"id":14},{"text":"i.e.\\","start":82,"end":87,"id":15},{"text":"the","start":88,"end":91,"id":16},{"text":"empirical","start":92,"end":101,"id":17},{"text":"SVM","start":102,"end":105,"id":18},{"text":",","start":105,"end":106,"id":19},{"text":"and","start":107,"end":110,"id":20},{"text":"the","start":111,"end":114,"id":21},{"text":"theoretical","start":115,"end":126,"id":22},{"text":"SVM","start":127,"end":130,"id":23},{"text":"is","start":131,"end":133,"id":24},{"text":"asymptotically","start":134,"end":148,"id":25},{"text":"normal","start":149,"end":155,"id":26},{"text":"with","start":156,"end":160,"id":27},{"text":"rate","start":161,"end":165,"id":28},{"text":"$","start":166,"end":167,"id":29},{"text":"\\sqrt{n}$.","start":167,"end":177,"id":30},{"text":"That","start":178,"end":182,"id":31},{"text":"is","start":183,"end":185,"id":32},{"text":",","start":185,"end":186,"id":33},{"text":"the","start":187,"end":190,"id":34},{"text":"standardized","start":191,"end":203,"id":35},{"text":"difference","start":204,"end":214,"id":36},{"text":"converges","start":215,"end":224,"id":37},{"text":"weakly","start":225,"end":231,"id":38},{"text":"to","start":232,"end":234,"id":39},{"text":"a","start":235,"end":236,"id":40},{"text":"Gaussian","start":237,"end":245,"id":41},{"text":"process","start":246,"end":253,"id":42},{"text":"in","start":254,"end":256,"id":43},{"text":"the","start":257,"end":260,"id":44},{"text":"reproducing","start":261,"end":272,"id":45},{"text":"kernel","start":273,"end":279,"id":46},{"text":"Hilbert","start":280,"end":287,"id":47},{"text":"space","start":288,"end":293,"id":48},{"text":".","start":293,"end":294,"id":49}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":102,"end":105,"token_start":18,"token_end":18,"label":"ALGO","answer":"accept"},{"start":127,"end":130,"token_start":23,"token_end":23,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"this is a common setup in recent bioinformatics experiments, of which we analyze metabolite profiles in different conditions (disease vs. control and treatment vs. untreated) in different tissues (views).","_input_hash":1779557843,"_task_hash":-1576667569,"tokens":[{"text":"this","start":0,"end":4,"id":0},{"text":"is","start":5,"end":7,"id":1},{"text":"a","start":8,"end":9,"id":2},{"text":"common","start":10,"end":16,"id":3},{"text":"setup","start":17,"end":22,"id":4},{"text":"in","start":23,"end":25,"id":5},{"text":"recent","start":26,"end":32,"id":6},{"text":"bioinformatics","start":33,"end":47,"id":7},{"text":"experiments","start":48,"end":59,"id":8},{"text":",","start":59,"end":60,"id":9},{"text":"of","start":61,"end":63,"id":10},{"text":"which","start":64,"end":69,"id":11},{"text":"we","start":70,"end":72,"id":12},{"text":"analyze","start":73,"end":80,"id":13},{"text":"metabolite","start":81,"end":91,"id":14},{"text":"profiles","start":92,"end":100,"id":15},{"text":"in","start":101,"end":103,"id":16},{"text":"different","start":104,"end":113,"id":17},{"text":"conditions","start":114,"end":124,"id":18},{"text":"(","start":125,"end":126,"id":19},{"text":"disease","start":126,"end":133,"id":20},{"text":"vs.","start":134,"end":137,"id":21},{"text":"control","start":138,"end":145,"id":22},{"text":"and","start":146,"end":149,"id":23},{"text":"treatment","start":150,"end":159,"id":24},{"text":"vs.","start":160,"end":163,"id":25},{"text":"untreated","start":164,"end":173,"id":26},{"text":")","start":173,"end":174,"id":27},{"text":"in","start":175,"end":177,"id":28},{"text":"different","start":178,"end":187,"id":29},{"text":"tissues","start":188,"end":195,"id":30},{"text":"(","start":196,"end":197,"id":31},{"text":"views","start":197,"end":202,"id":32},{"text":")","start":202,"end":203,"id":33},{"text":".","start":203,"end":204,"id":34}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We propose a new training strategy and obtain improved generalisation performance and better density estimates in comparative evaluations on several benchmark data sets.","_input_hash":-234269161,"_task_hash":-341524968,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"propose","start":3,"end":10,"id":1},{"text":"a","start":11,"end":12,"id":2},{"text":"new","start":13,"end":16,"id":3},{"text":"training","start":17,"end":25,"id":4},{"text":"strategy","start":26,"end":34,"id":5},{"text":"and","start":35,"end":38,"id":6},{"text":"obtain","start":39,"end":45,"id":7},{"text":"improved","start":46,"end":54,"id":8},{"text":"generalisation","start":55,"end":69,"id":9},{"text":"performance","start":70,"end":81,"id":10},{"text":"and","start":82,"end":85,"id":11},{"text":"better","start":86,"end":92,"id":12},{"text":"density","start":93,"end":100,"id":13},{"text":"estimates","start":101,"end":110,"id":14},{"text":"in","start":111,"end":113,"id":15},{"text":"comparative","start":114,"end":125,"id":16},{"text":"evaluations","start":126,"end":137,"id":17},{"text":"on","start":138,"end":140,"id":18},{"text":"several","start":141,"end":148,"id":19},{"text":"benchmark","start":149,"end":158,"id":20},{"text":"data","start":159,"end":163,"id":21},{"text":"sets","start":164,"end":168,"id":22},{"text":".","start":168,"end":169,"id":23}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We show how higher-order Bayesian decision-making problems, such as optimizing image acquisition in magnetic resonance scanners, can be addressed by querying the SLM posterior covariance, unrelated to the density's mode.","_input_hash":1008301903,"_task_hash":811203434,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"show","start":3,"end":7,"id":1},{"text":"how","start":8,"end":11,"id":2},{"text":"higher","start":12,"end":18,"id":3},{"text":"-","start":18,"end":19,"id":4},{"text":"order","start":19,"end":24,"id":5},{"text":"Bayesian","start":25,"end":33,"id":6},{"text":"decision","start":34,"end":42,"id":7},{"text":"-","start":42,"end":43,"id":8},{"text":"making","start":43,"end":49,"id":9},{"text":"problems","start":50,"end":58,"id":10},{"text":",","start":58,"end":59,"id":11},{"text":"such","start":60,"end":64,"id":12},{"text":"as","start":65,"end":67,"id":13},{"text":"optimizing","start":68,"end":78,"id":14},{"text":"image","start":79,"end":84,"id":15},{"text":"acquisition","start":85,"end":96,"id":16},{"text":"in","start":97,"end":99,"id":17},{"text":"magnetic","start":100,"end":108,"id":18},{"text":"resonance","start":109,"end":118,"id":19},{"text":"scanners","start":119,"end":127,"id":20},{"text":",","start":127,"end":128,"id":21},{"text":"can","start":129,"end":132,"id":22},{"text":"be","start":133,"end":135,"id":23},{"text":"addressed","start":136,"end":145,"id":24},{"text":"by","start":146,"end":148,"id":25},{"text":"querying","start":149,"end":157,"id":26},{"text":"the","start":158,"end":161,"id":27},{"text":"SLM","start":162,"end":165,"id":28},{"text":"posterior","start":166,"end":175,"id":29},{"text":"covariance","start":176,"end":186,"id":30},{"text":",","start":186,"end":187,"id":31},{"text":"unrelated","start":188,"end":197,"id":32},{"text":"to","start":198,"end":200,"id":33},{"text":"the","start":201,"end":204,"id":34},{"text":"density","start":205,"end":212,"id":35},{"text":"'s","start":212,"end":214,"id":36},{"text":"mode","start":215,"end":219,"id":37},{"text":".","start":219,"end":220,"id":38}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We study the inference problem of density ratios and apply a semi-parametric density-ratio estimator to the two-sample homogeneity test.","_input_hash":1098462718,"_task_hash":570824939,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"study","start":3,"end":8,"id":1},{"text":"the","start":9,"end":12,"id":2},{"text":"inference","start":13,"end":22,"id":3},{"text":"problem","start":23,"end":30,"id":4},{"text":"of","start":31,"end":33,"id":5},{"text":"density","start":34,"end":41,"id":6},{"text":"ratios","start":42,"end":48,"id":7},{"text":"and","start":49,"end":52,"id":8},{"text":"apply","start":53,"end":58,"id":9},{"text":"a","start":59,"end":60,"id":10},{"text":"semi","start":61,"end":65,"id":11},{"text":"-","start":65,"end":66,"id":12},{"text":"parametric","start":66,"end":76,"id":13},{"text":"density","start":77,"end":84,"id":14},{"text":"-","start":84,"end":85,"id":15},{"text":"ratio","start":85,"end":90,"id":16},{"text":"estimator","start":91,"end":100,"id":17},{"text":"to","start":101,"end":103,"id":18},{"text":"the","start":104,"end":107,"id":19},{"text":"two","start":108,"end":111,"id":20},{"text":"-","start":111,"end":112,"id":21},{"text":"sample","start":112,"end":118,"id":22},{"text":"homogeneity","start":119,"end":130,"id":23},{"text":"test","start":131,"end":135,"id":24},{"text":".","start":135,"end":136,"id":25}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"simulations|NOUN","word":"simulations","sense":"NOUN","meta":{"score":0.7911999822,"sense":"NOUN"},"_input_hash":-376248382,"_task_hash":-247528788,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"simulations|NOUN","start":0,"end":16,"id":0}]}
{"text":"biological_organisms|NOUN","word":"biological organisms","sense":"NOUN","meta":{"score":0.7682999969,"sense":"NOUN"},"_input_hash":1379092264,"_task_hash":2145074869,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"biological_organisms|NOUN","start":0,"end":25,"id":0}]}
{"text":"Probabilities are uniquely characterized by the mean of the canonical map to the RKHS.","_input_hash":826563427,"_task_hash":2108635876,"tokens":[{"text":"Probabilities","start":0,"end":13,"id":0},{"text":"are","start":14,"end":17,"id":1},{"text":"uniquely","start":18,"end":26,"id":2},{"text":"characterized","start":27,"end":40,"id":3},{"text":"by","start":41,"end":43,"id":4},{"text":"the","start":44,"end":47,"id":5},{"text":"mean","start":48,"end":52,"id":6},{"text":"of","start":53,"end":55,"id":7},{"text":"the","start":56,"end":59,"id":8},{"text":"canonical","start":60,"end":69,"id":9},{"text":"map","start":70,"end":73,"id":10},{"text":"to","start":74,"end":76,"id":11},{"text":"the","start":77,"end":80,"id":12},{"text":"RKHS","start":81,"end":85,"id":13},{"text":".","start":85,"end":86,"id":14}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"it is a vectorial space, it is a set of pointwise defined functions, and the evaluation functional on this set is a continuous mapping.","_input_hash":-1724788611,"_task_hash":1965285496,"tokens":[{"text":"it","start":0,"end":2,"id":0},{"text":"is","start":3,"end":5,"id":1},{"text":"a","start":6,"end":7,"id":2},{"text":"vectorial","start":8,"end":17,"id":3},{"text":"space","start":18,"end":23,"id":4},{"text":",","start":23,"end":24,"id":5},{"text":"it","start":25,"end":27,"id":6},{"text":"is","start":28,"end":30,"id":7},{"text":"a","start":31,"end":32,"id":8},{"text":"set","start":33,"end":36,"id":9},{"text":"of","start":37,"end":39,"id":10},{"text":"pointwise","start":40,"end":49,"id":11},{"text":"defined","start":50,"end":57,"id":12},{"text":"functions","start":58,"end":67,"id":13},{"text":",","start":67,"end":68,"id":14},{"text":"and","start":69,"end":72,"id":15},{"text":"the","start":73,"end":76,"id":16},{"text":"evaluation","start":77,"end":87,"id":17},{"text":"functional","start":88,"end":98,"id":18},{"text":"on","start":99,"end":101,"id":19},{"text":"this","start":102,"end":106,"id":20},{"text":"set","start":107,"end":110,"id":21},{"text":"is","start":111,"end":113,"id":22},{"text":"a","start":114,"end":115,"id":23},{"text":"continuous","start":116,"end":126,"id":24},{"text":"mapping","start":127,"end":134,"id":25},{"text":".","start":134,"end":135,"id":26}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":8,"end":17,"token_start":3,"token_end":3,"label":"ALGO","answer":"reject"}],"answer":"reject"}
{"text":"We define a family of smooth densities and conditional densities by second order exponential models, i.e., by maximizing conditional entropy subject to first and second statistical moments.","_input_hash":-1906282048,"_task_hash":-557100930,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"define","start":3,"end":9,"id":1},{"text":"a","start":10,"end":11,"id":2},{"text":"family","start":12,"end":18,"id":3},{"text":"of","start":19,"end":21,"id":4},{"text":"smooth","start":22,"end":28,"id":5},{"text":"densities","start":29,"end":38,"id":6},{"text":"and","start":39,"end":42,"id":7},{"text":"conditional","start":43,"end":54,"id":8},{"text":"densities","start":55,"end":64,"id":9},{"text":"by","start":65,"end":67,"id":10},{"text":"second","start":68,"end":74,"id":11},{"text":"order","start":75,"end":80,"id":12},{"text":"exponential","start":81,"end":92,"id":13},{"text":"models","start":93,"end":99,"id":14},{"text":",","start":99,"end":100,"id":15},{"text":"i.e.","start":101,"end":105,"id":16},{"text":",","start":105,"end":106,"id":17},{"text":"by","start":107,"end":109,"id":18},{"text":"maximizing","start":110,"end":120,"id":19},{"text":"conditional","start":121,"end":132,"id":20},{"text":"entropy","start":133,"end":140,"id":21},{"text":"subject","start":141,"end":148,"id":22},{"text":"to","start":149,"end":151,"id":23},{"text":"first","start":152,"end":157,"id":24},{"text":"and","start":158,"end":161,"id":25},{"text":"second","start":162,"end":168,"id":26},{"text":"statistical","start":169,"end":180,"id":27},{"text":"moments","start":181,"end":188,"id":28},{"text":".","start":188,"end":189,"id":29}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"In this contribution we analyze the performance of a particular method -- online centroid anomaly detection -- in the presence of adversarial noise.","_input_hash":1971747661,"_task_hash":689003305,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"this","start":3,"end":7,"id":1},{"text":"contribution","start":8,"end":20,"id":2},{"text":"we","start":21,"end":23,"id":3},{"text":"analyze","start":24,"end":31,"id":4},{"text":"the","start":32,"end":35,"id":5},{"text":"performance","start":36,"end":47,"id":6},{"text":"of","start":48,"end":50,"id":7},{"text":"a","start":51,"end":52,"id":8},{"text":"particular","start":53,"end":63,"id":9},{"text":"method","start":64,"end":70,"id":10},{"text":"--","start":71,"end":73,"id":11},{"text":"online","start":74,"end":80,"id":12},{"text":"centroid","start":81,"end":89,"id":13},{"text":"anomaly","start":90,"end":97,"id":14},{"text":"detection","start":98,"end":107,"id":15},{"text":"--","start":108,"end":110,"id":16},{"text":"in","start":111,"end":113,"id":17},{"text":"the","start":114,"end":117,"id":18},{"text":"presence","start":118,"end":126,"id":19},{"text":"of","start":127,"end":129,"id":20},{"text":"adversarial","start":130,"end":141,"id":21},{"text":"noise","start":142,"end":147,"id":22},{"text":".","start":147,"end":148,"id":23}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"As common in real applications, the choice of the regularization parameter may depend on the data.","_input_hash":-548434850,"_task_hash":-1935574815,"tokens":[{"text":"As","start":0,"end":2,"id":0},{"text":"common","start":3,"end":9,"id":1},{"text":"in","start":10,"end":12,"id":2},{"text":"real","start":13,"end":17,"id":3},{"text":"applications","start":18,"end":30,"id":4},{"text":",","start":30,"end":31,"id":5},{"text":"the","start":32,"end":35,"id":6},{"text":"choice","start":36,"end":42,"id":7},{"text":"of","start":43,"end":45,"id":8},{"text":"the","start":46,"end":49,"id":9},{"text":"regularization","start":50,"end":64,"id":10},{"text":"parameter","start":65,"end":74,"id":11},{"text":"may","start":75,"end":78,"id":12},{"text":"depend","start":79,"end":85,"id":13},{"text":"on","start":86,"end":88,"id":14},{"text":"the","start":89,"end":92,"id":15},{"text":"data","start":93,"end":97,"id":16},{"text":".","start":97,"end":98,"id":17}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We provide an overview of numerous data-analysis methods which take advantage of reproducing kernel Hilbert spaces and discuss the idea of combining several kernels to improve the performance on certain tasks.","_input_hash":982284119,"_task_hash":313016093,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"provide","start":3,"end":10,"id":1},{"text":"an","start":11,"end":13,"id":2},{"text":"overview","start":14,"end":22,"id":3},{"text":"of","start":23,"end":25,"id":4},{"text":"numerous","start":26,"end":34,"id":5},{"text":"data","start":35,"end":39,"id":6},{"text":"-","start":39,"end":40,"id":7},{"text":"analysis","start":40,"end":48,"id":8},{"text":"methods","start":49,"end":56,"id":9},{"text":"which","start":57,"end":62,"id":10},{"text":"take","start":63,"end":67,"id":11},{"text":"advantage","start":68,"end":77,"id":12},{"text":"of","start":78,"end":80,"id":13},{"text":"reproducing","start":81,"end":92,"id":14},{"text":"kernel","start":93,"end":99,"id":15},{"text":"Hilbert","start":100,"end":107,"id":16},{"text":"spaces","start":108,"end":114,"id":17},{"text":"and","start":115,"end":118,"id":18},{"text":"discuss","start":119,"end":126,"id":19},{"text":"the","start":127,"end":130,"id":20},{"text":"idea","start":131,"end":135,"id":21},{"text":"of","start":136,"end":138,"id":22},{"text":"combining","start":139,"end":148,"id":23},{"text":"several","start":149,"end":156,"id":24},{"text":"kernels","start":157,"end":164,"id":25},{"text":"to","start":165,"end":167,"id":26},{"text":"improve","start":168,"end":175,"id":27},{"text":"the","start":176,"end":179,"id":28},{"text":"performance","start":180,"end":191,"id":29},{"text":"on","start":192,"end":194,"id":30},{"text":"certain","start":195,"end":202,"id":31},{"text":"tasks","start":203,"end":208,"id":32},{"text":".","start":208,"end":209,"id":33}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":35,"end":56,"token_start":6,"token_end":9,"label":"ALGO","answer":"reject"}],"answer":"reject"}
{"text":"The impact of missing data is then investigated and decision forrests are found to improve the results.","_input_hash":-1496861163,"_task_hash":595301050,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"impact","start":4,"end":10,"id":1},{"text":"of","start":11,"end":13,"id":2},{"text":"missing","start":14,"end":21,"id":3},{"text":"data","start":22,"end":26,"id":4},{"text":"is","start":27,"end":29,"id":5},{"text":"then","start":30,"end":34,"id":6},{"text":"investigated","start":35,"end":47,"id":7},{"text":"and","start":48,"end":51,"id":8},{"text":"decision","start":52,"end":60,"id":9},{"text":"forrests","start":61,"end":69,"id":10},{"text":"are","start":70,"end":73,"id":11},{"text":"found","start":74,"end":79,"id":12},{"text":"to","start":80,"end":82,"id":13},{"text":"improve","start":83,"end":90,"id":14},{"text":"the","start":91,"end":94,"id":15},{"text":"results","start":95,"end":102,"id":16},{"text":".","start":102,"end":103,"id":17}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":52,"end":69,"token_start":9,"token_end":10,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"However, in many applications, additional conditions on the structure of the regression vector and its sparsity pattern are available.","_input_hash":1534893567,"_task_hash":357376953,"tokens":[{"text":"However","start":0,"end":7,"id":0},{"text":",","start":7,"end":8,"id":1},{"text":"in","start":9,"end":11,"id":2},{"text":"many","start":12,"end":16,"id":3},{"text":"applications","start":17,"end":29,"id":4},{"text":",","start":29,"end":30,"id":5},{"text":"additional","start":31,"end":41,"id":6},{"text":"conditions","start":42,"end":52,"id":7},{"text":"on","start":53,"end":55,"id":8},{"text":"the","start":56,"end":59,"id":9},{"text":"structure","start":60,"end":69,"id":10},{"text":"of","start":70,"end":72,"id":11},{"text":"the","start":73,"end":76,"id":12},{"text":"regression","start":77,"end":87,"id":13},{"text":"vector","start":88,"end":94,"id":14},{"text":"and","start":95,"end":98,"id":15},{"text":"its","start":99,"end":102,"id":16},{"text":"sparsity","start":103,"end":111,"id":17},{"text":"pattern","start":112,"end":119,"id":18},{"text":"are","start":120,"end":123,"id":19},{"text":"available","start":124,"end":133,"id":20},{"text":".","start":133,"end":134,"id":21}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"The theorem appears to be new but is closely related to results achieved by other investigators.","_input_hash":-1493657758,"_task_hash":1946779967,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"theorem","start":4,"end":11,"id":1},{"text":"appears","start":12,"end":19,"id":2},{"text":"to","start":20,"end":22,"id":3},{"text":"be","start":23,"end":25,"id":4},{"text":"new","start":26,"end":29,"id":5},{"text":"but","start":30,"end":33,"id":6},{"text":"is","start":34,"end":36,"id":7},{"text":"closely","start":37,"end":44,"id":8},{"text":"related","start":45,"end":52,"id":9},{"text":"to","start":53,"end":55,"id":10},{"text":"results","start":56,"end":63,"id":11},{"text":"achieved","start":64,"end":72,"id":12},{"text":"by","start":73,"end":75,"id":13},{"text":"other","start":76,"end":81,"id":14},{"text":"investigators","start":82,"end":95,"id":15},{"text":".","start":95,"end":96,"id":16}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"neural_networks|NOUN","word":"neural networks","sense":"NOUN","meta":{"score":0.8971999884,"sense":"NOUN"},"_input_hash":-160342458,"_task_hash":-1395219987,"_session_id":null,"_view_id":"html","answer":"accept","spans":[],"tokens":[{"text":"neural_networks|NOUN","start":0,"end":20,"id":0}]}
{"text":"Through the years, clinicians have determined combinations of different fluorescent markers which generate relatively known expression patterns for specific subtypes of leukemia and lymphoma -- cancers of the hematopoietic system.","_input_hash":-770091940,"_task_hash":2008927478,"tokens":[{"text":"Through","start":0,"end":7,"id":0},{"text":"the","start":8,"end":11,"id":1},{"text":"years","start":12,"end":17,"id":2},{"text":",","start":17,"end":18,"id":3},{"text":"clinicians","start":19,"end":29,"id":4},{"text":"have","start":30,"end":34,"id":5},{"text":"determined","start":35,"end":45,"id":6},{"text":"combinations","start":46,"end":58,"id":7},{"text":"of","start":59,"end":61,"id":8},{"text":"different","start":62,"end":71,"id":9},{"text":"fluorescent","start":72,"end":83,"id":10},{"text":"markers","start":84,"end":91,"id":11},{"text":"which","start":92,"end":97,"id":12},{"text":"generate","start":98,"end":106,"id":13},{"text":"relatively","start":107,"end":117,"id":14},{"text":"known","start":118,"end":123,"id":15},{"text":"expression","start":124,"end":134,"id":16},{"text":"patterns","start":135,"end":143,"id":17},{"text":"for","start":144,"end":147,"id":18},{"text":"specific","start":148,"end":156,"id":19},{"text":"subtypes","start":157,"end":165,"id":20},{"text":"of","start":166,"end":168,"id":21},{"text":"leukemia","start":169,"end":177,"id":22},{"text":"and","start":178,"end":181,"id":23},{"text":"lymphoma","start":182,"end":190,"id":24},{"text":"--","start":191,"end":193,"id":25},{"text":"cancers","start":194,"end":201,"id":26},{"text":"of","start":202,"end":204,"id":27},{"text":"the","start":205,"end":208,"id":28},{"text":"hematopoietic","start":209,"end":222,"id":29},{"text":"system","start":223,"end":229,"id":30},{"text":".","start":229,"end":230,"id":31}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"We define a class of Euclidean distances on weighted graphs, enabling to perform thermodynamic soft graph clustering.","_input_hash":1698259657,"_task_hash":-1710680142,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"define","start":3,"end":9,"id":1},{"text":"a","start":10,"end":11,"id":2},{"text":"class","start":12,"end":17,"id":3},{"text":"of","start":18,"end":20,"id":4},{"text":"Euclidean","start":21,"end":30,"id":5},{"text":"distances","start":31,"end":40,"id":6},{"text":"on","start":41,"end":43,"id":7},{"text":"weighted","start":44,"end":52,"id":8},{"text":"graphs","start":53,"end":59,"id":9},{"text":",","start":59,"end":60,"id":10},{"text":"enabling","start":61,"end":69,"id":11},{"text":"to","start":70,"end":72,"id":12},{"text":"perform","start":73,"end":80,"id":13},{"text":"thermodynamic","start":81,"end":94,"id":14},{"text":"soft","start":95,"end":99,"id":15},{"text":"graph","start":100,"end":105,"id":16},{"text":"clustering","start":106,"end":116,"id":17},{"text":".","start":116,"end":117,"id":18}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"human_intelligence|NOUN","word":"human intelligence","sense":"NOUN","meta":{"score":0.7709000111,"sense":"NOUN"},"_input_hash":759673472,"_task_hash":1745561240,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"human_intelligence|NOUN","start":0,"end":23,"id":0}]}
{"text":"Autoencoder neural network is implemented to estimate the missing data.","_input_hash":909308757,"_task_hash":1784407609,"tokens":[{"text":"Autoencoder","start":0,"end":11,"id":0},{"text":"neural","start":12,"end":18,"id":1},{"text":"network","start":19,"end":26,"id":2},{"text":"is","start":27,"end":29,"id":3},{"text":"implemented","start":30,"end":41,"id":4},{"text":"to","start":42,"end":44,"id":5},{"text":"estimate","start":45,"end":53,"id":6},{"text":"the","start":54,"end":57,"id":7},{"text":"missing","start":58,"end":65,"id":8},{"text":"data","start":66,"end":70,"id":9},{"text":".","start":70,"end":71,"id":10}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":0,"end":26,"token_start":0,"token_end":2,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"complex_math|NOUN","word":"complex math","sense":"NOUN","meta":{"score":0.780099988,"sense":"NOUN"},"_input_hash":-1911515581,"_task_hash":-830619799,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"complex_math|NOUN","start":0,"end":17,"id":0}]}
{"text":"simple_algorithms|NOUN","word":"simple algorithms","sense":"NOUN","meta":{"score":0.7637000084,"sense":"NOUN"},"_input_hash":1093452053,"_task_hash":-313010988,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"simple_algorithms|NOUN","start":0,"end":22,"id":0}]}
{"text":"FPGAs|NOUN","word":"FPGAs","sense":"NOUN","meta":{"score":0.7603999972,"sense":"NOUN"},"_input_hash":380850520,"_task_hash":1470416076,"_session_id":null,"_view_id":"html","answer":"accept","spans":[],"tokens":[{"text":"FPGAs|NOUN","start":0,"end":10,"id":0}]}
{"text":"Genetic algorithm is implemented for network optimization and estimating the missing data.","_input_hash":-1636818188,"_task_hash":-1540476072,"tokens":[{"text":"Genetic","start":0,"end":7,"id":0},{"text":"algorithm","start":8,"end":17,"id":1},{"text":"is","start":18,"end":20,"id":2},{"text":"implemented","start":21,"end":32,"id":3},{"text":"for","start":33,"end":36,"id":4},{"text":"network","start":37,"end":44,"id":5},{"text":"optimization","start":45,"end":57,"id":6},{"text":"and","start":58,"end":61,"id":7},{"text":"estimating","start":62,"end":72,"id":8},{"text":"the","start":73,"end":76,"id":9},{"text":"missing","start":77,"end":84,"id":10},{"text":"data","start":85,"end":89,"id":11},{"text":".","start":89,"end":90,"id":12}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":0,"end":17,"token_start":0,"token_end":1,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"In this paper, we consider the problem of \"hyper-sparse aggregation\".","_input_hash":1984426396,"_task_hash":1798868814,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"this","start":3,"end":7,"id":1},{"text":"paper","start":8,"end":13,"id":2},{"text":",","start":13,"end":14,"id":3},{"text":"we","start":15,"end":17,"id":4},{"text":"consider","start":18,"end":26,"id":5},{"text":"the","start":27,"end":30,"id":6},{"text":"problem","start":31,"end":38,"id":7},{"text":"of","start":39,"end":41,"id":8},{"text":"\"","start":42,"end":43,"id":9},{"text":"hyper","start":43,"end":48,"id":10},{"text":"-","start":48,"end":49,"id":11},{"text":"sparse","start":49,"end":55,"id":12},{"text":"aggregation","start":56,"end":67,"id":13},{"text":"\"","start":67,"end":68,"id":14},{"text":".","start":68,"end":69,"id":15}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"The embedding function is then determined by solving a semidefinite program which has an interesting connection to the soft-margin linear binary support vector machine classifier.","_input_hash":1743355789,"_task_hash":-1834582324,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"embedding","start":4,"end":13,"id":1},{"text":"function","start":14,"end":22,"id":2},{"text":"is","start":23,"end":25,"id":3},{"text":"then","start":26,"end":30,"id":4},{"text":"determined","start":31,"end":41,"id":5},{"text":"by","start":42,"end":44,"id":6},{"text":"solving","start":45,"end":52,"id":7},{"text":"a","start":53,"end":54,"id":8},{"text":"semidefinite","start":55,"end":67,"id":9},{"text":"program","start":68,"end":75,"id":10},{"text":"which","start":76,"end":81,"id":11},{"text":"has","start":82,"end":85,"id":12},{"text":"an","start":86,"end":88,"id":13},{"text":"interesting","start":89,"end":100,"id":14},{"text":"connection","start":101,"end":111,"id":15},{"text":"to","start":112,"end":114,"id":16},{"text":"the","start":115,"end":118,"id":17},{"text":"soft","start":119,"end":123,"id":18},{"text":"-","start":123,"end":124,"id":19},{"text":"margin","start":124,"end":130,"id":20},{"text":"linear","start":131,"end":137,"id":21},{"text":"binary","start":138,"end":144,"id":22},{"text":"support","start":145,"end":152,"id":23},{"text":"vector","start":153,"end":159,"id":24},{"text":"machine","start":160,"end":167,"id":25},{"text":"classifier","start":168,"end":178,"id":26},{"text":".","start":178,"end":179,"id":27}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"In nonparametric classification and regression problems, regularized kernel methods, in particular support vector machines, attract much attention in theoretical and in applied statistics.","_input_hash":-318180059,"_task_hash":396971579,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"nonparametric","start":3,"end":16,"id":1},{"text":"classification","start":17,"end":31,"id":2},{"text":"and","start":32,"end":35,"id":3},{"text":"regression","start":36,"end":46,"id":4},{"text":"problems","start":47,"end":55,"id":5},{"text":",","start":55,"end":56,"id":6},{"text":"regularized","start":57,"end":68,"id":7},{"text":"kernel","start":69,"end":75,"id":8},{"text":"methods","start":76,"end":83,"id":9},{"text":",","start":83,"end":84,"id":10},{"text":"in","start":85,"end":87,"id":11},{"text":"particular","start":88,"end":98,"id":12},{"text":"support","start":99,"end":106,"id":13},{"text":"vector","start":107,"end":113,"id":14},{"text":"machines","start":114,"end":122,"id":15},{"text":",","start":122,"end":123,"id":16},{"text":"attract","start":124,"end":131,"id":17},{"text":"much","start":132,"end":136,"id":18},{"text":"attention","start":137,"end":146,"id":19},{"text":"in","start":147,"end":149,"id":20},{"text":"theoretical","start":150,"end":161,"id":21},{"text":"and","start":162,"end":165,"id":22},{"text":"in","start":166,"end":168,"id":23},{"text":"applied","start":169,"end":176,"id":24},{"text":"statistics","start":177,"end":187,"id":25},{"text":".","start":187,"end":188,"id":26}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":99,"end":122,"token_start":13,"token_end":15,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We prove that finding a maximum weight spanning forest with restricted tree size is NP-hard, and develop an approximation algorithm for this problem.","_input_hash":-1043233372,"_task_hash":961878719,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"prove","start":3,"end":8,"id":1},{"text":"that","start":9,"end":13,"id":2},{"text":"finding","start":14,"end":21,"id":3},{"text":"a","start":22,"end":23,"id":4},{"text":"maximum","start":24,"end":31,"id":5},{"text":"weight","start":32,"end":38,"id":6},{"text":"spanning","start":39,"end":47,"id":7},{"text":"forest","start":48,"end":54,"id":8},{"text":"with","start":55,"end":59,"id":9},{"text":"restricted","start":60,"end":70,"id":10},{"text":"tree","start":71,"end":75,"id":11},{"text":"size","start":76,"end":80,"id":12},{"text":"is","start":81,"end":83,"id":13},{"text":"NP","start":84,"end":86,"id":14},{"text":"-","start":86,"end":87,"id":15},{"text":"hard","start":87,"end":91,"id":16},{"text":",","start":91,"end":92,"id":17},{"text":"and","start":93,"end":96,"id":18},{"text":"develop","start":97,"end":104,"id":19},{"text":"an","start":105,"end":107,"id":20},{"text":"approximation","start":108,"end":121,"id":21},{"text":"algorithm","start":122,"end":131,"id":22},{"text":"for","start":132,"end":135,"id":23},{"text":"this","start":136,"end":140,"id":24},{"text":"problem","start":141,"end":148,"id":25},{"text":".","start":148,"end":149,"id":26}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":48,"end":54,"token_start":8,"token_end":8,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"The equations show that optimization is computationally expensive.","_input_hash":1928351634,"_task_hash":1322417302,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"equations","start":4,"end":13,"id":1},{"text":"show","start":14,"end":18,"id":2},{"text":"that","start":19,"end":23,"id":3},{"text":"optimization","start":24,"end":36,"id":4},{"text":"is","start":37,"end":39,"id":5},{"text":"computationally","start":40,"end":55,"id":6},{"text":"expensive","start":56,"end":65,"id":7},{"text":".","start":65,"end":66,"id":8}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Completed by a point at infinity, they can be viewed as subtrees of the Bruhat-Tits tree associated to the $p$-adic projective line.","_input_hash":1227531399,"_task_hash":-1429731402,"tokens":[{"text":"Completed","start":0,"end":9,"id":0},{"text":"by","start":10,"end":12,"id":1},{"text":"a","start":13,"end":14,"id":2},{"text":"point","start":15,"end":20,"id":3},{"text":"at","start":21,"end":23,"id":4},{"text":"infinity","start":24,"end":32,"id":5},{"text":",","start":32,"end":33,"id":6},{"text":"they","start":34,"end":38,"id":7},{"text":"can","start":39,"end":42,"id":8},{"text":"be","start":43,"end":45,"id":9},{"text":"viewed","start":46,"end":52,"id":10},{"text":"as","start":53,"end":55,"id":11},{"text":"subtrees","start":56,"end":64,"id":12},{"text":"of","start":65,"end":67,"id":13},{"text":"the","start":68,"end":71,"id":14},{"text":"Bruhat","start":72,"end":78,"id":15},{"text":"-","start":78,"end":79,"id":16},{"text":"Tits","start":79,"end":83,"id":17},{"text":"tree","start":84,"end":88,"id":18},{"text":"associated","start":89,"end":99,"id":19},{"text":"to","start":100,"end":102,"id":20},{"text":"the","start":103,"end":106,"id":21},{"text":"$","start":107,"end":108,"id":22},{"text":"p$-adic","start":108,"end":115,"id":23},{"text":"projective","start":116,"end":126,"id":24},{"text":"line","start":127,"end":131,"id":25},{"text":".","start":131,"end":132,"id":26}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"To overcome both of these problems, we propose to compute the weight vectors using a low-dimensional neighborhood representation.","_input_hash":-371971599,"_task_hash":1006070794,"tokens":[{"text":"To","start":0,"end":2,"id":0},{"text":"overcome","start":3,"end":11,"id":1},{"text":"both","start":12,"end":16,"id":2},{"text":"of","start":17,"end":19,"id":3},{"text":"these","start":20,"end":25,"id":4},{"text":"problems","start":26,"end":34,"id":5},{"text":",","start":34,"end":35,"id":6},{"text":"we","start":36,"end":38,"id":7},{"text":"propose","start":39,"end":46,"id":8},{"text":"to","start":47,"end":49,"id":9},{"text":"compute","start":50,"end":57,"id":10},{"text":"the","start":58,"end":61,"id":11},{"text":"weight","start":62,"end":68,"id":12},{"text":"vectors","start":69,"end":76,"id":13},{"text":"using","start":77,"end":82,"id":14},{"text":"a","start":83,"end":84,"id":15},{"text":"low","start":85,"end":88,"id":16},{"text":"-","start":88,"end":89,"id":17},{"text":"dimensional","start":89,"end":100,"id":18},{"text":"neighborhood","start":101,"end":113,"id":19},{"text":"representation","start":114,"end":128,"id":20},{"text":".","start":128,"end":129,"id":21}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"2001) in p-adic geometry.","_input_hash":-933078138,"_task_hash":-914511922,"tokens":[{"text":"2001","start":0,"end":4,"id":0},{"text":")","start":4,"end":5,"id":1},{"text":"in","start":6,"end":8,"id":2},{"text":"p","start":9,"end":10,"id":3},{"text":"-","start":10,"end":11,"id":4},{"text":"adic","start":11,"end":15,"id":5},{"text":"geometry","start":16,"end":24,"id":6},{"text":".","start":24,"end":25,"id":7}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"  We consider the case of one binary and one real-valued variable where the method can distinguish between cause and effect.","_input_hash":470315968,"_task_hash":-613938129,"tokens":[{"text":"  ","start":0,"end":2,"id":0},{"text":"We","start":2,"end":4,"id":1},{"text":"consider","start":5,"end":13,"id":2},{"text":"the","start":14,"end":17,"id":3},{"text":"case","start":18,"end":22,"id":4},{"text":"of","start":23,"end":25,"id":5},{"text":"one","start":26,"end":29,"id":6},{"text":"binary","start":30,"end":36,"id":7},{"text":"and","start":37,"end":40,"id":8},{"text":"one","start":41,"end":44,"id":9},{"text":"real","start":45,"end":49,"id":10},{"text":"-","start":49,"end":50,"id":11},{"text":"valued","start":50,"end":56,"id":12},{"text":"variable","start":57,"end":65,"id":13},{"text":"where","start":66,"end":71,"id":14},{"text":"the","start":72,"end":75,"id":15},{"text":"method","start":76,"end":82,"id":16},{"text":"can","start":83,"end":86,"id":17},{"text":"distinguish","start":87,"end":98,"id":18},{"text":"between","start":99,"end":106,"id":19},{"text":"cause","start":107,"end":112,"id":20},{"text":"and","start":113,"end":116,"id":21},{"text":"effect","start":117,"end":123,"id":22},{"text":".","start":123,"end":124,"id":23}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":72,"end":82,"token_start":15,"token_end":16,"label":"ALGO","answer":"reject"}],"answer":"reject"}
{"text":"Computational evidence in support of the claim is provided, together with an outline of the theoretical explanation.","_input_hash":-630027830,"_task_hash":1994262169,"tokens":[{"text":"Computational","start":0,"end":13,"id":0},{"text":"evidence","start":14,"end":22,"id":1},{"text":"in","start":23,"end":25,"id":2},{"text":"support","start":26,"end":33,"id":3},{"text":"of","start":34,"end":36,"id":4},{"text":"the","start":37,"end":40,"id":5},{"text":"claim","start":41,"end":46,"id":6},{"text":"is","start":47,"end":49,"id":7},{"text":"provided","start":50,"end":58,"id":8},{"text":",","start":58,"end":59,"id":9},{"text":"together","start":60,"end":68,"id":10},{"text":"with","start":69,"end":73,"id":11},{"text":"an","start":74,"end":76,"id":12},{"text":"outline","start":77,"end":84,"id":13},{"text":"of","start":85,"end":87,"id":14},{"text":"the","start":88,"end":91,"id":15},{"text":"theoretical","start":92,"end":103,"id":16},{"text":"explanation","start":104,"end":115,"id":17},{"text":".","start":115,"end":116,"id":18}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"learning_algorithm|NOUN","word":"learning algorithm","sense":"NOUN","meta":{"score":0.7911999822,"sense":"NOUN"},"_input_hash":-527166335,"_task_hash":-936644162,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"learning_algorithm|NOUN","start":0,"end":23,"id":0}]}
{"text":"The performance for learning drifting concepts of one of the presented algorithms is analysed and compared with the Baldi-Chauvin algorithm in the same situations.","_input_hash":1678323752,"_task_hash":-449989881,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"performance","start":4,"end":15,"id":1},{"text":"for","start":16,"end":19,"id":2},{"text":"learning","start":20,"end":28,"id":3},{"text":"drifting","start":29,"end":37,"id":4},{"text":"concepts","start":38,"end":46,"id":5},{"text":"of","start":47,"end":49,"id":6},{"text":"one","start":50,"end":53,"id":7},{"text":"of","start":54,"end":56,"id":8},{"text":"the","start":57,"end":60,"id":9},{"text":"presented","start":61,"end":70,"id":10},{"text":"algorithms","start":71,"end":81,"id":11},{"text":"is","start":82,"end":84,"id":12},{"text":"analysed","start":85,"end":93,"id":13},{"text":"and","start":94,"end":97,"id":14},{"text":"compared","start":98,"end":106,"id":15},{"text":"with","start":107,"end":111,"id":16},{"text":"the","start":112,"end":115,"id":17},{"text":"Baldi","start":116,"end":121,"id":18},{"text":"-","start":121,"end":122,"id":19},{"text":"Chauvin","start":122,"end":129,"id":20},{"text":"algorithm","start":130,"end":139,"id":21},{"text":"in","start":140,"end":142,"id":22},{"text":"the","start":143,"end":146,"id":23},{"text":"same","start":147,"end":151,"id":24},{"text":"situations","start":152,"end":162,"id":25},{"text":".","start":162,"end":163,"id":26}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":116,"end":139,"token_start":18,"token_end":21,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"In contrast to standard raw feature weighting, FIRM takes the underlying correlation structure of the features into account.","_input_hash":2114874348,"_task_hash":794646834,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"contrast","start":3,"end":11,"id":1},{"text":"to","start":12,"end":14,"id":2},{"text":"standard","start":15,"end":23,"id":3},{"text":"raw","start":24,"end":27,"id":4},{"text":"feature","start":28,"end":35,"id":5},{"text":"weighting","start":36,"end":45,"id":6},{"text":",","start":45,"end":46,"id":7},{"text":"FIRM","start":47,"end":51,"id":8},{"text":"takes","start":52,"end":57,"id":9},{"text":"the","start":58,"end":61,"id":10},{"text":"underlying","start":62,"end":72,"id":11},{"text":"correlation","start":73,"end":84,"id":12},{"text":"structure","start":85,"end":94,"id":13},{"text":"of","start":95,"end":97,"id":14},{"text":"the","start":98,"end":101,"id":15},{"text":"features","start":102,"end":110,"id":16},{"text":"into","start":111,"end":115,"id":17},{"text":"account","start":116,"end":123,"id":18},{"text":".","start":123,"end":124,"id":19}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"Motivated by an existing graph partitioning framework, we derive relationships between optimizing relevance information, as defined in the Information Bottleneck method, and the regularized cut in a K-partitioned graph.","_input_hash":909894230,"_task_hash":1668761734,"tokens":[{"text":"Motivated","start":0,"end":9,"id":0},{"text":"by","start":10,"end":12,"id":1},{"text":"an","start":13,"end":15,"id":2},{"text":"existing","start":16,"end":24,"id":3},{"text":"graph","start":25,"end":30,"id":4},{"text":"partitioning","start":31,"end":43,"id":5},{"text":"framework","start":44,"end":53,"id":6},{"text":",","start":53,"end":54,"id":7},{"text":"we","start":55,"end":57,"id":8},{"text":"derive","start":58,"end":64,"id":9},{"text":"relationships","start":65,"end":78,"id":10},{"text":"between","start":79,"end":86,"id":11},{"text":"optimizing","start":87,"end":97,"id":12},{"text":"relevance","start":98,"end":107,"id":13},{"text":"information","start":108,"end":119,"id":14},{"text":",","start":119,"end":120,"id":15},{"text":"as","start":121,"end":123,"id":16},{"text":"defined","start":124,"end":131,"id":17},{"text":"in","start":132,"end":134,"id":18},{"text":"the","start":135,"end":138,"id":19},{"text":"Information","start":139,"end":150,"id":20},{"text":"Bottleneck","start":151,"end":161,"id":21},{"text":"method","start":162,"end":168,"id":22},{"text":",","start":168,"end":169,"id":23},{"text":"and","start":170,"end":173,"id":24},{"text":"the","start":174,"end":177,"id":25},{"text":"regularized","start":178,"end":189,"id":26},{"text":"cut","start":190,"end":193,"id":27},{"text":"in","start":194,"end":196,"id":28},{"text":"a","start":197,"end":198,"id":29},{"text":"K","start":199,"end":200,"id":30},{"text":"-","start":200,"end":201,"id":31},{"text":"partitioned","start":201,"end":212,"id":32},{"text":"graph","start":213,"end":218,"id":33},{"text":".","start":218,"end":219,"id":34}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We derive an approximate maximum-likelihood procedure for parameter estimation, which relies on variational methods to handle intractable posterior expectations.","_input_hash":-1403220553,"_task_hash":251824613,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"derive","start":3,"end":9,"id":1},{"text":"an","start":10,"end":12,"id":2},{"text":"approximate","start":13,"end":24,"id":3},{"text":"maximum","start":25,"end":32,"id":4},{"text":"-","start":32,"end":33,"id":5},{"text":"likelihood","start":33,"end":43,"id":6},{"text":"procedure","start":44,"end":53,"id":7},{"text":"for","start":54,"end":57,"id":8},{"text":"parameter","start":58,"end":67,"id":9},{"text":"estimation","start":68,"end":78,"id":10},{"text":",","start":78,"end":79,"id":11},{"text":"which","start":80,"end":85,"id":12},{"text":"relies","start":86,"end":92,"id":13},{"text":"on","start":93,"end":95,"id":14},{"text":"variational","start":96,"end":107,"id":15},{"text":"methods","start":108,"end":115,"id":16},{"text":"to","start":116,"end":118,"id":17},{"text":"handle","start":119,"end":125,"id":18},{"text":"intractable","start":126,"end":137,"id":19},{"text":"posterior","start":138,"end":147,"id":20},{"text":"expectations","start":148,"end":160,"id":21},{"text":".","start":160,"end":161,"id":22}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"In the last few years, many different performance measures have been introduced to overcome the weakness of the most natural metric, the Accuracy.","_input_hash":498447663,"_task_hash":-1036298045,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"the","start":3,"end":6,"id":1},{"text":"last","start":7,"end":11,"id":2},{"text":"few","start":12,"end":15,"id":3},{"text":"years","start":16,"end":21,"id":4},{"text":",","start":21,"end":22,"id":5},{"text":"many","start":23,"end":27,"id":6},{"text":"different","start":28,"end":37,"id":7},{"text":"performance","start":38,"end":49,"id":8},{"text":"measures","start":50,"end":58,"id":9},{"text":"have","start":59,"end":63,"id":10},{"text":"been","start":64,"end":68,"id":11},{"text":"introduced","start":69,"end":79,"id":12},{"text":"to","start":80,"end":82,"id":13},{"text":"overcome","start":83,"end":91,"id":14},{"text":"the","start":92,"end":95,"id":15},{"text":"weakness","start":96,"end":104,"id":16},{"text":"of","start":105,"end":107,"id":17},{"text":"the","start":108,"end":111,"id":18},{"text":"most","start":112,"end":116,"id":19},{"text":"natural","start":117,"end":124,"id":20},{"text":"metric","start":125,"end":131,"id":21},{"text":",","start":131,"end":132,"id":22},{"text":"the","start":133,"end":136,"id":23},{"text":"Accuracy","start":137,"end":145,"id":24},{"text":".","start":145,"end":146,"id":25}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"An interesting consequence is that the size of the test sample is not required to grow to infinity for the consistency of the cross-validation procedure.","_input_hash":-1544597140,"_task_hash":-844183427,"tokens":[{"text":"An","start":0,"end":2,"id":0},{"text":"interesting","start":3,"end":14,"id":1},{"text":"consequence","start":15,"end":26,"id":2},{"text":"is","start":27,"end":29,"id":3},{"text":"that","start":30,"end":34,"id":4},{"text":"the","start":35,"end":38,"id":5},{"text":"size","start":39,"end":43,"id":6},{"text":"of","start":44,"end":46,"id":7},{"text":"the","start":47,"end":50,"id":8},{"text":"test","start":51,"end":55,"id":9},{"text":"sample","start":56,"end":62,"id":10},{"text":"is","start":63,"end":65,"id":11},{"text":"not","start":66,"end":69,"id":12},{"text":"required","start":70,"end":78,"id":13},{"text":"to","start":79,"end":81,"id":14},{"text":"grow","start":82,"end":86,"id":15},{"text":"to","start":87,"end":89,"id":16},{"text":"infinity","start":90,"end":98,"id":17},{"text":"for","start":99,"end":102,"id":18},{"text":"the","start":103,"end":106,"id":19},{"text":"consistency","start":107,"end":118,"id":20},{"text":"of","start":119,"end":121,"id":21},{"text":"the","start":122,"end":125,"id":22},{"text":"cross","start":126,"end":131,"id":23},{"text":"-","start":131,"end":132,"id":24},{"text":"validation","start":132,"end":142,"id":25},{"text":"procedure","start":143,"end":152,"id":26},{"text":".","start":152,"end":153,"id":27}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"However, learning the structure of a belief network, particularly one with hidden units, is difficult.","_input_hash":-1921318332,"_task_hash":1826700889,"tokens":[{"text":"However","start":0,"end":7,"id":0},{"text":",","start":7,"end":8,"id":1},{"text":"learning","start":9,"end":17,"id":2},{"text":"the","start":18,"end":21,"id":3},{"text":"structure","start":22,"end":31,"id":4},{"text":"of","start":32,"end":34,"id":5},{"text":"a","start":35,"end":36,"id":6},{"text":"belief","start":37,"end":43,"id":7},{"text":"network","start":44,"end":51,"id":8},{"text":",","start":51,"end":52,"id":9},{"text":"particularly","start":53,"end":65,"id":10},{"text":"one","start":66,"end":69,"id":11},{"text":"with","start":70,"end":74,"id":12},{"text":"hidden","start":75,"end":81,"id":13},{"text":"units","start":82,"end":87,"id":14},{"text":",","start":87,"end":88,"id":15},{"text":"is","start":89,"end":91,"id":16},{"text":"difficult","start":92,"end":101,"id":17},{"text":".","start":101,"end":102,"id":18}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"This yields a parsimonious function fit, which lends itself easily to visualization and interpretation.","_input_hash":257907864,"_task_hash":1998647606,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"yields","start":5,"end":11,"id":1},{"text":"a","start":12,"end":13,"id":2},{"text":"parsimonious","start":14,"end":26,"id":3},{"text":"function","start":27,"end":35,"id":4},{"text":"fit","start":36,"end":39,"id":5},{"text":",","start":39,"end":40,"id":6},{"text":"which","start":41,"end":46,"id":7},{"text":"lends","start":47,"end":52,"id":8},{"text":"itself","start":53,"end":59,"id":9},{"text":"easily","start":60,"end":66,"id":10},{"text":"to","start":67,"end":69,"id":11},{"text":"visualization","start":70,"end":83,"id":12},{"text":"and","start":84,"end":87,"id":13},{"text":"interpretation","start":88,"end":102,"id":14},{"text":".","start":102,"end":103,"id":15}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"The idea is to use an asymmetry between the distributions of cause and effect that occurs if both the covariance matrix of the cause and the structure matrix mapping cause to the effect are independently chosen.","_input_hash":1554371149,"_task_hash":-110771222,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"idea","start":4,"end":8,"id":1},{"text":"is","start":9,"end":11,"id":2},{"text":"to","start":12,"end":14,"id":3},{"text":"use","start":15,"end":18,"id":4},{"text":"an","start":19,"end":21,"id":5},{"text":"asymmetry","start":22,"end":31,"id":6},{"text":"between","start":32,"end":39,"id":7},{"text":"the","start":40,"end":43,"id":8},{"text":"distributions","start":44,"end":57,"id":9},{"text":"of","start":58,"end":60,"id":10},{"text":"cause","start":61,"end":66,"id":11},{"text":"and","start":67,"end":70,"id":12},{"text":"effect","start":71,"end":77,"id":13},{"text":"that","start":78,"end":82,"id":14},{"text":"occurs","start":83,"end":89,"id":15},{"text":"if","start":90,"end":92,"id":16},{"text":"both","start":93,"end":97,"id":17},{"text":"the","start":98,"end":101,"id":18},{"text":"covariance","start":102,"end":112,"id":19},{"text":"matrix","start":113,"end":119,"id":20},{"text":"of","start":120,"end":122,"id":21},{"text":"the","start":123,"end":126,"id":22},{"text":"cause","start":127,"end":132,"id":23},{"text":"and","start":133,"end":136,"id":24},{"text":"the","start":137,"end":140,"id":25},{"text":"structure","start":141,"end":150,"id":26},{"text":"matrix","start":151,"end":157,"id":27},{"text":"mapping","start":158,"end":165,"id":28},{"text":"cause","start":166,"end":171,"id":29},{"text":"to","start":172,"end":174,"id":30},{"text":"the","start":175,"end":178,"id":31},{"text":"effect","start":179,"end":185,"id":32},{"text":"are","start":186,"end":189,"id":33},{"text":"independently","start":190,"end":203,"id":34},{"text":"chosen","start":204,"end":210,"id":35},{"text":".","start":210,"end":211,"id":36}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"movie ratings predicted from reviews, and the political tone of amendments in the U.S. Senate based on the amendment text.","_input_hash":-1436499935,"_task_hash":691054206,"tokens":[{"text":"movie","start":0,"end":5,"id":0},{"text":"ratings","start":6,"end":13,"id":1},{"text":"predicted","start":14,"end":23,"id":2},{"text":"from","start":24,"end":28,"id":3},{"text":"reviews","start":29,"end":36,"id":4},{"text":",","start":36,"end":37,"id":5},{"text":"and","start":38,"end":41,"id":6},{"text":"the","start":42,"end":45,"id":7},{"text":"political","start":46,"end":55,"id":8},{"text":"tone","start":56,"end":60,"id":9},{"text":"of","start":61,"end":63,"id":10},{"text":"amendments","start":64,"end":74,"id":11},{"text":"in","start":75,"end":77,"id":12},{"text":"the","start":78,"end":81,"id":13},{"text":"U.S.","start":82,"end":86,"id":14},{"text":"Senate","start":87,"end":93,"id":15},{"text":"based","start":94,"end":99,"id":16},{"text":"on","start":100,"end":102,"id":17},{"text":"the","start":103,"end":106,"id":18},{"text":"amendment","start":107,"end":116,"id":19},{"text":"text","start":117,"end":121,"id":20},{"text":".","start":121,"end":122,"id":21}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Our method is constructed from an ensemble of interacting, learning agents, that acquire and process incoming information using various types, or different versions of machine learning algorithms.","_input_hash":2074196084,"_task_hash":-547819629,"tokens":[{"text":"Our","start":0,"end":3,"id":0},{"text":"method","start":4,"end":10,"id":1},{"text":"is","start":11,"end":13,"id":2},{"text":"constructed","start":14,"end":25,"id":3},{"text":"from","start":26,"end":30,"id":4},{"text":"an","start":31,"end":33,"id":5},{"text":"ensemble","start":34,"end":42,"id":6},{"text":"of","start":43,"end":45,"id":7},{"text":"interacting","start":46,"end":57,"id":8},{"text":",","start":57,"end":58,"id":9},{"text":"learning","start":59,"end":67,"id":10},{"text":"agents","start":68,"end":74,"id":11},{"text":",","start":74,"end":75,"id":12},{"text":"that","start":76,"end":80,"id":13},{"text":"acquire","start":81,"end":88,"id":14},{"text":"and","start":89,"end":92,"id":15},{"text":"process","start":93,"end":100,"id":16},{"text":"incoming","start":101,"end":109,"id":17},{"text":"information","start":110,"end":121,"id":18},{"text":"using","start":122,"end":127,"id":19},{"text":"various","start":128,"end":135,"id":20},{"text":"types","start":136,"end":141,"id":21},{"text":",","start":141,"end":142,"id":22},{"text":"or","start":143,"end":145,"id":23},{"text":"different","start":146,"end":155,"id":24},{"text":"versions","start":156,"end":164,"id":25},{"text":"of","start":165,"end":167,"id":26},{"text":"machine","start":168,"end":175,"id":27},{"text":"learning","start":176,"end":184,"id":28},{"text":"algorithms","start":185,"end":195,"id":29},{"text":".","start":195,"end":196,"id":30}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Turing_machines|NOUN","word":"Turing machines","sense":"NOUN","meta":{"score":0.7954000235,"sense":"NOUN"},"_input_hash":-1915503930,"_task_hash":1649559953,"_session_id":null,"_view_id":"html","answer":"accept","spans":[],"tokens":[{"text":"Turing_machines|NOUN","start":0,"end":20,"id":0}]}
{"text":"The Indian buffet process has been used as a nonparametric Bayesian prior on the directed structure of a belief network with a single infinitely wide hidden layer.","_input_hash":-2052689914,"_task_hash":437347962,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"Indian","start":4,"end":10,"id":1},{"text":"buffet","start":11,"end":17,"id":2},{"text":"process","start":18,"end":25,"id":3},{"text":"has","start":26,"end":29,"id":4},{"text":"been","start":30,"end":34,"id":5},{"text":"used","start":35,"end":39,"id":6},{"text":"as","start":40,"end":42,"id":7},{"text":"a","start":43,"end":44,"id":8},{"text":"nonparametric","start":45,"end":58,"id":9},{"text":"Bayesian","start":59,"end":67,"id":10},{"text":"prior","start":68,"end":73,"id":11},{"text":"on","start":74,"end":76,"id":12},{"text":"the","start":77,"end":80,"id":13},{"text":"directed","start":81,"end":89,"id":14},{"text":"structure","start":90,"end":99,"id":15},{"text":"of","start":100,"end":102,"id":16},{"text":"a","start":103,"end":104,"id":17},{"text":"belief","start":105,"end":111,"id":18},{"text":"network","start":112,"end":119,"id":19},{"text":"with","start":120,"end":124,"id":20},{"text":"a","start":125,"end":126,"id":21},{"text":"single","start":127,"end":133,"id":22},{"text":"infinitely","start":134,"end":144,"id":23},{"text":"wide","start":145,"end":149,"id":24},{"text":"hidden","start":150,"end":156,"id":25},{"text":"layer","start":157,"end":162,"id":26},{"text":".","start":162,"end":163,"id":27}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":4,"end":25,"token_start":1,"token_end":3,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"In this article, we derive concentration inequalities for the cross-validation estimate of the generalization error for empirical risk minimizers.","_input_hash":1902169871,"_task_hash":-1167396275,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"this","start":3,"end":7,"id":1},{"text":"article","start":8,"end":15,"id":2},{"text":",","start":15,"end":16,"id":3},{"text":"we","start":17,"end":19,"id":4},{"text":"derive","start":20,"end":26,"id":5},{"text":"concentration","start":27,"end":40,"id":6},{"text":"inequalities","start":41,"end":53,"id":7},{"text":"for","start":54,"end":57,"id":8},{"text":"the","start":58,"end":61,"id":9},{"text":"cross","start":62,"end":67,"id":10},{"text":"-","start":67,"end":68,"id":11},{"text":"validation","start":68,"end":78,"id":12},{"text":"estimate","start":79,"end":87,"id":13},{"text":"of","start":88,"end":90,"id":14},{"text":"the","start":91,"end":94,"id":15},{"text":"generalization","start":95,"end":109,"id":16},{"text":"error","start":110,"end":115,"id":17},{"text":"for","start":116,"end":119,"id":18},{"text":"empirical","start":120,"end":129,"id":19},{"text":"risk","start":130,"end":134,"id":20},{"text":"minimizers","start":135,"end":145,"id":21},{"text":".","start":145,"end":146,"id":22}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"matrix_operations|NOUN","word":"matrix operations","sense":"NOUN","meta":{"score":0.7754999995,"sense":"NOUN"},"_input_hash":1826928171,"_task_hash":1383931697,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"matrix_operations|NOUN","start":0,"end":22,"id":0}]}
{"text":"We show that if 0<p <= 1, then both all underlying subspaces and the best l0 subspace can be precisely recovered by lp minimization with overwhelming probability.","_input_hash":-1173300096,"_task_hash":-972853506,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"show","start":3,"end":7,"id":1},{"text":"that","start":8,"end":12,"id":2},{"text":"if","start":13,"end":15,"id":3},{"text":"0","start":16,"end":17,"id":4},{"text":"<","start":17,"end":18,"id":5},{"text":"p","start":18,"end":19,"id":6},{"text":"<","start":20,"end":21,"id":7},{"text":"=","start":21,"end":22,"id":8},{"text":"1","start":23,"end":24,"id":9},{"text":",","start":24,"end":25,"id":10},{"text":"then","start":26,"end":30,"id":11},{"text":"both","start":31,"end":35,"id":12},{"text":"all","start":36,"end":39,"id":13},{"text":"underlying","start":40,"end":50,"id":14},{"text":"subspaces","start":51,"end":60,"id":15},{"text":"and","start":61,"end":64,"id":16},{"text":"the","start":65,"end":68,"id":17},{"text":"best","start":69,"end":73,"id":18},{"text":"l0","start":74,"end":76,"id":19},{"text":"subspace","start":77,"end":85,"id":20},{"text":"can","start":86,"end":89,"id":21},{"text":"be","start":90,"end":92,"id":22},{"text":"precisely","start":93,"end":102,"id":23},{"text":"recovered","start":103,"end":112,"id":24},{"text":"by","start":113,"end":115,"id":25},{"text":"lp","start":116,"end":118,"id":26},{"text":"minimization","start":119,"end":131,"id":27},{"text":"with","start":132,"end":136,"id":28},{"text":"overwhelming","start":137,"end":149,"id":29},{"text":"probability","start":150,"end":161,"id":30},{"text":".","start":161,"end":162,"id":31}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"The iterative algorithm, KNIFE, alternates between finding the coefficients of the original problem and finding the feature weights through kernel linearization.","_input_hash":131542730,"_task_hash":865355697,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"iterative","start":4,"end":13,"id":1},{"text":"algorithm","start":14,"end":23,"id":2},{"text":",","start":23,"end":24,"id":3},{"text":"KNIFE","start":25,"end":30,"id":4},{"text":",","start":30,"end":31,"id":5},{"text":"alternates","start":32,"end":42,"id":6},{"text":"between","start":43,"end":50,"id":7},{"text":"finding","start":51,"end":58,"id":8},{"text":"the","start":59,"end":62,"id":9},{"text":"coefficients","start":63,"end":75,"id":10},{"text":"of","start":76,"end":78,"id":11},{"text":"the","start":79,"end":82,"id":12},{"text":"original","start":83,"end":91,"id":13},{"text":"problem","start":92,"end":99,"id":14},{"text":"and","start":100,"end":103,"id":15},{"text":"finding","start":104,"end":111,"id":16},{"text":"the","start":112,"end":115,"id":17},{"text":"feature","start":116,"end":123,"id":18},{"text":"weights","start":124,"end":131,"id":19},{"text":"through","start":132,"end":139,"id":20},{"text":"kernel","start":140,"end":146,"id":21},{"text":"linearization","start":147,"end":160,"id":22},{"text":".","start":160,"end":161,"id":23}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We propose and analyze an optimal approach for allocations, if continuous-valued allocations are allowed.","_input_hash":-30090669,"_task_hash":-2055234131,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"propose","start":3,"end":10,"id":1},{"text":"and","start":11,"end":14,"id":2},{"text":"analyze","start":15,"end":22,"id":3},{"text":"an","start":23,"end":25,"id":4},{"text":"optimal","start":26,"end":33,"id":5},{"text":"approach","start":34,"end":42,"id":6},{"text":"for","start":43,"end":46,"id":7},{"text":"allocations","start":47,"end":58,"id":8},{"text":",","start":58,"end":59,"id":9},{"text":"if","start":60,"end":62,"id":10},{"text":"continuous","start":63,"end":73,"id":11},{"text":"-","start":73,"end":74,"id":12},{"text":"valued","start":74,"end":80,"id":13},{"text":"allocations","start":81,"end":92,"id":14},{"text":"are","start":93,"end":96,"id":15},{"text":"allowed","start":97,"end":104,"id":16},{"text":".","start":104,"end":105,"id":17}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"Whereas training a LapSVM in the dual requires two steps, using the primal form allows us to collapse training to a single step.","_input_hash":251554342,"_task_hash":1468334110,"tokens":[{"text":"Whereas","start":0,"end":7,"id":0},{"text":"training","start":8,"end":16,"id":1},{"text":"a","start":17,"end":18,"id":2},{"text":"LapSVM","start":19,"end":25,"id":3},{"text":"in","start":26,"end":28,"id":4},{"text":"the","start":29,"end":32,"id":5},{"text":"dual","start":33,"end":37,"id":6},{"text":"requires","start":38,"end":46,"id":7},{"text":"two","start":47,"end":50,"id":8},{"text":"steps","start":51,"end":56,"id":9},{"text":",","start":56,"end":57,"id":10},{"text":"using","start":58,"end":63,"id":11},{"text":"the","start":64,"end":67,"id":12},{"text":"primal","start":68,"end":74,"id":13},{"text":"form","start":75,"end":79,"id":14},{"text":"allows","start":80,"end":86,"id":15},{"text":"us","start":87,"end":89,"id":16},{"text":"to","start":90,"end":92,"id":17},{"text":"collapse","start":93,"end":101,"id":18},{"text":"training","start":102,"end":110,"id":19},{"text":"to","start":111,"end":113,"id":20},{"text":"a","start":114,"end":115,"id":21},{"text":"single","start":116,"end":122,"id":22},{"text":"step","start":123,"end":127,"id":23},{"text":".","start":127,"end":128,"id":24}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"doin","meta":{"score":0},"_input_hash":-501034240,"_task_hash":1097500615,"_session_id":null,"_view_id":"text","answer":"reject","spans":[],"tokens":[{"text":"doin","start":0,"end":4,"id":0}]}
{"text":"MCMC|ORG","word":"MCMC","sense":"ORG","meta":{"score":0.7773000002,"sense":"ORG"},"_input_hash":1992951261,"_task_hash":2101909211,"_session_id":null,"_view_id":"html","answer":"accept","spans":[],"tokens":[{"text":"MCMC|ORG","start":0,"end":8,"id":0}]}
{"text":"Metrics used for measuring similarity between multi-dimensional data points are based on symmetrical divergences.","_input_hash":-834676812,"_task_hash":921118485,"tokens":[{"text":"Metrics","start":0,"end":7,"id":0},{"text":"used","start":8,"end":12,"id":1},{"text":"for","start":13,"end":16,"id":2},{"text":"measuring","start":17,"end":26,"id":3},{"text":"similarity","start":27,"end":37,"id":4},{"text":"between","start":38,"end":45,"id":5},{"text":"multi","start":46,"end":51,"id":6},{"text":"-","start":51,"end":52,"id":7},{"text":"dimensional","start":52,"end":63,"id":8},{"text":"data","start":64,"end":68,"id":9},{"text":"points","start":69,"end":75,"id":10},{"text":"are","start":76,"end":79,"id":11},{"text":"based","start":80,"end":85,"id":12},{"text":"on","start":86,"end":88,"id":13},{"text":"symmetrical","start":89,"end":100,"id":14},{"text":"divergences","start":101,"end":112,"id":15},{"text":".","start":112,"end":113,"id":16}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"complex_system|NOUN","word":"complex system","sense":"NOUN","meta":{"score":0.7659000158,"sense":"NOUN"},"_input_hash":1763720330,"_task_hash":2078950481,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"complex_system|NOUN","start":0,"end":19,"id":0}]}
{"text":"This way, our method is in the same spirit as faithfulness-based causal inference because it also rejects non-generic mutual adjustments among DAG-parameters.","_input_hash":1196395389,"_task_hash":-599186881,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"way","start":5,"end":8,"id":1},{"text":",","start":8,"end":9,"id":2},{"text":"our","start":10,"end":13,"id":3},{"text":"method","start":14,"end":20,"id":4},{"text":"is","start":21,"end":23,"id":5},{"text":"in","start":24,"end":26,"id":6},{"text":"the","start":27,"end":30,"id":7},{"text":"same","start":31,"end":35,"id":8},{"text":"spirit","start":36,"end":42,"id":9},{"text":"as","start":43,"end":45,"id":10},{"text":"faithfulness","start":46,"end":58,"id":11},{"text":"-","start":58,"end":59,"id":12},{"text":"based","start":59,"end":64,"id":13},{"text":"causal","start":65,"end":71,"id":14},{"text":"inference","start":72,"end":81,"id":15},{"text":"because","start":82,"end":89,"id":16},{"text":"it","start":90,"end":92,"id":17},{"text":"also","start":93,"end":97,"id":18},{"text":"rejects","start":98,"end":105,"id":19},{"text":"non","start":106,"end":109,"id":20},{"text":"-","start":109,"end":110,"id":21},{"text":"generic","start":110,"end":117,"id":22},{"text":"mutual","start":118,"end":124,"id":23},{"text":"adjustments","start":125,"end":136,"id":24},{"text":"among","start":137,"end":142,"id":25},{"text":"DAG","start":143,"end":146,"id":26},{"text":"-","start":146,"end":147,"id":27},{"text":"parameters","start":147,"end":157,"id":28},{"text":".","start":157,"end":158,"id":29}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":46,"end":64,"token_start":11,"token_end":13,"label":"ALGO","answer":"reject"},{"start":143,"end":157,"token_start":26,"token_end":28,"label":"ALGO","answer":"reject"}],"answer":"reject"}
{"text":"This article addresses the modeling of reverberant recording environments in the context of under-determined convolutive blind source separation.","_input_hash":1679859491,"_task_hash":1344595225,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"article","start":5,"end":12,"id":1},{"text":"addresses","start":13,"end":22,"id":2},{"text":"the","start":23,"end":26,"id":3},{"text":"modeling","start":27,"end":35,"id":4},{"text":"of","start":36,"end":38,"id":5},{"text":"reverberant","start":39,"end":50,"id":6},{"text":"recording","start":51,"end":60,"id":7},{"text":"environments","start":61,"end":73,"id":8},{"text":"in","start":74,"end":76,"id":9},{"text":"the","start":77,"end":80,"id":10},{"text":"context","start":81,"end":88,"id":11},{"text":"of","start":89,"end":91,"id":12},{"text":"under","start":92,"end":97,"id":13},{"text":"-","start":97,"end":98,"id":14},{"text":"determined","start":98,"end":108,"id":15},{"text":"convolutive","start":109,"end":120,"id":16},{"text":"blind","start":121,"end":126,"id":17},{"text":"source","start":127,"end":133,"id":18},{"text":"separation","start":134,"end":144,"id":19},{"text":".","start":144,"end":145,"id":20}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"If some of the variables take only values in proper subsets of R^n, these conditionals can induce different families of joint distributions even for Markov-equivalent graphs.","_input_hash":827848449,"_task_hash":1511977818,"tokens":[{"text":"If","start":0,"end":2,"id":0},{"text":"some","start":3,"end":7,"id":1},{"text":"of","start":8,"end":10,"id":2},{"text":"the","start":11,"end":14,"id":3},{"text":"variables","start":15,"end":24,"id":4},{"text":"take","start":25,"end":29,"id":5},{"text":"only","start":30,"end":34,"id":6},{"text":"values","start":35,"end":41,"id":7},{"text":"in","start":42,"end":44,"id":8},{"text":"proper","start":45,"end":51,"id":9},{"text":"subsets","start":52,"end":59,"id":10},{"text":"of","start":60,"end":62,"id":11},{"text":"R^n","start":63,"end":66,"id":12},{"text":",","start":66,"end":67,"id":13},{"text":"these","start":68,"end":73,"id":14},{"text":"conditionals","start":74,"end":86,"id":15},{"text":"can","start":87,"end":90,"id":16},{"text":"induce","start":91,"end":97,"id":17},{"text":"different","start":98,"end":107,"id":18},{"text":"families","start":108,"end":116,"id":19},{"text":"of","start":117,"end":119,"id":20},{"text":"joint","start":120,"end":125,"id":21},{"text":"distributions","start":126,"end":139,"id":22},{"text":"even","start":140,"end":144,"id":23},{"text":"for","start":145,"end":148,"id":24},{"text":"Markov","start":149,"end":155,"id":25},{"text":"-","start":155,"end":156,"id":26},{"text":"equivalent","start":156,"end":166,"id":27},{"text":"graphs","start":167,"end":173,"id":28},{"text":".","start":173,"end":174,"id":29}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"We observe a phase transition from one regime to the other and it is the purpose of this work to quantify the influence of various parameters on this phase transition.","_input_hash":-726240648,"_task_hash":-1905187295,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"observe","start":3,"end":10,"id":1},{"text":"a","start":11,"end":12,"id":2},{"text":"phase","start":13,"end":18,"id":3},{"text":"transition","start":19,"end":29,"id":4},{"text":"from","start":30,"end":34,"id":5},{"text":"one","start":35,"end":38,"id":6},{"text":"regime","start":39,"end":45,"id":7},{"text":"to","start":46,"end":48,"id":8},{"text":"the","start":49,"end":52,"id":9},{"text":"other","start":53,"end":58,"id":10},{"text":"and","start":59,"end":62,"id":11},{"text":"it","start":63,"end":65,"id":12},{"text":"is","start":66,"end":68,"id":13},{"text":"the","start":69,"end":72,"id":14},{"text":"purpose","start":73,"end":80,"id":15},{"text":"of","start":81,"end":83,"id":16},{"text":"this","start":84,"end":88,"id":17},{"text":"work","start":89,"end":93,"id":18},{"text":"to","start":94,"end":96,"id":19},{"text":"quantify","start":97,"end":105,"id":20},{"text":"the","start":106,"end":109,"id":21},{"text":"influence","start":110,"end":119,"id":22},{"text":"of","start":120,"end":122,"id":23},{"text":"various","start":123,"end":130,"id":24},{"text":"parameters","start":131,"end":141,"id":25},{"text":"on","start":142,"end":144,"id":26},{"text":"this","start":145,"end":149,"id":27},{"text":"phase","start":150,"end":155,"id":28},{"text":"transition","start":156,"end":166,"id":29},{"text":".","start":166,"end":167,"id":30}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"The posterior is likewise an RKHS mean of a weighted sample.","_input_hash":1556689185,"_task_hash":3684682,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"posterior","start":4,"end":13,"id":1},{"text":"is","start":14,"end":16,"id":2},{"text":"likewise","start":17,"end":25,"id":3},{"text":"an","start":26,"end":28,"id":4},{"text":"RKHS","start":29,"end":33,"id":5},{"text":"mean","start":34,"end":38,"id":6},{"text":"of","start":39,"end":41,"id":7},{"text":"a","start":42,"end":43,"id":8},{"text":"weighted","start":44,"end":52,"id":9},{"text":"sample","start":53,"end":59,"id":10},{"text":".","start":59,"end":60,"id":11}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"Nontrivial surrogate regret bounds are shown to exist precisely when the surrogate loss satisfies a \"calibration\" condition that is easily verified for many common losses.","_input_hash":498887588,"_task_hash":738081253,"tokens":[{"text":"Nontrivial","start":0,"end":10,"id":0},{"text":"surrogate","start":11,"end":20,"id":1},{"text":"regret","start":21,"end":27,"id":2},{"text":"bounds","start":28,"end":34,"id":3},{"text":"are","start":35,"end":38,"id":4},{"text":"shown","start":39,"end":44,"id":5},{"text":"to","start":45,"end":47,"id":6},{"text":"exist","start":48,"end":53,"id":7},{"text":"precisely","start":54,"end":63,"id":8},{"text":"when","start":64,"end":68,"id":9},{"text":"the","start":69,"end":72,"id":10},{"text":"surrogate","start":73,"end":82,"id":11},{"text":"loss","start":83,"end":87,"id":12},{"text":"satisfies","start":88,"end":97,"id":13},{"text":"a","start":98,"end":99,"id":14},{"text":"\"","start":100,"end":101,"id":15},{"text":"calibration","start":101,"end":112,"id":16},{"text":"\"","start":112,"end":113,"id":17},{"text":"condition","start":114,"end":123,"id":18},{"text":"that","start":124,"end":128,"id":19},{"text":"is","start":129,"end":131,"id":20},{"text":"easily","start":132,"end":138,"id":21},{"text":"verified","start":139,"end":147,"id":22},{"text":"for","start":148,"end":151,"id":23},{"text":"many","start":152,"end":156,"id":24},{"text":"common","start":157,"end":163,"id":25},{"text":"losses","start":164,"end":170,"id":26},{"text":".","start":170,"end":171,"id":27}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"This is achieved using a recently introduced tree-structured sparse regularization norm, which has proven useful in several applications.","_input_hash":7370038,"_task_hash":1378610804,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"is","start":5,"end":7,"id":1},{"text":"achieved","start":8,"end":16,"id":2},{"text":"using","start":17,"end":22,"id":3},{"text":"a","start":23,"end":24,"id":4},{"text":"recently","start":25,"end":33,"id":5},{"text":"introduced","start":34,"end":44,"id":6},{"text":"tree","start":45,"end":49,"id":7},{"text":"-","start":49,"end":50,"id":8},{"text":"structured","start":50,"end":60,"id":9},{"text":"sparse","start":61,"end":67,"id":10},{"text":"regularization","start":68,"end":82,"id":11},{"text":"norm","start":83,"end":87,"id":12},{"text":",","start":87,"end":88,"id":13},{"text":"which","start":89,"end":94,"id":14},{"text":"has","start":95,"end":98,"id":15},{"text":"proven","start":99,"end":105,"id":16},{"text":"useful","start":106,"end":112,"id":17},{"text":"in","start":113,"end":115,"id":18},{"text":"several","start":116,"end":123,"id":19},{"text":"applications","start":124,"end":136,"id":20},{"text":".","start":136,"end":137,"id":21}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"Modeling data with linear combinations of a few elements from a learned dictionary has been the focus of much recent research in machine learning, neuroscience and signal processing.","_input_hash":1318071673,"_task_hash":2133471701,"tokens":[{"text":"Modeling","start":0,"end":8,"id":0},{"text":"data","start":9,"end":13,"id":1},{"text":"with","start":14,"end":18,"id":2},{"text":"linear","start":19,"end":25,"id":3},{"text":"combinations","start":26,"end":38,"id":4},{"text":"of","start":39,"end":41,"id":5},{"text":"a","start":42,"end":43,"id":6},{"text":"few","start":44,"end":47,"id":7},{"text":"elements","start":48,"end":56,"id":8},{"text":"from","start":57,"end":61,"id":9},{"text":"a","start":62,"end":63,"id":10},{"text":"learned","start":64,"end":71,"id":11},{"text":"dictionary","start":72,"end":82,"id":12},{"text":"has","start":83,"end":86,"id":13},{"text":"been","start":87,"end":91,"id":14},{"text":"the","start":92,"end":95,"id":15},{"text":"focus","start":96,"end":101,"id":16},{"text":"of","start":102,"end":104,"id":17},{"text":"much","start":105,"end":109,"id":18},{"text":"recent","start":110,"end":116,"id":19},{"text":"research","start":117,"end":125,"id":20},{"text":"in","start":126,"end":128,"id":21},{"text":"machine","start":129,"end":136,"id":22},{"text":"learning","start":137,"end":145,"id":23},{"text":",","start":145,"end":146,"id":24},{"text":"neuroscience","start":147,"end":159,"id":25},{"text":"and","start":160,"end":163,"id":26},{"text":"signal","start":164,"end":170,"id":27},{"text":"processing","start":171,"end":181,"id":28},{"text":".","start":181,"end":182,"id":29}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"StARS outperforms all these competing procedures.","_input_hash":2127472659,"_task_hash":792510777,"tokens":[{"text":"StARS","start":0,"end":5,"id":0},{"text":"outperforms","start":6,"end":17,"id":1},{"text":"all","start":18,"end":21,"id":2},{"text":"these","start":22,"end":27,"id":3},{"text":"competing","start":28,"end":37,"id":4},{"text":"procedures","start":38,"end":48,"id":5},{"text":".","start":48,"end":49,"id":6}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"computer_architecture|NOUN","word":"computer architecture","sense":"NOUN","meta":{"score":0.7868999839,"sense":"NOUN"},"_input_hash":837210475,"_task_hash":1832653932,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"computer_architecture|NOUN","start":0,"end":26,"id":0}]}
{"text":"We conclude that what is percieved as slow by SFA varies and that a more or less fast switching from one regime to the other occurs, perhaps showing some similarity to human perception.","_input_hash":1471156297,"_task_hash":981867864,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"conclude","start":3,"end":11,"id":1},{"text":"that","start":12,"end":16,"id":2},{"text":"what","start":17,"end":21,"id":3},{"text":"is","start":22,"end":24,"id":4},{"text":"percieved","start":25,"end":34,"id":5},{"text":"as","start":35,"end":37,"id":6},{"text":"slow","start":38,"end":42,"id":7},{"text":"by","start":43,"end":45,"id":8},{"text":"SFA","start":46,"end":49,"id":9},{"text":"varies","start":50,"end":56,"id":10},{"text":"and","start":57,"end":60,"id":11},{"text":"that","start":61,"end":65,"id":12},{"text":"a","start":66,"end":67,"id":13},{"text":"more","start":68,"end":72,"id":14},{"text":"or","start":73,"end":75,"id":15},{"text":"less","start":76,"end":80,"id":16},{"text":"fast","start":81,"end":85,"id":17},{"text":"switching","start":86,"end":95,"id":18},{"text":"from","start":96,"end":100,"id":19},{"text":"one","start":101,"end":104,"id":20},{"text":"regime","start":105,"end":111,"id":21},{"text":"to","start":112,"end":114,"id":22},{"text":"the","start":115,"end":118,"id":23},{"text":"other","start":119,"end":124,"id":24},{"text":"occurs","start":125,"end":131,"id":25},{"text":",","start":131,"end":132,"id":26},{"text":"perhaps","start":133,"end":140,"id":27},{"text":"showing","start":141,"end":148,"id":28},{"text":"some","start":149,"end":153,"id":29},{"text":"similarity","start":154,"end":164,"id":30},{"text":"to","start":165,"end":167,"id":31},{"text":"human","start":168,"end":173,"id":32},{"text":"perception","start":174,"end":184,"id":33},{"text":".","start":184,"end":185,"id":34}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"Distributions over permutations arise in applications ranging from multi-object tracking to ranking of instances.","_input_hash":-578156914,"_task_hash":860254932,"tokens":[{"text":"Distributions","start":0,"end":13,"id":0},{"text":"over","start":14,"end":18,"id":1},{"text":"permutations","start":19,"end":31,"id":2},{"text":"arise","start":32,"end":37,"id":3},{"text":"in","start":38,"end":40,"id":4},{"text":"applications","start":41,"end":53,"id":5},{"text":"ranging","start":54,"end":61,"id":6},{"text":"from","start":62,"end":66,"id":7},{"text":"multi","start":67,"end":72,"id":8},{"text":"-","start":72,"end":73,"id":9},{"text":"object","start":73,"end":79,"id":10},{"text":"tracking","start":80,"end":88,"id":11},{"text":"to","start":89,"end":91,"id":12},{"text":"ranking","start":92,"end":99,"id":13},{"text":"of","start":100,"end":102,"id":14},{"text":"instances","start":103,"end":112,"id":15},{"text":".","start":112,"end":113,"id":16}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"We also propose a modification for the case when only integer-valued allocations are possible.","_input_hash":675297407,"_task_hash":-189230073,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"also","start":3,"end":7,"id":1},{"text":"propose","start":8,"end":15,"id":2},{"text":"a","start":16,"end":17,"id":3},{"text":"modification","start":18,"end":30,"id":4},{"text":"for","start":31,"end":34,"id":5},{"text":"the","start":35,"end":38,"id":6},{"text":"case","start":39,"end":43,"id":7},{"text":"when","start":44,"end":48,"id":8},{"text":"only","start":49,"end":53,"id":9},{"text":"integer","start":54,"end":61,"id":10},{"text":"-","start":61,"end":62,"id":11},{"text":"valued","start":62,"end":68,"id":12},{"text":"allocations","start":69,"end":80,"id":13},{"text":"are","start":81,"end":84,"id":14},{"text":"possible","start":85,"end":93,"id":15},{"text":".","start":93,"end":94,"id":16}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"Based on this observation, we propose using $l_1/l_2$ regularization upon the activation possibilities of hidden units in restricted Boltzmann machines to capture the loacal dependencies among hidden units.","_input_hash":-779051655,"_task_hash":-748010239,"tokens":[{"text":"Based","start":0,"end":5,"id":0},{"text":"on","start":6,"end":8,"id":1},{"text":"this","start":9,"end":13,"id":2},{"text":"observation","start":14,"end":25,"id":3},{"text":",","start":25,"end":26,"id":4},{"text":"we","start":27,"end":29,"id":5},{"text":"propose","start":30,"end":37,"id":6},{"text":"using","start":38,"end":43,"id":7},{"text":"$","start":44,"end":45,"id":8},{"text":"l_1","start":45,"end":48,"id":9},{"text":"/","start":48,"end":49,"id":10},{"text":"l_2","start":49,"end":52,"id":11},{"text":"$","start":52,"end":53,"id":12},{"text":"regularization","start":54,"end":68,"id":13},{"text":"upon","start":69,"end":73,"id":14},{"text":"the","start":74,"end":77,"id":15},{"text":"activation","start":78,"end":88,"id":16},{"text":"possibilities","start":89,"end":102,"id":17},{"text":"of","start":103,"end":105,"id":18},{"text":"hidden","start":106,"end":112,"id":19},{"text":"units","start":113,"end":118,"id":20},{"text":"in","start":119,"end":121,"id":21},{"text":"restricted","start":122,"end":132,"id":22},{"text":"Boltzmann","start":133,"end":142,"id":23},{"text":"machines","start":143,"end":151,"id":24},{"text":"to","start":152,"end":154,"id":25},{"text":"capture","start":155,"end":162,"id":26},{"text":"the","start":163,"end":166,"id":27},{"text":"loacal","start":167,"end":173,"id":28},{"text":"dependencies","start":174,"end":186,"id":29},{"text":"among","start":187,"end":192,"id":30},{"text":"hidden","start":193,"end":199,"id":31},{"text":"units","start":200,"end":205,"id":32},{"text":".","start":205,"end":206,"id":33}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":122,"end":151,"token_start":22,"token_end":24,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"When using the K-nearest neighbors method, one often ignores uncertainty in the choice of K. To account for such uncertainty, Holmes and Adams (2002) proposed a Bayesian framework for K-nearest neighbors (KNN).","_input_hash":434432237,"_task_hash":-133365338,"tokens":[{"text":"When","start":0,"end":4,"id":0},{"text":"using","start":5,"end":10,"id":1},{"text":"the","start":11,"end":14,"id":2},{"text":"K","start":15,"end":16,"id":3},{"text":"-","start":16,"end":17,"id":4},{"text":"nearest","start":17,"end":24,"id":5},{"text":"neighbors","start":25,"end":34,"id":6},{"text":"method","start":35,"end":41,"id":7},{"text":",","start":41,"end":42,"id":8},{"text":"one","start":43,"end":46,"id":9},{"text":"often","start":47,"end":52,"id":10},{"text":"ignores","start":53,"end":60,"id":11},{"text":"uncertainty","start":61,"end":72,"id":12},{"text":"in","start":73,"end":75,"id":13},{"text":"the","start":76,"end":79,"id":14},{"text":"choice","start":80,"end":86,"id":15},{"text":"of","start":87,"end":89,"id":16},{"text":"K.","start":90,"end":92,"id":17},{"text":"To","start":93,"end":95,"id":18},{"text":"account","start":96,"end":103,"id":19},{"text":"for","start":104,"end":107,"id":20},{"text":"such","start":108,"end":112,"id":21},{"text":"uncertainty","start":113,"end":124,"id":22},{"text":",","start":124,"end":125,"id":23},{"text":"Holmes","start":126,"end":132,"id":24},{"text":"and","start":133,"end":136,"id":25},{"text":"Adams","start":137,"end":142,"id":26},{"text":"(","start":143,"end":144,"id":27},{"text":"2002","start":144,"end":148,"id":28},{"text":")","start":148,"end":149,"id":29},{"text":"proposed","start":150,"end":158,"id":30},{"text":"a","start":159,"end":160,"id":31},{"text":"Bayesian","start":161,"end":169,"id":32},{"text":"framework","start":170,"end":179,"id":33},{"text":"for","start":180,"end":183,"id":34},{"text":"K","start":184,"end":185,"id":35},{"text":"-","start":185,"end":186,"id":36},{"text":"nearest","start":186,"end":193,"id":37},{"text":"neighbors","start":194,"end":203,"id":38},{"text":"(","start":204,"end":205,"id":39},{"text":"KNN","start":205,"end":208,"id":40},{"text":")","start":208,"end":209,"id":41},{"text":".","start":209,"end":210,"id":42}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":15,"end":34,"token_start":3,"token_end":6,"label":"ALGO","answer":"accept"},{"start":184,"end":203,"token_start":35,"token_end":38,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"The predictive power is maintained at least at the same level as the original tree ensemble.","_input_hash":1495663278,"_task_hash":-595697248,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"predictive","start":4,"end":14,"id":1},{"text":"power","start":15,"end":20,"id":2},{"text":"is","start":21,"end":23,"id":3},{"text":"maintained","start":24,"end":34,"id":4},{"text":"at","start":35,"end":37,"id":5},{"text":"least","start":38,"end":43,"id":6},{"text":"at","start":44,"end":46,"id":7},{"text":"the","start":47,"end":50,"id":8},{"text":"same","start":51,"end":55,"id":9},{"text":"level","start":56,"end":61,"id":10},{"text":"as","start":62,"end":64,"id":11},{"text":"the","start":65,"end":68,"id":12},{"text":"original","start":69,"end":77,"id":13},{"text":"tree","start":78,"end":82,"id":14},{"text":"ensemble","start":83,"end":91,"id":15},{"text":".","start":91,"end":92,"id":16}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":78,"end":91,"token_start":14,"token_end":15,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"we","meta":{"score":0},"_input_hash":517814815,"_task_hash":-1635728891,"_session_id":null,"_view_id":"text","answer":"reject","spans":[],"tokens":[{"text":"we","start":0,"end":2,"id":0}]}
{"text":"The step size leading to the global maximum of the contrast along the search direction is found among the roots of a fourth-degree polynomial.","_input_hash":-602567725,"_task_hash":644587622,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"step","start":4,"end":8,"id":1},{"text":"size","start":9,"end":13,"id":2},{"text":"leading","start":14,"end":21,"id":3},{"text":"to","start":22,"end":24,"id":4},{"text":"the","start":25,"end":28,"id":5},{"text":"global","start":29,"end":35,"id":6},{"text":"maximum","start":36,"end":43,"id":7},{"text":"of","start":44,"end":46,"id":8},{"text":"the","start":47,"end":50,"id":9},{"text":"contrast","start":51,"end":59,"id":10},{"text":"along","start":60,"end":65,"id":11},{"text":"the","start":66,"end":69,"id":12},{"text":"search","start":70,"end":76,"id":13},{"text":"direction","start":77,"end":86,"id":14},{"text":"is","start":87,"end":89,"id":15},{"text":"found","start":90,"end":95,"id":16},{"text":"among","start":96,"end":101,"id":17},{"text":"the","start":102,"end":105,"id":18},{"text":"roots","start":106,"end":111,"id":19},{"text":"of","start":112,"end":114,"id":20},{"text":"a","start":115,"end":116,"id":21},{"text":"fourth","start":117,"end":123,"id":22},{"text":"-","start":123,"end":124,"id":23},{"text":"degree","start":124,"end":130,"id":24},{"text":"polynomial","start":131,"end":141,"id":25},{"text":".","start":141,"end":142,"id":26}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We propose a method to infer causal structures containing both discrete and continuous variables.","_input_hash":494398072,"_task_hash":-611349042,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"propose","start":3,"end":10,"id":1},{"text":"a","start":11,"end":12,"id":2},{"text":"method","start":13,"end":19,"id":3},{"text":"to","start":20,"end":22,"id":4},{"text":"infer","start":23,"end":28,"id":5},{"text":"causal","start":29,"end":35,"id":6},{"text":"structures","start":36,"end":46,"id":7},{"text":"containing","start":47,"end":57,"id":8},{"text":"both","start":58,"end":62,"id":9},{"text":"discrete","start":63,"end":71,"id":10},{"text":"and","start":72,"end":75,"id":11},{"text":"continuous","start":76,"end":86,"id":12},{"text":"variables","start":87,"end":96,"id":13},{"text":".","start":96,"end":97,"id":14}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":13,"end":19,"token_start":3,"token_end":3,"label":"ALGO","answer":"reject"}],"answer":"reject"}
{"text":"parsimonious models are obtained, which are very suitable for interpretation.","_input_hash":1352466466,"_task_hash":-1367422401,"tokens":[{"text":"parsimonious","start":0,"end":12,"id":0},{"text":"models","start":13,"end":19,"id":1},{"text":"are","start":20,"end":23,"id":2},{"text":"obtained","start":24,"end":32,"id":3},{"text":",","start":32,"end":33,"id":4},{"text":"which","start":34,"end":39,"id":5},{"text":"are","start":40,"end":43,"id":6},{"text":"very","start":44,"end":48,"id":7},{"text":"suitable","start":49,"end":57,"id":8},{"text":"for","start":58,"end":61,"id":9},{"text":"interpretation","start":62,"end":76,"id":10},{"text":".","start":76,"end":77,"id":11}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We propose Dirichlet Process mixtures of Generalized Linear Models (DP-GLM), a new method of nonparametric regression that accommodates continuous and categorical inputs, and responses that can be modeled by a generalized linear model.","_input_hash":-1048931128,"_task_hash":1723282706,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"propose","start":3,"end":10,"id":1},{"text":"Dirichlet","start":11,"end":20,"id":2},{"text":"Process","start":21,"end":28,"id":3},{"text":"mixtures","start":29,"end":37,"id":4},{"text":"of","start":38,"end":40,"id":5},{"text":"Generalized","start":41,"end":52,"id":6},{"text":"Linear","start":53,"end":59,"id":7},{"text":"Models","start":60,"end":66,"id":8},{"text":"(","start":67,"end":68,"id":9},{"text":"DP","start":68,"end":70,"id":10},{"text":"-","start":70,"end":71,"id":11},{"text":"GLM","start":71,"end":74,"id":12},{"text":")","start":74,"end":75,"id":13},{"text":",","start":75,"end":76,"id":14},{"text":"a","start":77,"end":78,"id":15},{"text":"new","start":79,"end":82,"id":16},{"text":"method","start":83,"end":89,"id":17},{"text":"of","start":90,"end":92,"id":18},{"text":"nonparametric","start":93,"end":106,"id":19},{"text":"regression","start":107,"end":117,"id":20},{"text":"that","start":118,"end":122,"id":21},{"text":"accommodates","start":123,"end":135,"id":22},{"text":"continuous","start":136,"end":146,"id":23},{"text":"and","start":147,"end":150,"id":24},{"text":"categorical","start":151,"end":162,"id":25},{"text":"inputs","start":163,"end":169,"id":26},{"text":",","start":169,"end":170,"id":27},{"text":"and","start":171,"end":174,"id":28},{"text":"responses","start":175,"end":184,"id":29},{"text":"that","start":185,"end":189,"id":30},{"text":"can","start":190,"end":193,"id":31},{"text":"be","start":194,"end":196,"id":32},{"text":"modeled","start":197,"end":204,"id":33},{"text":"by","start":205,"end":207,"id":34},{"text":"a","start":208,"end":209,"id":35},{"text":"generalized","start":210,"end":221,"id":36},{"text":"linear","start":222,"end":228,"id":37},{"text":"model","start":229,"end":234,"id":38},{"text":".","start":234,"end":235,"id":39}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":11,"end":66,"token_start":2,"token_end":8,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"This method is in practice difficult, because it requires a global optimization of a complicated function, the joint distribution by fixed input variables.","_input_hash":-2105218453,"_task_hash":-1582819205,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"method","start":5,"end":11,"id":1},{"text":"is","start":12,"end":14,"id":2},{"text":"in","start":15,"end":17,"id":3},{"text":"practice","start":18,"end":26,"id":4},{"text":"difficult","start":27,"end":36,"id":5},{"text":",","start":36,"end":37,"id":6},{"text":"because","start":38,"end":45,"id":7},{"text":"it","start":46,"end":48,"id":8},{"text":"requires","start":49,"end":57,"id":9},{"text":"a","start":58,"end":59,"id":10},{"text":"global","start":60,"end":66,"id":11},{"text":"optimization","start":67,"end":79,"id":12},{"text":"of","start":80,"end":82,"id":13},{"text":"a","start":83,"end":84,"id":14},{"text":"complicated","start":85,"end":96,"id":15},{"text":"function","start":97,"end":105,"id":16},{"text":",","start":105,"end":106,"id":17},{"text":"the","start":107,"end":110,"id":18},{"text":"joint","start":111,"end":116,"id":19},{"text":"distribution","start":117,"end":129,"id":20},{"text":"by","start":130,"end":132,"id":21},{"text":"fixed","start":133,"end":138,"id":22},{"text":"input","start":139,"end":144,"id":23},{"text":"variables","start":145,"end":154,"id":24},{"text":".","start":154,"end":155,"id":25}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"This paper re-examines the issues in two parts.","_input_hash":-2035294479,"_task_hash":965280369,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"paper","start":5,"end":10,"id":1},{"text":"re","start":11,"end":13,"id":2},{"text":"-","start":13,"end":14,"id":3},{"text":"examines","start":14,"end":22,"id":4},{"text":"the","start":23,"end":26,"id":5},{"text":"issues","start":27,"end":33,"id":6},{"text":"in","start":34,"end":36,"id":7},{"text":"two","start":37,"end":40,"id":8},{"text":"parts","start":41,"end":46,"id":9},{"text":".","start":46,"end":47,"id":10}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"This technical report derives the first two posterior moments of the maximum of two correlated Gaussian variables and the first two posterior moments of the two generating variables (corresponding to Gaussian approximations minimizing relative entropy).","_input_hash":-1369732246,"_task_hash":-1544135136,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"technical","start":5,"end":14,"id":1},{"text":"report","start":15,"end":21,"id":2},{"text":"derives","start":22,"end":29,"id":3},{"text":"the","start":30,"end":33,"id":4},{"text":"first","start":34,"end":39,"id":5},{"text":"two","start":40,"end":43,"id":6},{"text":"posterior","start":44,"end":53,"id":7},{"text":"moments","start":54,"end":61,"id":8},{"text":"of","start":62,"end":64,"id":9},{"text":"the","start":65,"end":68,"id":10},{"text":"maximum","start":69,"end":76,"id":11},{"text":"of","start":77,"end":79,"id":12},{"text":"two","start":80,"end":83,"id":13},{"text":"correlated","start":84,"end":94,"id":14},{"text":"Gaussian","start":95,"end":103,"id":15},{"text":"variables","start":104,"end":113,"id":16},{"text":"and","start":114,"end":117,"id":17},{"text":"the","start":118,"end":121,"id":18},{"text":"first","start":122,"end":127,"id":19},{"text":"two","start":128,"end":131,"id":20},{"text":"posterior","start":132,"end":141,"id":21},{"text":"moments","start":142,"end":149,"id":22},{"text":"of","start":150,"end":152,"id":23},{"text":"the","start":153,"end":156,"id":24},{"text":"two","start":157,"end":160,"id":25},{"text":"generating","start":161,"end":171,"id":26},{"text":"variables","start":172,"end":181,"id":27},{"text":"(","start":182,"end":183,"id":28},{"text":"corresponding","start":183,"end":196,"id":29},{"text":"to","start":197,"end":199,"id":30},{"text":"Gaussian","start":200,"end":208,"id":31},{"text":"approximations","start":209,"end":223,"id":32},{"text":"minimizing","start":224,"end":234,"id":33},{"text":"relative","start":235,"end":243,"id":34},{"text":"entropy","start":244,"end":251,"id":35},{"text":")","start":251,"end":252,"id":36},{"text":".","start":252,"end":253,"id":37}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"modeling patches of natural images, modeling handwritten digits and pretaining a deep networks for a classification task.","_input_hash":-508554862,"_task_hash":419712666,"tokens":[{"text":"modeling","start":0,"end":8,"id":0},{"text":"patches","start":9,"end":16,"id":1},{"text":"of","start":17,"end":19,"id":2},{"text":"natural","start":20,"end":27,"id":3},{"text":"images","start":28,"end":34,"id":4},{"text":",","start":34,"end":35,"id":5},{"text":"modeling","start":36,"end":44,"id":6},{"text":"handwritten","start":45,"end":56,"id":7},{"text":"digits","start":57,"end":63,"id":8},{"text":"and","start":64,"end":67,"id":9},{"text":"pretaining","start":68,"end":78,"id":10},{"text":"a","start":79,"end":80,"id":11},{"text":"deep","start":81,"end":85,"id":12},{"text":"networks","start":86,"end":94,"id":13},{"text":"for","start":95,"end":98,"id":14},{"text":"a","start":99,"end":100,"id":15},{"text":"classification","start":101,"end":115,"id":16},{"text":"task","start":116,"end":120,"id":17},{"text":".","start":120,"end":121,"id":18}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"This result extends to additive homoscedastic uniform noise around the subspaces (i.e., uniform distribution in a strip around them) and near recovery with an error proportional to the noise level.","_input_hash":-1900850711,"_task_hash":-1377835114,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"result","start":5,"end":11,"id":1},{"text":"extends","start":12,"end":19,"id":2},{"text":"to","start":20,"end":22,"id":3},{"text":"additive","start":23,"end":31,"id":4},{"text":"homoscedastic","start":32,"end":45,"id":5},{"text":"uniform","start":46,"end":53,"id":6},{"text":"noise","start":54,"end":59,"id":7},{"text":"around","start":60,"end":66,"id":8},{"text":"the","start":67,"end":70,"id":9},{"text":"subspaces","start":71,"end":80,"id":10},{"text":"(","start":81,"end":82,"id":11},{"text":"i.e.","start":82,"end":86,"id":12},{"text":",","start":86,"end":87,"id":13},{"text":"uniform","start":88,"end":95,"id":14},{"text":"distribution","start":96,"end":108,"id":15},{"text":"in","start":109,"end":111,"id":16},{"text":"a","start":112,"end":113,"id":17},{"text":"strip","start":114,"end":119,"id":18},{"text":"around","start":120,"end":126,"id":19},{"text":"them","start":127,"end":131,"id":20},{"text":")","start":131,"end":132,"id":21},{"text":"and","start":133,"end":136,"id":22},{"text":"near","start":137,"end":141,"id":23},{"text":"recovery","start":142,"end":150,"id":24},{"text":"with","start":151,"end":155,"id":25},{"text":"an","start":156,"end":158,"id":26},{"text":"error","start":159,"end":164,"id":27},{"text":"proportional","start":165,"end":177,"id":28},{"text":"to","start":178,"end":180,"id":29},{"text":"the","start":181,"end":184,"id":30},{"text":"noise","start":185,"end":190,"id":31},{"text":"level","start":191,"end":196,"id":32},{"text":".","start":196,"end":197,"id":33}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"We propose a simple kernel based nearest neighbor approach for handwritten digit classification.","_input_hash":-1778104689,"_task_hash":-872252738,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"propose","start":3,"end":10,"id":1},{"text":"a","start":11,"end":12,"id":2},{"text":"simple","start":13,"end":19,"id":3},{"text":"kernel","start":20,"end":26,"id":4},{"text":"based","start":27,"end":32,"id":5},{"text":"nearest","start":33,"end":40,"id":6},{"text":"neighbor","start":41,"end":49,"id":7},{"text":"approach","start":50,"end":58,"id":8},{"text":"for","start":59,"end":62,"id":9},{"text":"handwritten","start":63,"end":74,"id":10},{"text":"digit","start":75,"end":80,"id":11},{"text":"classification","start":81,"end":95,"id":12},{"text":".","start":95,"end":96,"id":13}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":20,"end":58,"token_start":4,"token_end":8,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We give experimental evidence that the new algorithm is significantly more robust against label noise than existing boosting algorithm.","_input_hash":-1045206845,"_task_hash":-2094188250,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"give","start":3,"end":7,"id":1},{"text":"experimental","start":8,"end":20,"id":2},{"text":"evidence","start":21,"end":29,"id":3},{"text":"that","start":30,"end":34,"id":4},{"text":"the","start":35,"end":38,"id":5},{"text":"new","start":39,"end":42,"id":6},{"text":"algorithm","start":43,"end":52,"id":7},{"text":"is","start":53,"end":55,"id":8},{"text":"significantly","start":56,"end":69,"id":9},{"text":"more","start":70,"end":74,"id":10},{"text":"robust","start":75,"end":81,"id":11},{"text":"against","start":82,"end":89,"id":12},{"text":"label","start":90,"end":95,"id":13},{"text":"noise","start":96,"end":101,"id":14},{"text":"than","start":102,"end":106,"id":15},{"text":"existing","start":107,"end":115,"id":16},{"text":"boosting","start":116,"end":124,"id":17},{"text":"algorithm","start":125,"end":134,"id":18},{"text":".","start":134,"end":135,"id":19}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":116,"end":124,"token_start":17,"token_end":17,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"it","meta":{"score":0},"_input_hash":-781753652,"_task_hash":-1725579662,"_session_id":null,"_view_id":"text","answer":"reject","spans":[],"tokens":[{"text":"it","start":0,"end":2,"id":0}]}
{"text":"Here we examine the case where the model parameters before and after the changepoint are independent and we derive an online algorithm for exact inference of the most recent changepoint.","_input_hash":852536986,"_task_hash":1584886922,"tokens":[{"text":"Here","start":0,"end":4,"id":0},{"text":"we","start":5,"end":7,"id":1},{"text":"examine","start":8,"end":15,"id":2},{"text":"the","start":16,"end":19,"id":3},{"text":"case","start":20,"end":24,"id":4},{"text":"where","start":25,"end":30,"id":5},{"text":"the","start":31,"end":34,"id":6},{"text":"model","start":35,"end":40,"id":7},{"text":"parameters","start":41,"end":51,"id":8},{"text":"before","start":52,"end":58,"id":9},{"text":"and","start":59,"end":62,"id":10},{"text":"after","start":63,"end":68,"id":11},{"text":"the","start":69,"end":72,"id":12},{"text":"changepoint","start":73,"end":84,"id":13},{"text":"are","start":85,"end":88,"id":14},{"text":"independent","start":89,"end":100,"id":15},{"text":"and","start":101,"end":104,"id":16},{"text":"we","start":105,"end":107,"id":17},{"text":"derive","start":108,"end":114,"id":18},{"text":"an","start":115,"end":117,"id":19},{"text":"online","start":118,"end":124,"id":20},{"text":"algorithm","start":125,"end":134,"id":21},{"text":"for","start":135,"end":138,"id":22},{"text":"exact","start":139,"end":144,"id":23},{"text":"inference","start":145,"end":154,"id":24},{"text":"of","start":155,"end":157,"id":25},{"text":"the","start":158,"end":161,"id":26},{"text":"most","start":162,"end":166,"id":27},{"text":"recent","start":167,"end":173,"id":28},{"text":"changepoint","start":174,"end":185,"id":29},{"text":".","start":185,"end":186,"id":30}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"quantum_computers|NOUN","word":"quantum computers","sense":"NOUN","meta":{"score":0.7752000093,"sense":"NOUN"},"_input_hash":1832293551,"_task_hash":6642805,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"quantum_computers|NOUN","start":0,"end":22,"id":0}]}
{"text":"Due to practial limitations the model can be restricted to a class such as linear regression models, which we address in this study.","_input_hash":-1146729600,"_task_hash":-396792118,"tokens":[{"text":"Due","start":0,"end":3,"id":0},{"text":"to","start":4,"end":6,"id":1},{"text":"practial","start":7,"end":15,"id":2},{"text":"limitations","start":16,"end":27,"id":3},{"text":"the","start":28,"end":31,"id":4},{"text":"model","start":32,"end":37,"id":5},{"text":"can","start":38,"end":41,"id":6},{"text":"be","start":42,"end":44,"id":7},{"text":"restricted","start":45,"end":55,"id":8},{"text":"to","start":56,"end":58,"id":9},{"text":"a","start":59,"end":60,"id":10},{"text":"class","start":61,"end":66,"id":11},{"text":"such","start":67,"end":71,"id":12},{"text":"as","start":72,"end":74,"id":13},{"text":"linear","start":75,"end":81,"id":14},{"text":"regression","start":82,"end":92,"id":15},{"text":"models","start":93,"end":99,"id":16},{"text":",","start":99,"end":100,"id":17},{"text":"which","start":101,"end":106,"id":18},{"text":"we","start":107,"end":109,"id":19},{"text":"address","start":110,"end":117,"id":20},{"text":"in","start":118,"end":120,"id":21},{"text":"this","start":121,"end":125,"id":22},{"text":"study","start":126,"end":131,"id":23},{"text":".","start":131,"end":132,"id":24}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"The first encodes the idea that the output of a clustering scheme should carry a multiresolution structure, the second the idea that one should be able to compare the results of clustering algorithms as one varies the data set, for example by adding points or by applying functions to it.","_input_hash":-595449267,"_task_hash":1934641529,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"first","start":4,"end":9,"id":1},{"text":"encodes","start":10,"end":17,"id":2},{"text":"the","start":18,"end":21,"id":3},{"text":"idea","start":22,"end":26,"id":4},{"text":"that","start":27,"end":31,"id":5},{"text":"the","start":32,"end":35,"id":6},{"text":"output","start":36,"end":42,"id":7},{"text":"of","start":43,"end":45,"id":8},{"text":"a","start":46,"end":47,"id":9},{"text":"clustering","start":48,"end":58,"id":10},{"text":"scheme","start":59,"end":65,"id":11},{"text":"should","start":66,"end":72,"id":12},{"text":"carry","start":73,"end":78,"id":13},{"text":"a","start":79,"end":80,"id":14},{"text":"multiresolution","start":81,"end":96,"id":15},{"text":"structure","start":97,"end":106,"id":16},{"text":",","start":106,"end":107,"id":17},{"text":"the","start":108,"end":111,"id":18},{"text":"second","start":112,"end":118,"id":19},{"text":"the","start":119,"end":122,"id":20},{"text":"idea","start":123,"end":127,"id":21},{"text":"that","start":128,"end":132,"id":22},{"text":"one","start":133,"end":136,"id":23},{"text":"should","start":137,"end":143,"id":24},{"text":"be","start":144,"end":146,"id":25},{"text":"able","start":147,"end":151,"id":26},{"text":"to","start":152,"end":154,"id":27},{"text":"compare","start":155,"end":162,"id":28},{"text":"the","start":163,"end":166,"id":29},{"text":"results","start":167,"end":174,"id":30},{"text":"of","start":175,"end":177,"id":31},{"text":"clustering","start":178,"end":188,"id":32},{"text":"algorithms","start":189,"end":199,"id":33},{"text":"as","start":200,"end":202,"id":34},{"text":"one","start":203,"end":206,"id":35},{"text":"varies","start":207,"end":213,"id":36},{"text":"the","start":214,"end":217,"id":37},{"text":"data","start":218,"end":222,"id":38},{"text":"set","start":223,"end":226,"id":39},{"text":",","start":226,"end":227,"id":40},{"text":"for","start":228,"end":231,"id":41},{"text":"example","start":232,"end":239,"id":42},{"text":"by","start":240,"end":242,"id":43},{"text":"adding","start":243,"end":249,"id":44},{"text":"points","start":250,"end":256,"id":45},{"text":"or","start":257,"end":259,"id":46},{"text":"by","start":260,"end":262,"id":47},{"text":"applying","start":263,"end":271,"id":48},{"text":"functions","start":272,"end":281,"id":49},{"text":"to","start":282,"end":284,"id":50},{"text":"it","start":285,"end":287,"id":51},{"text":".","start":287,"end":288,"id":52}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":178,"end":199,"token_start":32,"token_end":33,"label":"ALGO","answer":"reject"}],"answer":"reject"}
{"text":"We detail optimal algorithms to predict sequences generated in this way.","_input_hash":1746763958,"_task_hash":-1597840849,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"detail","start":3,"end":9,"id":1},{"text":"optimal","start":10,"end":17,"id":2},{"text":"algorithms","start":18,"end":28,"id":3},{"text":"to","start":29,"end":31,"id":4},{"text":"predict","start":32,"end":39,"id":5},{"text":"sequences","start":40,"end":49,"id":6},{"text":"generated","start":50,"end":59,"id":7},{"text":"in","start":60,"end":62,"id":8},{"text":"this","start":63,"end":67,"id":9},{"text":"way","start":68,"end":71,"id":10},{"text":".","start":71,"end":72,"id":11}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":10,"end":28,"token_start":2,"token_end":3,"label":"ALGO","answer":"reject"}],"answer":"reject"}
{"text":"Among other practical benefits, RobustICA can avoid prewhitening and deals with real- and complex-valued mixtures of possibly noncircular sources alike.","_input_hash":918322072,"_task_hash":1687500086,"tokens":[{"text":"Among","start":0,"end":5,"id":0},{"text":"other","start":6,"end":11,"id":1},{"text":"practical","start":12,"end":21,"id":2},{"text":"benefits","start":22,"end":30,"id":3},{"text":",","start":30,"end":31,"id":4},{"text":"RobustICA","start":32,"end":41,"id":5},{"text":"can","start":42,"end":45,"id":6},{"text":"avoid","start":46,"end":51,"id":7},{"text":"prewhitening","start":52,"end":64,"id":8},{"text":"and","start":65,"end":68,"id":9},{"text":"deals","start":69,"end":74,"id":10},{"text":"with","start":75,"end":79,"id":11},{"text":"real-","start":80,"end":85,"id":12},{"text":"and","start":86,"end":89,"id":13},{"text":"complex","start":90,"end":97,"id":14},{"text":"-","start":97,"end":98,"id":15},{"text":"valued","start":98,"end":104,"id":16},{"text":"mixtures","start":105,"end":113,"id":17},{"text":"of","start":114,"end":116,"id":18},{"text":"possibly","start":117,"end":125,"id":19},{"text":"noncircular","start":126,"end":137,"id":20},{"text":"sources","start":138,"end":145,"id":21},{"text":"alike","start":146,"end":151,"id":22},{"text":".","start":151,"end":152,"id":23}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":32,"end":41,"token_start":5,"token_end":5,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"It is known that there exist $p$-adic representation of dendrograms.","_input_hash":-2145759776,"_task_hash":-591247322,"tokens":[{"text":"It","start":0,"end":2,"id":0},{"text":"is","start":3,"end":5,"id":1},{"text":"known","start":6,"end":11,"id":2},{"text":"that","start":12,"end":16,"id":3},{"text":"there","start":17,"end":22,"id":4},{"text":"exist","start":23,"end":28,"id":5},{"text":"$","start":29,"end":30,"id":6},{"text":"p$-adic","start":30,"end":37,"id":7},{"text":"representation","start":38,"end":52,"id":8},{"text":"of","start":53,"end":55,"id":9},{"text":"dendrograms","start":56,"end":67,"id":10},{"text":".","start":67,"end":68,"id":11}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"complex_functions|NOUN","word":"complex functions","sense":"NOUN","meta":{"score":0.7585999966,"sense":"NOUN"},"_input_hash":1359682905,"_task_hash":-254686403,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"complex_functions|NOUN","start":0,"end":22,"id":0}]}
{"text":"General loss functions and class of predictors with finite VC-dimension are considered.","_input_hash":1006505184,"_task_hash":413456921,"tokens":[{"text":"General","start":0,"end":7,"id":0},{"text":"loss","start":8,"end":12,"id":1},{"text":"functions","start":13,"end":22,"id":2},{"text":"and","start":23,"end":26,"id":3},{"text":"class","start":27,"end":32,"id":4},{"text":"of","start":33,"end":35,"id":5},{"text":"predictors","start":36,"end":46,"id":6},{"text":"with","start":47,"end":51,"id":7},{"text":"finite","start":52,"end":58,"id":8},{"text":"VC","start":59,"end":61,"id":9},{"text":"-","start":61,"end":62,"id":10},{"text":"dimension","start":62,"end":71,"id":11},{"text":"are","start":72,"end":75,"id":12},{"text":"considered","start":76,"end":86,"id":13},{"text":".","start":86,"end":87,"id":14}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"natural_language_processing|NOUN","word":"natural language processing","sense":"NOUN","meta":{"score":0.7961999774,"sense":"NOUN"},"_input_hash":679294529,"_task_hash":-601715570,"_session_id":null,"_view_id":"html","answer":"accept","spans":[],"tokens":[{"text":"natural_language_processing|NOUN","start":0,"end":32,"id":0}]}
{"text":"It contains 1-NN as a special case.","_input_hash":1500027186,"_task_hash":2092447590,"tokens":[{"text":"It","start":0,"end":2,"id":0},{"text":"contains","start":3,"end":11,"id":1},{"text":"1-NN","start":12,"end":16,"id":2},{"text":"as","start":17,"end":19,"id":3},{"text":"a","start":20,"end":21,"id":4},{"text":"special","start":22,"end":29,"id":5},{"text":"case","start":30,"end":34,"id":6},{"text":".","start":34,"end":35,"id":7}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"other_algorithms|NOUN","word":"other algorithms","sense":"NOUN","meta":{"score":0.7588999867,"sense":"NOUN"},"_input_hash":1936176820,"_task_hash":-1483717678,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"other_algorithms|NOUN","start":0,"end":21,"id":0}]}
{"text":"This series expansion is shown to be useful for numerical calculations of the JSD, when the probability distributions are nearly equal, and for which, consequently, small numerical errors dominate evaluation.","_input_hash":-508395004,"_task_hash":296377828,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"series","start":5,"end":11,"id":1},{"text":"expansion","start":12,"end":21,"id":2},{"text":"is","start":22,"end":24,"id":3},{"text":"shown","start":25,"end":30,"id":4},{"text":"to","start":31,"end":33,"id":5},{"text":"be","start":34,"end":36,"id":6},{"text":"useful","start":37,"end":43,"id":7},{"text":"for","start":44,"end":47,"id":8},{"text":"numerical","start":48,"end":57,"id":9},{"text":"calculations","start":58,"end":70,"id":10},{"text":"of","start":71,"end":73,"id":11},{"text":"the","start":74,"end":77,"id":12},{"text":"JSD","start":78,"end":81,"id":13},{"text":",","start":81,"end":82,"id":14},{"text":"when","start":83,"end":87,"id":15},{"text":"the","start":88,"end":91,"id":16},{"text":"probability","start":92,"end":103,"id":17},{"text":"distributions","start":104,"end":117,"id":18},{"text":"are","start":118,"end":121,"id":19},{"text":"nearly","start":122,"end":128,"id":20},{"text":"equal","start":129,"end":134,"id":21},{"text":",","start":134,"end":135,"id":22},{"text":"and","start":136,"end":139,"id":23},{"text":"for","start":140,"end":143,"id":24},{"text":"which","start":144,"end":149,"id":25},{"text":",","start":149,"end":150,"id":26},{"text":"consequently","start":151,"end":163,"id":27},{"text":",","start":163,"end":164,"id":28},{"text":"small","start":165,"end":170,"id":29},{"text":"numerical","start":171,"end":180,"id":30},{"text":"errors","start":181,"end":187,"id":31},{"text":"dominate","start":188,"end":196,"id":32},{"text":"evaluation","start":197,"end":207,"id":33},{"text":".","start":207,"end":208,"id":34}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"The final classification of incoming input data is therefore defined as the stationary state of the meta-learning system using simple majority rule, yet the minority clusters that share opposite classification outcome can be observed in the system.","_input_hash":360348176,"_task_hash":53910799,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"final","start":4,"end":9,"id":1},{"text":"classification","start":10,"end":24,"id":2},{"text":"of","start":25,"end":27,"id":3},{"text":"incoming","start":28,"end":36,"id":4},{"text":"input","start":37,"end":42,"id":5},{"text":"data","start":43,"end":47,"id":6},{"text":"is","start":48,"end":50,"id":7},{"text":"therefore","start":51,"end":60,"id":8},{"text":"defined","start":61,"end":68,"id":9},{"text":"as","start":69,"end":71,"id":10},{"text":"the","start":72,"end":75,"id":11},{"text":"stationary","start":76,"end":86,"id":12},{"text":"state","start":87,"end":92,"id":13},{"text":"of","start":93,"end":95,"id":14},{"text":"the","start":96,"end":99,"id":15},{"text":"meta","start":100,"end":104,"id":16},{"text":"-","start":104,"end":105,"id":17},{"text":"learning","start":105,"end":113,"id":18},{"text":"system","start":114,"end":120,"id":19},{"text":"using","start":121,"end":126,"id":20},{"text":"simple","start":127,"end":133,"id":21},{"text":"majority","start":134,"end":142,"id":22},{"text":"rule","start":143,"end":147,"id":23},{"text":",","start":147,"end":148,"id":24},{"text":"yet","start":149,"end":152,"id":25},{"text":"the","start":153,"end":156,"id":26},{"text":"minority","start":157,"end":165,"id":27},{"text":"clusters","start":166,"end":174,"id":28},{"text":"that","start":175,"end":179,"id":29},{"text":"share","start":180,"end":185,"id":30},{"text":"opposite","start":186,"end":194,"id":31},{"text":"classification","start":195,"end":209,"id":32},{"text":"outcome","start":210,"end":217,"id":33},{"text":"can","start":218,"end":221,"id":34},{"text":"be","start":222,"end":224,"id":35},{"text":"observed","start":225,"end":233,"id":36},{"text":"in","start":234,"end":236,"id":37},{"text":"the","start":237,"end":240,"id":38},{"text":"system","start":241,"end":247,"id":39},{"text":".","start":247,"end":248,"id":40}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We give conditions under which sample variance penalization is effective.","_input_hash":1848389840,"_task_hash":-1980547899,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"give","start":3,"end":7,"id":1},{"text":"conditions","start":8,"end":18,"id":2},{"text":"under","start":19,"end":24,"id":3},{"text":"which","start":25,"end":30,"id":4},{"text":"sample","start":31,"end":37,"id":5},{"text":"variance","start":38,"end":46,"id":6},{"text":"penalization","start":47,"end":59,"id":7},{"text":"is","start":60,"end":62,"id":8},{"text":"effective","start":63,"end":72,"id":9},{"text":".","start":72,"end":73,"id":10}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"We investigate the role of the initialization for the stability of the k-means clustering algorithm.","_input_hash":-858948277,"_task_hash":-1783413372,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"investigate","start":3,"end":14,"id":1},{"text":"the","start":15,"end":18,"id":2},{"text":"role","start":19,"end":23,"id":3},{"text":"of","start":24,"end":26,"id":4},{"text":"the","start":27,"end":30,"id":5},{"text":"initialization","start":31,"end":45,"id":6},{"text":"for","start":46,"end":49,"id":7},{"text":"the","start":50,"end":53,"id":8},{"text":"stability","start":54,"end":63,"id":9},{"text":"of","start":64,"end":66,"id":10},{"text":"the","start":67,"end":70,"id":11},{"text":"k","start":71,"end":72,"id":12},{"text":"-","start":72,"end":73,"id":13},{"text":"means","start":73,"end":78,"id":14},{"text":"clustering","start":79,"end":89,"id":15},{"text":"algorithm","start":90,"end":99,"id":16},{"text":".","start":99,"end":100,"id":17}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":71,"end":99,"token_start":12,"token_end":16,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Thus we can index the complexity of our random sequence by the number of states of the automata.","_input_hash":-530968138,"_task_hash":-70775467,"tokens":[{"text":"Thus","start":0,"end":4,"id":0},{"text":"we","start":5,"end":7,"id":1},{"text":"can","start":8,"end":11,"id":2},{"text":"index","start":12,"end":17,"id":3},{"text":"the","start":18,"end":21,"id":4},{"text":"complexity","start":22,"end":32,"id":5},{"text":"of","start":33,"end":35,"id":6},{"text":"our","start":36,"end":39,"id":7},{"text":"random","start":40,"end":46,"id":8},{"text":"sequence","start":47,"end":55,"id":9},{"text":"by","start":56,"end":58,"id":10},{"text":"the","start":59,"end":62,"id":11},{"text":"number","start":63,"end":69,"id":12},{"text":"of","start":70,"end":72,"id":13},{"text":"states","start":73,"end":79,"id":14},{"text":"of","start":80,"end":82,"id":15},{"text":"the","start":83,"end":86,"id":16},{"text":"automata","start":87,"end":95,"id":17},{"text":".","start":95,"end":96,"id":18}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"formalization of learning and attack processes, derivation of an optimal attack, analysis of its efficiency and constraints.","_input_hash":-1898018722,"_task_hash":1991588775,"tokens":[{"text":"formalization","start":0,"end":13,"id":0},{"text":"of","start":14,"end":16,"id":1},{"text":"learning","start":17,"end":25,"id":2},{"text":"and","start":26,"end":29,"id":3},{"text":"attack","start":30,"end":36,"id":4},{"text":"processes","start":37,"end":46,"id":5},{"text":",","start":46,"end":47,"id":6},{"text":"derivation","start":48,"end":58,"id":7},{"text":"of","start":59,"end":61,"id":8},{"text":"an","start":62,"end":64,"id":9},{"text":"optimal","start":65,"end":72,"id":10},{"text":"attack","start":73,"end":79,"id":11},{"text":",","start":79,"end":80,"id":12},{"text":"analysis","start":81,"end":89,"id":13},{"text":"of","start":90,"end":92,"id":14},{"text":"its","start":93,"end":96,"id":15},{"text":"efficiency","start":97,"end":107,"id":16},{"text":"and","start":108,"end":111,"id":17},{"text":"constraints","start":112,"end":123,"id":18},{"text":".","start":123,"end":124,"id":19}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"In this context, learning the dictionary amounts to solving a large-scale matrix factorization problem, which can be done efficiently with classical optimization tools.","_input_hash":1100707809,"_task_hash":-1652280002,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"this","start":3,"end":7,"id":1},{"text":"context","start":8,"end":15,"id":2},{"text":",","start":15,"end":16,"id":3},{"text":"learning","start":17,"end":25,"id":4},{"text":"the","start":26,"end":29,"id":5},{"text":"dictionary","start":30,"end":40,"id":6},{"text":"amounts","start":41,"end":48,"id":7},{"text":"to","start":49,"end":51,"id":8},{"text":"solving","start":52,"end":59,"id":9},{"text":"a","start":60,"end":61,"id":10},{"text":"large","start":62,"end":67,"id":11},{"text":"-","start":67,"end":68,"id":12},{"text":"scale","start":68,"end":73,"id":13},{"text":"matrix","start":74,"end":80,"id":14},{"text":"factorization","start":81,"end":94,"id":15},{"text":"problem","start":95,"end":102,"id":16},{"text":",","start":102,"end":103,"id":17},{"text":"which","start":104,"end":109,"id":18},{"text":"can","start":110,"end":113,"id":19},{"text":"be","start":114,"end":116,"id":20},{"text":"done","start":117,"end":121,"id":21},{"text":"efficiently","start":122,"end":133,"id":22},{"text":"with","start":134,"end":138,"id":23},{"text":"classical","start":139,"end":148,"id":24},{"text":"optimization","start":149,"end":161,"id":25},{"text":"tools","start":162,"end":167,"id":26},{"text":".","start":167,"end":168,"id":27}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":149,"end":167,"token_start":25,"token_end":26,"label":"ALGO","answer":"reject"}],"answer":"reject"}
{"text":"It makes the direct definition of a multinomial distribution over permutation space impractical for all but a very small $n$. In this work we propose an embedding of all $n!$ permutations for a given $n$ in a surface of a hypersphere defined in $\\mathbbm{R}^{(n-1)^2}$. As a result of the embedding, we acquire ability to define continuous distributions over a hypersphere with all the benefits of directional statistics.","_input_hash":708902930,"_task_hash":188494324,"tokens":[{"text":"It","start":0,"end":2,"id":0},{"text":"makes","start":3,"end":8,"id":1},{"text":"the","start":9,"end":12,"id":2},{"text":"direct","start":13,"end":19,"id":3},{"text":"definition","start":20,"end":30,"id":4},{"text":"of","start":31,"end":33,"id":5},{"text":"a","start":34,"end":35,"id":6},{"text":"multinomial","start":36,"end":47,"id":7},{"text":"distribution","start":48,"end":60,"id":8},{"text":"over","start":61,"end":65,"id":9},{"text":"permutation","start":66,"end":77,"id":10},{"text":"space","start":78,"end":83,"id":11},{"text":"impractical","start":84,"end":95,"id":12},{"text":"for","start":96,"end":99,"id":13},{"text":"all","start":100,"end":103,"id":14},{"text":"but","start":104,"end":107,"id":15},{"text":"a","start":108,"end":109,"id":16},{"text":"very","start":110,"end":114,"id":17},{"text":"small","start":115,"end":120,"id":18},{"text":"$","start":121,"end":122,"id":19},{"text":"n$.","start":122,"end":125,"id":20},{"text":"In","start":126,"end":128,"id":21},{"text":"this","start":129,"end":133,"id":22},{"text":"work","start":134,"end":138,"id":23},{"text":"we","start":139,"end":141,"id":24},{"text":"propose","start":142,"end":149,"id":25},{"text":"an","start":150,"end":152,"id":26},{"text":"embedding","start":153,"end":162,"id":27},{"text":"of","start":163,"end":165,"id":28},{"text":"all","start":166,"end":169,"id":29},{"text":"$","start":170,"end":171,"id":30},{"text":"n!$","start":171,"end":174,"id":31},{"text":"permutations","start":175,"end":187,"id":32},{"text":"for","start":188,"end":191,"id":33},{"text":"a","start":192,"end":193,"id":34},{"text":"given","start":194,"end":199,"id":35},{"text":"$","start":200,"end":201,"id":36},{"text":"n$","start":201,"end":203,"id":37},{"text":"in","start":204,"end":206,"id":38},{"text":"a","start":207,"end":208,"id":39},{"text":"surface","start":209,"end":216,"id":40},{"text":"of","start":217,"end":219,"id":41},{"text":"a","start":220,"end":221,"id":42},{"text":"hypersphere","start":222,"end":233,"id":43},{"text":"defined","start":234,"end":241,"id":44},{"text":"in","start":242,"end":244,"id":45},{"text":"$","start":245,"end":246,"id":46},{"text":"\\mathbbm{R}^{(n-1)^2}$.","start":246,"end":269,"id":47},{"text":"As","start":270,"end":272,"id":48},{"text":"a","start":273,"end":274,"id":49},{"text":"result","start":275,"end":281,"id":50},{"text":"of","start":282,"end":284,"id":51},{"text":"the","start":285,"end":288,"id":52},{"text":"embedding","start":289,"end":298,"id":53},{"text":",","start":298,"end":299,"id":54},{"text":"we","start":300,"end":302,"id":55},{"text":"acquire","start":303,"end":310,"id":56},{"text":"ability","start":311,"end":318,"id":57},{"text":"to","start":319,"end":321,"id":58},{"text":"define","start":322,"end":328,"id":59},{"text":"continuous","start":329,"end":339,"id":60},{"text":"distributions","start":340,"end":353,"id":61},{"text":"over","start":354,"end":358,"id":62},{"text":"a","start":359,"end":360,"id":63},{"text":"hypersphere","start":361,"end":372,"id":64},{"text":"with","start":373,"end":377,"id":65},{"text":"all","start":378,"end":381,"id":66},{"text":"the","start":382,"end":385,"id":67},{"text":"benefits","start":386,"end":394,"id":68},{"text":"of","start":395,"end":397,"id":69},{"text":"directional","start":398,"end":409,"id":70},{"text":"statistics","start":410,"end":420,"id":71},{"text":".","start":420,"end":421,"id":72}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"Second, it is short and simple and it follows from a direct application of Blackwell's approachability theorem to carefully chosen vector-valued payoff function and convex target set.","_input_hash":-1499825962,"_task_hash":792909598,"tokens":[{"text":"Second","start":0,"end":6,"id":0},{"text":",","start":6,"end":7,"id":1},{"text":"it","start":8,"end":10,"id":2},{"text":"is","start":11,"end":13,"id":3},{"text":"short","start":14,"end":19,"id":4},{"text":"and","start":20,"end":23,"id":5},{"text":"simple","start":24,"end":30,"id":6},{"text":"and","start":31,"end":34,"id":7},{"text":"it","start":35,"end":37,"id":8},{"text":"follows","start":38,"end":45,"id":9},{"text":"from","start":46,"end":50,"id":10},{"text":"a","start":51,"end":52,"id":11},{"text":"direct","start":53,"end":59,"id":12},{"text":"application","start":60,"end":71,"id":13},{"text":"of","start":72,"end":74,"id":14},{"text":"Blackwell","start":75,"end":84,"id":15},{"text":"'s","start":84,"end":86,"id":16},{"text":"approachability","start":87,"end":102,"id":17},{"text":"theorem","start":103,"end":110,"id":18},{"text":"to","start":111,"end":113,"id":19},{"text":"carefully","start":114,"end":123,"id":20},{"text":"chosen","start":124,"end":130,"id":21},{"text":"vector","start":131,"end":137,"id":22},{"text":"-","start":137,"end":138,"id":23},{"text":"valued","start":138,"end":144,"id":24},{"text":"payoff","start":145,"end":151,"id":25},{"text":"function","start":152,"end":160,"id":26},{"text":"and","start":161,"end":164,"id":27},{"text":"convex","start":165,"end":171,"id":28},{"text":"target","start":172,"end":178,"id":29},{"text":"set","start":179,"end":182,"id":30},{"text":".","start":182,"end":183,"id":31}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"Two approaches to surrogate regret bounds are developed.","_input_hash":1489249856,"_task_hash":307187196,"tokens":[{"text":"Two","start":0,"end":3,"id":0},{"text":"approaches","start":4,"end":14,"id":1},{"text":"to","start":15,"end":17,"id":2},{"text":"surrogate","start":18,"end":27,"id":3},{"text":"regret","start":28,"end":34,"id":4},{"text":"bounds","start":35,"end":41,"id":5},{"text":"are","start":42,"end":45,"id":6},{"text":"developed","start":46,"end":55,"id":7},{"text":".","start":55,"end":56,"id":8}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"Among them, Matthews Correlation Coefficient has recently gained popularity among researchers not only in machine learning but also in several application fields such as bioinformatics.","_input_hash":-1101186323,"_task_hash":222177656,"tokens":[{"text":"Among","start":0,"end":5,"id":0},{"text":"them","start":6,"end":10,"id":1},{"text":",","start":10,"end":11,"id":2},{"text":"Matthews","start":12,"end":20,"id":3},{"text":"Correlation","start":21,"end":32,"id":4},{"text":"Coefficient","start":33,"end":44,"id":5},{"text":"has","start":45,"end":48,"id":6},{"text":"recently","start":49,"end":57,"id":7},{"text":"gained","start":58,"end":64,"id":8},{"text":"popularity","start":65,"end":75,"id":9},{"text":"among","start":76,"end":81,"id":10},{"text":"researchers","start":82,"end":93,"id":11},{"text":"not","start":94,"end":97,"id":12},{"text":"only","start":98,"end":102,"id":13},{"text":"in","start":103,"end":105,"id":14},{"text":"machine","start":106,"end":113,"id":15},{"text":"learning","start":114,"end":122,"id":16},{"text":"but","start":123,"end":126,"id":17},{"text":"also","start":127,"end":131,"id":18},{"text":"in","start":132,"end":134,"id":19},{"text":"several","start":135,"end":142,"id":20},{"text":"application","start":143,"end":154,"id":21},{"text":"fields","start":155,"end":161,"id":22},{"text":"such","start":162,"end":166,"id":23},{"text":"as","start":167,"end":169,"id":24},{"text":"bioinformatics","start":170,"end":184,"id":25},{"text":".","start":184,"end":185,"id":26}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"We show experimental results with synthetic and real data, in particular, we show results in pollution prediction, school exams score prediction and gene expression data.","_input_hash":1904077874,"_task_hash":1745400184,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"show","start":3,"end":7,"id":1},{"text":"experimental","start":8,"end":20,"id":2},{"text":"results","start":21,"end":28,"id":3},{"text":"with","start":29,"end":33,"id":4},{"text":"synthetic","start":34,"end":43,"id":5},{"text":"and","start":44,"end":47,"id":6},{"text":"real","start":48,"end":52,"id":7},{"text":"data","start":53,"end":57,"id":8},{"text":",","start":57,"end":58,"id":9},{"text":"in","start":59,"end":61,"id":10},{"text":"particular","start":62,"end":72,"id":11},{"text":",","start":72,"end":73,"id":12},{"text":"we","start":74,"end":76,"id":13},{"text":"show","start":77,"end":81,"id":14},{"text":"results","start":82,"end":89,"id":15},{"text":"in","start":90,"end":92,"id":16},{"text":"pollution","start":93,"end":102,"id":17},{"text":"prediction","start":103,"end":113,"id":18},{"text":",","start":113,"end":114,"id":19},{"text":"school","start":115,"end":121,"id":20},{"text":"exams","start":122,"end":127,"id":21},{"text":"score","start":128,"end":133,"id":22},{"text":"prediction","start":134,"end":144,"id":23},{"text":"and","start":145,"end":148,"id":24},{"text":"gene","start":149,"end":153,"id":25},{"text":"expression","start":154,"end":164,"id":26},{"text":"data","start":165,"end":169,"id":27},{"text":".","start":169,"end":170,"id":28}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"human_brain|NOUN","word":"human brain","sense":"NOUN","meta":{"score":0.8493000269,"sense":"NOUN"},"_input_hash":-1320410847,"_task_hash":499081280,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"human_brain|NOUN","start":0,"end":16,"id":0}]}
{"text":"Experiments on handwritten digit classification, digital art identification, nonlinear inverse image problems, and compressed sensing demonstrate that our approach is effective in large-scale settings, and is well suited to supervised and semi-supervised classification, as well as regression tasks for data that admit sparse representations.","_input_hash":-1230483569,"_task_hash":-1836899765,"tokens":[{"text":"Experiments","start":0,"end":11,"id":0},{"text":"on","start":12,"end":14,"id":1},{"text":"handwritten","start":15,"end":26,"id":2},{"text":"digit","start":27,"end":32,"id":3},{"text":"classification","start":33,"end":47,"id":4},{"text":",","start":47,"end":48,"id":5},{"text":"digital","start":49,"end":56,"id":6},{"text":"art","start":57,"end":60,"id":7},{"text":"identification","start":61,"end":75,"id":8},{"text":",","start":75,"end":76,"id":9},{"text":"nonlinear","start":77,"end":86,"id":10},{"text":"inverse","start":87,"end":94,"id":11},{"text":"image","start":95,"end":100,"id":12},{"text":"problems","start":101,"end":109,"id":13},{"text":",","start":109,"end":110,"id":14},{"text":"and","start":111,"end":114,"id":15},{"text":"compressed","start":115,"end":125,"id":16},{"text":"sensing","start":126,"end":133,"id":17},{"text":"demonstrate","start":134,"end":145,"id":18},{"text":"that","start":146,"end":150,"id":19},{"text":"our","start":151,"end":154,"id":20},{"text":"approach","start":155,"end":163,"id":21},{"text":"is","start":164,"end":166,"id":22},{"text":"effective","start":167,"end":176,"id":23},{"text":"in","start":177,"end":179,"id":24},{"text":"large","start":180,"end":185,"id":25},{"text":"-","start":185,"end":186,"id":26},{"text":"scale","start":186,"end":191,"id":27},{"text":"settings","start":192,"end":200,"id":28},{"text":",","start":200,"end":201,"id":29},{"text":"and","start":202,"end":205,"id":30},{"text":"is","start":206,"end":208,"id":31},{"text":"well","start":209,"end":213,"id":32},{"text":"suited","start":214,"end":220,"id":33},{"text":"to","start":221,"end":223,"id":34},{"text":"supervised","start":224,"end":234,"id":35},{"text":"and","start":235,"end":238,"id":36},{"text":"semi","start":239,"end":243,"id":37},{"text":"-","start":243,"end":244,"id":38},{"text":"supervised","start":244,"end":254,"id":39},{"text":"classification","start":255,"end":269,"id":40},{"text":",","start":269,"end":270,"id":41},{"text":"as","start":271,"end":273,"id":42},{"text":"well","start":274,"end":278,"id":43},{"text":"as","start":279,"end":281,"id":44},{"text":"regression","start":282,"end":292,"id":45},{"text":"tasks","start":293,"end":298,"id":46},{"text":"for","start":299,"end":302,"id":47},{"text":"data","start":303,"end":307,"id":48},{"text":"that","start":308,"end":312,"id":49},{"text":"admit","start":313,"end":318,"id":50},{"text":"sparse","start":319,"end":325,"id":51},{"text":"representations","start":326,"end":341,"id":52},{"text":".","start":341,"end":342,"id":53}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":239,"end":269,"token_start":37,"token_end":40,"label":"ALGO","answer":"reject"},{"start":282,"end":298,"token_start":45,"token_end":46,"label":"ALGO","answer":"reject"}],"answer":"reject"}
{"text":"We give improved constants for data dependent and variance sensitive confidence bounds, called empirical Bernstein bounds, and extend these inequalities to hold uniformly over classes of functionswhose growth function is polynomial in the sample size n. The bounds lead us to consider sample variance penalization, a novel learning method which takes into account the empirical variance of the loss function.","_input_hash":-1242792690,"_task_hash":-1953945584,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"give","start":3,"end":7,"id":1},{"text":"improved","start":8,"end":16,"id":2},{"text":"constants","start":17,"end":26,"id":3},{"text":"for","start":27,"end":30,"id":4},{"text":"data","start":31,"end":35,"id":5},{"text":"dependent","start":36,"end":45,"id":6},{"text":"and","start":46,"end":49,"id":7},{"text":"variance","start":50,"end":58,"id":8},{"text":"sensitive","start":59,"end":68,"id":9},{"text":"confidence","start":69,"end":79,"id":10},{"text":"bounds","start":80,"end":86,"id":11},{"text":",","start":86,"end":87,"id":12},{"text":"called","start":88,"end":94,"id":13},{"text":"empirical","start":95,"end":104,"id":14},{"text":"Bernstein","start":105,"end":114,"id":15},{"text":"bounds","start":115,"end":121,"id":16},{"text":",","start":121,"end":122,"id":17},{"text":"and","start":123,"end":126,"id":18},{"text":"extend","start":127,"end":133,"id":19},{"text":"these","start":134,"end":139,"id":20},{"text":"inequalities","start":140,"end":152,"id":21},{"text":"to","start":153,"end":155,"id":22},{"text":"hold","start":156,"end":160,"id":23},{"text":"uniformly","start":161,"end":170,"id":24},{"text":"over","start":171,"end":175,"id":25},{"text":"classes","start":176,"end":183,"id":26},{"text":"of","start":184,"end":186,"id":27},{"text":"functionswhose","start":187,"end":201,"id":28},{"text":"growth","start":202,"end":208,"id":29},{"text":"function","start":209,"end":217,"id":30},{"text":"is","start":218,"end":220,"id":31},{"text":"polynomial","start":221,"end":231,"id":32},{"text":"in","start":232,"end":234,"id":33},{"text":"the","start":235,"end":238,"id":34},{"text":"sample","start":239,"end":245,"id":35},{"text":"size","start":246,"end":250,"id":36},{"text":"n.","start":251,"end":253,"id":37},{"text":"The","start":254,"end":257,"id":38},{"text":"bounds","start":258,"end":264,"id":39},{"text":"lead","start":265,"end":269,"id":40},{"text":"us","start":270,"end":272,"id":41},{"text":"to","start":273,"end":275,"id":42},{"text":"consider","start":276,"end":284,"id":43},{"text":"sample","start":285,"end":291,"id":44},{"text":"variance","start":292,"end":300,"id":45},{"text":"penalization","start":301,"end":313,"id":46},{"text":",","start":313,"end":314,"id":47},{"text":"a","start":315,"end":316,"id":48},{"text":"novel","start":317,"end":322,"id":49},{"text":"learning","start":323,"end":331,"id":50},{"text":"method","start":332,"end":338,"id":51},{"text":"which","start":339,"end":344,"id":52},{"text":"takes","start":345,"end":350,"id":53},{"text":"into","start":351,"end":355,"id":54},{"text":"account","start":356,"end":363,"id":55},{"text":"the","start":364,"end":367,"id":56},{"text":"empirical","start":368,"end":377,"id":57},{"text":"variance","start":378,"end":386,"id":58},{"text":"of","start":387,"end":389,"id":59},{"text":"the","start":390,"end":393,"id":60},{"text":"loss","start":394,"end":398,"id":61},{"text":"function","start":399,"end":407,"id":62},{"text":".","start":407,"end":408,"id":63}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"problem_space|NOUN","word":"problem space","sense":"NOUN","meta":{"score":0.7639999986,"sense":"NOUN"},"_input_hash":1451865425,"_task_hash":-930631885,"_session_id":null,"_view_id":"html","answer":"reject","spans":[],"tokens":[{"text":"problem_space|NOUN","start":0,"end":18,"id":0}]}
{"text":"neural_network|NOUN","word":"neural network","sense":"NOUN","meta":{"score":1.0,"sense":"NOUN"},"answer":"accept","_input_hash":2073216112,"_task_hash":-399424270,"spans":[],"tokens":[{"text":"neural_network|NOUN","start":0,"end":19,"id":0}]}
{"text":"We demonstrate this algorithm on collections of scientific abstracts from several journals.","_input_hash":1728424860,"_task_hash":-1387184893,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"demonstrate","start":3,"end":14,"id":1},{"text":"this","start":15,"end":19,"id":2},{"text":"algorithm","start":20,"end":29,"id":3},{"text":"on","start":30,"end":32,"id":4},{"text":"collections","start":33,"end":44,"id":5},{"text":"of","start":45,"end":47,"id":6},{"text":"scientific","start":48,"end":58,"id":7},{"text":"abstracts","start":59,"end":68,"id":8},{"text":"from","start":69,"end":73,"id":9},{"text":"several","start":74,"end":81,"id":10},{"text":"journals","start":82,"end":90,"id":11},{"text":".","start":90,"end":91,"id":12}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We point out the interest of each cross-validation procedure in terms of rate of convergence.","_input_hash":-791264148,"_task_hash":-395434608,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"point","start":3,"end":8,"id":1},{"text":"out","start":9,"end":12,"id":2},{"text":"the","start":13,"end":16,"id":3},{"text":"interest","start":17,"end":25,"id":4},{"text":"of","start":26,"end":28,"id":5},{"text":"each","start":29,"end":33,"id":6},{"text":"cross","start":34,"end":39,"id":7},{"text":"-","start":39,"end":40,"id":8},{"text":"validation","start":40,"end":50,"id":9},{"text":"procedure","start":51,"end":60,"id":10},{"text":"in","start":61,"end":63,"id":11},{"text":"terms","start":64,"end":69,"id":12},{"text":"of","start":70,"end":72,"id":13},{"text":"rate","start":73,"end":77,"id":14},{"text":"of","start":78,"end":80,"id":15},{"text":"convergence","start":81,"end":92,"id":16},{"text":".","start":92,"end":93,"id":17}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"the number of variables and n:","_input_hash":-1356592422,"_task_hash":1522437034,"tokens":[{"text":"the","start":0,"end":3,"id":0},{"text":"number","start":4,"end":10,"id":1},{"text":"of","start":11,"end":13,"id":2},{"text":"variables","start":14,"end":23,"id":3},{"text":"and","start":24,"end":27,"id":4},{"text":"n","start":28,"end":29,"id":5},{"text":":","start":29,"end":30,"id":6}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"Indeed, we demonstrate on the much studied ZIP code data how the PVM can reap the benefits of a problem-specific metric.","_input_hash":811333583,"_task_hash":1760649759,"tokens":[{"text":"Indeed","start":0,"end":6,"id":0},{"text":",","start":6,"end":7,"id":1},{"text":"we","start":8,"end":10,"id":2},{"text":"demonstrate","start":11,"end":22,"id":3},{"text":"on","start":23,"end":25,"id":4},{"text":"the","start":26,"end":29,"id":5},{"text":"much","start":30,"end":34,"id":6},{"text":"studied","start":35,"end":42,"id":7},{"text":"ZIP","start":43,"end":46,"id":8},{"text":"code","start":47,"end":51,"id":9},{"text":"data","start":52,"end":56,"id":10},{"text":"how","start":57,"end":60,"id":11},{"text":"the","start":61,"end":64,"id":12},{"text":"PVM","start":65,"end":68,"id":13},{"text":"can","start":69,"end":72,"id":14},{"text":"reap","start":73,"end":77,"id":15},{"text":"the","start":78,"end":81,"id":16},{"text":"benefits","start":82,"end":90,"id":17},{"text":"of","start":91,"end":93,"id":18},{"text":"a","start":94,"end":95,"id":19},{"text":"problem","start":96,"end":103,"id":20},{"text":"-","start":103,"end":104,"id":21},{"text":"specific","start":104,"end":112,"id":22},{"text":"metric","start":113,"end":119,"id":23},{"text":".","start":119,"end":120,"id":24}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We show that such a family of kernels has close bonds with the laplace transforms of nonnegative-valued functions defined on the cone of positive semidefinite matrices, and we present some closed formulas that can be derived as special cases of such integral expressions.","_input_hash":-672109701,"_task_hash":-95035663,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"show","start":3,"end":7,"id":1},{"text":"that","start":8,"end":12,"id":2},{"text":"such","start":13,"end":17,"id":3},{"text":"a","start":18,"end":19,"id":4},{"text":"family","start":20,"end":26,"id":5},{"text":"of","start":27,"end":29,"id":6},{"text":"kernels","start":30,"end":37,"id":7},{"text":"has","start":38,"end":41,"id":8},{"text":"close","start":42,"end":47,"id":9},{"text":"bonds","start":48,"end":53,"id":10},{"text":"with","start":54,"end":58,"id":11},{"text":"the","start":59,"end":62,"id":12},{"text":"laplace","start":63,"end":70,"id":13},{"text":"transforms","start":71,"end":81,"id":14},{"text":"of","start":82,"end":84,"id":15},{"text":"nonnegative","start":85,"end":96,"id":16},{"text":"-","start":96,"end":97,"id":17},{"text":"valued","start":97,"end":103,"id":18},{"text":"functions","start":104,"end":113,"id":19},{"text":"defined","start":114,"end":121,"id":20},{"text":"on","start":122,"end":124,"id":21},{"text":"the","start":125,"end":128,"id":22},{"text":"cone","start":129,"end":133,"id":23},{"text":"of","start":134,"end":136,"id":24},{"text":"positive","start":137,"end":145,"id":25},{"text":"semidefinite","start":146,"end":158,"id":26},{"text":"matrices","start":159,"end":167,"id":27},{"text":",","start":167,"end":168,"id":28},{"text":"and","start":169,"end":172,"id":29},{"text":"we","start":173,"end":175,"id":30},{"text":"present","start":176,"end":183,"id":31},{"text":"some","start":184,"end":188,"id":32},{"text":"closed","start":189,"end":195,"id":33},{"text":"formulas","start":196,"end":204,"id":34},{"text":"that","start":205,"end":209,"id":35},{"text":"can","start":210,"end":213,"id":36},{"text":"be","start":214,"end":216,"id":37},{"text":"derived","start":217,"end":224,"id":38},{"text":"as","start":225,"end":227,"id":39},{"text":"special","start":228,"end":235,"id":40},{"text":"cases","start":236,"end":241,"id":41},{"text":"of","start":242,"end":244,"id":42},{"text":"such","start":245,"end":249,"id":43},{"text":"integral","start":250,"end":258,"id":44},{"text":"expressions","start":259,"end":270,"id":45},{"text":".","start":270,"end":271,"id":46}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"On synthetic data, we compare our algorithm on the two estimation tasks to the other existing methods.","_input_hash":517898554,"_task_hash":-1271085399,"tokens":[{"text":"On","start":0,"end":2,"id":0},{"text":"synthetic","start":3,"end":12,"id":1},{"text":"data","start":13,"end":17,"id":2},{"text":",","start":17,"end":18,"id":3},{"text":"we","start":19,"end":21,"id":4},{"text":"compare","start":22,"end":29,"id":5},{"text":"our","start":30,"end":33,"id":6},{"text":"algorithm","start":34,"end":43,"id":7},{"text":"on","start":44,"end":46,"id":8},{"text":"the","start":47,"end":50,"id":9},{"text":"two","start":51,"end":54,"id":10},{"text":"estimation","start":55,"end":65,"id":11},{"text":"tasks","start":66,"end":71,"id":12},{"text":"to","start":72,"end":74,"id":13},{"text":"the","start":75,"end":78,"id":14},{"text":"other","start":79,"end":84,"id":15},{"text":"existing","start":85,"end":93,"id":16},{"text":"methods","start":94,"end":101,"id":17},{"text":".","start":101,"end":102,"id":18}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":30,"end":43,"token_start":6,"token_end":7,"label":"ALGO","answer":"reject"},{"start":85,"end":101,"token_start":16,"token_end":17,"label":"ALGO","answer":"reject"}],"answer":"reject"}
{"text":"It is applicable to Gaussian as well as non-Gaussian data.","_input_hash":-488972745,"_task_hash":1654397282,"tokens":[{"text":"It","start":0,"end":2,"id":0},{"text":"is","start":3,"end":5,"id":1},{"text":"applicable","start":6,"end":16,"id":2},{"text":"to","start":17,"end":19,"id":3},{"text":"Gaussian","start":20,"end":28,"id":4},{"text":"as","start":29,"end":31,"id":5},{"text":"well","start":32,"end":36,"id":6},{"text":"as","start":37,"end":39,"id":7},{"text":"non","start":40,"end":43,"id":8},{"text":"-","start":43,"end":44,"id":9},{"text":"Gaussian","start":44,"end":52,"id":10},{"text":"data","start":53,"end":57,"id":11},{"text":".","start":57,"end":58,"id":12}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"Unlike previous methods, its quantization error depends only on the intrinsic dimension of the data distribution, rather than the apparent dimension of the space in which the data happen to lie.","_input_hash":1862424817,"_task_hash":383444665,"tokens":[{"text":"Unlike","start":0,"end":6,"id":0},{"text":"previous","start":7,"end":15,"id":1},{"text":"methods","start":16,"end":23,"id":2},{"text":",","start":23,"end":24,"id":3},{"text":"its","start":25,"end":28,"id":4},{"text":"quantization","start":29,"end":41,"id":5},{"text":"error","start":42,"end":47,"id":6},{"text":"depends","start":48,"end":55,"id":7},{"text":"only","start":56,"end":60,"id":8},{"text":"on","start":61,"end":63,"id":9},{"text":"the","start":64,"end":67,"id":10},{"text":"intrinsic","start":68,"end":77,"id":11},{"text":"dimension","start":78,"end":87,"id":12},{"text":"of","start":88,"end":90,"id":13},{"text":"the","start":91,"end":94,"id":14},{"text":"data","start":95,"end":99,"id":15},{"text":"distribution","start":100,"end":112,"id":16},{"text":",","start":112,"end":113,"id":17},{"text":"rather","start":114,"end":120,"id":18},{"text":"than","start":121,"end":125,"id":19},{"text":"the","start":126,"end":129,"id":20},{"text":"apparent","start":130,"end":138,"id":21},{"text":"dimension","start":139,"end":148,"id":22},{"text":"of","start":149,"end":151,"id":23},{"text":"the","start":152,"end":155,"id":24},{"text":"space","start":156,"end":161,"id":25},{"text":"in","start":162,"end":164,"id":26},{"text":"which","start":165,"end":170,"id":27},{"text":"the","start":171,"end":174,"id":28},{"text":"data","start":175,"end":179,"id":29},{"text":"happen","start":180,"end":186,"id":30},{"text":"to","start":187,"end":189,"id":31},{"text":"lie","start":190,"end":193,"id":32},{"text":".","start":193,"end":194,"id":33}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":7,"end":23,"token_start":1,"token_end":2,"label":"ALGO","answer":"reject"}],"answer":"reject"}
{"text":"The first is a direct generalization of Bartlett et al. [","_input_hash":-845656059,"_task_hash":1605296307,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"first","start":4,"end":9,"id":1},{"text":"is","start":10,"end":12,"id":2},{"text":"a","start":13,"end":14,"id":3},{"text":"direct","start":15,"end":21,"id":4},{"text":"generalization","start":22,"end":36,"id":5},{"text":"of","start":37,"end":39,"id":6},{"text":"Bartlett","start":40,"end":48,"id":7},{"text":"et","start":49,"end":51,"id":8},{"text":"al","start":52,"end":54,"id":9},{"text":".","start":54,"end":55,"id":10},{"text":"[","start":56,"end":57,"id":11}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"Comparison with the results obtained in the literature shows that the bound presented here provides an additional $36-40\\%$ reduction.","_input_hash":-1095710320,"_task_hash":502475128,"tokens":[{"text":"Comparison","start":0,"end":10,"id":0},{"text":"with","start":11,"end":15,"id":1},{"text":"the","start":16,"end":19,"id":2},{"text":"results","start":20,"end":27,"id":3},{"text":"obtained","start":28,"end":36,"id":4},{"text":"in","start":37,"end":39,"id":5},{"text":"the","start":40,"end":43,"id":6},{"text":"literature","start":44,"end":54,"id":7},{"text":"shows","start":55,"end":60,"id":8},{"text":"that","start":61,"end":65,"id":9},{"text":"the","start":66,"end":69,"id":10},{"text":"bound","start":70,"end":75,"id":11},{"text":"presented","start":76,"end":85,"id":12},{"text":"here","start":86,"end":90,"id":13},{"text":"provides","start":91,"end":99,"id":14},{"text":"an","start":100,"end":102,"id":15},{"text":"additional","start":103,"end":113,"id":16},{"text":"$","start":114,"end":115,"id":17},{"text":"36","start":115,"end":117,"id":18},{"text":"-","start":117,"end":118,"id":19},{"text":"40\\%$","start":118,"end":123,"id":20},{"text":"reduction","start":124,"end":133,"id":21},{"text":".","start":133,"end":134,"id":22}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
