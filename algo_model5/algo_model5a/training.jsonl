{"text":"We then discuss relative bounds, comparing the generalization error of two classification rules, showing how the margin assumption of Mammen and Tsybakov can be replaced with some empirical measure of the covariance structure of the classification model.","_input_hash":-1789333198,"_task_hash":1784519736,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"then","start":3,"end":7,"id":1},{"text":"discuss","start":8,"end":15,"id":2},{"text":"relative","start":16,"end":24,"id":3},{"text":"bounds","start":25,"end":31,"id":4},{"text":",","start":31,"end":32,"id":5},{"text":"comparing","start":33,"end":42,"id":6},{"text":"the","start":43,"end":46,"id":7},{"text":"generalization","start":47,"end":61,"id":8},{"text":"error","start":62,"end":67,"id":9},{"text":"of","start":68,"end":70,"id":10},{"text":"two","start":71,"end":74,"id":11},{"text":"classification","start":75,"end":89,"id":12},{"text":"rules","start":90,"end":95,"id":13},{"text":",","start":95,"end":96,"id":14},{"text":"showing","start":97,"end":104,"id":15},{"text":"how","start":105,"end":108,"id":16},{"text":"the","start":109,"end":112,"id":17},{"text":"margin","start":113,"end":119,"id":18},{"text":"assumption","start":120,"end":130,"id":19},{"text":"of","start":131,"end":133,"id":20},{"text":"Mammen","start":134,"end":140,"id":21},{"text":"and","start":141,"end":144,"id":22},{"text":"Tsybakov","start":145,"end":153,"id":23},{"text":"can","start":154,"end":157,"id":24},{"text":"be","start":158,"end":160,"id":25},{"text":"replaced","start":161,"end":169,"id":26},{"text":"with","start":170,"end":174,"id":27},{"text":"some","start":175,"end":179,"id":28},{"text":"empirical","start":180,"end":189,"id":29},{"text":"measure","start":190,"end":197,"id":30},{"text":"of","start":198,"end":200,"id":31},{"text":"the","start":201,"end":204,"id":32},{"text":"covariance","start":205,"end":215,"id":33},{"text":"structure","start":216,"end":225,"id":34},{"text":"of","start":226,"end":228,"id":35},{"text":"the","start":229,"end":232,"id":36},{"text":"classification","start":233,"end":247,"id":37},{"text":"model","start":248,"end":253,"id":38},{"text":".","start":253,"end":254,"id":39}],"spans":[{"start":233,"end":247,"token_start":37,"token_end":37,"label":"ALGO","answer":"accept"}],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"Their Bayesian KNN (BKNN) approach uses a pseudo-likelihood function, and standard Markov chain Monte Carlo (MCMC) techniques to draw posterior samples.","_input_hash":-1316639529,"_task_hash":167483520,"tokens":[{"text":"Their","start":0,"end":5,"id":0},{"text":"Bayesian","start":6,"end":14,"id":1},{"text":"KNN","start":15,"end":18,"id":2},{"text":"(","start":19,"end":20,"id":3},{"text":"BKNN","start":20,"end":24,"id":4},{"text":")","start":24,"end":25,"id":5},{"text":"approach","start":26,"end":34,"id":6},{"text":"uses","start":35,"end":39,"id":7},{"text":"a","start":40,"end":41,"id":8},{"text":"pseudo","start":42,"end":48,"id":9},{"text":"-","start":48,"end":49,"id":10},{"text":"likelihood","start":49,"end":59,"id":11},{"text":"function","start":60,"end":68,"id":12},{"text":",","start":68,"end":69,"id":13},{"text":"and","start":70,"end":73,"id":14},{"text":"standard","start":74,"end":82,"id":15},{"text":"Markov","start":83,"end":89,"id":16},{"text":"chain","start":90,"end":95,"id":17},{"text":"Monte","start":96,"end":101,"id":18},{"text":"Carlo","start":102,"end":107,"id":19},{"text":"(","start":108,"end":109,"id":20},{"text":"MCMC","start":109,"end":113,"id":21},{"text":")","start":113,"end":114,"id":22},{"text":"techniques","start":115,"end":125,"id":23},{"text":"to","start":126,"end":128,"id":24},{"text":"draw","start":129,"end":133,"id":25},{"text":"posterior","start":134,"end":143,"id":26},{"text":"samples","start":144,"end":151,"id":27},{"text":".","start":151,"end":152,"id":28}],"spans":[{"token_start":1,"token_end":2,"start":6,"end":18,"text":"Bayesian KNN","label":"ALGO","source":"./algo_model5","input_hash":-1316639529,"answer":"accept"},{"token_start":16,"token_end":19,"start":83,"end":107,"text":"Markov chain Monte Carlo","label":"ALGO","source":"./algo_model5","input_hash":-1316639529,"answer":"accept"}],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"Second, we apply our results to the analysis of ERG models.","_input_hash":-1181044587,"_task_hash":-86563709,"tokens":[{"text":"Second","start":0,"end":6,"id":0},{"text":",","start":6,"end":7,"id":1},{"text":"we","start":8,"end":10,"id":2},{"text":"apply","start":11,"end":16,"id":3},{"text":"our","start":17,"end":20,"id":4},{"text":"results","start":21,"end":28,"id":5},{"text":"to","start":29,"end":31,"id":6},{"text":"the","start":32,"end":35,"id":7},{"text":"analysis","start":36,"end":44,"id":8},{"text":"of","start":45,"end":47,"id":9},{"text":"ERG","start":48,"end":51,"id":10},{"text":"models","start":52,"end":58,"id":11},{"text":".","start":58,"end":59,"id":12}],"spans":[{"start":48,"end":51,"token_start":10,"token_end":10,"label":"ALGO","answer":"accept"}],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"The most direct way to express arbitrary dependencies in datasets is to estimate the joint distribution and to apply afterwards the argmax-function to obtain the mode of the corresponding conditional distribution.","_input_hash":-1027595124,"_task_hash":179847809,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"most","start":4,"end":8,"id":1},{"text":"direct","start":9,"end":15,"id":2},{"text":"way","start":16,"end":19,"id":3},{"text":"to","start":20,"end":22,"id":4},{"text":"express","start":23,"end":30,"id":5},{"text":"arbitrary","start":31,"end":40,"id":6},{"text":"dependencies","start":41,"end":53,"id":7},{"text":"in","start":54,"end":56,"id":8},{"text":"datasets","start":57,"end":65,"id":9},{"text":"is","start":66,"end":68,"id":10},{"text":"to","start":69,"end":71,"id":11},{"text":"estimate","start":72,"end":80,"id":12},{"text":"the","start":81,"end":84,"id":13},{"text":"joint","start":85,"end":90,"id":14},{"text":"distribution","start":91,"end":103,"id":15},{"text":"and","start":104,"end":107,"id":16},{"text":"to","start":108,"end":110,"id":17},{"text":"apply","start":111,"end":116,"id":18},{"text":"afterwards","start":117,"end":127,"id":19},{"text":"the","start":128,"end":131,"id":20},{"text":"argmax","start":132,"end":138,"id":21},{"text":"-","start":138,"end":139,"id":22},{"text":"function","start":139,"end":147,"id":23},{"text":"to","start":148,"end":150,"id":24},{"text":"obtain","start":151,"end":157,"id":25},{"text":"the","start":158,"end":161,"id":26},{"text":"mode","start":162,"end":166,"id":27},{"text":"of","start":167,"end":169,"id":28},{"text":"the","start":170,"end":173,"id":29},{"text":"corresponding","start":174,"end":187,"id":30},{"text":"conditional","start":188,"end":199,"id":31},{"text":"distribution","start":200,"end":212,"id":32},{"text":".","start":212,"end":213,"id":33}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"We analyze the method in the high-dimensional setting, where the number of dimensions $p$ is allowed to grow with the number of observations $n$. The rate of convergence of the estimate is demonstrated to depend explicitly on the sparsity of the underlying graph.","_input_hash":418561057,"_task_hash":812657583,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"analyze","start":3,"end":10,"id":1},{"text":"the","start":11,"end":14,"id":2},{"text":"method","start":15,"end":21,"id":3},{"text":"in","start":22,"end":24,"id":4},{"text":"the","start":25,"end":28,"id":5},{"text":"high","start":29,"end":33,"id":6},{"text":"-","start":33,"end":34,"id":7},{"text":"dimensional","start":34,"end":45,"id":8},{"text":"setting","start":46,"end":53,"id":9},{"text":",","start":53,"end":54,"id":10},{"text":"where","start":55,"end":60,"id":11},{"text":"the","start":61,"end":64,"id":12},{"text":"number","start":65,"end":71,"id":13},{"text":"of","start":72,"end":74,"id":14},{"text":"dimensions","start":75,"end":85,"id":15},{"text":"$","start":86,"end":87,"id":16},{"text":"p$","start":87,"end":89,"id":17},{"text":"is","start":90,"end":92,"id":18},{"text":"allowed","start":93,"end":100,"id":19},{"text":"to","start":101,"end":103,"id":20},{"text":"grow","start":104,"end":108,"id":21},{"text":"with","start":109,"end":113,"id":22},{"text":"the","start":114,"end":117,"id":23},{"text":"number","start":118,"end":124,"id":24},{"text":"of","start":125,"end":127,"id":25},{"text":"observations","start":128,"end":140,"id":26},{"text":"$","start":141,"end":142,"id":27},{"text":"n$.","start":142,"end":145,"id":28},{"text":"The","start":146,"end":149,"id":29},{"text":"rate","start":150,"end":154,"id":30},{"text":"of","start":155,"end":157,"id":31},{"text":"convergence","start":158,"end":169,"id":32},{"text":"of","start":170,"end":172,"id":33},{"text":"the","start":173,"end":176,"id":34},{"text":"estimate","start":177,"end":185,"id":35},{"text":"is","start":186,"end":188,"id":36},{"text":"demonstrated","start":189,"end":201,"id":37},{"text":"to","start":202,"end":204,"id":38},{"text":"depend","start":205,"end":211,"id":39},{"text":"explicitly","start":212,"end":222,"id":40},{"text":"on","start":223,"end":225,"id":41},{"text":"the","start":226,"end":229,"id":42},{"text":"sparsity","start":230,"end":238,"id":43},{"text":"of","start":239,"end":241,"id":44},{"text":"the","start":242,"end":245,"id":45},{"text":"underlying","start":246,"end":256,"id":46},{"text":"graph","start":257,"end":262,"id":47},{"text":".","start":262,"end":263,"id":48}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"We explore further properties of this unique scheme, stability and convergence are established.","_input_hash":-1864270311,"_task_hash":647865512,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"explore","start":3,"end":10,"id":1},{"text":"further","start":11,"end":18,"id":2},{"text":"properties","start":19,"end":29,"id":3},{"text":"of","start":30,"end":32,"id":4},{"text":"this","start":33,"end":37,"id":5},{"text":"unique","start":38,"end":44,"id":6},{"text":"scheme","start":45,"end":51,"id":7},{"text":",","start":51,"end":52,"id":8},{"text":"stability","start":53,"end":62,"id":9},{"text":"and","start":63,"end":66,"id":10},{"text":"convergence","start":67,"end":78,"id":11},{"text":"are","start":79,"end":82,"id":12},{"text":"established","start":83,"end":94,"id":13},{"text":".","start":94,"end":95,"id":14}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"  High dimensional features also arise when we consider high-order interactions.","_input_hash":-910918860,"_task_hash":852057726,"tokens":[{"text":"  ","start":0,"end":2,"id":0},{"text":"High","start":2,"end":6,"id":1},{"text":"dimensional","start":7,"end":18,"id":2},{"text":"features","start":19,"end":27,"id":3},{"text":"also","start":28,"end":32,"id":4},{"text":"arise","start":33,"end":38,"id":5},{"text":"when","start":39,"end":43,"id":6},{"text":"we","start":44,"end":46,"id":7},{"text":"consider","start":47,"end":55,"id":8},{"text":"high","start":56,"end":60,"id":9},{"text":"-","start":60,"end":61,"id":10},{"text":"order","start":61,"end":66,"id":11},{"text":"interactions","start":67,"end":79,"id":12},{"text":".","start":79,"end":80,"id":13}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"We present a new online boosting algorithm for adapting the weights of a boosted classifier, which yields a closer approximation to Freund and Schapire's AdaBoost algorithm than previous online boosting algorithms.","_input_hash":199535872,"_task_hash":-1707277270,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"present","start":3,"end":10,"id":1},{"text":"a","start":11,"end":12,"id":2},{"text":"new","start":13,"end":16,"id":3},{"text":"online","start":17,"end":23,"id":4},{"text":"boosting","start":24,"end":32,"id":5},{"text":"algorithm","start":33,"end":42,"id":6},{"text":"for","start":43,"end":46,"id":7},{"text":"adapting","start":47,"end":55,"id":8},{"text":"the","start":56,"end":59,"id":9},{"text":"weights","start":60,"end":67,"id":10},{"text":"of","start":68,"end":70,"id":11},{"text":"a","start":71,"end":72,"id":12},{"text":"boosted","start":73,"end":80,"id":13},{"text":"classifier","start":81,"end":91,"id":14},{"text":",","start":91,"end":92,"id":15},{"text":"which","start":93,"end":98,"id":16},{"text":"yields","start":99,"end":105,"id":17},{"text":"a","start":106,"end":107,"id":18},{"text":"closer","start":108,"end":114,"id":19},{"text":"approximation","start":115,"end":128,"id":20},{"text":"to","start":129,"end":131,"id":21},{"text":"Freund","start":132,"end":138,"id":22},{"text":"and","start":139,"end":142,"id":23},{"text":"Schapire","start":143,"end":151,"id":24},{"text":"'s","start":151,"end":153,"id":25},{"text":"AdaBoost","start":154,"end":162,"id":26},{"text":"algorithm","start":163,"end":172,"id":27},{"text":"than","start":173,"end":177,"id":28},{"text":"previous","start":178,"end":186,"id":29},{"text":"online","start":187,"end":193,"id":30},{"text":"boosting","start":194,"end":202,"id":31},{"text":"algorithms","start":203,"end":213,"id":32},{"text":".","start":213,"end":214,"id":33}],"spans":[{"start":17,"end":32,"token_start":4,"token_end":5,"label":"ALGO","answer":"accept"},{"start":154,"end":162,"token_start":26,"token_end":26,"label":"ALGO","answer":"accept"},{"start":187,"end":202,"token_start":30,"token_end":31,"label":"ALGO","answer":"accept"}],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"At the end, we calculate the topology of the hidden part of a dendrogram.","_input_hash":1928286706,"_task_hash":1441321189,"tokens":[{"text":"At","start":0,"end":2,"id":0},{"text":"the","start":3,"end":6,"id":1},{"text":"end","start":7,"end":10,"id":2},{"text":",","start":10,"end":11,"id":3},{"text":"we","start":12,"end":14,"id":4},{"text":"calculate","start":15,"end":24,"id":5},{"text":"the","start":25,"end":28,"id":6},{"text":"topology","start":29,"end":37,"id":7},{"text":"of","start":38,"end":40,"id":8},{"text":"the","start":41,"end":44,"id":9},{"text":"hidden","start":45,"end":51,"id":10},{"text":"part","start":52,"end":56,"id":11},{"text":"of","start":57,"end":59,"id":12},{"text":"a","start":60,"end":61,"id":13},{"text":"dendrogram","start":62,"end":72,"id":14},{"text":".","start":72,"end":73,"id":15}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"Typically, flow cytometric data analysis is performed through a series of 2-dimensional projections onto the axes of the data set.","_input_hash":-1351284336,"_task_hash":-39802717,"tokens":[{"text":"Typically","start":0,"end":9,"id":0},{"text":",","start":9,"end":10,"id":1},{"text":"flow","start":11,"end":15,"id":2},{"text":"cytometric","start":16,"end":26,"id":3},{"text":"data","start":27,"end":31,"id":4},{"text":"analysis","start":32,"end":40,"id":5},{"text":"is","start":41,"end":43,"id":6},{"text":"performed","start":44,"end":53,"id":7},{"text":"through","start":54,"end":61,"id":8},{"text":"a","start":62,"end":63,"id":9},{"text":"series","start":64,"end":70,"id":10},{"text":"of","start":71,"end":73,"id":11},{"text":"2-dimensional","start":74,"end":87,"id":12},{"text":"projections","start":88,"end":99,"id":13},{"text":"onto","start":100,"end":104,"id":14},{"text":"the","start":105,"end":108,"id":15},{"text":"axes","start":109,"end":113,"id":16},{"text":"of","start":114,"end":116,"id":17},{"text":"the","start":117,"end":120,"id":18},{"text":"data","start":121,"end":125,"id":19},{"text":"set","start":126,"end":129,"id":20},{"text":".","start":129,"end":130,"id":21}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"We show how to associate to any posterior distribution an effective temperature relating it to the Gibbs prior distribution with the same level of expected error rate, and how to estimate this effective temperature from data, resulting in an estimator whose expected error rate converges according to the best possible power of the sample size adaptively under any margin and parametric complexity assumptions.","_input_hash":1038190572,"_task_hash":655357634,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"show","start":3,"end":7,"id":1},{"text":"how","start":8,"end":11,"id":2},{"text":"to","start":12,"end":14,"id":3},{"text":"associate","start":15,"end":24,"id":4},{"text":"to","start":25,"end":27,"id":5},{"text":"any","start":28,"end":31,"id":6},{"text":"posterior","start":32,"end":41,"id":7},{"text":"distribution","start":42,"end":54,"id":8},{"text":"an","start":55,"end":57,"id":9},{"text":"effective","start":58,"end":67,"id":10},{"text":"temperature","start":68,"end":79,"id":11},{"text":"relating","start":80,"end":88,"id":12},{"text":"it","start":89,"end":91,"id":13},{"text":"to","start":92,"end":94,"id":14},{"text":"the","start":95,"end":98,"id":15},{"text":"Gibbs","start":99,"end":104,"id":16},{"text":"prior","start":105,"end":110,"id":17},{"text":"distribution","start":111,"end":123,"id":18},{"text":"with","start":124,"end":128,"id":19},{"text":"the","start":129,"end":132,"id":20},{"text":"same","start":133,"end":137,"id":21},{"text":"level","start":138,"end":143,"id":22},{"text":"of","start":144,"end":146,"id":23},{"text":"expected","start":147,"end":155,"id":24},{"text":"error","start":156,"end":161,"id":25},{"text":"rate","start":162,"end":166,"id":26},{"text":",","start":166,"end":167,"id":27},{"text":"and","start":168,"end":171,"id":28},{"text":"how","start":172,"end":175,"id":29},{"text":"to","start":176,"end":178,"id":30},{"text":"estimate","start":179,"end":187,"id":31},{"text":"this","start":188,"end":192,"id":32},{"text":"effective","start":193,"end":202,"id":33},{"text":"temperature","start":203,"end":214,"id":34},{"text":"from","start":215,"end":219,"id":35},{"text":"data","start":220,"end":224,"id":36},{"text":",","start":224,"end":225,"id":37},{"text":"resulting","start":226,"end":235,"id":38},{"text":"in","start":236,"end":238,"id":39},{"text":"an","start":239,"end":241,"id":40},{"text":"estimator","start":242,"end":251,"id":41},{"text":"whose","start":252,"end":257,"id":42},{"text":"expected","start":258,"end":266,"id":43},{"text":"error","start":267,"end":272,"id":44},{"text":"rate","start":273,"end":277,"id":45},{"text":"converges","start":278,"end":287,"id":46},{"text":"according","start":288,"end":297,"id":47},{"text":"to","start":298,"end":300,"id":48},{"text":"the","start":301,"end":304,"id":49},{"text":"best","start":305,"end":309,"id":50},{"text":"possible","start":310,"end":318,"id":51},{"text":"power","start":319,"end":324,"id":52},{"text":"of","start":325,"end":327,"id":53},{"text":"the","start":328,"end":331,"id":54},{"text":"sample","start":332,"end":338,"id":55},{"text":"size","start":339,"end":343,"id":56},{"text":"adaptively","start":344,"end":354,"id":57},{"text":"under","start":355,"end":360,"id":58},{"text":"any","start":361,"end":364,"id":59},{"text":"margin","start":365,"end":371,"id":60},{"text":"and","start":372,"end":375,"id":61},{"text":"parametric","start":376,"end":386,"id":62},{"text":"complexity","start":387,"end":397,"id":63},{"text":"assumptions","start":398,"end":409,"id":64},{"text":".","start":409,"end":410,"id":65}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"The combination of sparsity and smoothness is crucial for mathematical theory as well as performance for finite-sample data.","_input_hash":161615895,"_task_hash":-1344135724,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"combination","start":4,"end":15,"id":1},{"text":"of","start":16,"end":18,"id":2},{"text":"sparsity","start":19,"end":27,"id":3},{"text":"and","start":28,"end":31,"id":4},{"text":"smoothness","start":32,"end":42,"id":5},{"text":"is","start":43,"end":45,"id":6},{"text":"crucial","start":46,"end":53,"id":7},{"text":"for","start":54,"end":57,"id":8},{"text":"mathematical","start":58,"end":70,"id":9},{"text":"theory","start":71,"end":77,"id":10},{"text":"as","start":78,"end":80,"id":11},{"text":"well","start":81,"end":85,"id":12},{"text":"as","start":86,"end":88,"id":13},{"text":"performance","start":89,"end":100,"id":14},{"text":"for","start":101,"end":104,"id":15},{"text":"finite","start":105,"end":111,"id":16},{"text":"-","start":111,"end":112,"id":17},{"text":"sample","start":112,"end":118,"id":18},{"text":"data","start":119,"end":123,"id":19},{"text":".","start":123,"end":124,"id":20}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"Online detection of changepoints is useful in modelling and prediction of time series in application areas such as finance, biometrics, and robotics.","_input_hash":-2147316778,"_task_hash":392928165,"tokens":[{"text":"Online","start":0,"end":6,"id":0},{"text":"detection","start":7,"end":16,"id":1},{"text":"of","start":17,"end":19,"id":2},{"text":"changepoints","start":20,"end":32,"id":3},{"text":"is","start":33,"end":35,"id":4},{"text":"useful","start":36,"end":42,"id":5},{"text":"in","start":43,"end":45,"id":6},{"text":"modelling","start":46,"end":55,"id":7},{"text":"and","start":56,"end":59,"id":8},{"text":"prediction","start":60,"end":70,"id":9},{"text":"of","start":71,"end":73,"id":10},{"text":"time","start":74,"end":78,"id":11},{"text":"series","start":79,"end":85,"id":12},{"text":"in","start":86,"end":88,"id":13},{"text":"application","start":89,"end":100,"id":14},{"text":"areas","start":101,"end":106,"id":15},{"text":"such","start":107,"end":111,"id":16},{"text":"as","start":112,"end":114,"id":17},{"text":"finance","start":115,"end":122,"id":18},{"text":",","start":122,"end":123,"id":19},{"text":"biometrics","start":124,"end":134,"id":20},{"text":",","start":134,"end":135,"id":21},{"text":"and","start":136,"end":139,"id":22},{"text":"robotics","start":140,"end":148,"id":23},{"text":".","start":148,"end":149,"id":24}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"The strategy involves a ranking of explanatory variables using the random forests score of importance and a stepwise ascending variable introduction strategy.","_input_hash":-1863389377,"_task_hash":874675346,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"strategy","start":4,"end":12,"id":1},{"text":"involves","start":13,"end":21,"id":2},{"text":"a","start":22,"end":23,"id":3},{"text":"ranking","start":24,"end":31,"id":4},{"text":"of","start":32,"end":34,"id":5},{"text":"explanatory","start":35,"end":46,"id":6},{"text":"variables","start":47,"end":56,"id":7},{"text":"using","start":57,"end":62,"id":8},{"text":"the","start":63,"end":66,"id":9},{"text":"random","start":67,"end":73,"id":10},{"text":"forests","start":74,"end":81,"id":11},{"text":"score","start":82,"end":87,"id":12},{"text":"of","start":88,"end":90,"id":13},{"text":"importance","start":91,"end":101,"id":14},{"text":"and","start":102,"end":105,"id":15},{"text":"a","start":106,"end":107,"id":16},{"text":"stepwise","start":108,"end":116,"id":17},{"text":"ascending","start":117,"end":126,"id":18},{"text":"variable","start":127,"end":135,"id":19},{"text":"introduction","start":136,"end":148,"id":20},{"text":"strategy","start":149,"end":157,"id":21},{"text":".","start":157,"end":158,"id":22}],"spans":[{"start":67,"end":81,"token_start":10,"token_end":11,"label":"ALGO","answer":"accept"}],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"2001) in p-adic geometry.","_input_hash":-933078138,"_task_hash":-914511922,"tokens":[{"text":"2001","start":0,"end":4,"id":0},{"text":")","start":4,"end":5,"id":1},{"text":"in","start":6,"end":8,"id":2},{"text":"p","start":9,"end":10,"id":3},{"text":"-","start":10,"end":11,"id":4},{"text":"adic","start":11,"end":15,"id":5},{"text":"geometry","start":16,"end":24,"id":6},{"text":".","start":24,"end":25,"id":7}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"We present and prove conditions on the manifold that are necessary for the success of the algorithms.","_input_hash":-1272831787,"_task_hash":-387941999,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"present","start":3,"end":10,"id":1},{"text":"and","start":11,"end":14,"id":2},{"text":"prove","start":15,"end":20,"id":3},{"text":"conditions","start":21,"end":31,"id":4},{"text":"on","start":32,"end":34,"id":5},{"text":"the","start":35,"end":38,"id":6},{"text":"manifold","start":39,"end":47,"id":7},{"text":"that","start":48,"end":52,"id":8},{"text":"are","start":53,"end":56,"id":9},{"text":"necessary","start":57,"end":66,"id":10},{"text":"for","start":67,"end":70,"id":11},{"text":"the","start":71,"end":74,"id":12},{"text":"success","start":75,"end":82,"id":13},{"text":"of","start":83,"end":85,"id":14},{"text":"the","start":86,"end":89,"id":15},{"text":"algorithms","start":90,"end":100,"id":16},{"text":".","start":100,"end":101,"id":17}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"With Dirichlet Process priors and an efficient implementation the models are highly scalable, as demonstrated with a social network from the Last.fm web site, with 670,000 nodes and 1.89 million links.","_input_hash":913960638,"_task_hash":-2089473856,"tokens":[{"text":"With","start":0,"end":4,"id":0},{"text":"Dirichlet","start":5,"end":14,"id":1},{"text":"Process","start":15,"end":22,"id":2},{"text":"priors","start":23,"end":29,"id":3},{"text":"and","start":30,"end":33,"id":4},{"text":"an","start":34,"end":36,"id":5},{"text":"efficient","start":37,"end":46,"id":6},{"text":"implementation","start":47,"end":61,"id":7},{"text":"the","start":62,"end":65,"id":8},{"text":"models","start":66,"end":72,"id":9},{"text":"are","start":73,"end":76,"id":10},{"text":"highly","start":77,"end":83,"id":11},{"text":"scalable","start":84,"end":92,"id":12},{"text":",","start":92,"end":93,"id":13},{"text":"as","start":94,"end":96,"id":14},{"text":"demonstrated","start":97,"end":109,"id":15},{"text":"with","start":110,"end":114,"id":16},{"text":"a","start":115,"end":116,"id":17},{"text":"social","start":117,"end":123,"id":18},{"text":"network","start":124,"end":131,"id":19},{"text":"from","start":132,"end":136,"id":20},{"text":"the","start":137,"end":140,"id":21},{"text":"Last.fm","start":141,"end":148,"id":22},{"text":"web","start":149,"end":152,"id":23},{"text":"site","start":153,"end":157,"id":24},{"text":",","start":157,"end":158,"id":25},{"text":"with","start":159,"end":163,"id":26},{"text":"670,000","start":164,"end":171,"id":27},{"text":"nodes","start":172,"end":177,"id":28},{"text":"and","start":178,"end":181,"id":29},{"text":"1.89","start":182,"end":186,"id":30},{"text":"million","start":187,"end":194,"id":31},{"text":"links","start":195,"end":200,"id":32},{"text":".","start":200,"end":201,"id":33}],"spans":[{"token_start":1,"token_end":2,"start":5,"end":22,"text":"Dirichlet Process","label":"ALGO","source":"./algo_model5","input_hash":913960638,"answer":"accept"}],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"This series expansion is shown to be useful for numerical calculations of the JSD, when the probability distributions are nearly equal, and for which, consequently, small numerical errors dominate evaluation.","_input_hash":-508395004,"_task_hash":296377828,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"series","start":5,"end":11,"id":1},{"text":"expansion","start":12,"end":21,"id":2},{"text":"is","start":22,"end":24,"id":3},{"text":"shown","start":25,"end":30,"id":4},{"text":"to","start":31,"end":33,"id":5},{"text":"be","start":34,"end":36,"id":6},{"text":"useful","start":37,"end":43,"id":7},{"text":"for","start":44,"end":47,"id":8},{"text":"numerical","start":48,"end":57,"id":9},{"text":"calculations","start":58,"end":70,"id":10},{"text":"of","start":71,"end":73,"id":11},{"text":"the","start":74,"end":77,"id":12},{"text":"JSD","start":78,"end":81,"id":13},{"text":",","start":81,"end":82,"id":14},{"text":"when","start":83,"end":87,"id":15},{"text":"the","start":88,"end":91,"id":16},{"text":"probability","start":92,"end":103,"id":17},{"text":"distributions","start":104,"end":117,"id":18},{"text":"are","start":118,"end":121,"id":19},{"text":"nearly","start":122,"end":128,"id":20},{"text":"equal","start":129,"end":134,"id":21},{"text":",","start":134,"end":135,"id":22},{"text":"and","start":136,"end":139,"id":23},{"text":"for","start":140,"end":143,"id":24},{"text":"which","start":144,"end":149,"id":25},{"text":",","start":149,"end":150,"id":26},{"text":"consequently","start":151,"end":163,"id":27},{"text":",","start":163,"end":164,"id":28},{"text":"small","start":165,"end":170,"id":29},{"text":"numerical","start":171,"end":180,"id":30},{"text":"errors","start":181,"end":187,"id":31},{"text":"dominate","start":188,"end":196,"id":32},{"text":"evaluation","start":197,"end":207,"id":33},{"text":".","start":207,"end":208,"id":34}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"In this paper we present a means of determining a low-dimensional projection which maintains the high-dimensional relationships (i.e. information) between differing oncological data sets.","_input_hash":1027560018,"_task_hash":989776101,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"this","start":3,"end":7,"id":1},{"text":"paper","start":8,"end":13,"id":2},{"text":"we","start":14,"end":16,"id":3},{"text":"present","start":17,"end":24,"id":4},{"text":"a","start":25,"end":26,"id":5},{"text":"means","start":27,"end":32,"id":6},{"text":"of","start":33,"end":35,"id":7},{"text":"determining","start":36,"end":47,"id":8},{"text":"a","start":48,"end":49,"id":9},{"text":"low","start":50,"end":53,"id":10},{"text":"-","start":53,"end":54,"id":11},{"text":"dimensional","start":54,"end":65,"id":12},{"text":"projection","start":66,"end":76,"id":13},{"text":"which","start":77,"end":82,"id":14},{"text":"maintains","start":83,"end":92,"id":15},{"text":"the","start":93,"end":96,"id":16},{"text":"high","start":97,"end":101,"id":17},{"text":"-","start":101,"end":102,"id":18},{"text":"dimensional","start":102,"end":113,"id":19},{"text":"relationships","start":114,"end":127,"id":20},{"text":"(","start":128,"end":129,"id":21},{"text":"i.e.","start":129,"end":133,"id":22},{"text":"information","start":134,"end":145,"id":23},{"text":")","start":145,"end":146,"id":24},{"text":"between","start":147,"end":154,"id":25},{"text":"differing","start":155,"end":164,"id":26},{"text":"oncological","start":165,"end":176,"id":27},{"text":"data","start":177,"end":181,"id":28},{"text":"sets","start":182,"end":186,"id":29},{"text":".","start":186,"end":187,"id":30}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"Existing results on its performance apply to discrete combinatorial optimization where the optimization variables can assume only a finite set of possible values.","_input_hash":-1427032138,"_task_hash":36898530,"tokens":[{"text":"Existing","start":0,"end":8,"id":0},{"text":"results","start":9,"end":16,"id":1},{"text":"on","start":17,"end":19,"id":2},{"text":"its","start":20,"end":23,"id":3},{"text":"performance","start":24,"end":35,"id":4},{"text":"apply","start":36,"end":41,"id":5},{"text":"to","start":42,"end":44,"id":6},{"text":"discrete","start":45,"end":53,"id":7},{"text":"combinatorial","start":54,"end":67,"id":8},{"text":"optimization","start":68,"end":80,"id":9},{"text":"where","start":81,"end":86,"id":10},{"text":"the","start":87,"end":90,"id":11},{"text":"optimization","start":91,"end":103,"id":12},{"text":"variables","start":104,"end":113,"id":13},{"text":"can","start":114,"end":117,"id":14},{"text":"assume","start":118,"end":124,"id":15},{"text":"only","start":125,"end":129,"id":16},{"text":"a","start":130,"end":131,"id":17},{"text":"finite","start":132,"end":138,"id":18},{"text":"set","start":139,"end":142,"id":19},{"text":"of","start":143,"end":145,"id":20},{"text":"possible","start":146,"end":154,"id":21},{"text":"values","start":155,"end":161,"id":22},{"text":".","start":161,"end":162,"id":23}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"However, a fast online approximation is possible.","_input_hash":-1349805728,"_task_hash":2002389883,"tokens":[{"text":"However","start":0,"end":7,"id":0},{"text":",","start":7,"end":8,"id":1},{"text":"a","start":9,"end":10,"id":2},{"text":"fast","start":11,"end":15,"id":3},{"text":"online","start":16,"end":22,"id":4},{"text":"approximation","start":23,"end":36,"id":5},{"text":"is","start":37,"end":39,"id":6},{"text":"possible","start":40,"end":48,"id":7},{"text":".","start":48,"end":49,"id":8}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"One is when high dimensional measurements are available, for example, gene expression data produced by microarray techniques.","_input_hash":492717479,"_task_hash":1185154125,"tokens":[{"text":"One","start":0,"end":3,"id":0},{"text":"is","start":4,"end":6,"id":1},{"text":"when","start":7,"end":11,"id":2},{"text":"high","start":12,"end":16,"id":3},{"text":"dimensional","start":17,"end":28,"id":4},{"text":"measurements","start":29,"end":41,"id":5},{"text":"are","start":42,"end":45,"id":6},{"text":"available","start":46,"end":55,"id":7},{"text":",","start":55,"end":56,"id":8},{"text":"for","start":57,"end":60,"id":9},{"text":"example","start":61,"end":68,"id":10},{"text":",","start":68,"end":69,"id":11},{"text":"gene","start":70,"end":74,"id":12},{"text":"expression","start":75,"end":85,"id":13},{"text":"data","start":86,"end":90,"id":14},{"text":"produced","start":91,"end":99,"id":15},{"text":"by","start":100,"end":102,"id":16},{"text":"microarray","start":103,"end":113,"id":17},{"text":"techniques","start":114,"end":124,"id":18},{"text":".","start":124,"end":125,"id":19}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"  We use both simulated data and real data to test our methods in both chapters.","_input_hash":-389650462,"_task_hash":67022977,"tokens":[{"text":"  ","start":0,"end":2,"id":0},{"text":"We","start":2,"end":4,"id":1},{"text":"use","start":5,"end":8,"id":2},{"text":"both","start":9,"end":13,"id":3},{"text":"simulated","start":14,"end":23,"id":4},{"text":"data","start":24,"end":28,"id":5},{"text":"and","start":29,"end":32,"id":6},{"text":"real","start":33,"end":37,"id":7},{"text":"data","start":38,"end":42,"id":8},{"text":"to","start":43,"end":45,"id":9},{"text":"test","start":46,"end":50,"id":10},{"text":"our","start":51,"end":54,"id":11},{"text":"methods","start":55,"end":62,"id":12},{"text":"in","start":63,"end":65,"id":13},{"text":"both","start":66,"end":70,"id":14},{"text":"chapters","start":71,"end":79,"id":15},{"text":".","start":79,"end":80,"id":16}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"We characterize and study variable importance (VIMP) and pairwise variable associations in binary regression trees.","_input_hash":-1220427477,"_task_hash":-1404360791,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"characterize","start":3,"end":15,"id":1},{"text":"and","start":16,"end":19,"id":2},{"text":"study","start":20,"end":25,"id":3},{"text":"variable","start":26,"end":34,"id":4},{"text":"importance","start":35,"end":45,"id":5},{"text":"(","start":46,"end":47,"id":6},{"text":"VIMP","start":47,"end":51,"id":7},{"text":")","start":51,"end":52,"id":8},{"text":"and","start":53,"end":56,"id":9},{"text":"pairwise","start":57,"end":65,"id":10},{"text":"variable","start":66,"end":74,"id":11},{"text":"associations","start":75,"end":87,"id":12},{"text":"in","start":88,"end":90,"id":13},{"text":"binary","start":91,"end":97,"id":14},{"text":"regression","start":98,"end":108,"id":15},{"text":"trees","start":109,"end":114,"id":16},{"text":".","start":114,"end":115,"id":17}],"spans":[{"start":91,"end":114,"token_start":14,"token_end":16,"label":"ALGO","answer":"accept"}],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"The new REMAPF algorithm estimates both the epsilon-machine and the decisional states from data.","_input_hash":-1763963857,"_task_hash":-989788376,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"new","start":4,"end":7,"id":1},{"text":"REMAPF","start":8,"end":14,"id":2},{"text":"algorithm","start":15,"end":24,"id":3},{"text":"estimates","start":25,"end":34,"id":4},{"text":"both","start":35,"end":39,"id":5},{"text":"the","start":40,"end":43,"id":6},{"text":"epsilon","start":44,"end":51,"id":7},{"text":"-","start":51,"end":52,"id":8},{"text":"machine","start":52,"end":59,"id":9},{"text":"and","start":60,"end":63,"id":10},{"text":"the","start":64,"end":67,"id":11},{"text":"decisional","start":68,"end":78,"id":12},{"text":"states","start":79,"end":85,"id":13},{"text":"from","start":86,"end":90,"id":14},{"text":"data","start":91,"end":95,"id":15},{"text":".","start":95,"end":96,"id":16}],"spans":[{"start":8,"end":14,"token_start":2,"token_end":2,"label":"ALGO","answer":"accept"},{"start":44,"end":59,"token_start":7,"token_end":9,"label":"ALGO","answer":"accept"}],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"Using convex analysis on the set of posterior probability measures, we show how to get local measures of the complexity of the classification model involving the relative entropy of posterior distributions with respect to Gibbs posterior measures.","_input_hash":-1112714564,"_task_hash":-26508879,"tokens":[{"text":"Using","start":0,"end":5,"id":0},{"text":"convex","start":6,"end":12,"id":1},{"text":"analysis","start":13,"end":21,"id":2},{"text":"on","start":22,"end":24,"id":3},{"text":"the","start":25,"end":28,"id":4},{"text":"set","start":29,"end":32,"id":5},{"text":"of","start":33,"end":35,"id":6},{"text":"posterior","start":36,"end":45,"id":7},{"text":"probability","start":46,"end":57,"id":8},{"text":"measures","start":58,"end":66,"id":9},{"text":",","start":66,"end":67,"id":10},{"text":"we","start":68,"end":70,"id":11},{"text":"show","start":71,"end":75,"id":12},{"text":"how","start":76,"end":79,"id":13},{"text":"to","start":80,"end":82,"id":14},{"text":"get","start":83,"end":86,"id":15},{"text":"local","start":87,"end":92,"id":16},{"text":"measures","start":93,"end":101,"id":17},{"text":"of","start":102,"end":104,"id":18},{"text":"the","start":105,"end":108,"id":19},{"text":"complexity","start":109,"end":119,"id":20},{"text":"of","start":120,"end":122,"id":21},{"text":"the","start":123,"end":126,"id":22},{"text":"classification","start":127,"end":141,"id":23},{"text":"model","start":142,"end":147,"id":24},{"text":"involving","start":148,"end":157,"id":25},{"text":"the","start":158,"end":161,"id":26},{"text":"relative","start":162,"end":170,"id":27},{"text":"entropy","start":171,"end":178,"id":28},{"text":"of","start":179,"end":181,"id":29},{"text":"posterior","start":182,"end":191,"id":30},{"text":"distributions","start":192,"end":205,"id":31},{"text":"with","start":206,"end":210,"id":32},{"text":"respect","start":211,"end":218,"id":33},{"text":"to","start":219,"end":221,"id":34},{"text":"Gibbs","start":222,"end":227,"id":35},{"text":"posterior","start":228,"end":237,"id":36},{"text":"measures","start":238,"end":246,"id":37},{"text":".","start":246,"end":247,"id":38}],"spans":[{"start":127,"end":141,"token_start":23,"token_end":23,"label":"ALGO","answer":"accept"}],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"This class consists of Locally Linear Embedding (LLE), Laplacian Eigenmap, Local Tangent Space Alignment (LTSA), Hessian Eigenmaps (HLLE), and Diffusion maps.","_input_hash":86645528,"_task_hash":537964960,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"class","start":5,"end":10,"id":1},{"text":"consists","start":11,"end":19,"id":2},{"text":"of","start":20,"end":22,"id":3},{"text":"Locally","start":23,"end":30,"id":4},{"text":"Linear","start":31,"end":37,"id":5},{"text":"Embedding","start":38,"end":47,"id":6},{"text":"(","start":48,"end":49,"id":7},{"text":"LLE","start":49,"end":52,"id":8},{"text":")","start":52,"end":53,"id":9},{"text":",","start":53,"end":54,"id":10},{"text":"Laplacian","start":55,"end":64,"id":11},{"text":"Eigenmap","start":65,"end":73,"id":12},{"text":",","start":73,"end":74,"id":13},{"text":"Local","start":75,"end":80,"id":14},{"text":"Tangent","start":81,"end":88,"id":15},{"text":"Space","start":89,"end":94,"id":16},{"text":"Alignment","start":95,"end":104,"id":17},{"text":"(","start":105,"end":106,"id":18},{"text":"LTSA","start":106,"end":110,"id":19},{"text":")","start":110,"end":111,"id":20},{"text":",","start":111,"end":112,"id":21},{"text":"Hessian","start":113,"end":120,"id":22},{"text":"Eigenmaps","start":121,"end":130,"id":23},{"text":"(","start":131,"end":132,"id":24},{"text":"HLLE","start":132,"end":136,"id":25},{"text":")","start":136,"end":137,"id":26},{"text":",","start":137,"end":138,"id":27},{"text":"and","start":139,"end":142,"id":28},{"text":"Diffusion","start":143,"end":152,"id":29},{"text":"maps","start":153,"end":157,"id":30},{"text":".","start":157,"end":158,"id":31}],"spans":[{"start":23,"end":47,"token_start":4,"token_end":6,"label":"ALGO","answer":"accept"}],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"Flow cytometry is often used to characterize the malignant cells in leukemia and lymphoma patients, traced to the level of the individual cell.","_input_hash":-186932865,"_task_hash":2014982531,"tokens":[{"text":"Flow","start":0,"end":4,"id":0},{"text":"cytometry","start":5,"end":14,"id":1},{"text":"is","start":15,"end":17,"id":2},{"text":"often","start":18,"end":23,"id":3},{"text":"used","start":24,"end":28,"id":4},{"text":"to","start":29,"end":31,"id":5},{"text":"characterize","start":32,"end":44,"id":6},{"text":"the","start":45,"end":48,"id":7},{"text":"malignant","start":49,"end":58,"id":8},{"text":"cells","start":59,"end":64,"id":9},{"text":"in","start":65,"end":67,"id":10},{"text":"leukemia","start":68,"end":76,"id":11},{"text":"and","start":77,"end":80,"id":12},{"text":"lymphoma","start":81,"end":89,"id":13},{"text":"patients","start":90,"end":98,"id":14},{"text":",","start":98,"end":99,"id":15},{"text":"traced","start":100,"end":106,"id":16},{"text":"to","start":107,"end":109,"id":17},{"text":"the","start":110,"end":113,"id":18},{"text":"level","start":114,"end":119,"id":19},{"text":"of","start":120,"end":122,"id":20},{"text":"the","start":123,"end":126,"id":21},{"text":"individual","start":127,"end":137,"id":22},{"text":"cell","start":138,"end":142,"id":23},{"text":".","start":142,"end":143,"id":24}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"We provide self-contained proof of a theorem relating probabilistic coherence of forecasts to their non-domination by rival forecasts with respect to any proper scoring rule.","_input_hash":419135333,"_task_hash":954605435,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"provide","start":3,"end":10,"id":1},{"text":"self","start":11,"end":15,"id":2},{"text":"-","start":15,"end":16,"id":3},{"text":"contained","start":16,"end":25,"id":4},{"text":"proof","start":26,"end":31,"id":5},{"text":"of","start":32,"end":34,"id":6},{"text":"a","start":35,"end":36,"id":7},{"text":"theorem","start":37,"end":44,"id":8},{"text":"relating","start":45,"end":53,"id":9},{"text":"probabilistic","start":54,"end":67,"id":10},{"text":"coherence","start":68,"end":77,"id":11},{"text":"of","start":78,"end":80,"id":12},{"text":"forecasts","start":81,"end":90,"id":13},{"text":"to","start":91,"end":93,"id":14},{"text":"their","start":94,"end":99,"id":15},{"text":"non","start":100,"end":103,"id":16},{"text":"-","start":103,"end":104,"id":17},{"text":"domination","start":104,"end":114,"id":18},{"text":"by","start":115,"end":117,"id":19},{"text":"rival","start":118,"end":123,"id":20},{"text":"forecasts","start":124,"end":133,"id":21},{"text":"with","start":134,"end":138,"id":22},{"text":"respect","start":139,"end":146,"id":23},{"text":"to","start":147,"end":149,"id":24},{"text":"any","start":150,"end":153,"id":25},{"text":"proper","start":154,"end":160,"id":26},{"text":"scoring","start":161,"end":168,"id":27},{"text":"rule","start":169,"end":173,"id":28},{"text":".","start":173,"end":174,"id":29}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"These methods successfully drive the optimization of sampling trajectories for real-world magnetic resonance imaging through Bayesian experimental design, which has not been attempted before.","_input_hash":-1437027727,"_task_hash":884314657,"tokens":[{"text":"These","start":0,"end":5,"id":0},{"text":"methods","start":6,"end":13,"id":1},{"text":"successfully","start":14,"end":26,"id":2},{"text":"drive","start":27,"end":32,"id":3},{"text":"the","start":33,"end":36,"id":4},{"text":"optimization","start":37,"end":49,"id":5},{"text":"of","start":50,"end":52,"id":6},{"text":"sampling","start":53,"end":61,"id":7},{"text":"trajectories","start":62,"end":74,"id":8},{"text":"for","start":75,"end":78,"id":9},{"text":"real","start":79,"end":83,"id":10},{"text":"-","start":83,"end":84,"id":11},{"text":"world","start":84,"end":89,"id":12},{"text":"magnetic","start":90,"end":98,"id":13},{"text":"resonance","start":99,"end":108,"id":14},{"text":"imaging","start":109,"end":116,"id":15},{"text":"through","start":117,"end":124,"id":16},{"text":"Bayesian","start":125,"end":133,"id":17},{"text":"experimental","start":134,"end":146,"id":18},{"text":"design","start":147,"end":153,"id":19},{"text":",","start":153,"end":154,"id":20},{"text":"which","start":155,"end":160,"id":21},{"text":"has","start":161,"end":164,"id":22},{"text":"not","start":165,"end":168,"id":23},{"text":"been","start":169,"end":173,"id":24},{"text":"attempted","start":174,"end":183,"id":25},{"text":"before","start":184,"end":190,"id":26},{"text":".","start":190,"end":191,"id":27}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"We show that the reconstruction weights computed by LLE capture the high-dimensional structure of the neighborhoods, and not the low-dimensional manifold structure.","_input_hash":-1196170794,"_task_hash":17773513,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"show","start":3,"end":7,"id":1},{"text":"that","start":8,"end":12,"id":2},{"text":"the","start":13,"end":16,"id":3},{"text":"reconstruction","start":17,"end":31,"id":4},{"text":"weights","start":32,"end":39,"id":5},{"text":"computed","start":40,"end":48,"id":6},{"text":"by","start":49,"end":51,"id":7},{"text":"LLE","start":52,"end":55,"id":8},{"text":"capture","start":56,"end":63,"id":9},{"text":"the","start":64,"end":67,"id":10},{"text":"high","start":68,"end":72,"id":11},{"text":"-","start":72,"end":73,"id":12},{"text":"dimensional","start":73,"end":84,"id":13},{"text":"structure","start":85,"end":94,"id":14},{"text":"of","start":95,"end":97,"id":15},{"text":"the","start":98,"end":101,"id":16},{"text":"neighborhoods","start":102,"end":115,"id":17},{"text":",","start":115,"end":116,"id":18},{"text":"and","start":117,"end":120,"id":19},{"text":"not","start":121,"end":124,"id":20},{"text":"the","start":125,"end":128,"id":21},{"text":"low","start":129,"end":132,"id":22},{"text":"-","start":132,"end":133,"id":23},{"text":"dimensional","start":133,"end":144,"id":24},{"text":"manifold","start":145,"end":153,"id":25},{"text":"structure","start":154,"end":163,"id":26},{"text":".","start":163,"end":164,"id":27}],"spans":[{"start":52,"end":55,"token_start":8,"token_end":8,"label":"ALGO","answer":"accept"}],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"We introduce an alternative, interaction component model for communities (ICMc), where the whole network is a bag of links, stemming from different components.","_input_hash":-485852851,"_task_hash":-2009781620,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"introduce","start":3,"end":12,"id":1},{"text":"an","start":13,"end":15,"id":2},{"text":"alternative","start":16,"end":27,"id":3},{"text":",","start":27,"end":28,"id":4},{"text":"interaction","start":29,"end":40,"id":5},{"text":"component","start":41,"end":50,"id":6},{"text":"model","start":51,"end":56,"id":7},{"text":"for","start":57,"end":60,"id":8},{"text":"communities","start":61,"end":72,"id":9},{"text":"(","start":73,"end":74,"id":10},{"text":"ICMc","start":74,"end":78,"id":11},{"text":")","start":78,"end":79,"id":12},{"text":",","start":79,"end":80,"id":13},{"text":"where","start":81,"end":86,"id":14},{"text":"the","start":87,"end":90,"id":15},{"text":"whole","start":91,"end":96,"id":16},{"text":"network","start":97,"end":104,"id":17},{"text":"is","start":105,"end":107,"id":18},{"text":"a","start":108,"end":109,"id":19},{"text":"bag","start":110,"end":113,"id":20},{"text":"of","start":114,"end":116,"id":21},{"text":"links","start":117,"end":122,"id":22},{"text":",","start":122,"end":123,"id":23},{"text":"stemming","start":124,"end":132,"id":24},{"text":"from","start":133,"end":137,"id":25},{"text":"different","start":138,"end":147,"id":26},{"text":"components","start":148,"end":158,"id":27},{"text":".","start":158,"end":159,"id":28}],"spans":[{"token_start":5,"token_end":6,"start":29,"end":50,"text":"interaction component","label":"ALGO","source":"./algo_model5","input_hash":-485852851,"answer":"accept"}],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"LLE first linearly reconstructs each input point from its nearest neighbors and then preserves these neighborhood relations in the low-dimensional embedding.","_input_hash":-258775660,"_task_hash":-1878114212,"tokens":[{"text":"LLE","start":0,"end":3,"id":0},{"text":"first","start":4,"end":9,"id":1},{"text":"linearly","start":10,"end":18,"id":2},{"text":"reconstructs","start":19,"end":31,"id":3},{"text":"each","start":32,"end":36,"id":4},{"text":"input","start":37,"end":42,"id":5},{"text":"point","start":43,"end":48,"id":6},{"text":"from","start":49,"end":53,"id":7},{"text":"its","start":54,"end":57,"id":8},{"text":"nearest","start":58,"end":65,"id":9},{"text":"neighbors","start":66,"end":75,"id":10},{"text":"and","start":76,"end":79,"id":11},{"text":"then","start":80,"end":84,"id":12},{"text":"preserves","start":85,"end":94,"id":13},{"text":"these","start":95,"end":100,"id":14},{"text":"neighborhood","start":101,"end":113,"id":15},{"text":"relations","start":114,"end":123,"id":16},{"text":"in","start":124,"end":126,"id":17},{"text":"the","start":127,"end":130,"id":18},{"text":"low","start":131,"end":134,"id":19},{"text":"-","start":134,"end":135,"id":20},{"text":"dimensional","start":135,"end":146,"id":21},{"text":"embedding","start":147,"end":156,"id":22},{"text":".","start":156,"end":157,"id":23}],"spans":[{"token_start":0,"token_end":0,"start":0,"end":3,"text":"LLE","label":"ALGO","source":"./algo_model5","input_hash":-258775660,"answer":"accept"}],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"Finally, experimental evidence of the performance of the proposed approach on both artificial data and a speaker verification task is provided.","_input_hash":-1272997971,"_task_hash":799308236,"tokens":[{"text":"Finally","start":0,"end":7,"id":0},{"text":",","start":7,"end":8,"id":1},{"text":"experimental","start":9,"end":21,"id":2},{"text":"evidence","start":22,"end":30,"id":3},{"text":"of","start":31,"end":33,"id":4},{"text":"the","start":34,"end":37,"id":5},{"text":"performance","start":38,"end":49,"id":6},{"text":"of","start":50,"end":52,"id":7},{"text":"the","start":53,"end":56,"id":8},{"text":"proposed","start":57,"end":65,"id":9},{"text":"approach","start":66,"end":74,"id":10},{"text":"on","start":75,"end":77,"id":11},{"text":"both","start":78,"end":82,"id":12},{"text":"artificial","start":83,"end":93,"id":13},{"text":"data","start":94,"end":98,"id":14},{"text":"and","start":99,"end":102,"id":15},{"text":"a","start":103,"end":104,"id":16},{"text":"speaker","start":105,"end":112,"id":17},{"text":"verification","start":113,"end":125,"id":18},{"text":"task","start":126,"end":130,"id":19},{"text":"is","start":131,"end":133,"id":20},{"text":"provided","start":134,"end":142,"id":21},{"text":".","start":142,"end":143,"id":22}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"The distance metric plays an important role in nearest neighbor (NN) classification.","_input_hash":533487841,"_task_hash":-1008111325,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"distance","start":4,"end":12,"id":1},{"text":"metric","start":13,"end":19,"id":2},{"text":"plays","start":20,"end":25,"id":3},{"text":"an","start":26,"end":28,"id":4},{"text":"important","start":29,"end":38,"id":5},{"text":"role","start":39,"end":43,"id":6},{"text":"in","start":44,"end":46,"id":7},{"text":"nearest","start":47,"end":54,"id":8},{"text":"neighbor","start":55,"end":63,"id":9},{"text":"(","start":64,"end":65,"id":10},{"text":"NN","start":65,"end":67,"id":11},{"text":")","start":67,"end":68,"id":12},{"text":"classification","start":69,"end":83,"id":13},{"text":".","start":83,"end":84,"id":14}],"spans":[{"start":47,"end":63,"token_start":8,"token_end":9,"label":"ALGO","answer":"accept"}],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"Usually the Euclidean distance metric is assumed or a Mahalanobis distance metric is optimized to improve the NN performance.","_input_hash":184187675,"_task_hash":1331312961,"tokens":[{"text":"Usually","start":0,"end":7,"id":0},{"text":"the","start":8,"end":11,"id":1},{"text":"Euclidean","start":12,"end":21,"id":2},{"text":"distance","start":22,"end":30,"id":3},{"text":"metric","start":31,"end":37,"id":4},{"text":"is","start":38,"end":40,"id":5},{"text":"assumed","start":41,"end":48,"id":6},{"text":"or","start":49,"end":51,"id":7},{"text":"a","start":52,"end":53,"id":8},{"text":"Mahalanobis","start":54,"end":65,"id":9},{"text":"distance","start":66,"end":74,"id":10},{"text":"metric","start":75,"end":81,"id":11},{"text":"is","start":82,"end":84,"id":12},{"text":"optimized","start":85,"end":94,"id":13},{"text":"to","start":95,"end":97,"id":14},{"text":"improve","start":98,"end":105,"id":15},{"text":"the","start":106,"end":109,"id":16},{"text":"NN","start":110,"end":112,"id":17},{"text":"performance","start":113,"end":124,"id":18},{"text":".","start":124,"end":125,"id":19}],"spans":[{"start":110,"end":112,"token_start":17,"token_end":17,"label":"ALGO","answer":"accept"}],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"For computational or other reasons, people may select only a small subset of features when modelling such data, by looking at how relevant the features are to predicting the response, based on some measure such as correlation with the response in the training data.","_input_hash":-901972611,"_task_hash":240848819,"tokens":[{"text":"For","start":0,"end":3,"id":0},{"text":"computational","start":4,"end":17,"id":1},{"text":"or","start":18,"end":20,"id":2},{"text":"other","start":21,"end":26,"id":3},{"text":"reasons","start":27,"end":34,"id":4},{"text":",","start":34,"end":35,"id":5},{"text":"people","start":36,"end":42,"id":6},{"text":"may","start":43,"end":46,"id":7},{"text":"select","start":47,"end":53,"id":8},{"text":"only","start":54,"end":58,"id":9},{"text":"a","start":59,"end":60,"id":10},{"text":"small","start":61,"end":66,"id":11},{"text":"subset","start":67,"end":73,"id":12},{"text":"of","start":74,"end":76,"id":13},{"text":"features","start":77,"end":85,"id":14},{"text":"when","start":86,"end":90,"id":15},{"text":"modelling","start":91,"end":100,"id":16},{"text":"such","start":101,"end":105,"id":17},{"text":"data","start":106,"end":110,"id":18},{"text":",","start":110,"end":111,"id":19},{"text":"by","start":112,"end":114,"id":20},{"text":"looking","start":115,"end":122,"id":21},{"text":"at","start":123,"end":125,"id":22},{"text":"how","start":126,"end":129,"id":23},{"text":"relevant","start":130,"end":138,"id":24},{"text":"the","start":139,"end":142,"id":25},{"text":"features","start":143,"end":151,"id":26},{"text":"are","start":152,"end":155,"id":27},{"text":"to","start":156,"end":158,"id":28},{"text":"predicting","start":159,"end":169,"id":29},{"text":"the","start":170,"end":173,"id":30},{"text":"response","start":174,"end":182,"id":31},{"text":",","start":182,"end":183,"id":32},{"text":"based","start":184,"end":189,"id":33},{"text":"on","start":190,"end":192,"id":34},{"text":"some","start":193,"end":197,"id":35},{"text":"measure","start":198,"end":205,"id":36},{"text":"such","start":206,"end":210,"id":37},{"text":"as","start":211,"end":213,"id":38},{"text":"correlation","start":214,"end":225,"id":39},{"text":"with","start":226,"end":230,"id":40},{"text":"the","start":231,"end":234,"id":41},{"text":"response","start":235,"end":243,"id":42},{"text":"in","start":244,"end":246,"id":43},{"text":"the","start":247,"end":250,"id":44},{"text":"training","start":251,"end":259,"id":45},{"text":"data","start":260,"end":264,"id":46},{"text":".","start":264,"end":265,"id":47}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"We compute the probability distribution of the length of the current ``run,'' or time since the last changepoint, using a simple message-passing algorithm.","_input_hash":-2035327106,"_task_hash":2044654912,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"compute","start":3,"end":10,"id":1},{"text":"the","start":11,"end":14,"id":2},{"text":"probability","start":15,"end":26,"id":3},{"text":"distribution","start":27,"end":39,"id":4},{"text":"of","start":40,"end":42,"id":5},{"text":"the","start":43,"end":46,"id":6},{"text":"length","start":47,"end":53,"id":7},{"text":"of","start":54,"end":56,"id":8},{"text":"the","start":57,"end":60,"id":9},{"text":"current","start":61,"end":68,"id":10},{"text":"`","start":69,"end":70,"id":11},{"text":"`","start":70,"end":71,"id":12},{"text":"run","start":71,"end":74,"id":13},{"text":",","start":74,"end":75,"id":14},{"text":"'","start":75,"end":76,"id":15},{"text":"'","start":76,"end":77,"id":16},{"text":"or","start":78,"end":80,"id":17},{"text":"time","start":81,"end":85,"id":18},{"text":"since","start":86,"end":91,"id":19},{"text":"the","start":92,"end":95,"id":20},{"text":"last","start":96,"end":100,"id":21},{"text":"changepoint","start":101,"end":112,"id":22},{"text":",","start":112,"end":113,"id":23},{"text":"using","start":114,"end":119,"id":24},{"text":"a","start":120,"end":121,"id":25},{"text":"simple","start":122,"end":128,"id":26},{"text":"message","start":129,"end":136,"id":27},{"text":"-","start":136,"end":137,"id":28},{"text":"passing","start":137,"end":144,"id":29},{"text":"algorithm","start":145,"end":154,"id":30},{"text":".","start":154,"end":155,"id":31}],"spans":[{"start":129,"end":144,"token_start":27,"token_end":29,"label":"ALGO","answer":"accept"}],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"They are simple, computationally fast and scalable, interpretable, and admit nonparametric priors.","_input_hash":438256361,"_task_hash":333327110,"tokens":[{"text":"They","start":0,"end":4,"id":0},{"text":"are","start":5,"end":8,"id":1},{"text":"simple","start":9,"end":15,"id":2},{"text":",","start":15,"end":16,"id":3},{"text":"computationally","start":17,"end":32,"id":4},{"text":"fast","start":33,"end":37,"id":5},{"text":"and","start":38,"end":41,"id":6},{"text":"scalable","start":42,"end":50,"id":7},{"text":",","start":50,"end":51,"id":8},{"text":"interpretable","start":52,"end":65,"id":9},{"text":",","start":65,"end":66,"id":10},{"text":"and","start":67,"end":70,"id":11},{"text":"admit","start":71,"end":76,"id":12},{"text":"nonparametric","start":77,"end":90,"id":13},{"text":"priors","start":91,"end":97,"id":14},{"text":".","start":97,"end":98,"id":15}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"Autoencoder neural network is implemented to estimate the missing data.","_input_hash":909308757,"_task_hash":-1057027017,"tokens":[{"text":"Autoencoder","start":0,"end":11,"id":0},{"text":"neural","start":12,"end":18,"id":1},{"text":"network","start":19,"end":26,"id":2},{"text":"is","start":27,"end":29,"id":3},{"text":"implemented","start":30,"end":41,"id":4},{"text":"to","start":42,"end":44,"id":5},{"text":"estimate","start":45,"end":53,"id":6},{"text":"the","start":54,"end":57,"id":7},{"text":"missing","start":58,"end":65,"id":8},{"text":"data","start":66,"end":70,"id":9},{"text":".","start":70,"end":71,"id":10}],"spans":[{"token_start":0,"token_end":2,"start":0,"end":26,"text":"Autoencoder neural network","label":"ALGO","source":"./algo_model5","input_hash":909308757,"answer":"accept"}],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"Our implementation is highly modular so that the algorithm may be applied to a variety of types of data.","_input_hash":-1056800728,"_task_hash":-1521443138,"tokens":[{"text":"Our","start":0,"end":3,"id":0},{"text":"implementation","start":4,"end":18,"id":1},{"text":"is","start":19,"end":21,"id":2},{"text":"highly","start":22,"end":28,"id":3},{"text":"modular","start":29,"end":36,"id":4},{"text":"so","start":37,"end":39,"id":5},{"text":"that","start":40,"end":44,"id":6},{"text":"the","start":45,"end":48,"id":7},{"text":"algorithm","start":49,"end":58,"id":8},{"text":"may","start":59,"end":62,"id":9},{"text":"be","start":63,"end":65,"id":10},{"text":"applied","start":66,"end":73,"id":11},{"text":"to","start":74,"end":76,"id":12},{"text":"a","start":77,"end":78,"id":13},{"text":"variety","start":79,"end":86,"id":14},{"text":"of","start":87,"end":89,"id":15},{"text":"types","start":90,"end":95,"id":16},{"text":"of","start":96,"end":98,"id":17},{"text":"data","start":99,"end":103,"id":18},{"text":".","start":103,"end":104,"id":19}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"Concerning design of kernel density estimators, one essential issue is how fast the pointwise mean square error (MSE) and/or the integrated mean square error (IMSE) diminish as the number of sampling instances increases.","_input_hash":2132921784,"_task_hash":233238422,"tokens":[{"text":"Concerning","start":0,"end":10,"id":0},{"text":"design","start":11,"end":17,"id":1},{"text":"of","start":18,"end":20,"id":2},{"text":"kernel","start":21,"end":27,"id":3},{"text":"density","start":28,"end":35,"id":4},{"text":"estimators","start":36,"end":46,"id":5},{"text":",","start":46,"end":47,"id":6},{"text":"one","start":48,"end":51,"id":7},{"text":"essential","start":52,"end":61,"id":8},{"text":"issue","start":62,"end":67,"id":9},{"text":"is","start":68,"end":70,"id":10},{"text":"how","start":71,"end":74,"id":11},{"text":"fast","start":75,"end":79,"id":12},{"text":"the","start":80,"end":83,"id":13},{"text":"pointwise","start":84,"end":93,"id":14},{"text":"mean","start":94,"end":98,"id":15},{"text":"square","start":99,"end":105,"id":16},{"text":"error","start":106,"end":111,"id":17},{"text":"(","start":112,"end":113,"id":18},{"text":"MSE","start":113,"end":116,"id":19},{"text":")","start":116,"end":117,"id":20},{"text":"and/or","start":118,"end":124,"id":21},{"text":"the","start":125,"end":128,"id":22},{"text":"integrated","start":129,"end":139,"id":23},{"text":"mean","start":140,"end":144,"id":24},{"text":"square","start":145,"end":151,"id":25},{"text":"error","start":152,"end":157,"id":26},{"text":"(","start":158,"end":159,"id":27},{"text":"IMSE","start":159,"end":163,"id":28},{"text":")","start":163,"end":164,"id":29},{"text":"diminish","start":165,"end":173,"id":30},{"text":"as","start":174,"end":176,"id":31},{"text":"the","start":177,"end":180,"id":32},{"text":"number","start":181,"end":187,"id":33},{"text":"of","start":188,"end":190,"id":34},{"text":"sampling","start":191,"end":199,"id":35},{"text":"instances","start":200,"end":209,"id":36},{"text":"increases","start":210,"end":219,"id":37},{"text":".","start":219,"end":220,"id":38}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"We show that there are simple manifolds in which the necessary conditions are violated, and hence the algorithms cannot recover the underlying manifolds.","_input_hash":-1117391181,"_task_hash":-775241696,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"show","start":3,"end":7,"id":1},{"text":"that","start":8,"end":12,"id":2},{"text":"there","start":13,"end":18,"id":3},{"text":"are","start":19,"end":22,"id":4},{"text":"simple","start":23,"end":29,"id":5},{"text":"manifolds","start":30,"end":39,"id":6},{"text":"in","start":40,"end":42,"id":7},{"text":"which","start":43,"end":48,"id":8},{"text":"the","start":49,"end":52,"id":9},{"text":"necessary","start":53,"end":62,"id":10},{"text":"conditions","start":63,"end":73,"id":11},{"text":"are","start":74,"end":77,"id":12},{"text":"violated","start":78,"end":86,"id":13},{"text":",","start":86,"end":87,"id":14},{"text":"and","start":88,"end":91,"id":15},{"text":"hence","start":92,"end":97,"id":16},{"text":"the","start":98,"end":101,"id":17},{"text":"algorithms","start":102,"end":112,"id":18},{"text":"can","start":113,"end":116,"id":19},{"text":"not","start":116,"end":119,"id":20},{"text":"recover","start":120,"end":127,"id":21},{"text":"the","start":128,"end":131,"id":22},{"text":"underlying","start":132,"end":142,"id":23},{"text":"manifolds","start":143,"end":152,"id":24},{"text":".","start":152,"end":153,"id":25}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"A computer program is available that implements the proposed shrinkage estimator.","_input_hash":-701394453,"_task_hash":-179084809,"tokens":[{"text":"A","start":0,"end":1,"id":0},{"text":"computer","start":2,"end":10,"id":1},{"text":"program","start":11,"end":18,"id":2},{"text":"is","start":19,"end":21,"id":3},{"text":"available","start":22,"end":31,"id":4},{"text":"that","start":32,"end":36,"id":5},{"text":"implements","start":37,"end":47,"id":6},{"text":"the","start":48,"end":51,"id":7},{"text":"proposed","start":52,"end":60,"id":8},{"text":"shrinkage","start":61,"end":70,"id":9},{"text":"estimator","start":71,"end":80,"id":10},{"text":".","start":80,"end":81,"id":11}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"We present a procedure for effective estimation of entropy and mutual information from small-sample data, and apply it to the problem of inferring high-dimensional gene association networks.","_input_hash":1305692642,"_task_hash":408214070,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"present","start":3,"end":10,"id":1},{"text":"a","start":11,"end":12,"id":2},{"text":"procedure","start":13,"end":22,"id":3},{"text":"for","start":23,"end":26,"id":4},{"text":"effective","start":27,"end":36,"id":5},{"text":"estimation","start":37,"end":47,"id":6},{"text":"of","start":48,"end":50,"id":7},{"text":"entropy","start":51,"end":58,"id":8},{"text":"and","start":59,"end":62,"id":9},{"text":"mutual","start":63,"end":69,"id":10},{"text":"information","start":70,"end":81,"id":11},{"text":"from","start":82,"end":86,"id":12},{"text":"small","start":87,"end":92,"id":13},{"text":"-","start":92,"end":93,"id":14},{"text":"sample","start":93,"end":99,"id":15},{"text":"data","start":100,"end":104,"id":16},{"text":",","start":104,"end":105,"id":17},{"text":"and","start":106,"end":109,"id":18},{"text":"apply","start":110,"end":115,"id":19},{"text":"it","start":116,"end":118,"id":20},{"text":"to","start":119,"end":121,"id":21},{"text":"the","start":122,"end":125,"id":22},{"text":"problem","start":126,"end":133,"id":23},{"text":"of","start":134,"end":136,"id":24},{"text":"inferring","start":137,"end":146,"id":25},{"text":"high","start":147,"end":151,"id":26},{"text":"-","start":151,"end":152,"id":27},{"text":"dimensional","start":152,"end":163,"id":28},{"text":"gene","start":164,"end":168,"id":29},{"text":"association","start":169,"end":180,"id":30},{"text":"networks","start":181,"end":189,"id":31},{"text":".","start":189,"end":190,"id":32}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"We propose to investigate test statistics for testing homogeneity in reproducing kernel Hilbert spaces.","_input_hash":-1337820549,"_task_hash":-1342743489,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"propose","start":3,"end":10,"id":1},{"text":"to","start":11,"end":13,"id":2},{"text":"investigate","start":14,"end":25,"id":3},{"text":"test","start":26,"end":30,"id":4},{"text":"statistics","start":31,"end":41,"id":5},{"text":"for","start":42,"end":45,"id":6},{"text":"testing","start":46,"end":53,"id":7},{"text":"homogeneity","start":54,"end":65,"id":8},{"text":"in","start":66,"end":68,"id":9},{"text":"reproducing","start":69,"end":80,"id":10},{"text":"kernel","start":81,"end":87,"id":11},{"text":"Hilbert","start":88,"end":95,"id":12},{"text":"spaces","start":96,"end":102,"id":13},{"text":".","start":102,"end":103,"id":14}],"spans":[{"token_start":11,"token_end":13,"start":81,"end":102,"text":"kernel Hilbert spaces","label":"ALGO","source":"./algo_model5","input_hash":-1337820549,"answer":"accept"}],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"We formulate the problem as a maximization of $\\ell_1$-regularized surrogate likelihood that allows us to find a sparse solution.","_input_hash":255427274,"_task_hash":-1941332186,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"formulate","start":3,"end":12,"id":1},{"text":"the","start":13,"end":16,"id":2},{"text":"problem","start":17,"end":24,"id":3},{"text":"as","start":25,"end":27,"id":4},{"text":"a","start":28,"end":29,"id":5},{"text":"maximization","start":30,"end":42,"id":6},{"text":"of","start":43,"end":45,"id":7},{"text":"$","start":46,"end":47,"id":8},{"text":"\\ell_1$-regularized","start":47,"end":66,"id":9},{"text":"surrogate","start":67,"end":76,"id":10},{"text":"likelihood","start":77,"end":87,"id":11},{"text":"that","start":88,"end":92,"id":12},{"text":"allows","start":93,"end":99,"id":13},{"text":"us","start":100,"end":102,"id":14},{"text":"to","start":103,"end":105,"id":15},{"text":"find","start":106,"end":110,"id":16},{"text":"a","start":111,"end":112,"id":17},{"text":"sparse","start":113,"end":119,"id":18},{"text":"solution","start":120,"end":128,"id":19},{"text":".","start":128,"end":129,"id":20}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"Our optimization technique efficiently incorporates the cutting-plane algorithm in order to obtain a tighter outer bound on the marginal polytope, which results in improvement of both parameter estimates and approximation to marginals.","_input_hash":150698891,"_task_hash":1712358823,"tokens":[{"text":"Our","start":0,"end":3,"id":0},{"text":"optimization","start":4,"end":16,"id":1},{"text":"technique","start":17,"end":26,"id":2},{"text":"efficiently","start":27,"end":38,"id":3},{"text":"incorporates","start":39,"end":51,"id":4},{"text":"the","start":52,"end":55,"id":5},{"text":"cutting","start":56,"end":63,"id":6},{"text":"-","start":63,"end":64,"id":7},{"text":"plane","start":64,"end":69,"id":8},{"text":"algorithm","start":70,"end":79,"id":9},{"text":"in","start":80,"end":82,"id":10},{"text":"order","start":83,"end":88,"id":11},{"text":"to","start":89,"end":91,"id":12},{"text":"obtain","start":92,"end":98,"id":13},{"text":"a","start":99,"end":100,"id":14},{"text":"tighter","start":101,"end":108,"id":15},{"text":"outer","start":109,"end":114,"id":16},{"text":"bound","start":115,"end":120,"id":17},{"text":"on","start":121,"end":123,"id":18},{"text":"the","start":124,"end":127,"id":19},{"text":"marginal","start":128,"end":136,"id":20},{"text":"polytope","start":137,"end":145,"id":21},{"text":",","start":145,"end":146,"id":22},{"text":"which","start":147,"end":152,"id":23},{"text":"results","start":153,"end":160,"id":24},{"text":"in","start":161,"end":163,"id":25},{"text":"improvement","start":164,"end":175,"id":26},{"text":"of","start":176,"end":178,"id":27},{"text":"both","start":179,"end":183,"id":28},{"text":"parameter","start":184,"end":193,"id":29},{"text":"estimates","start":194,"end":203,"id":30},{"text":"and","start":204,"end":207,"id":31},{"text":"approximation","start":208,"end":221,"id":32},{"text":"to","start":222,"end":224,"id":33},{"text":"marginals","start":225,"end":234,"id":34},{"text":".","start":234,"end":235,"id":35}],"spans":[{"start":56,"end":69,"token_start":6,"token_end":8,"label":"ALGO","answer":"accept"}],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"But the main contribution of this paper is twofold:","_input_hash":839304675,"_task_hash":946156102,"tokens":[{"text":"But","start":0,"end":3,"id":0},{"text":"the","start":4,"end":7,"id":1},{"text":"main","start":8,"end":12,"id":2},{"text":"contribution","start":13,"end":25,"id":3},{"text":"of","start":26,"end":28,"id":4},{"text":"this","start":29,"end":33,"id":5},{"text":"paper","start":34,"end":39,"id":6},{"text":"is","start":40,"end":42,"id":7},{"text":"twofold","start":43,"end":50,"id":8},{"text":":","start":50,"end":51,"id":9}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"The former finds both disassortative and assortative structure, while the alternative assumes assortativity and finds community-like structures like the earlier methods motivated by physics.","_input_hash":251252844,"_task_hash":-328374854,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"former","start":4,"end":10,"id":1},{"text":"finds","start":11,"end":16,"id":2},{"text":"both","start":17,"end":21,"id":3},{"text":"disassortative","start":22,"end":36,"id":4},{"text":"and","start":37,"end":40,"id":5},{"text":"assortative","start":41,"end":52,"id":6},{"text":"structure","start":53,"end":62,"id":7},{"text":",","start":62,"end":63,"id":8},{"text":"while","start":64,"end":69,"id":9},{"text":"the","start":70,"end":73,"id":10},{"text":"alternative","start":74,"end":85,"id":11},{"text":"assumes","start":86,"end":93,"id":12},{"text":"assortativity","start":94,"end":107,"id":13},{"text":"and","start":108,"end":111,"id":14},{"text":"finds","start":112,"end":117,"id":15},{"text":"community","start":118,"end":127,"id":16},{"text":"-","start":127,"end":128,"id":17},{"text":"like","start":128,"end":132,"id":18},{"text":"structures","start":133,"end":143,"id":19},{"text":"like","start":144,"end":148,"id":20},{"text":"the","start":149,"end":152,"id":21},{"text":"earlier","start":153,"end":160,"id":22},{"text":"methods","start":161,"end":168,"id":23},{"text":"motivated","start":169,"end":178,"id":24},{"text":"by","start":179,"end":181,"id":25},{"text":"physics","start":182,"end":189,"id":26},{"text":".","start":189,"end":190,"id":27}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"The theory naturally extends from single trees to ensembles of trees and applies to methods like random forests.","_input_hash":316397493,"_task_hash":-1475633716,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"theory","start":4,"end":10,"id":1},{"text":"naturally","start":11,"end":20,"id":2},{"text":"extends","start":21,"end":28,"id":3},{"text":"from","start":29,"end":33,"id":4},{"text":"single","start":34,"end":40,"id":5},{"text":"trees","start":41,"end":46,"id":6},{"text":"to","start":47,"end":49,"id":7},{"text":"ensembles","start":50,"end":59,"id":8},{"text":"of","start":60,"end":62,"id":9},{"text":"trees","start":63,"end":68,"id":10},{"text":"and","start":69,"end":72,"id":11},{"text":"applies","start":73,"end":80,"id":12},{"text":"to","start":81,"end":83,"id":13},{"text":"methods","start":84,"end":91,"id":14},{"text":"like","start":92,"end":96,"id":15},{"text":"random","start":97,"end":103,"id":16},{"text":"forests","start":104,"end":111,"id":17},{"text":".","start":111,"end":112,"id":18}],"spans":[{"start":50,"end":68,"token_start":8,"token_end":10,"label":"ALGO","answer":"accept"},{"start":97,"end":111,"token_start":16,"token_end":17,"label":"ALGO","answer":"accept"}],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"Although the main focus of this paper is to present a general, theoretical framework for metric embedding in a NN setting, we demonstrate the performance of the proposed method on some benchmark datasets and show that it performs better than the Mahalanobis metric learning algorithm in terms of leave-one-out and generalization errors.","_input_hash":-1031192136,"_task_hash":-1587680190,"tokens":[{"text":"Although","start":0,"end":8,"id":0},{"text":"the","start":9,"end":12,"id":1},{"text":"main","start":13,"end":17,"id":2},{"text":"focus","start":18,"end":23,"id":3},{"text":"of","start":24,"end":26,"id":4},{"text":"this","start":27,"end":31,"id":5},{"text":"paper","start":32,"end":37,"id":6},{"text":"is","start":38,"end":40,"id":7},{"text":"to","start":41,"end":43,"id":8},{"text":"present","start":44,"end":51,"id":9},{"text":"a","start":52,"end":53,"id":10},{"text":"general","start":54,"end":61,"id":11},{"text":",","start":61,"end":62,"id":12},{"text":"theoretical","start":63,"end":74,"id":13},{"text":"framework","start":75,"end":84,"id":14},{"text":"for","start":85,"end":88,"id":15},{"text":"metric","start":89,"end":95,"id":16},{"text":"embedding","start":96,"end":105,"id":17},{"text":"in","start":106,"end":108,"id":18},{"text":"a","start":109,"end":110,"id":19},{"text":"NN","start":111,"end":113,"id":20},{"text":"setting","start":114,"end":121,"id":21},{"text":",","start":121,"end":122,"id":22},{"text":"we","start":123,"end":125,"id":23},{"text":"demonstrate","start":126,"end":137,"id":24},{"text":"the","start":138,"end":141,"id":25},{"text":"performance","start":142,"end":153,"id":26},{"text":"of","start":154,"end":156,"id":27},{"text":"the","start":157,"end":160,"id":28},{"text":"proposed","start":161,"end":169,"id":29},{"text":"method","start":170,"end":176,"id":30},{"text":"on","start":177,"end":179,"id":31},{"text":"some","start":180,"end":184,"id":32},{"text":"benchmark","start":185,"end":194,"id":33},{"text":"datasets","start":195,"end":203,"id":34},{"text":"and","start":204,"end":207,"id":35},{"text":"show","start":208,"end":212,"id":36},{"text":"that","start":213,"end":217,"id":37},{"text":"it","start":218,"end":220,"id":38},{"text":"performs","start":221,"end":229,"id":39},{"text":"better","start":230,"end":236,"id":40},{"text":"than","start":237,"end":241,"id":41},{"text":"the","start":242,"end":245,"id":42},{"text":"Mahalanobis","start":246,"end":257,"id":43},{"text":"metric","start":258,"end":264,"id":44},{"text":"learning","start":265,"end":273,"id":45},{"text":"algorithm","start":274,"end":283,"id":46},{"text":"in","start":284,"end":286,"id":47},{"text":"terms","start":287,"end":292,"id":48},{"text":"of","start":293,"end":295,"id":49},{"text":"leave","start":296,"end":301,"id":50},{"text":"-","start":301,"end":302,"id":51},{"text":"one","start":302,"end":305,"id":52},{"text":"-","start":305,"end":306,"id":53},{"text":"out","start":306,"end":309,"id":54},{"text":"and","start":310,"end":313,"id":55},{"text":"generalization","start":314,"end":328,"id":56},{"text":"errors","start":329,"end":335,"id":57},{"text":".","start":335,"end":336,"id":58}],"spans":[{"token_start":20,"token_end":20,"start":111,"end":113,"text":"NN","label":"ALGO","source":"./algo_model5","input_hash":-1031192136,"answer":"accept"},{"token_start":43,"token_end":45,"start":246,"end":273,"text":"Mahalanobis metric learning","label":"ALGO","source":"./algo_model5","input_hash":-1031192136,"answer":"accept"}],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"By using machine learning techniques, we allow clinicians to visualize data in a low dimension defined by a linear combination of all of the available markers, rather than just 2 at a time.","_input_hash":-1723825170,"_task_hash":1629234304,"tokens":[{"text":"By","start":0,"end":2,"id":0},{"text":"using","start":3,"end":8,"id":1},{"text":"machine","start":9,"end":16,"id":2},{"text":"learning","start":17,"end":25,"id":3},{"text":"techniques","start":26,"end":36,"id":4},{"text":",","start":36,"end":37,"id":5},{"text":"we","start":38,"end":40,"id":6},{"text":"allow","start":41,"end":46,"id":7},{"text":"clinicians","start":47,"end":57,"id":8},{"text":"to","start":58,"end":60,"id":9},{"text":"visualize","start":61,"end":70,"id":10},{"text":"data","start":71,"end":75,"id":11},{"text":"in","start":76,"end":78,"id":12},{"text":"a","start":79,"end":80,"id":13},{"text":"low","start":81,"end":84,"id":14},{"text":"dimension","start":85,"end":94,"id":15},{"text":"defined","start":95,"end":102,"id":16},{"text":"by","start":103,"end":105,"id":17},{"text":"a","start":106,"end":107,"id":18},{"text":"linear","start":108,"end":114,"id":19},{"text":"combination","start":115,"end":126,"id":20},{"text":"of","start":127,"end":129,"id":21},{"text":"all","start":130,"end":133,"id":22},{"text":"of","start":134,"end":136,"id":23},{"text":"the","start":137,"end":140,"id":24},{"text":"available","start":141,"end":150,"id":25},{"text":"markers","start":151,"end":158,"id":26},{"text":",","start":158,"end":159,"id":27},{"text":"rather","start":160,"end":166,"id":28},{"text":"than","start":167,"end":171,"id":29},{"text":"just","start":172,"end":176,"id":30},{"text":"2","start":177,"end":178,"id":31},{"text":"at","start":179,"end":181,"id":32},{"text":"a","start":182,"end":183,"id":33},{"text":"time","start":184,"end":188,"id":34},{"text":".","start":188,"end":189,"id":35}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"In recent years, kernel density estimation has been exploited by computer scientists to model machine learning problems.","_input_hash":-74833356,"_task_hash":1868453665,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"recent","start":3,"end":9,"id":1},{"text":"years","start":10,"end":15,"id":2},{"text":",","start":15,"end":16,"id":3},{"text":"kernel","start":17,"end":23,"id":4},{"text":"density","start":24,"end":31,"id":5},{"text":"estimation","start":32,"end":42,"id":6},{"text":"has","start":43,"end":46,"id":7},{"text":"been","start":47,"end":51,"id":8},{"text":"exploited","start":52,"end":61,"id":9},{"text":"by","start":62,"end":64,"id":10},{"text":"computer","start":65,"end":73,"id":11},{"text":"scientists","start":74,"end":84,"id":12},{"text":"to","start":85,"end":87,"id":13},{"text":"model","start":88,"end":93,"id":14},{"text":"machine","start":94,"end":101,"id":15},{"text":"learning","start":102,"end":110,"id":16},{"text":"problems","start":111,"end":119,"id":17},{"text":".","start":119,"end":120,"id":18}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"We demonstrate the performance gain for both local and global classifiers and demonstrate a 10% improvement of the $k$-nearest neighbors algorithm performance.","_input_hash":-1173540332,"_task_hash":949256519,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"demonstrate","start":3,"end":14,"id":1},{"text":"the","start":15,"end":18,"id":2},{"text":"performance","start":19,"end":30,"id":3},{"text":"gain","start":31,"end":35,"id":4},{"text":"for","start":36,"end":39,"id":5},{"text":"both","start":40,"end":44,"id":6},{"text":"local","start":45,"end":50,"id":7},{"text":"and","start":51,"end":54,"id":8},{"text":"global","start":55,"end":61,"id":9},{"text":"classifiers","start":62,"end":73,"id":10},{"text":"and","start":74,"end":77,"id":11},{"text":"demonstrate","start":78,"end":89,"id":12},{"text":"a","start":90,"end":91,"id":13},{"text":"10","start":92,"end":94,"id":14},{"text":"%","start":94,"end":95,"id":15},{"text":"improvement","start":96,"end":107,"id":16},{"text":"of","start":108,"end":110,"id":17},{"text":"the","start":111,"end":114,"id":18},{"text":"$","start":115,"end":116,"id":19},{"text":"k$-nearest","start":116,"end":126,"id":20},{"text":"neighbors","start":127,"end":136,"id":21},{"text":"algorithm","start":137,"end":146,"id":22},{"text":"performance","start":147,"end":158,"id":23},{"text":".","start":158,"end":159,"id":24}],"spans":[{"start":115,"end":136,"token_start":19,"token_end":21,"label":"ALGO","answer":"accept"}],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"To overcome both of these problems, we propose to compute the weight vectors using a low-dimensional neighborhood representation.","_input_hash":-371971599,"_task_hash":1006070794,"tokens":[{"text":"To","start":0,"end":2,"id":0},{"text":"overcome","start":3,"end":11,"id":1},{"text":"both","start":12,"end":16,"id":2},{"text":"of","start":17,"end":19,"id":3},{"text":"these","start":20,"end":25,"id":4},{"text":"problems","start":26,"end":34,"id":5},{"text":",","start":34,"end":35,"id":6},{"text":"we","start":36,"end":38,"id":7},{"text":"propose","start":39,"end":46,"id":8},{"text":"to","start":47,"end":49,"id":9},{"text":"compute","start":50,"end":57,"id":10},{"text":"the","start":58,"end":61,"id":11},{"text":"weight","start":62,"end":68,"id":12},{"text":"vectors","start":69,"end":76,"id":13},{"text":"using","start":77,"end":82,"id":14},{"text":"a","start":83,"end":84,"id":15},{"text":"low","start":85,"end":88,"id":16},{"text":"-","start":88,"end":89,"id":17},{"text":"dimensional","start":89,"end":100,"id":18},{"text":"neighborhood","start":101,"end":113,"id":19},{"text":"representation","start":114,"end":128,"id":20},{"text":".","start":128,"end":129,"id":21}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"Despite its simplicity, we show that it outperforms eight other entropy estimation procedures across a diverse range of sampling scenarios and data-generating models, even in cases of severe undersampling.","_input_hash":1789338829,"_task_hash":-723976366,"tokens":[{"text":"Despite","start":0,"end":7,"id":0},{"text":"its","start":8,"end":11,"id":1},{"text":"simplicity","start":12,"end":22,"id":2},{"text":",","start":22,"end":23,"id":3},{"text":"we","start":24,"end":26,"id":4},{"text":"show","start":27,"end":31,"id":5},{"text":"that","start":32,"end":36,"id":6},{"text":"it","start":37,"end":39,"id":7},{"text":"outperforms","start":40,"end":51,"id":8},{"text":"eight","start":52,"end":57,"id":9},{"text":"other","start":58,"end":63,"id":10},{"text":"entropy","start":64,"end":71,"id":11},{"text":"estimation","start":72,"end":82,"id":12},{"text":"procedures","start":83,"end":93,"id":13},{"text":"across","start":94,"end":100,"id":14},{"text":"a","start":101,"end":102,"id":15},{"text":"diverse","start":103,"end":110,"id":16},{"text":"range","start":111,"end":116,"id":17},{"text":"of","start":117,"end":119,"id":18},{"text":"sampling","start":120,"end":128,"id":19},{"text":"scenarios","start":129,"end":138,"id":20},{"text":"and","start":139,"end":142,"id":21},{"text":"data","start":143,"end":147,"id":22},{"text":"-","start":147,"end":148,"id":23},{"text":"generating","start":148,"end":158,"id":24},{"text":"models","start":159,"end":165,"id":25},{"text":",","start":165,"end":166,"id":26},{"text":"even","start":167,"end":171,"id":27},{"text":"in","start":172,"end":174,"id":28},{"text":"cases","start":175,"end":180,"id":29},{"text":"of","start":181,"end":183,"id":30},{"text":"severe","start":184,"end":190,"id":31},{"text":"undersampling","start":191,"end":204,"id":32},{"text":".","start":204,"end":205,"id":33}],"spans":[{"start":143,"end":158,"token_start":22,"token_end":24,"label":"ALGO","answer":"accept"}],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"However, the necessity of obtaining sensitivity measures as degrees of freedom for model selection or confidence intervals for more detailed analysis requires cubic runtime, and thus constitutes a computational bottleneck in real-world data analysis.","_input_hash":-487395949,"_task_hash":-102871396,"tokens":[{"text":"However","start":0,"end":7,"id":0},{"text":",","start":7,"end":8,"id":1},{"text":"the","start":9,"end":12,"id":2},{"text":"necessity","start":13,"end":22,"id":3},{"text":"of","start":23,"end":25,"id":4},{"text":"obtaining","start":26,"end":35,"id":5},{"text":"sensitivity","start":36,"end":47,"id":6},{"text":"measures","start":48,"end":56,"id":7},{"text":"as","start":57,"end":59,"id":8},{"text":"degrees","start":60,"end":67,"id":9},{"text":"of","start":68,"end":70,"id":10},{"text":"freedom","start":71,"end":78,"id":11},{"text":"for","start":79,"end":82,"id":12},{"text":"model","start":83,"end":88,"id":13},{"text":"selection","start":89,"end":98,"id":14},{"text":"or","start":99,"end":101,"id":15},{"text":"confidence","start":102,"end":112,"id":16},{"text":"intervals","start":113,"end":122,"id":17},{"text":"for","start":123,"end":126,"id":18},{"text":"more","start":127,"end":131,"id":19},{"text":"detailed","start":132,"end":140,"id":20},{"text":"analysis","start":141,"end":149,"id":21},{"text":"requires","start":150,"end":158,"id":22},{"text":"cubic","start":159,"end":164,"id":23},{"text":"runtime","start":165,"end":172,"id":24},{"text":",","start":172,"end":173,"id":25},{"text":"and","start":174,"end":177,"id":26},{"text":"thus","start":178,"end":182,"id":27},{"text":"constitutes","start":183,"end":194,"id":28},{"text":"a","start":195,"end":196,"id":29},{"text":"computational","start":197,"end":210,"id":30},{"text":"bottleneck","start":211,"end":221,"id":31},{"text":"in","start":222,"end":224,"id":32},{"text":"real","start":225,"end":229,"id":33},{"text":"-","start":229,"end":230,"id":34},{"text":"world","start":230,"end":235,"id":35},{"text":"data","start":236,"end":240,"id":36},{"text":"analysis","start":241,"end":249,"id":37},{"text":".","start":249,"end":250,"id":38}],"spans":[{"start":0,"end":8,"token_start":0,"token_end":1,"label":"ALGO","answer":"reject"},{"start":9,"end":22,"token_start":2,"token_end":3,"label":"ALGO","answer":"reject"},{"start":23,"end":35,"token_start":4,"token_end":5,"label":"ALGO","answer":"reject"},{"start":36,"end":56,"token_start":6,"token_end":7,"label":"ALGO","answer":"reject"},{"start":57,"end":67,"token_start":8,"token_end":9,"label":"ALGO","answer":"reject"},{"start":68,"end":78,"token_start":10,"token_end":11,"label":"ALGO","answer":"reject"},{"start":79,"end":88,"token_start":12,"token_end":13,"label":"ALGO","answer":"reject"},{"start":89,"end":101,"token_start":14,"token_end":15,"label":"ALGO","answer":"reject"},{"start":102,"end":122,"token_start":16,"token_end":17,"label":"ALGO","answer":"reject"},{"start":123,"end":131,"token_start":18,"token_end":19,"label":"ALGO","answer":"reject"},{"start":132,"end":149,"token_start":20,"token_end":21,"label":"ALGO","answer":"reject"},{"start":150,"end":164,"token_start":22,"token_end":23,"label":"ALGO","answer":"reject"},{"start":165,"end":173,"token_start":24,"token_end":25,"label":"ALGO","answer":"reject"},{"start":229,"end":230,"token_start":34,"token_end":34,"label":"ALGO","answer":"reject"}],"_session_id":null,"_view_id":"ner_manual","answer":"reject"}
{"text":"This monograph deals with adaptive supervised classification, using tools borrowed from statistical mechanics and information theory, stemming from the PACBayesian approach pioneered by David McAllester and applied to a conception of statistical learning theory forged by Vladimir Vapnik.","_input_hash":291673665,"_task_hash":255261176,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"monograph","start":5,"end":14,"id":1},{"text":"deals","start":15,"end":20,"id":2},{"text":"with","start":21,"end":25,"id":3},{"text":"adaptive","start":26,"end":34,"id":4},{"text":"supervised","start":35,"end":45,"id":5},{"text":"classification","start":46,"end":60,"id":6},{"text":",","start":60,"end":61,"id":7},{"text":"using","start":62,"end":67,"id":8},{"text":"tools","start":68,"end":73,"id":9},{"text":"borrowed","start":74,"end":82,"id":10},{"text":"from","start":83,"end":87,"id":11},{"text":"statistical","start":88,"end":99,"id":12},{"text":"mechanics","start":100,"end":109,"id":13},{"text":"and","start":110,"end":113,"id":14},{"text":"information","start":114,"end":125,"id":15},{"text":"theory","start":126,"end":132,"id":16},{"text":",","start":132,"end":133,"id":17},{"text":"stemming","start":134,"end":142,"id":18},{"text":"from","start":143,"end":147,"id":19},{"text":"the","start":148,"end":151,"id":20},{"text":"PACBayesian","start":152,"end":163,"id":21},{"text":"approach","start":164,"end":172,"id":22},{"text":"pioneered","start":173,"end":182,"id":23},{"text":"by","start":183,"end":185,"id":24},{"text":"David","start":186,"end":191,"id":25},{"text":"McAllester","start":192,"end":202,"id":26},{"text":"and","start":203,"end":206,"id":27},{"text":"applied","start":207,"end":214,"id":28},{"text":"to","start":215,"end":217,"id":29},{"text":"a","start":218,"end":219,"id":30},{"text":"conception","start":220,"end":230,"id":31},{"text":"of","start":231,"end":233,"id":32},{"text":"statistical","start":234,"end":245,"id":33},{"text":"learning","start":246,"end":254,"id":34},{"text":"theory","start":255,"end":261,"id":35},{"text":"forged","start":262,"end":268,"id":36},{"text":"by","start":269,"end":271,"id":37},{"text":"Vladimir","start":272,"end":280,"id":38},{"text":"Vapnik","start":281,"end":287,"id":39},{"text":".","start":287,"end":288,"id":40}],"spans":[{"start":26,"end":60,"token_start":4,"token_end":6,"label":"ALGO","answer":"accept"},{"start":152,"end":163,"token_start":21,"token_end":21,"label":"ALGO","answer":"accept"}],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"Finally, we present numerical results that demonstrate our claims.","_input_hash":896022460,"_task_hash":-851466995,"tokens":[{"text":"Finally","start":0,"end":7,"id":0},{"text":",","start":7,"end":8,"id":1},{"text":"we","start":9,"end":11,"id":2},{"text":"present","start":12,"end":19,"id":3},{"text":"numerical","start":20,"end":29,"id":4},{"text":"results","start":30,"end":37,"id":5},{"text":"that","start":38,"end":42,"id":6},{"text":"demonstrate","start":43,"end":54,"id":7},{"text":"our","start":55,"end":58,"id":8},{"text":"claims","start":59,"end":65,"id":9},{"text":".","start":65,"end":66,"id":10}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"Dimensionality reduction is a topic of recent interest.","_input_hash":2135991398,"_task_hash":90492257,"tokens":[{"text":"Dimensionality","start":0,"end":14,"id":0},{"text":"reduction","start":15,"end":24,"id":1},{"text":"is","start":25,"end":27,"id":2},{"text":"a","start":28,"end":29,"id":3},{"text":"topic","start":30,"end":35,"id":4},{"text":"of","start":36,"end":38,"id":5},{"text":"recent","start":39,"end":45,"id":6},{"text":"interest","start":46,"end":54,"id":7},{"text":".","start":54,"end":55,"id":8}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"This method embeds a dendrogram as a subtree into the Bruhat-Tits tree associated to the p-adic numbers, and goes back to Cornelissen et al. (","_input_hash":778461077,"_task_hash":-1638456363,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"method","start":5,"end":11,"id":1},{"text":"embeds","start":12,"end":18,"id":2},{"text":"a","start":19,"end":20,"id":3},{"text":"dendrogram","start":21,"end":31,"id":4},{"text":"as","start":32,"end":34,"id":5},{"text":"a","start":35,"end":36,"id":6},{"text":"subtree","start":37,"end":44,"id":7},{"text":"into","start":45,"end":49,"id":8},{"text":"the","start":50,"end":53,"id":9},{"text":"Bruhat","start":54,"end":60,"id":10},{"text":"-","start":60,"end":61,"id":11},{"text":"Tits","start":61,"end":65,"id":12},{"text":"tree","start":66,"end":70,"id":13},{"text":"associated","start":71,"end":81,"id":14},{"text":"to","start":82,"end":84,"id":15},{"text":"the","start":85,"end":88,"id":16},{"text":"p","start":89,"end":90,"id":17},{"text":"-","start":90,"end":91,"id":18},{"text":"adic","start":91,"end":95,"id":19},{"text":"numbers","start":96,"end":103,"id":20},{"text":",","start":103,"end":104,"id":21},{"text":"and","start":105,"end":108,"id":22},{"text":"goes","start":109,"end":113,"id":23},{"text":"back","start":114,"end":118,"id":24},{"text":"to","start":119,"end":121,"id":25},{"text":"Cornelissen","start":122,"end":133,"id":26},{"text":"et","start":134,"end":136,"id":27},{"text":"al","start":137,"end":139,"id":28},{"text":".","start":139,"end":140,"id":29},{"text":"(","start":141,"end":142,"id":30}],"spans":[{"token_start":10,"token_end":13,"start":54,"end":70,"text":"Bruhat-Tits tree","label":"ALGO","source":"./algo_model5","input_hash":778461077,"answer":"accept"}],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"We investigate classification performance using the CCDR algorithm on hyper-spectral satellite imagery data.","_input_hash":-1472662451,"_task_hash":-238272890,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"investigate","start":3,"end":14,"id":1},{"text":"classification","start":15,"end":29,"id":2},{"text":"performance","start":30,"end":41,"id":3},{"text":"using","start":42,"end":47,"id":4},{"text":"the","start":48,"end":51,"id":5},{"text":"CCDR","start":52,"end":56,"id":6},{"text":"algorithm","start":57,"end":66,"id":7},{"text":"on","start":67,"end":69,"id":8},{"text":"hyper","start":70,"end":75,"id":9},{"text":"-","start":75,"end":76,"id":10},{"text":"spectral","start":76,"end":84,"id":11},{"text":"satellite","start":85,"end":94,"id":12},{"text":"imagery","start":95,"end":102,"id":13},{"text":"data","start":103,"end":107,"id":14},{"text":".","start":107,"end":108,"id":15}],"spans":[{"token_start":6,"token_end":6,"start":52,"end":56,"text":"CCDR","label":"ALGO","source":"./algo_model5","input_hash":-1472662451,"answer":"accept"}],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"Changepoints are abrupt variations in the generative parameters of a data sequence.","_input_hash":1519314402,"_task_hash":-980663315,"tokens":[{"text":"Changepoints","start":0,"end":12,"id":0},{"text":"are","start":13,"end":16,"id":1},{"text":"abrupt","start":17,"end":23,"id":2},{"text":"variations","start":24,"end":34,"id":3},{"text":"in","start":35,"end":37,"id":4},{"text":"the","start":38,"end":41,"id":5},{"text":"generative","start":42,"end":52,"id":6},{"text":"parameters","start":53,"end":63,"id":7},{"text":"of","start":64,"end":66,"id":8},{"text":"a","start":67,"end":68,"id":9},{"text":"data","start":69,"end":73,"id":10},{"text":"sequence","start":74,"end":82,"id":11},{"text":".","start":82,"end":83,"id":12}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"Many problems of low-level computer vision and image processing, such as denoising, deconvolution, tomographic reconstruction or super-resolution, can be addressed by maximizing the posterior distribution of a sparse linear model (SLM).","_input_hash":1701962311,"_task_hash":1236810890,"tokens":[{"text":"Many","start":0,"end":4,"id":0},{"text":"problems","start":5,"end":13,"id":1},{"text":"of","start":14,"end":16,"id":2},{"text":"low","start":17,"end":20,"id":3},{"text":"-","start":20,"end":21,"id":4},{"text":"level","start":21,"end":26,"id":5},{"text":"computer","start":27,"end":35,"id":6},{"text":"vision","start":36,"end":42,"id":7},{"text":"and","start":43,"end":46,"id":8},{"text":"image","start":47,"end":52,"id":9},{"text":"processing","start":53,"end":63,"id":10},{"text":",","start":63,"end":64,"id":11},{"text":"such","start":65,"end":69,"id":12},{"text":"as","start":70,"end":72,"id":13},{"text":"denoising","start":73,"end":82,"id":14},{"text":",","start":82,"end":83,"id":15},{"text":"deconvolution","start":84,"end":97,"id":16},{"text":",","start":97,"end":98,"id":17},{"text":"tomographic","start":99,"end":110,"id":18},{"text":"reconstruction","start":111,"end":125,"id":19},{"text":"or","start":126,"end":128,"id":20},{"text":"super","start":129,"end":134,"id":21},{"text":"-","start":134,"end":135,"id":22},{"text":"resolution","start":135,"end":145,"id":23},{"text":",","start":145,"end":146,"id":24},{"text":"can","start":147,"end":150,"id":25},{"text":"be","start":151,"end":153,"id":26},{"text":"addressed","start":154,"end":163,"id":27},{"text":"by","start":164,"end":166,"id":28},{"text":"maximizing","start":167,"end":177,"id":29},{"text":"the","start":178,"end":181,"id":30},{"text":"posterior","start":182,"end":191,"id":31},{"text":"distribution","start":192,"end":204,"id":32},{"text":"of","start":205,"end":207,"id":33},{"text":"a","start":208,"end":209,"id":34},{"text":"sparse","start":210,"end":216,"id":35},{"text":"linear","start":217,"end":223,"id":36},{"text":"model","start":224,"end":229,"id":37},{"text":"(","start":230,"end":231,"id":38},{"text":"SLM","start":231,"end":234,"id":39},{"text":")","start":234,"end":235,"id":40},{"text":".","start":235,"end":236,"id":41}],"spans":[{"start":27,"end":42,"token_start":6,"token_end":7,"label":"ALGO","answer":"accept"},{"start":47,"end":63,"token_start":9,"token_end":10,"label":"ALGO","answer":"accept"},{"start":210,"end":223,"token_start":35,"token_end":36,"label":"ALGO","answer":"accept"}],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"A brief discussion about learning and symmetry breaking based on our results is also presented.","_input_hash":-1615577558,"_task_hash":102416772,"tokens":[{"text":"A","start":0,"end":1,"id":0},{"text":"brief","start":2,"end":7,"id":1},{"text":"discussion","start":8,"end":18,"id":2},{"text":"about","start":19,"end":24,"id":3},{"text":"learning","start":25,"end":33,"id":4},{"text":"and","start":34,"end":37,"id":5},{"text":"symmetry","start":38,"end":46,"id":6},{"text":"breaking","start":47,"end":55,"id":7},{"text":"based","start":56,"end":61,"id":8},{"text":"on","start":62,"end":64,"id":9},{"text":"our","start":65,"end":68,"id":10},{"text":"results","start":69,"end":76,"id":11},{"text":"is","start":77,"end":79,"id":12},{"text":"also","start":80,"end":84,"id":13},{"text":"presented","start":85,"end":94,"id":14},{"text":".","start":94,"end":95,"id":15}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"We obtain an index of the complexity of a random sequence by allowing the role of the measure in classical probability theory to be played by a function we call the generating mechanism.","_input_hash":293701925,"_task_hash":-213590300,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"obtain","start":3,"end":9,"id":1},{"text":"an","start":10,"end":12,"id":2},{"text":"index","start":13,"end":18,"id":3},{"text":"of","start":19,"end":21,"id":4},{"text":"the","start":22,"end":25,"id":5},{"text":"complexity","start":26,"end":36,"id":6},{"text":"of","start":37,"end":39,"id":7},{"text":"a","start":40,"end":41,"id":8},{"text":"random","start":42,"end":48,"id":9},{"text":"sequence","start":49,"end":57,"id":10},{"text":"by","start":58,"end":60,"id":11},{"text":"allowing","start":61,"end":69,"id":12},{"text":"the","start":70,"end":73,"id":13},{"text":"role","start":74,"end":78,"id":14},{"text":"of","start":79,"end":81,"id":15},{"text":"the","start":82,"end":85,"id":16},{"text":"measure","start":86,"end":93,"id":17},{"text":"in","start":94,"end":96,"id":18},{"text":"classical","start":97,"end":106,"id":19},{"text":"probability","start":107,"end":118,"id":20},{"text":"theory","start":119,"end":125,"id":21},{"text":"to","start":126,"end":128,"id":22},{"text":"be","start":129,"end":131,"id":23},{"text":"played","start":132,"end":138,"id":24},{"text":"by","start":139,"end":141,"id":25},{"text":"a","start":142,"end":143,"id":26},{"text":"function","start":144,"end":152,"id":27},{"text":"we","start":153,"end":155,"id":28},{"text":"call","start":156,"end":160,"id":29},{"text":"the","start":161,"end":164,"id":30},{"text":"generating","start":165,"end":175,"id":31},{"text":"mechanism","start":176,"end":185,"id":32},{"text":".","start":185,"end":186,"id":33}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"In Chapter 3, we propose a method for compressing a group of parameters into a single one, by exploiting the fact that many predictor variables derived from high-order interactions have the same values for all the training cases.","_input_hash":82553369,"_task_hash":-1207129772,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"Chapter","start":3,"end":10,"id":1},{"text":"3","start":11,"end":12,"id":2},{"text":",","start":12,"end":13,"id":3},{"text":"we","start":14,"end":16,"id":4},{"text":"propose","start":17,"end":24,"id":5},{"text":"a","start":25,"end":26,"id":6},{"text":"method","start":27,"end":33,"id":7},{"text":"for","start":34,"end":37,"id":8},{"text":"compressing","start":38,"end":49,"id":9},{"text":"a","start":50,"end":51,"id":10},{"text":"group","start":52,"end":57,"id":11},{"text":"of","start":58,"end":60,"id":12},{"text":"parameters","start":61,"end":71,"id":13},{"text":"into","start":72,"end":76,"id":14},{"text":"a","start":77,"end":78,"id":15},{"text":"single","start":79,"end":85,"id":16},{"text":"one","start":86,"end":89,"id":17},{"text":",","start":89,"end":90,"id":18},{"text":"by","start":91,"end":93,"id":19},{"text":"exploiting","start":94,"end":104,"id":20},{"text":"the","start":105,"end":108,"id":21},{"text":"fact","start":109,"end":113,"id":22},{"text":"that","start":114,"end":118,"id":23},{"text":"many","start":119,"end":123,"id":24},{"text":"predictor","start":124,"end":133,"id":25},{"text":"variables","start":134,"end":143,"id":26},{"text":"derived","start":144,"end":151,"id":27},{"text":"from","start":152,"end":156,"id":28},{"text":"high","start":157,"end":161,"id":29},{"text":"-","start":161,"end":162,"id":30},{"text":"order","start":162,"end":167,"id":31},{"text":"interactions","start":168,"end":180,"id":32},{"text":"have","start":181,"end":185,"id":33},{"text":"the","start":186,"end":189,"id":34},{"text":"same","start":190,"end":194,"id":35},{"text":"values","start":195,"end":201,"id":36},{"text":"for","start":202,"end":205,"id":37},{"text":"all","start":206,"end":209,"id":38},{"text":"the","start":210,"end":213,"id":39},{"text":"training","start":214,"end":222,"id":40},{"text":"cases","start":223,"end":228,"id":41},{"text":".","start":228,"end":229,"id":42}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"In Chapter 2, we propose a Bayesian method to avoid this selection bias, with application to naive Bayes models and mixture models.","_input_hash":-134608525,"_task_hash":269575710,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"Chapter","start":3,"end":10,"id":1},{"text":"2","start":11,"end":12,"id":2},{"text":",","start":12,"end":13,"id":3},{"text":"we","start":14,"end":16,"id":4},{"text":"propose","start":17,"end":24,"id":5},{"text":"a","start":25,"end":26,"id":6},{"text":"Bayesian","start":27,"end":35,"id":7},{"text":"method","start":36,"end":42,"id":8},{"text":"to","start":43,"end":45,"id":9},{"text":"avoid","start":46,"end":51,"id":10},{"text":"this","start":52,"end":56,"id":11},{"text":"selection","start":57,"end":66,"id":12},{"text":"bias","start":67,"end":71,"id":13},{"text":",","start":71,"end":72,"id":14},{"text":"with","start":73,"end":77,"id":15},{"text":"application","start":78,"end":89,"id":16},{"text":"to","start":90,"end":92,"id":17},{"text":"naive","start":93,"end":98,"id":18},{"text":"Bayes","start":99,"end":104,"id":19},{"text":"models","start":105,"end":111,"id":20},{"text":"and","start":112,"end":115,"id":21},{"text":"mixture","start":116,"end":123,"id":22},{"text":"models","start":124,"end":130,"id":23},{"text":".","start":130,"end":131,"id":24}],"spans":[{"start":27,"end":35,"token_start":7,"token_end":7,"label":"ALGO","answer":"accept"},{"start":93,"end":104,"token_start":18,"token_end":19,"label":"ALGO","answer":"accept"},{"start":116,"end":123,"token_start":22,"token_end":22,"label":"ALGO","answer":"accept"}],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"This is carried out through a small simulation study and the analysis of several real functional data sets.","_input_hash":1263647153,"_task_hash":-1692974147,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"is","start":5,"end":7,"id":1},{"text":"carried","start":8,"end":15,"id":2},{"text":"out","start":16,"end":19,"id":3},{"text":"through","start":20,"end":27,"id":4},{"text":"a","start":28,"end":29,"id":5},{"text":"small","start":30,"end":35,"id":6},{"text":"simulation","start":36,"end":46,"id":7},{"text":"study","start":47,"end":52,"id":8},{"text":"and","start":53,"end":56,"id":9},{"text":"the","start":57,"end":60,"id":10},{"text":"analysis","start":61,"end":69,"id":11},{"text":"of","start":70,"end":72,"id":12},{"text":"several","start":73,"end":80,"id":13},{"text":"real","start":81,"end":85,"id":14},{"text":"functional","start":86,"end":96,"id":15},{"text":"data","start":97,"end":101,"id":16},{"text":"sets","start":102,"end":106,"id":17},{"text":".","start":106,"end":107,"id":18}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"This model exemplifies a recent trend in statistical machine learning--the use of Bayesian nonparametric methods to infer distributions on flexible data structures.","_input_hash":1038966363,"_task_hash":204625339,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"model","start":5,"end":10,"id":1},{"text":"exemplifies","start":11,"end":22,"id":2},{"text":"a","start":23,"end":24,"id":3},{"text":"recent","start":25,"end":31,"id":4},{"text":"trend","start":32,"end":37,"id":5},{"text":"in","start":38,"end":40,"id":6},{"text":"statistical","start":41,"end":52,"id":7},{"text":"machine","start":53,"end":60,"id":8},{"text":"learning","start":61,"end":69,"id":9},{"text":"--","start":69,"end":71,"id":10},{"text":"the","start":71,"end":74,"id":11},{"text":"use","start":75,"end":78,"id":12},{"text":"of","start":79,"end":81,"id":13},{"text":"Bayesian","start":82,"end":90,"id":14},{"text":"nonparametric","start":91,"end":104,"id":15},{"text":"methods","start":105,"end":112,"id":16},{"text":"to","start":113,"end":115,"id":17},{"text":"infer","start":116,"end":121,"id":18},{"text":"distributions","start":122,"end":135,"id":19},{"text":"on","start":136,"end":138,"id":20},{"text":"flexible","start":139,"end":147,"id":21},{"text":"data","start":148,"end":152,"id":22},{"text":"structures","start":153,"end":163,"id":23},{"text":".","start":163,"end":164,"id":24}],"spans":[{"start":82,"end":104,"token_start":14,"token_end":15,"label":"ALGO","answer":"accept"}],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"The equations show that optimization is computationally expensive.","_input_hash":1928351634,"_task_hash":1322417302,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"equations","start":4,"end":13,"id":1},{"text":"show","start":14,"end":18,"id":2},{"text":"that","start":19,"end":23,"id":3},{"text":"optimization","start":24,"end":36,"id":4},{"text":"is","start":37,"end":39,"id":5},{"text":"computationally","start":40,"end":55,"id":6},{"text":"expensive","start":56,"end":65,"id":7},{"text":".","start":65,"end":66,"id":8}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"We show how higher-order Bayesian decision-making problems, such as optimizing image acquisition in magnetic resonance scanners, can be addressed by querying the SLM posterior covariance, unrelated to the density's mode.","_input_hash":1008301903,"_task_hash":811203434,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"show","start":3,"end":7,"id":1},{"text":"how","start":8,"end":11,"id":2},{"text":"higher","start":12,"end":18,"id":3},{"text":"-","start":18,"end":19,"id":4},{"text":"order","start":19,"end":24,"id":5},{"text":"Bayesian","start":25,"end":33,"id":6},{"text":"decision","start":34,"end":42,"id":7},{"text":"-","start":42,"end":43,"id":8},{"text":"making","start":43,"end":49,"id":9},{"text":"problems","start":50,"end":58,"id":10},{"text":",","start":58,"end":59,"id":11},{"text":"such","start":60,"end":64,"id":12},{"text":"as","start":65,"end":67,"id":13},{"text":"optimizing","start":68,"end":78,"id":14},{"text":"image","start":79,"end":84,"id":15},{"text":"acquisition","start":85,"end":96,"id":16},{"text":"in","start":97,"end":99,"id":17},{"text":"magnetic","start":100,"end":108,"id":18},{"text":"resonance","start":109,"end":118,"id":19},{"text":"scanners","start":119,"end":127,"id":20},{"text":",","start":127,"end":128,"id":21},{"text":"can","start":129,"end":132,"id":22},{"text":"be","start":133,"end":135,"id":23},{"text":"addressed","start":136,"end":145,"id":24},{"text":"by","start":146,"end":148,"id":25},{"text":"querying","start":149,"end":157,"id":26},{"text":"the","start":158,"end":161,"id":27},{"text":"SLM","start":162,"end":165,"id":28},{"text":"posterior","start":166,"end":175,"id":29},{"text":"covariance","start":176,"end":186,"id":30},{"text":",","start":186,"end":187,"id":31},{"text":"unrelated","start":188,"end":197,"id":32},{"text":"to","start":198,"end":200,"id":33},{"text":"the","start":201,"end":204,"id":34},{"text":"density","start":205,"end":212,"id":35},{"text":"'s","start":212,"end":214,"id":36},{"text":"mode","start":215,"end":219,"id":37},{"text":".","start":219,"end":220,"id":38}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"Completed by a point at infinity, they can be viewed as subtrees of the Bruhat-Tits tree associated to the $p$-adic projective line.","_input_hash":1227531399,"_task_hash":378816634,"tokens":[{"text":"Completed","start":0,"end":9,"id":0},{"text":"by","start":10,"end":12,"id":1},{"text":"a","start":13,"end":14,"id":2},{"text":"point","start":15,"end":20,"id":3},{"text":"at","start":21,"end":23,"id":4},{"text":"infinity","start":24,"end":32,"id":5},{"text":",","start":32,"end":33,"id":6},{"text":"they","start":34,"end":38,"id":7},{"text":"can","start":39,"end":42,"id":8},{"text":"be","start":43,"end":45,"id":9},{"text":"viewed","start":46,"end":52,"id":10},{"text":"as","start":53,"end":55,"id":11},{"text":"subtrees","start":56,"end":64,"id":12},{"text":"of","start":65,"end":67,"id":13},{"text":"the","start":68,"end":71,"id":14},{"text":"Bruhat","start":72,"end":78,"id":15},{"text":"-","start":78,"end":79,"id":16},{"text":"Tits","start":79,"end":83,"id":17},{"text":"tree","start":84,"end":88,"id":18},{"text":"associated","start":89,"end":99,"id":19},{"text":"to","start":100,"end":102,"id":20},{"text":"the","start":103,"end":106,"id":21},{"text":"$","start":107,"end":108,"id":22},{"text":"p$-adic","start":108,"end":115,"id":23},{"text":"projective","start":116,"end":126,"id":24},{"text":"line","start":127,"end":131,"id":25},{"text":".","start":131,"end":132,"id":26}],"spans":[{"token_start":15,"token_end":18,"start":72,"end":88,"text":"Bruhat-Tits tree","label":"ALGO","source":"./algo_model5","input_hash":1227531399,"answer":"accept"}],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"Simulated annealing is a popular method for approaching the solution of a global optimization problem.","_input_hash":1054415795,"_task_hash":-184384468,"tokens":[{"text":"Simulated","start":0,"end":9,"id":0},{"text":"annealing","start":10,"end":19,"id":1},{"text":"is","start":20,"end":22,"id":2},{"text":"a","start":23,"end":24,"id":3},{"text":"popular","start":25,"end":32,"id":4},{"text":"method","start":33,"end":39,"id":5},{"text":"for","start":40,"end":43,"id":6},{"text":"approaching","start":44,"end":55,"id":7},{"text":"the","start":56,"end":59,"id":8},{"text":"solution","start":60,"end":68,"id":9},{"text":"of","start":69,"end":71,"id":10},{"text":"a","start":72,"end":73,"id":11},{"text":"global","start":74,"end":80,"id":12},{"text":"optimization","start":81,"end":93,"id":13},{"text":"problem","start":94,"end":101,"id":14},{"text":".","start":101,"end":102,"id":15}],"spans":[{"start":0,"end":19,"token_start":0,"token_end":1,"label":"ALGO","answer":"accept"}],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"The measure also serves as a natural tool when choosing dimension-reduction parameters.","_input_hash":-1067289464,"_task_hash":2026893910,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"measure","start":4,"end":11,"id":1},{"text":"also","start":12,"end":16,"id":2},{"text":"serves","start":17,"end":23,"id":3},{"text":"as","start":24,"end":26,"id":4},{"text":"a","start":27,"end":28,"id":5},{"text":"natural","start":29,"end":36,"id":6},{"text":"tool","start":37,"end":41,"id":7},{"text":"when","start":42,"end":46,"id":8},{"text":"choosing","start":47,"end":55,"id":9},{"text":"dimension","start":56,"end":65,"id":10},{"text":"-","start":65,"end":66,"id":11},{"text":"reduction","start":66,"end":75,"id":12},{"text":"parameters","start":76,"end":86,"id":13},{"text":".","start":86,"end":87,"id":14}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"Application examples are given for hidden model reconstruction, cellular automata filtering, and edge detection in images.","_input_hash":-23107903,"_task_hash":-1548598598,"tokens":[{"text":"Application","start":0,"end":11,"id":0},{"text":"examples","start":12,"end":20,"id":1},{"text":"are","start":21,"end":24,"id":2},{"text":"given","start":25,"end":30,"id":3},{"text":"for","start":31,"end":34,"id":4},{"text":"hidden","start":35,"end":41,"id":5},{"text":"model","start":42,"end":47,"id":6},{"text":"reconstruction","start":48,"end":62,"id":7},{"text":",","start":62,"end":63,"id":8},{"text":"cellular","start":64,"end":72,"id":9},{"text":"automata","start":73,"end":81,"id":10},{"text":"filtering","start":82,"end":91,"id":11},{"text":",","start":91,"end":92,"id":12},{"text":"and","start":93,"end":96,"id":13},{"text":"edge","start":97,"end":101,"id":14},{"text":"detection","start":102,"end":111,"id":15},{"text":"in","start":112,"end":114,"id":16},{"text":"images","start":115,"end":121,"id":17},{"text":".","start":121,"end":122,"id":18}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"The issues associated with these difficulties relate to the broader structure of discrete exponential families.","_input_hash":-1951682925,"_task_hash":-212268159,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"issues","start":4,"end":10,"id":1},{"text":"associated","start":11,"end":21,"id":2},{"text":"with","start":22,"end":26,"id":3},{"text":"these","start":27,"end":32,"id":4},{"text":"difficulties","start":33,"end":45,"id":5},{"text":"relate","start":46,"end":52,"id":6},{"text":"to","start":53,"end":55,"id":7},{"text":"the","start":56,"end":59,"id":8},{"text":"broader","start":60,"end":67,"id":9},{"text":"structure","start":68,"end":77,"id":10},{"text":"of","start":78,"end":80,"id":11},{"text":"discrete","start":81,"end":89,"id":12},{"text":"exponential","start":90,"end":101,"id":13},{"text":"families","start":102,"end":110,"id":14},{"text":".","start":110,"end":111,"id":15}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"Consequently, the weight vectors are highly sensitive to noise.","_input_hash":-1963221515,"_task_hash":508945984,"tokens":[{"text":"Consequently","start":0,"end":12,"id":0},{"text":",","start":12,"end":13,"id":1},{"text":"the","start":14,"end":17,"id":2},{"text":"weight","start":18,"end":24,"id":3},{"text":"vectors","start":25,"end":32,"id":4},{"text":"are","start":33,"end":36,"id":5},{"text":"highly","start":37,"end":43,"id":6},{"text":"sensitive","start":44,"end":53,"id":7},{"text":"to","start":54,"end":56,"id":8},{"text":"noise","start":57,"end":62,"id":9},{"text":".","start":62,"end":63,"id":10}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"Motivated by an existing graph partitioning framework, we derive relationships between optimizing relevance information, as defined in the Information Bottleneck method, and the regularized cut in a K-partitioned graph.","_input_hash":909894230,"_task_hash":1831309840,"tokens":[{"text":"Motivated","start":0,"end":9,"id":0},{"text":"by","start":10,"end":12,"id":1},{"text":"an","start":13,"end":15,"id":2},{"text":"existing","start":16,"end":24,"id":3},{"text":"graph","start":25,"end":30,"id":4},{"text":"partitioning","start":31,"end":43,"id":5},{"text":"framework","start":44,"end":53,"id":6},{"text":",","start":53,"end":54,"id":7},{"text":"we","start":55,"end":57,"id":8},{"text":"derive","start":58,"end":64,"id":9},{"text":"relationships","start":65,"end":78,"id":10},{"text":"between","start":79,"end":86,"id":11},{"text":"optimizing","start":87,"end":97,"id":12},{"text":"relevance","start":98,"end":107,"id":13},{"text":"information","start":108,"end":119,"id":14},{"text":",","start":119,"end":120,"id":15},{"text":"as","start":121,"end":123,"id":16},{"text":"defined","start":124,"end":131,"id":17},{"text":"in","start":132,"end":134,"id":18},{"text":"the","start":135,"end":138,"id":19},{"text":"Information","start":139,"end":150,"id":20},{"text":"Bottleneck","start":151,"end":161,"id":21},{"text":"method","start":162,"end":168,"id":22},{"text":",","start":168,"end":169,"id":23},{"text":"and","start":170,"end":173,"id":24},{"text":"the","start":174,"end":177,"id":25},{"text":"regularized","start":178,"end":189,"id":26},{"text":"cut","start":190,"end":193,"id":27},{"text":"in","start":194,"end":196,"id":28},{"text":"a","start":197,"end":198,"id":29},{"text":"K","start":199,"end":200,"id":30},{"text":"-","start":200,"end":201,"id":31},{"text":"partitioned","start":201,"end":212,"id":32},{"text":"graph","start":213,"end":218,"id":33},{"text":".","start":218,"end":219,"id":34}],"spans":[{"start":139,"end":161,"token_start":20,"token_end":21,"label":"ALGO","answer":"accept"}],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"Here we examine the case where the model parameters before and after the changepoint are independent and we derive an online algorithm for exact inference of the most recent changepoint.","_input_hash":852536986,"_task_hash":1144471029,"tokens":[{"text":"Here","start":0,"end":4,"id":0},{"text":"we","start":5,"end":7,"id":1},{"text":"examine","start":8,"end":15,"id":2},{"text":"the","start":16,"end":19,"id":3},{"text":"case","start":20,"end":24,"id":4},{"text":"where","start":25,"end":30,"id":5},{"text":"the","start":31,"end":34,"id":6},{"text":"model","start":35,"end":40,"id":7},{"text":"parameters","start":41,"end":51,"id":8},{"text":"before","start":52,"end":58,"id":9},{"text":"and","start":59,"end":62,"id":10},{"text":"after","start":63,"end":68,"id":11},{"text":"the","start":69,"end":72,"id":12},{"text":"changepoint","start":73,"end":84,"id":13},{"text":"are","start":85,"end":88,"id":14},{"text":"independent","start":89,"end":100,"id":15},{"text":"and","start":101,"end":104,"id":16},{"text":"we","start":105,"end":107,"id":17},{"text":"derive","start":108,"end":114,"id":18},{"text":"an","start":115,"end":117,"id":19},{"text":"online","start":118,"end":124,"id":20},{"text":"algorithm","start":125,"end":134,"id":21},{"text":"for","start":135,"end":138,"id":22},{"text":"exact","start":139,"end":144,"id":23},{"text":"inference","start":145,"end":154,"id":24},{"text":"of","start":155,"end":157,"id":25},{"text":"the","start":158,"end":161,"id":26},{"text":"most","start":162,"end":166,"id":27},{"text":"recent","start":167,"end":173,"id":28},{"text":"changepoint","start":174,"end":185,"id":29},{"text":".","start":185,"end":186,"id":30}],"spans":[{"token_start":20,"token_end":20,"start":118,"end":124,"text":"online","label":"ALGO","source":"./algo_model5","input_hash":852536986,"answer":"accept"}],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"Being among the easiest ways to find meaningful structure from discrete data, Latent Dirichlet Allocation (LDA) and related component models have been applied widely.","_input_hash":-96609974,"_task_hash":653154278,"tokens":[{"text":"Being","start":0,"end":5,"id":0},{"text":"among","start":6,"end":11,"id":1},{"text":"the","start":12,"end":15,"id":2},{"text":"easiest","start":16,"end":23,"id":3},{"text":"ways","start":24,"end":28,"id":4},{"text":"to","start":29,"end":31,"id":5},{"text":"find","start":32,"end":36,"id":6},{"text":"meaningful","start":37,"end":47,"id":7},{"text":"structure","start":48,"end":57,"id":8},{"text":"from","start":58,"end":62,"id":9},{"text":"discrete","start":63,"end":71,"id":10},{"text":"data","start":72,"end":76,"id":11},{"text":",","start":76,"end":77,"id":12},{"text":"Latent","start":78,"end":84,"id":13},{"text":"Dirichlet","start":85,"end":94,"id":14},{"text":"Allocation","start":95,"end":105,"id":15},{"text":"(","start":106,"end":107,"id":16},{"text":"LDA","start":107,"end":110,"id":17},{"text":")","start":110,"end":111,"id":18},{"text":"and","start":112,"end":115,"id":19},{"text":"related","start":116,"end":123,"id":20},{"text":"component","start":124,"end":133,"id":21},{"text":"models","start":134,"end":140,"id":22},{"text":"have","start":141,"end":145,"id":23},{"text":"been","start":146,"end":150,"id":24},{"text":"applied","start":151,"end":158,"id":25},{"text":"widely","start":159,"end":165,"id":26},{"text":".","start":165,"end":166,"id":27}],"spans":[{"token_start":13,"token_end":15,"start":78,"end":105,"text":"Latent Dirichlet Allocation","label":"ALGO","source":"./algo_model5","input_hash":-96609974,"answer":"accept"}],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"While no global \"uniform\" winner emerges from such comparisons, the overall performance of the k-NN method, together with its sound intuitive motivation and relative simplicity, suggests that it could represent a reasonable benchmark for the classification problem with functional data.","_input_hash":624564835,"_task_hash":74117629,"tokens":[{"text":"While","start":0,"end":5,"id":0},{"text":"no","start":6,"end":8,"id":1},{"text":"global","start":9,"end":15,"id":2},{"text":"\"","start":16,"end":17,"id":3},{"text":"uniform","start":17,"end":24,"id":4},{"text":"\"","start":24,"end":25,"id":5},{"text":"winner","start":26,"end":32,"id":6},{"text":"emerges","start":33,"end":40,"id":7},{"text":"from","start":41,"end":45,"id":8},{"text":"such","start":46,"end":50,"id":9},{"text":"comparisons","start":51,"end":62,"id":10},{"text":",","start":62,"end":63,"id":11},{"text":"the","start":64,"end":67,"id":12},{"text":"overall","start":68,"end":75,"id":13},{"text":"performance","start":76,"end":87,"id":14},{"text":"of","start":88,"end":90,"id":15},{"text":"the","start":91,"end":94,"id":16},{"text":"k","start":95,"end":96,"id":17},{"text":"-","start":96,"end":97,"id":18},{"text":"NN","start":97,"end":99,"id":19},{"text":"method","start":100,"end":106,"id":20},{"text":",","start":106,"end":107,"id":21},{"text":"together","start":108,"end":116,"id":22},{"text":"with","start":117,"end":121,"id":23},{"text":"its","start":122,"end":125,"id":24},{"text":"sound","start":126,"end":131,"id":25},{"text":"intuitive","start":132,"end":141,"id":26},{"text":"motivation","start":142,"end":152,"id":27},{"text":"and","start":153,"end":156,"id":28},{"text":"relative","start":157,"end":165,"id":29},{"text":"simplicity","start":166,"end":176,"id":30},{"text":",","start":176,"end":177,"id":31},{"text":"suggests","start":178,"end":186,"id":32},{"text":"that","start":187,"end":191,"id":33},{"text":"it","start":192,"end":194,"id":34},{"text":"could","start":195,"end":200,"id":35},{"text":"represent","start":201,"end":210,"id":36},{"text":"a","start":211,"end":212,"id":37},{"text":"reasonable","start":213,"end":223,"id":38},{"text":"benchmark","start":224,"end":233,"id":39},{"text":"for","start":234,"end":237,"id":40},{"text":"the","start":238,"end":241,"id":41},{"text":"classification","start":242,"end":256,"id":42},{"text":"problem","start":257,"end":264,"id":43},{"text":"with","start":265,"end":269,"id":44},{"text":"functional","start":270,"end":280,"id":45},{"text":"data","start":281,"end":285,"id":46},{"text":".","start":285,"end":286,"id":47}],"spans":[{"token_start":17,"token_end":19,"start":95,"end":99,"text":"k-NN","label":"ALGO","source":"./algo_model5","input_hash":624564835,"answer":"accept"}],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"The first one is to find important variables for interpretation and the second one is more restrictive and try to design a good prediction model.","_input_hash":1826510268,"_task_hash":-748678460,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"first","start":4,"end":9,"id":1},{"text":"one","start":10,"end":13,"id":2},{"text":"is","start":14,"end":16,"id":3},{"text":"to","start":17,"end":19,"id":4},{"text":"find","start":20,"end":24,"id":5},{"text":"important","start":25,"end":34,"id":6},{"text":"variables","start":35,"end":44,"id":7},{"text":"for","start":45,"end":48,"id":8},{"text":"interpretation","start":49,"end":63,"id":9},{"text":"and","start":64,"end":67,"id":10},{"text":"the","start":68,"end":71,"id":11},{"text":"second","start":72,"end":78,"id":12},{"text":"one","start":79,"end":82,"id":13},{"text":"is","start":83,"end":85,"id":14},{"text":"more","start":86,"end":90,"id":15},{"text":"restrictive","start":91,"end":102,"id":16},{"text":"and","start":103,"end":106,"id":17},{"text":"try","start":107,"end":110,"id":18},{"text":"to","start":111,"end":113,"id":19},{"text":"design","start":114,"end":120,"id":20},{"text":"a","start":121,"end":122,"id":21},{"text":"good","start":123,"end":127,"id":22},{"text":"prediction","start":128,"end":138,"id":23},{"text":"model","start":139,"end":144,"id":24},{"text":".","start":144,"end":145,"id":25}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"There has been an explosion of interest in statistical models for analyzing network data, and considerable interest in the class of exponential random graph (ERG) models, especially in connection with difficulties in computing maximum likelihood estimates.","_input_hash":1399533402,"_task_hash":-1380660147,"tokens":[{"text":"There","start":0,"end":5,"id":0},{"text":"has","start":6,"end":9,"id":1},{"text":"been","start":10,"end":14,"id":2},{"text":"an","start":15,"end":17,"id":3},{"text":"explosion","start":18,"end":27,"id":4},{"text":"of","start":28,"end":30,"id":5},{"text":"interest","start":31,"end":39,"id":6},{"text":"in","start":40,"end":42,"id":7},{"text":"statistical","start":43,"end":54,"id":8},{"text":"models","start":55,"end":61,"id":9},{"text":"for","start":62,"end":65,"id":10},{"text":"analyzing","start":66,"end":75,"id":11},{"text":"network","start":76,"end":83,"id":12},{"text":"data","start":84,"end":88,"id":13},{"text":",","start":88,"end":89,"id":14},{"text":"and","start":90,"end":93,"id":15},{"text":"considerable","start":94,"end":106,"id":16},{"text":"interest","start":107,"end":115,"id":17},{"text":"in","start":116,"end":118,"id":18},{"text":"the","start":119,"end":122,"id":19},{"text":"class","start":123,"end":128,"id":20},{"text":"of","start":129,"end":131,"id":21},{"text":"exponential","start":132,"end":143,"id":22},{"text":"random","start":144,"end":150,"id":23},{"text":"graph","start":151,"end":156,"id":24},{"text":"(","start":157,"end":158,"id":25},{"text":"ERG","start":158,"end":161,"id":26},{"text":")","start":161,"end":162,"id":27},{"text":"models","start":163,"end":169,"id":28},{"text":",","start":169,"end":170,"id":29},{"text":"especially","start":171,"end":181,"id":30},{"text":"in","start":182,"end":184,"id":31},{"text":"connection","start":185,"end":195,"id":32},{"text":"with","start":196,"end":200,"id":33},{"text":"difficulties","start":201,"end":213,"id":34},{"text":"in","start":214,"end":216,"id":35},{"text":"computing","start":217,"end":226,"id":36},{"text":"maximum","start":227,"end":234,"id":37},{"text":"likelihood","start":235,"end":245,"id":38},{"text":"estimates","start":246,"end":255,"id":39},{"text":".","start":255,"end":256,"id":40}],"spans":[{"token_start":22,"token_end":24,"start":132,"end":156,"text":"exponential random graph","label":"ALGO","source":"./algo_model5","input_hash":1399533402,"answer":"accept"}],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"We also contribute a new way of deriving the online algorithm that ties together previous online boosting work.","_input_hash":-821263039,"_task_hash":1623279092,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"also","start":3,"end":7,"id":1},{"text":"contribute","start":8,"end":18,"id":2},{"text":"a","start":19,"end":20,"id":3},{"text":"new","start":21,"end":24,"id":4},{"text":"way","start":25,"end":28,"id":5},{"text":"of","start":29,"end":31,"id":6},{"text":"deriving","start":32,"end":40,"id":7},{"text":"the","start":41,"end":44,"id":8},{"text":"online","start":45,"end":51,"id":9},{"text":"algorithm","start":52,"end":61,"id":10},{"text":"that","start":62,"end":66,"id":11},{"text":"ties","start":67,"end":71,"id":12},{"text":"together","start":72,"end":80,"id":13},{"text":"previous","start":81,"end":89,"id":14},{"text":"online","start":90,"end":96,"id":15},{"text":"boosting","start":97,"end":105,"id":16},{"text":"work","start":106,"end":110,"id":17},{"text":".","start":110,"end":111,"id":18}],"spans":[{"token_start":9,"token_end":9,"start":45,"end":51,"text":"online","label":"ALGO","source":"./algo_model5","input_hash":-821263039,"answer":"accept"}],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"The network is further optimized by implementing Decision Forest.","_input_hash":-191073578,"_task_hash":1861232210,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"network","start":4,"end":11,"id":1},{"text":"is","start":12,"end":14,"id":2},{"text":"further","start":15,"end":22,"id":3},{"text":"optimized","start":23,"end":32,"id":4},{"text":"by","start":33,"end":35,"id":5},{"text":"implementing","start":36,"end":48,"id":6},{"text":"Decision","start":49,"end":57,"id":7},{"text":"Forest","start":58,"end":64,"id":8},{"text":".","start":64,"end":65,"id":9}],"spans":[{"start":49,"end":64,"token_start":7,"token_end":8,"label":"ALGO","answer":"accept"}],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"We generate a set of biased sequences by applying a finite state automata with a specified number, $m$, of states to the set of all binary sequences.","_input_hash":1039097680,"_task_hash":1277283597,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"generate","start":3,"end":11,"id":1},{"text":"a","start":12,"end":13,"id":2},{"text":"set","start":14,"end":17,"id":3},{"text":"of","start":18,"end":20,"id":4},{"text":"biased","start":21,"end":27,"id":5},{"text":"sequences","start":28,"end":37,"id":6},{"text":"by","start":38,"end":40,"id":7},{"text":"applying","start":41,"end":49,"id":8},{"text":"a","start":50,"end":51,"id":9},{"text":"finite","start":52,"end":58,"id":10},{"text":"state","start":59,"end":64,"id":11},{"text":"automata","start":65,"end":73,"id":12},{"text":"with","start":74,"end":78,"id":13},{"text":"a","start":79,"end":80,"id":14},{"text":"specified","start":81,"end":90,"id":15},{"text":"number","start":91,"end":97,"id":16},{"text":",","start":97,"end":98,"id":17},{"text":"$","start":99,"end":100,"id":18},{"text":"m$","start":100,"end":102,"id":19},{"text":",","start":102,"end":103,"id":20},{"text":"of","start":104,"end":106,"id":21},{"text":"states","start":107,"end":113,"id":22},{"text":"to","start":114,"end":116,"id":23},{"text":"the","start":117,"end":120,"id":24},{"text":"set","start":121,"end":124,"id":25},{"text":"of","start":125,"end":127,"id":26},{"text":"all","start":128,"end":131,"id":27},{"text":"binary","start":132,"end":138,"id":28},{"text":"sequences","start":139,"end":148,"id":29},{"text":".","start":148,"end":149,"id":30}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"We prove theoretically that this straightforward and computationally simple modification of LLE reduces LLE's sensitivity to noise.","_input_hash":1337426058,"_task_hash":1796659381,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"prove","start":3,"end":8,"id":1},{"text":"theoretically","start":9,"end":22,"id":2},{"text":"that","start":23,"end":27,"id":3},{"text":"this","start":28,"end":32,"id":4},{"text":"straightforward","start":33,"end":48,"id":5},{"text":"and","start":49,"end":52,"id":6},{"text":"computationally","start":53,"end":68,"id":7},{"text":"simple","start":69,"end":75,"id":8},{"text":"modification","start":76,"end":88,"id":9},{"text":"of","start":89,"end":91,"id":10},{"text":"LLE","start":92,"end":95,"id":11},{"text":"reduces","start":96,"end":103,"id":12},{"text":"LLE","start":104,"end":107,"id":13},{"text":"'s","start":107,"end":109,"id":14},{"text":"sensitivity","start":110,"end":121,"id":15},{"text":"to","start":122,"end":124,"id":16},{"text":"noise","start":125,"end":130,"id":17},{"text":".","start":130,"end":131,"id":18}],"spans":[{"start":92,"end":95,"token_start":11,"token_end":11,"label":"ALGO","answer":"accept"},{"start":104,"end":107,"token_start":13,"token_end":13,"label":"ALGO","answer":"accept"}],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"Both the finite sample case and the limit case are analyzed.","_input_hash":-711154741,"_task_hash":-1814561714,"tokens":[{"text":"Both","start":0,"end":4,"id":0},{"text":"the","start":5,"end":8,"id":1},{"text":"finite","start":9,"end":15,"id":2},{"text":"sample","start":16,"end":22,"id":3},{"text":"case","start":23,"end":27,"id":4},{"text":"and","start":28,"end":31,"id":5},{"text":"the","start":32,"end":35,"id":6},{"text":"limit","start":36,"end":41,"id":7},{"text":"case","start":42,"end":46,"id":8},{"text":"are","start":47,"end":50,"id":9},{"text":"analyzed","start":51,"end":59,"id":10},{"text":".","start":59,"end":60,"id":11}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"We consider the problem of binary classification where one can, for a particular cost, choose not to classify an observation.","_input_hash":-1660335345,"_task_hash":-2061475922,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"consider","start":3,"end":11,"id":1},{"text":"the","start":12,"end":15,"id":2},{"text":"problem","start":16,"end":23,"id":3},{"text":"of","start":24,"end":26,"id":4},{"text":"binary","start":27,"end":33,"id":5},{"text":"classification","start":34,"end":48,"id":6},{"text":"where","start":49,"end":54,"id":7},{"text":"one","start":55,"end":58,"id":8},{"text":"can","start":59,"end":62,"id":9},{"text":",","start":62,"end":63,"id":10},{"text":"for","start":64,"end":67,"id":11},{"text":"a","start":68,"end":69,"id":12},{"text":"particular","start":70,"end":80,"id":13},{"text":"cost","start":81,"end":85,"id":14},{"text":",","start":85,"end":86,"id":15},{"text":"choose","start":87,"end":93,"id":16},{"text":"not","start":94,"end":97,"id":17},{"text":"to","start":98,"end":100,"id":18},{"text":"classify","start":101,"end":109,"id":19},{"text":"an","start":110,"end":112,"id":20},{"text":"observation","start":113,"end":124,"id":21},{"text":".","start":124,"end":125,"id":22}],"spans":[{"start":27,"end":48,"token_start":5,"token_end":6,"label":"ALGO","answer":"accept"}],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"Some experiments show advantages and shortcomings of the resulting regression method in comparison to the standard Nadaraya-Watson regression technique, which approximates the optimum by the expectation value.","_input_hash":730716699,"_task_hash":763636217,"tokens":[{"text":"Some","start":0,"end":4,"id":0},{"text":"experiments","start":5,"end":16,"id":1},{"text":"show","start":17,"end":21,"id":2},{"text":"advantages","start":22,"end":32,"id":3},{"text":"and","start":33,"end":36,"id":4},{"text":"shortcomings","start":37,"end":49,"id":5},{"text":"of","start":50,"end":52,"id":6},{"text":"the","start":53,"end":56,"id":7},{"text":"resulting","start":57,"end":66,"id":8},{"text":"regression","start":67,"end":77,"id":9},{"text":"method","start":78,"end":84,"id":10},{"text":"in","start":85,"end":87,"id":11},{"text":"comparison","start":88,"end":98,"id":12},{"text":"to","start":99,"end":101,"id":13},{"text":"the","start":102,"end":105,"id":14},{"text":"standard","start":106,"end":114,"id":15},{"text":"Nadaraya","start":115,"end":123,"id":16},{"text":"-","start":123,"end":124,"id":17},{"text":"Watson","start":124,"end":130,"id":18},{"text":"regression","start":131,"end":141,"id":19},{"text":"technique","start":142,"end":151,"id":20},{"text":",","start":151,"end":152,"id":21},{"text":"which","start":153,"end":158,"id":22},{"text":"approximates","start":159,"end":171,"id":23},{"text":"the","start":172,"end":175,"id":24},{"text":"optimum","start":176,"end":183,"id":25},{"text":"by","start":184,"end":186,"id":26},{"text":"the","start":187,"end":190,"id":27},{"text":"expectation","start":191,"end":202,"id":28},{"text":"value","start":203,"end":208,"id":29},{"text":".","start":208,"end":209,"id":30}],"spans":[{"token_start":16,"token_end":19,"start":115,"end":141,"text":"Nadaraya-Watson regression","label":"ALGO","source":"./algo_model5","input_hash":730716699,"answer":"accept"}],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"Thus we can index the complexity of our random sequence by the number of states of the automata.","_input_hash":-530968138,"_task_hash":-70775467,"tokens":[{"text":"Thus","start":0,"end":4,"id":0},{"text":"we","start":5,"end":7,"id":1},{"text":"can","start":8,"end":11,"id":2},{"text":"index","start":12,"end":17,"id":3},{"text":"the","start":18,"end":21,"id":4},{"text":"complexity","start":22,"end":32,"id":5},{"text":"of","start":33,"end":35,"id":6},{"text":"our","start":36,"end":39,"id":7},{"text":"random","start":40,"end":46,"id":8},{"text":"sequence","start":47,"end":55,"id":9},{"text":"by","start":56,"end":58,"id":10},{"text":"the","start":59,"end":62,"id":11},{"text":"number","start":63,"end":69,"id":12},{"text":"of","start":70,"end":72,"id":13},{"text":"states","start":73,"end":79,"id":14},{"text":"of","start":80,"end":82,"id":15},{"text":"the","start":83,"end":86,"id":16},{"text":"automata","start":87,"end":95,"id":17},{"text":".","start":95,"end":96,"id":18}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"We also present two novel dimension-reduction techniques that attempt to minimize the suggested measure, and compare the results of these techniques to the results of existing algorithms.","_input_hash":-589856197,"_task_hash":789082106,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"also","start":3,"end":7,"id":1},{"text":"present","start":8,"end":15,"id":2},{"text":"two","start":16,"end":19,"id":3},{"text":"novel","start":20,"end":25,"id":4},{"text":"dimension","start":26,"end":35,"id":5},{"text":"-","start":35,"end":36,"id":6},{"text":"reduction","start":36,"end":45,"id":7},{"text":"techniques","start":46,"end":56,"id":8},{"text":"that","start":57,"end":61,"id":9},{"text":"attempt","start":62,"end":69,"id":10},{"text":"to","start":70,"end":72,"id":11},{"text":"minimize","start":73,"end":81,"id":12},{"text":"the","start":82,"end":85,"id":13},{"text":"suggested","start":86,"end":95,"id":14},{"text":"measure","start":96,"end":103,"id":15},{"text":",","start":103,"end":104,"id":16},{"text":"and","start":105,"end":108,"id":17},{"text":"compare","start":109,"end":116,"id":18},{"text":"the","start":117,"end":120,"id":19},{"text":"results","start":121,"end":128,"id":20},{"text":"of","start":129,"end":131,"id":21},{"text":"these","start":132,"end":137,"id":22},{"text":"techniques","start":138,"end":148,"id":23},{"text":"to","start":149,"end":151,"id":24},{"text":"the","start":152,"end":155,"id":25},{"text":"results","start":156,"end":163,"id":26},{"text":"of","start":164,"end":166,"id":27},{"text":"existing","start":167,"end":175,"id":28},{"text":"algorithms","start":176,"end":186,"id":29},{"text":".","start":186,"end":187,"id":30}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"We present a connection between intrinsic dimension estimation and the optimal embedding dimension obtained using the CCDR algorithm.","_input_hash":-812107236,"_task_hash":1990113264,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"present","start":3,"end":10,"id":1},{"text":"a","start":11,"end":12,"id":2},{"text":"connection","start":13,"end":23,"id":3},{"text":"between","start":24,"end":31,"id":4},{"text":"intrinsic","start":32,"end":41,"id":5},{"text":"dimension","start":42,"end":51,"id":6},{"text":"estimation","start":52,"end":62,"id":7},{"text":"and","start":63,"end":66,"id":8},{"text":"the","start":67,"end":70,"id":9},{"text":"optimal","start":71,"end":78,"id":10},{"text":"embedding","start":79,"end":88,"id":11},{"text":"dimension","start":89,"end":98,"id":12},{"text":"obtained","start":99,"end":107,"id":13},{"text":"using","start":108,"end":113,"id":14},{"text":"the","start":114,"end":117,"id":15},{"text":"CCDR","start":118,"end":122,"id":16},{"text":"algorithm","start":123,"end":132,"id":17},{"text":".","start":132,"end":133,"id":18}],"spans":[{"start":118,"end":122,"token_start":16,"token_end":16,"label":"ALGO","answer":"accept"}],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"A conceptual framework for cluster analysis from the viewpoint of p-adic geometry is introduced by describing the space of all dendrograms for n datapoints and relating it to the moduli space of p-adic Riemannian spheres with punctures using a method recently applied by Murtagh (2004b).","_input_hash":-1260630185,"_task_hash":4913020,"tokens":[{"text":"A","start":0,"end":1,"id":0},{"text":"conceptual","start":2,"end":12,"id":1},{"text":"framework","start":13,"end":22,"id":2},{"text":"for","start":23,"end":26,"id":3},{"text":"cluster","start":27,"end":34,"id":4},{"text":"analysis","start":35,"end":43,"id":5},{"text":"from","start":44,"end":48,"id":6},{"text":"the","start":49,"end":52,"id":7},{"text":"viewpoint","start":53,"end":62,"id":8},{"text":"of","start":63,"end":65,"id":9},{"text":"p","start":66,"end":67,"id":10},{"text":"-","start":67,"end":68,"id":11},{"text":"adic","start":68,"end":72,"id":12},{"text":"geometry","start":73,"end":81,"id":13},{"text":"is","start":82,"end":84,"id":14},{"text":"introduced","start":85,"end":95,"id":15},{"text":"by","start":96,"end":98,"id":16},{"text":"describing","start":99,"end":109,"id":17},{"text":"the","start":110,"end":113,"id":18},{"text":"space","start":114,"end":119,"id":19},{"text":"of","start":120,"end":122,"id":20},{"text":"all","start":123,"end":126,"id":21},{"text":"dendrograms","start":127,"end":138,"id":22},{"text":"for","start":139,"end":142,"id":23},{"text":"n","start":143,"end":144,"id":24},{"text":"datapoints","start":145,"end":155,"id":25},{"text":"and","start":156,"end":159,"id":26},{"text":"relating","start":160,"end":168,"id":27},{"text":"it","start":169,"end":171,"id":28},{"text":"to","start":172,"end":174,"id":29},{"text":"the","start":175,"end":178,"id":30},{"text":"moduli","start":179,"end":185,"id":31},{"text":"space","start":186,"end":191,"id":32},{"text":"of","start":192,"end":194,"id":33},{"text":"p","start":195,"end":196,"id":34},{"text":"-","start":196,"end":197,"id":35},{"text":"adic","start":197,"end":201,"id":36},{"text":"Riemannian","start":202,"end":212,"id":37},{"text":"spheres","start":213,"end":220,"id":38},{"text":"with","start":221,"end":225,"id":39},{"text":"punctures","start":226,"end":235,"id":40},{"text":"using","start":236,"end":241,"id":41},{"text":"a","start":242,"end":243,"id":42},{"text":"method","start":244,"end":250,"id":43},{"text":"recently","start":251,"end":259,"id":44},{"text":"applied","start":260,"end":267,"id":45},{"text":"by","start":268,"end":270,"id":46},{"text":"Murtagh","start":271,"end":278,"id":47},{"text":"(","start":279,"end":280,"id":48},{"text":"2004b","start":280,"end":285,"id":49},{"text":")","start":285,"end":286,"id":50},{"text":".","start":286,"end":287,"id":51}],"spans":[{"token_start":0,"token_end":1,"start":0,"end":12,"text":"A conceptual","label":"ALGO","source":"./algo_model5","input_hash":-1260630185,"answer":"reject"},{"token_start":2,"token_end":3,"start":13,"end":26,"text":"framework for","label":"ALGO","source":"./algo_model5","input_hash":-1260630185,"answer":"reject"},{"token_start":4,"token_end":5,"start":27,"end":43,"text":"cluster analysis","label":"ALGO","source":"./algo_model5","input_hash":-1260630185,"answer":"reject"},{"token_start":6,"token_end":7,"start":44,"end":52,"text":"from the","label":"ALGO","source":"./algo_model5","input_hash":-1260630185,"answer":"reject"},{"token_start":8,"token_end":9,"start":53,"end":65,"text":"viewpoint of","label":"ALGO","source":"./algo_model5","input_hash":-1260630185,"answer":"reject"},{"token_start":10,"token_end":11,"start":66,"end":68,"text":"p-","label":"ALGO","source":"./algo_model5","input_hash":-1260630185,"answer":"reject"},{"token_start":12,"token_end":13,"start":68,"end":81,"text":"adic geometry","label":"ALGO","source":"./algo_model5","input_hash":-1260630185,"answer":"reject"},{"token_start":14,"token_end":15,"start":82,"end":95,"text":"is introduced","label":"ALGO","source":"./algo_model5","input_hash":-1260630185,"answer":"reject"},{"token_start":16,"token_end":17,"start":96,"end":109,"text":"by describing","label":"ALGO","source":"./algo_model5","input_hash":-1260630185,"answer":"reject"},{"token_start":18,"token_end":19,"start":110,"end":119,"text":"the space","label":"ALGO","source":"./algo_model5","input_hash":-1260630185,"answer":"reject"},{"token_start":20,"token_end":21,"start":120,"end":126,"text":"of all","label":"ALGO","source":"./algo_model5","input_hash":-1260630185,"answer":"reject"},{"token_start":22,"token_end":23,"start":127,"end":142,"text":"dendrograms for","label":"ALGO","source":"./algo_model5","input_hash":-1260630185,"answer":"reject"},{"token_start":24,"token_end":25,"start":143,"end":155,"text":"n datapoints","label":"ALGO","source":"./algo_model5","input_hash":-1260630185,"answer":"reject"},{"token_start":26,"token_end":27,"start":156,"end":168,"text":"and relating","label":"ALGO","source":"./algo_model5","input_hash":-1260630185,"answer":"reject"},{"token_start":28,"token_end":29,"start":169,"end":174,"text":"it to","label":"ALGO","source":"./algo_model5","input_hash":-1260630185,"answer":"reject"},{"token_start":30,"token_end":31,"start":175,"end":185,"text":"the moduli","label":"ALGO","source":"./algo_model5","input_hash":-1260630185,"answer":"reject"},{"token_start":32,"token_end":33,"start":186,"end":194,"text":"space of","label":"ALGO","source":"./algo_model5","input_hash":-1260630185,"answer":"reject"},{"token_start":34,"token_end":35,"start":195,"end":197,"text":"p-","label":"ALGO","source":"./algo_model5","input_hash":-1260630185,"answer":"reject"},{"token_start":36,"token_end":37,"start":197,"end":212,"text":"adic Riemannian","label":"ALGO","source":"./algo_model5","input_hash":-1260630185,"answer":"reject"},{"token_start":38,"token_end":39,"start":213,"end":225,"text":"spheres with","label":"ALGO","source":"./algo_model5","input_hash":-1260630185,"answer":"reject"},{"token_start":40,"token_end":41,"start":226,"end":241,"text":"punctures using","label":"ALGO","source":"./algo_model5","input_hash":-1260630185,"answer":"reject"},{"token_start":42,"token_end":43,"start":242,"end":250,"text":"a method","label":"ALGO","source":"./algo_model5","input_hash":-1260630185,"answer":"reject"},{"token_start":44,"token_end":45,"start":251,"end":267,"text":"recently applied","label":"ALGO","source":"./algo_model5","input_hash":-1260630185,"answer":"reject"},{"token_start":46,"token_end":47,"start":268,"end":278,"text":"by Murtagh","label":"ALGO","source":"./algo_model5","input_hash":-1260630185,"answer":"reject"},{"token_start":48,"token_end":48,"start":279,"end":280,"text":"(","label":"ALGO","source":"./algo_model5","input_hash":-1260630185,"answer":"reject"},{"token_start":49,"token_end":49,"start":280,"end":285,"text":"2004b","label":"ALGO","source":"./algo_model5","input_hash":-1260630185,"answer":"reject"},{"token_start":50,"token_end":51,"start":285,"end":287,"text":").","label":"ALGO","source":"./algo_model5","input_hash":-1260630185,"answer":"reject"}],"_session_id":null,"_view_id":"ner_manual","answer":"reject"}
{"text":"It is known that there exist $p$-adic representation of dendrograms.","_input_hash":-2145759776,"_task_hash":-591247322,"tokens":[{"text":"It","start":0,"end":2,"id":0},{"text":"is","start":3,"end":5,"id":1},{"text":"known","start":6,"end":11,"id":2},{"text":"that","start":12,"end":16,"id":3},{"text":"there","start":17,"end":22,"id":4},{"text":"exist","start":23,"end":28,"id":5},{"text":"$","start":29,"end":30,"id":6},{"text":"p$-adic","start":30,"end":37,"id":7},{"text":"representation","start":38,"end":52,"id":8},{"text":"of","start":53,"end":55,"id":9},{"text":"dendrograms","start":56,"end":67,"id":10},{"text":".","start":67,"end":68,"id":11}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"Typically, this generating mechanism will be a finite automata.","_input_hash":-1179024221,"_task_hash":-1078314631,"tokens":[{"text":"Typically","start":0,"end":9,"id":0},{"text":",","start":9,"end":10,"id":1},{"text":"this","start":11,"end":15,"id":2},{"text":"generating","start":16,"end":26,"id":3},{"text":"mechanism","start":27,"end":36,"id":4},{"text":"will","start":37,"end":41,"id":5},{"text":"be","start":42,"end":44,"id":6},{"text":"a","start":45,"end":46,"id":7},{"text":"finite","start":47,"end":53,"id":8},{"text":"automata","start":54,"end":62,"id":9},{"text":".","start":62,"end":63,"id":10}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"In this article, it is shown that with the proposed kernel function it is feasible to make the pointwise MSE of the density estimator converge at O(n^-2/3) regardless of the dimension of the vector space, provided that the probability density function at the point of interest meets certain conditions.","_input_hash":-521693369,"_task_hash":-1250994352,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"this","start":3,"end":7,"id":1},{"text":"article","start":8,"end":15,"id":2},{"text":",","start":15,"end":16,"id":3},{"text":"it","start":17,"end":19,"id":4},{"text":"is","start":20,"end":22,"id":5},{"text":"shown","start":23,"end":28,"id":6},{"text":"that","start":29,"end":33,"id":7},{"text":"with","start":34,"end":38,"id":8},{"text":"the","start":39,"end":42,"id":9},{"text":"proposed","start":43,"end":51,"id":10},{"text":"kernel","start":52,"end":58,"id":11},{"text":"function","start":59,"end":67,"id":12},{"text":"it","start":68,"end":70,"id":13},{"text":"is","start":71,"end":73,"id":14},{"text":"feasible","start":74,"end":82,"id":15},{"text":"to","start":83,"end":85,"id":16},{"text":"make","start":86,"end":90,"id":17},{"text":"the","start":91,"end":94,"id":18},{"text":"pointwise","start":95,"end":104,"id":19},{"text":"MSE","start":105,"end":108,"id":20},{"text":"of","start":109,"end":111,"id":21},{"text":"the","start":112,"end":115,"id":22},{"text":"density","start":116,"end":123,"id":23},{"text":"estimator","start":124,"end":133,"id":24},{"text":"converge","start":134,"end":142,"id":25},{"text":"at","start":143,"end":145,"id":26},{"text":"O(n^-2/3","start":146,"end":154,"id":27},{"text":")","start":154,"end":155,"id":28},{"text":"regardless","start":156,"end":166,"id":29},{"text":"of","start":167,"end":169,"id":30},{"text":"the","start":170,"end":173,"id":31},{"text":"dimension","start":174,"end":183,"id":32},{"text":"of","start":184,"end":186,"id":33},{"text":"the","start":187,"end":190,"id":34},{"text":"vector","start":191,"end":197,"id":35},{"text":"space","start":198,"end":203,"id":36},{"text":",","start":203,"end":204,"id":37},{"text":"provided","start":205,"end":213,"id":38},{"text":"that","start":214,"end":218,"id":39},{"text":"the","start":219,"end":222,"id":40},{"text":"probability","start":223,"end":234,"id":41},{"text":"density","start":235,"end":242,"id":42},{"text":"function","start":243,"end":251,"id":43},{"text":"at","start":252,"end":254,"id":44},{"text":"the","start":255,"end":258,"id":45},{"text":"point","start":259,"end":264,"id":46},{"text":"of","start":265,"end":267,"id":47},{"text":"interest","start":268,"end":276,"id":48},{"text":"meets","start":277,"end":282,"id":49},{"text":"certain","start":283,"end":290,"id":50},{"text":"conditions","start":291,"end":301,"id":51},{"text":".","start":301,"end":302,"id":52}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"Holmes and Adams (2002) focused on the performance of BKNN in terms of misclassification error but did not assess its ability to quantify uncertainty.","_input_hash":819004482,"_task_hash":1213736097,"tokens":[{"text":"Holmes","start":0,"end":6,"id":0},{"text":"and","start":7,"end":10,"id":1},{"text":"Adams","start":11,"end":16,"id":2},{"text":"(","start":17,"end":18,"id":3},{"text":"2002","start":18,"end":22,"id":4},{"text":")","start":22,"end":23,"id":5},{"text":"focused","start":24,"end":31,"id":6},{"text":"on","start":32,"end":34,"id":7},{"text":"the","start":35,"end":38,"id":8},{"text":"performance","start":39,"end":50,"id":9},{"text":"of","start":51,"end":53,"id":10},{"text":"BKNN","start":54,"end":58,"id":11},{"text":"in","start":59,"end":61,"id":12},{"text":"terms","start":62,"end":67,"id":13},{"text":"of","start":68,"end":70,"id":14},{"text":"misclassification","start":71,"end":88,"id":15},{"text":"error","start":89,"end":94,"id":16},{"text":"but","start":95,"end":98,"id":17},{"text":"did","start":99,"end":102,"id":18},{"text":"not","start":103,"end":106,"id":19},{"text":"assess","start":107,"end":113,"id":20},{"text":"its","start":114,"end":117,"id":21},{"text":"ability","start":118,"end":125,"id":22},{"text":"to","start":126,"end":128,"id":23},{"text":"quantify","start":129,"end":137,"id":24},{"text":"uncertainty","start":138,"end":149,"id":25},{"text":".","start":149,"end":150,"id":26}],"spans":[{"token_start":11,"token_end":11,"start":54,"end":58,"text":"BKNN","label":"ALGO","source":"./algo_model5","input_hash":819004482,"answer":"accept"}],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"This modification also removes the need for regularization when the number of neighbors is larger than the dimension of the input.","_input_hash":-46676230,"_task_hash":-509110954,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"modification","start":5,"end":17,"id":1},{"text":"also","start":18,"end":22,"id":2},{"text":"removes","start":23,"end":30,"id":3},{"text":"the","start":31,"end":34,"id":4},{"text":"need","start":35,"end":39,"id":5},{"text":"for","start":40,"end":43,"id":6},{"text":"regularization","start":44,"end":58,"id":7},{"text":"when","start":59,"end":63,"id":8},{"text":"the","start":64,"end":67,"id":9},{"text":"number","start":68,"end":74,"id":10},{"text":"of","start":75,"end":77,"id":11},{"text":"neighbors","start":78,"end":87,"id":12},{"text":"is","start":88,"end":90,"id":13},{"text":"larger","start":91,"end":97,"id":14},{"text":"than","start":98,"end":102,"id":15},{"text":"the","start":103,"end":106,"id":16},{"text":"dimension","start":107,"end":116,"id":17},{"text":"of","start":117,"end":119,"id":18},{"text":"the","start":120,"end":123,"id":19},{"text":"input","start":124,"end":129,"id":20},{"text":".","start":129,"end":130,"id":21}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"We present the nested Chinese restaurant process (nCRP), a stochastic process which assigns probability distributions to infinitely-deep, infinitely-branching trees.","_input_hash":1329355179,"_task_hash":-1452878183,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"present","start":3,"end":10,"id":1},{"text":"the","start":11,"end":14,"id":2},{"text":"nested","start":15,"end":21,"id":3},{"text":"Chinese","start":22,"end":29,"id":4},{"text":"restaurant","start":30,"end":40,"id":5},{"text":"process","start":41,"end":48,"id":6},{"text":"(","start":49,"end":50,"id":7},{"text":"nCRP","start":50,"end":54,"id":8},{"text":")","start":54,"end":55,"id":9},{"text":",","start":55,"end":56,"id":10},{"text":"a","start":57,"end":58,"id":11},{"text":"stochastic","start":59,"end":69,"id":12},{"text":"process","start":70,"end":77,"id":13},{"text":"which","start":78,"end":83,"id":14},{"text":"assigns","start":84,"end":91,"id":15},{"text":"probability","start":92,"end":103,"id":16},{"text":"distributions","start":104,"end":117,"id":17},{"text":"to","start":118,"end":120,"id":18},{"text":"infinitely","start":121,"end":131,"id":19},{"text":"-","start":131,"end":132,"id":20},{"text":"deep","start":132,"end":136,"id":21},{"text":",","start":136,"end":137,"id":22},{"text":"infinitely","start":138,"end":148,"id":23},{"text":"-","start":148,"end":149,"id":24},{"text":"branching","start":149,"end":158,"id":25},{"text":"trees","start":159,"end":164,"id":26},{"text":".","start":164,"end":165,"id":27}],"spans":[{"token_start":3,"token_end":5,"start":15,"end":40,"text":"nested Chinese restaurant","label":"ALGO","source":"./algo_model5","input_hash":1329355179,"answer":"accept"}],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"After explaining the definitions, the concept of classifiers is discussed in the context of moduli spaces, and upper bounds for the number of hidden vertices in dendrograms are given.","_input_hash":1788297260,"_task_hash":1090886645,"tokens":[{"text":"After","start":0,"end":5,"id":0},{"text":"explaining","start":6,"end":16,"id":1},{"text":"the","start":17,"end":20,"id":2},{"text":"definitions","start":21,"end":32,"id":3},{"text":",","start":32,"end":33,"id":4},{"text":"the","start":34,"end":37,"id":5},{"text":"concept","start":38,"end":45,"id":6},{"text":"of","start":46,"end":48,"id":7},{"text":"classifiers","start":49,"end":60,"id":8},{"text":"is","start":61,"end":63,"id":9},{"text":"discussed","start":64,"end":73,"id":10},{"text":"in","start":74,"end":76,"id":11},{"text":"the","start":77,"end":80,"id":12},{"text":"context","start":81,"end":88,"id":13},{"text":"of","start":89,"end":91,"id":14},{"text":"moduli","start":92,"end":98,"id":15},{"text":"spaces","start":99,"end":105,"id":16},{"text":",","start":105,"end":106,"id":17},{"text":"and","start":107,"end":110,"id":18},{"text":"upper","start":111,"end":116,"id":19},{"text":"bounds","start":117,"end":123,"id":20},{"text":"for","start":124,"end":127,"id":21},{"text":"the","start":128,"end":131,"id":22},{"text":"number","start":132,"end":138,"id":23},{"text":"of","start":139,"end":141,"id":24},{"text":"hidden","start":142,"end":148,"id":25},{"text":"vertices","start":149,"end":157,"id":26},{"text":"in","start":158,"end":160,"id":27},{"text":"dendrograms","start":161,"end":172,"id":28},{"text":"are","start":173,"end":176,"id":29},{"text":"given","start":177,"end":182,"id":30},{"text":".","start":182,"end":183,"id":31}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"When using the K-nearest neighbors method, one often ignores uncertainty in the choice of K. To account for such uncertainty, Holmes and Adams (2002) proposed a Bayesian framework for K-nearest neighbors (KNN).","_input_hash":434432237,"_task_hash":712121059,"tokens":[{"text":"When","start":0,"end":4,"id":0},{"text":"using","start":5,"end":10,"id":1},{"text":"the","start":11,"end":14,"id":2},{"text":"K","start":15,"end":16,"id":3},{"text":"-","start":16,"end":17,"id":4},{"text":"nearest","start":17,"end":24,"id":5},{"text":"neighbors","start":25,"end":34,"id":6},{"text":"method","start":35,"end":41,"id":7},{"text":",","start":41,"end":42,"id":8},{"text":"one","start":43,"end":46,"id":9},{"text":"often","start":47,"end":52,"id":10},{"text":"ignores","start":53,"end":60,"id":11},{"text":"uncertainty","start":61,"end":72,"id":12},{"text":"in","start":73,"end":75,"id":13},{"text":"the","start":76,"end":79,"id":14},{"text":"choice","start":80,"end":86,"id":15},{"text":"of","start":87,"end":89,"id":16},{"text":"K.","start":90,"end":92,"id":17},{"text":"To","start":93,"end":95,"id":18},{"text":"account","start":96,"end":103,"id":19},{"text":"for","start":104,"end":107,"id":20},{"text":"such","start":108,"end":112,"id":21},{"text":"uncertainty","start":113,"end":124,"id":22},{"text":",","start":124,"end":125,"id":23},{"text":"Holmes","start":126,"end":132,"id":24},{"text":"and","start":133,"end":136,"id":25},{"text":"Adams","start":137,"end":142,"id":26},{"text":"(","start":143,"end":144,"id":27},{"text":"2002","start":144,"end":148,"id":28},{"text":")","start":148,"end":149,"id":29},{"text":"proposed","start":150,"end":158,"id":30},{"text":"a","start":159,"end":160,"id":31},{"text":"Bayesian","start":161,"end":169,"id":32},{"text":"framework","start":170,"end":179,"id":33},{"text":"for","start":180,"end":183,"id":34},{"text":"K","start":184,"end":185,"id":35},{"text":"-","start":185,"end":186,"id":36},{"text":"nearest","start":186,"end":193,"id":37},{"text":"neighbors","start":194,"end":203,"id":38},{"text":"(","start":204,"end":205,"id":39},{"text":"KNN","start":205,"end":208,"id":40},{"text":")","start":208,"end":209,"id":41},{"text":".","start":209,"end":210,"id":42}],"spans":[{"token_start":3,"token_end":6,"start":15,"end":34,"text":"K-nearest neighbors","label":"ALGO","source":"./algo_model5","input_hash":434432237,"answer":"accept"},{"token_start":35,"token_end":38,"start":184,"end":203,"text":"K-nearest neighbors","label":"ALGO","source":"./algo_model5","input_hash":434432237,"answer":"accept"}],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"This is useful because while importance values from random forests are used to screen variables, for example they are used to filter high throughput genomic data in Bioinformatics, very little theory exists about their properties.","_input_hash":1109894544,"_task_hash":-1965616499,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"is","start":5,"end":7,"id":1},{"text":"useful","start":8,"end":14,"id":2},{"text":"because","start":15,"end":22,"id":3},{"text":"while","start":23,"end":28,"id":4},{"text":"importance","start":29,"end":39,"id":5},{"text":"values","start":40,"end":46,"id":6},{"text":"from","start":47,"end":51,"id":7},{"text":"random","start":52,"end":58,"id":8},{"text":"forests","start":59,"end":66,"id":9},{"text":"are","start":67,"end":70,"id":10},{"text":"used","start":71,"end":75,"id":11},{"text":"to","start":76,"end":78,"id":12},{"text":"screen","start":79,"end":85,"id":13},{"text":"variables","start":86,"end":95,"id":14},{"text":",","start":95,"end":96,"id":15},{"text":"for","start":97,"end":100,"id":16},{"text":"example","start":101,"end":108,"id":17},{"text":"they","start":109,"end":113,"id":18},{"text":"are","start":114,"end":117,"id":19},{"text":"used","start":118,"end":122,"id":20},{"text":"to","start":123,"end":125,"id":21},{"text":"filter","start":126,"end":132,"id":22},{"text":"high","start":133,"end":137,"id":23},{"text":"throughput","start":138,"end":148,"id":24},{"text":"genomic","start":149,"end":156,"id":25},{"text":"data","start":157,"end":161,"id":26},{"text":"in","start":162,"end":164,"id":27},{"text":"Bioinformatics","start":165,"end":179,"id":28},{"text":",","start":179,"end":180,"id":29},{"text":"very","start":181,"end":185,"id":30},{"text":"little","start":186,"end":192,"id":31},{"text":"theory","start":193,"end":199,"id":32},{"text":"exists","start":200,"end":206,"id":33},{"text":"about","start":207,"end":212,"id":34},{"text":"their","start":213,"end":218,"id":35},{"text":"properties","start":219,"end":229,"id":36},{"text":".","start":229,"end":230,"id":37}],"spans":[{"start":52,"end":66,"token_start":8,"token_end":9,"label":"ALGO","answer":"accept"}],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"The update rule is derived by minimizing AdaBoost's loss when viewed in an incremental form.","_input_hash":1538992120,"_task_hash":-1688558635,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"update","start":4,"end":10,"id":1},{"text":"rule","start":11,"end":15,"id":2},{"text":"is","start":16,"end":18,"id":3},{"text":"derived","start":19,"end":26,"id":4},{"text":"by","start":27,"end":29,"id":5},{"text":"minimizing","start":30,"end":40,"id":6},{"text":"AdaBoost","start":41,"end":49,"id":7},{"text":"'s","start":49,"end":51,"id":8},{"text":"loss","start":52,"end":56,"id":9},{"text":"when","start":57,"end":61,"id":10},{"text":"viewed","start":62,"end":68,"id":11},{"text":"in","start":69,"end":71,"id":12},{"text":"an","start":72,"end":74,"id":13},{"text":"incremental","start":75,"end":86,"id":14},{"text":"form","start":87,"end":91,"id":15},{"text":".","start":91,"end":92,"id":16}],"spans":[{"token_start":7,"token_end":7,"start":41,"end":49,"text":"AdaBoost","label":"ALGO","source":"./algo_model5","input_hash":1538992120,"answer":"accept"}],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"First we consider the closure of $k$-dimensional exponential families of distribution with discrete base measure and polyhedral convex support $\\mathrm{P}$. We show that the normal fan of $\\mathrm{P}$ is a geometric object that plays a fundamental role in deriving the statistical and geometric properties of the corresponding extended exponential families.","_input_hash":1504062085,"_task_hash":1397639665,"tokens":[{"text":"First","start":0,"end":5,"id":0},{"text":"we","start":6,"end":8,"id":1},{"text":"consider","start":9,"end":17,"id":2},{"text":"the","start":18,"end":21,"id":3},{"text":"closure","start":22,"end":29,"id":4},{"text":"of","start":30,"end":32,"id":5},{"text":"$","start":33,"end":34,"id":6},{"text":"k$-dimensional","start":34,"end":48,"id":7},{"text":"exponential","start":49,"end":60,"id":8},{"text":"families","start":61,"end":69,"id":9},{"text":"of","start":70,"end":72,"id":10},{"text":"distribution","start":73,"end":85,"id":11},{"text":"with","start":86,"end":90,"id":12},{"text":"discrete","start":91,"end":99,"id":13},{"text":"base","start":100,"end":104,"id":14},{"text":"measure","start":105,"end":112,"id":15},{"text":"and","start":113,"end":116,"id":16},{"text":"polyhedral","start":117,"end":127,"id":17},{"text":"convex","start":128,"end":134,"id":18},{"text":"support","start":135,"end":142,"id":19},{"text":"$","start":143,"end":144,"id":20},{"text":"\\mathrm{P}$.","start":144,"end":156,"id":21},{"text":"We","start":157,"end":159,"id":22},{"text":"show","start":160,"end":164,"id":23},{"text":"that","start":165,"end":169,"id":24},{"text":"the","start":170,"end":173,"id":25},{"text":"normal","start":174,"end":180,"id":26},{"text":"fan","start":181,"end":184,"id":27},{"text":"of","start":185,"end":187,"id":28},{"text":"$","start":188,"end":189,"id":29},{"text":"\\mathrm{P}$","start":189,"end":200,"id":30},{"text":"is","start":201,"end":203,"id":31},{"text":"a","start":204,"end":205,"id":32},{"text":"geometric","start":206,"end":215,"id":33},{"text":"object","start":216,"end":222,"id":34},{"text":"that","start":223,"end":227,"id":35},{"text":"plays","start":228,"end":233,"id":36},{"text":"a","start":234,"end":235,"id":37},{"text":"fundamental","start":236,"end":247,"id":38},{"text":"role","start":248,"end":252,"id":39},{"text":"in","start":253,"end":255,"id":40},{"text":"deriving","start":256,"end":264,"id":41},{"text":"the","start":265,"end":268,"id":42},{"text":"statistical","start":269,"end":280,"id":43},{"text":"and","start":281,"end":284,"id":44},{"text":"geometric","start":285,"end":294,"id":45},{"text":"properties","start":295,"end":305,"id":46},{"text":"of","start":306,"end":308,"id":47},{"text":"the","start":309,"end":312,"id":48},{"text":"corresponding","start":313,"end":326,"id":49},{"text":"extended","start":327,"end":335,"id":50},{"text":"exponential","start":336,"end":347,"id":51},{"text":"families","start":348,"end":356,"id":52},{"text":".","start":356,"end":357,"id":53}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"The implications are that certain moduli spaces known in algebraic geometry are $p$-adic parameter spaces of (families of) dendrograms, and stochastic classification can also be handled within this framework.","_input_hash":-1428625400,"_task_hash":2022874463,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"implications","start":4,"end":16,"id":1},{"text":"are","start":17,"end":20,"id":2},{"text":"that","start":21,"end":25,"id":3},{"text":"certain","start":26,"end":33,"id":4},{"text":"moduli","start":34,"end":40,"id":5},{"text":"spaces","start":41,"end":47,"id":6},{"text":"known","start":48,"end":53,"id":7},{"text":"in","start":54,"end":56,"id":8},{"text":"algebraic","start":57,"end":66,"id":9},{"text":"geometry","start":67,"end":75,"id":10},{"text":"are","start":76,"end":79,"id":11},{"text":"$","start":80,"end":81,"id":12},{"text":"p$-adic","start":81,"end":88,"id":13},{"text":"parameter","start":89,"end":98,"id":14},{"text":"spaces","start":99,"end":105,"id":15},{"text":"of","start":106,"end":108,"id":16},{"text":"(","start":109,"end":110,"id":17},{"text":"families","start":110,"end":118,"id":18},{"text":"of","start":119,"end":121,"id":19},{"text":")","start":121,"end":122,"id":20},{"text":"dendrograms","start":123,"end":134,"id":21},{"text":",","start":134,"end":135,"id":22},{"text":"and","start":136,"end":139,"id":23},{"text":"stochastic","start":140,"end":150,"id":24},{"text":"classification","start":151,"end":165,"id":25},{"text":"can","start":166,"end":169,"id":26},{"text":"also","start":170,"end":174,"id":27},{"text":"be","start":175,"end":177,"id":28},{"text":"handled","start":178,"end":185,"id":29},{"text":"within","start":186,"end":192,"id":30},{"text":"this","start":193,"end":197,"id":31},{"text":"framework","start":198,"end":207,"id":32},{"text":".","start":207,"end":208,"id":33}],"spans":[{"start":140,"end":165,"token_start":24,"token_end":25,"label":"ALGO","answer":"accept"}],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"Second, on a more practical side, we check the behavior of the k-NN method when compared with a few other functional classifiers.","_input_hash":1170064707,"_task_hash":1414753003,"tokens":[{"text":"Second","start":0,"end":6,"id":0},{"text":",","start":6,"end":7,"id":1},{"text":"on","start":8,"end":10,"id":2},{"text":"a","start":11,"end":12,"id":3},{"text":"more","start":13,"end":17,"id":4},{"text":"practical","start":18,"end":27,"id":5},{"text":"side","start":28,"end":32,"id":6},{"text":",","start":32,"end":33,"id":7},{"text":"we","start":34,"end":36,"id":8},{"text":"check","start":37,"end":42,"id":9},{"text":"the","start":43,"end":46,"id":10},{"text":"behavior","start":47,"end":55,"id":11},{"text":"of","start":56,"end":58,"id":12},{"text":"the","start":59,"end":62,"id":13},{"text":"k","start":63,"end":64,"id":14},{"text":"-","start":64,"end":65,"id":15},{"text":"NN","start":65,"end":67,"id":16},{"text":"method","start":68,"end":74,"id":17},{"text":"when","start":75,"end":79,"id":18},{"text":"compared","start":80,"end":88,"id":19},{"text":"with","start":89,"end":93,"id":20},{"text":"a","start":94,"end":95,"id":21},{"text":"few","start":96,"end":99,"id":22},{"text":"other","start":100,"end":105,"id":23},{"text":"functional","start":106,"end":116,"id":24},{"text":"classifiers","start":117,"end":128,"id":25},{"text":".","start":128,"end":129,"id":26}],"spans":[{"token_start":14,"token_end":16,"start":63,"end":67,"text":"k-NN","label":"ALGO","source":"./algo_model5","input_hash":1170064707,"answer":"accept"}],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"We assume that the weak hypotheses were selected beforehand, and only their weights are updated during online boosting.","_input_hash":366728403,"_task_hash":1360844684,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"assume","start":3,"end":9,"id":1},{"text":"that","start":10,"end":14,"id":2},{"text":"the","start":15,"end":18,"id":3},{"text":"weak","start":19,"end":23,"id":4},{"text":"hypotheses","start":24,"end":34,"id":5},{"text":"were","start":35,"end":39,"id":6},{"text":"selected","start":40,"end":48,"id":7},{"text":"beforehand","start":49,"end":59,"id":8},{"text":",","start":59,"end":60,"id":9},{"text":"and","start":61,"end":64,"id":10},{"text":"only","start":65,"end":69,"id":11},{"text":"their","start":70,"end":75,"id":12},{"text":"weights","start":76,"end":83,"id":13},{"text":"are","start":84,"end":87,"id":14},{"text":"updated","start":88,"end":95,"id":15},{"text":"during","start":96,"end":102,"id":16},{"text":"online","start":103,"end":109,"id":17},{"text":"boosting","start":110,"end":118,"id":18},{"text":".","start":118,"end":119,"id":19}],"spans":[{"start":103,"end":118,"token_start":17,"token_end":18,"label":"ALGO","answer":"accept"}],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"First, relying on a recent result by Cerou and Guyader (2006), we prove the consistency of the k-NN classifier for functional data whose distribution belongs to a broad family of Gaussian processes with triangular covariance functions.","_input_hash":1795532205,"_task_hash":-1819038566,"tokens":[{"text":"First","start":0,"end":5,"id":0},{"text":",","start":5,"end":6,"id":1},{"text":"relying","start":7,"end":14,"id":2},{"text":"on","start":15,"end":17,"id":3},{"text":"a","start":18,"end":19,"id":4},{"text":"recent","start":20,"end":26,"id":5},{"text":"result","start":27,"end":33,"id":6},{"text":"by","start":34,"end":36,"id":7},{"text":"Cerou","start":37,"end":42,"id":8},{"text":"and","start":43,"end":46,"id":9},{"text":"Guyader","start":47,"end":54,"id":10},{"text":"(","start":55,"end":56,"id":11},{"text":"2006","start":56,"end":60,"id":12},{"text":")","start":60,"end":61,"id":13},{"text":",","start":61,"end":62,"id":14},{"text":"we","start":63,"end":65,"id":15},{"text":"prove","start":66,"end":71,"id":16},{"text":"the","start":72,"end":75,"id":17},{"text":"consistency","start":76,"end":87,"id":18},{"text":"of","start":88,"end":90,"id":19},{"text":"the","start":91,"end":94,"id":20},{"text":"k","start":95,"end":96,"id":21},{"text":"-","start":96,"end":97,"id":22},{"text":"NN","start":97,"end":99,"id":23},{"text":"classifier","start":100,"end":110,"id":24},{"text":"for","start":111,"end":114,"id":25},{"text":"functional","start":115,"end":125,"id":26},{"text":"data","start":126,"end":130,"id":27},{"text":"whose","start":131,"end":136,"id":28},{"text":"distribution","start":137,"end":149,"id":29},{"text":"belongs","start":150,"end":157,"id":30},{"text":"to","start":158,"end":160,"id":31},{"text":"a","start":161,"end":162,"id":32},{"text":"broad","start":163,"end":168,"id":33},{"text":"family","start":169,"end":175,"id":34},{"text":"of","start":176,"end":178,"id":35},{"text":"Gaussian","start":179,"end":187,"id":36},{"text":"processes","start":188,"end":197,"id":37},{"text":"with","start":198,"end":202,"id":38},{"text":"triangular","start":203,"end":213,"id":39},{"text":"covariance","start":214,"end":224,"id":40},{"text":"functions","start":225,"end":234,"id":41},{"text":".","start":234,"end":235,"id":42}],"spans":[{"token_start":21,"token_end":23,"start":95,"end":99,"text":"k-NN","label":"ALGO","source":"./algo_model5","input_hash":1795532205,"answer":"accept"}],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"We consider the problem of jointly estimating the parameters as well as the structure of binary valued Markov Random Fields, in contrast to earlier work that focus on one of the two problems.","_input_hash":680968016,"_task_hash":-1466320610,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"consider","start":3,"end":11,"id":1},{"text":"the","start":12,"end":15,"id":2},{"text":"problem","start":16,"end":23,"id":3},{"text":"of","start":24,"end":26,"id":4},{"text":"jointly","start":27,"end":34,"id":5},{"text":"estimating","start":35,"end":45,"id":6},{"text":"the","start":46,"end":49,"id":7},{"text":"parameters","start":50,"end":60,"id":8},{"text":"as","start":61,"end":63,"id":9},{"text":"well","start":64,"end":68,"id":10},{"text":"as","start":69,"end":71,"id":11},{"text":"the","start":72,"end":75,"id":12},{"text":"structure","start":76,"end":85,"id":13},{"text":"of","start":86,"end":88,"id":14},{"text":"binary","start":89,"end":95,"id":15},{"text":"valued","start":96,"end":102,"id":16},{"text":"Markov","start":103,"end":109,"id":17},{"text":"Random","start":110,"end":116,"id":18},{"text":"Fields","start":117,"end":123,"id":19},{"text":",","start":123,"end":124,"id":20},{"text":"in","start":125,"end":127,"id":21},{"text":"contrast","start":128,"end":136,"id":22},{"text":"to","start":137,"end":139,"id":23},{"text":"earlier","start":140,"end":147,"id":24},{"text":"work","start":148,"end":152,"id":25},{"text":"that","start":153,"end":157,"id":26},{"text":"focus","start":158,"end":163,"id":27},{"text":"on","start":164,"end":166,"id":28},{"text":"one","start":167,"end":170,"id":29},{"text":"of","start":171,"end":173,"id":30},{"text":"the","start":174,"end":177,"id":31},{"text":"two","start":178,"end":181,"id":32},{"text":"problems","start":182,"end":190,"id":33},{"text":".","start":190,"end":191,"id":34}],"spans":[{"start":103,"end":123,"token_start":17,"token_end":19,"label":"ALGO","answer":"accept"}],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"We show how to extend systematically all the results obtained in the inductive setting to transductive learning, and use this to improve Vapnik's generalization bounds, extending them to the case when the sample is made of independent non-identically distributed pairs of patterns and labels.","_input_hash":-979179916,"_task_hash":399736153,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"show","start":3,"end":7,"id":1},{"text":"how","start":8,"end":11,"id":2},{"text":"to","start":12,"end":14,"id":3},{"text":"extend","start":15,"end":21,"id":4},{"text":"systematically","start":22,"end":36,"id":5},{"text":"all","start":37,"end":40,"id":6},{"text":"the","start":41,"end":44,"id":7},{"text":"results","start":45,"end":52,"id":8},{"text":"obtained","start":53,"end":61,"id":9},{"text":"in","start":62,"end":64,"id":10},{"text":"the","start":65,"end":68,"id":11},{"text":"inductive","start":69,"end":78,"id":12},{"text":"setting","start":79,"end":86,"id":13},{"text":"to","start":87,"end":89,"id":14},{"text":"transductive","start":90,"end":102,"id":15},{"text":"learning","start":103,"end":111,"id":16},{"text":",","start":111,"end":112,"id":17},{"text":"and","start":113,"end":116,"id":18},{"text":"use","start":117,"end":120,"id":19},{"text":"this","start":121,"end":125,"id":20},{"text":"to","start":126,"end":128,"id":21},{"text":"improve","start":129,"end":136,"id":22},{"text":"Vapnik","start":137,"end":143,"id":23},{"text":"'s","start":143,"end":145,"id":24},{"text":"generalization","start":146,"end":160,"id":25},{"text":"bounds","start":161,"end":167,"id":26},{"text":",","start":167,"end":168,"id":27},{"text":"extending","start":169,"end":178,"id":28},{"text":"them","start":179,"end":183,"id":29},{"text":"to","start":184,"end":186,"id":30},{"text":"the","start":187,"end":190,"id":31},{"text":"case","start":191,"end":195,"id":32},{"text":"when","start":196,"end":200,"id":33},{"text":"the","start":201,"end":204,"id":34},{"text":"sample","start":205,"end":211,"id":35},{"text":"is","start":212,"end":214,"id":36},{"text":"made","start":215,"end":219,"id":37},{"text":"of","start":220,"end":222,"id":38},{"text":"independent","start":223,"end":234,"id":39},{"text":"non","start":235,"end":238,"id":40},{"text":"-","start":238,"end":239,"id":41},{"text":"identically","start":239,"end":250,"id":42},{"text":"distributed","start":251,"end":262,"id":43},{"text":"pairs","start":263,"end":268,"id":44},{"text":"of","start":269,"end":271,"id":45},{"text":"patterns","start":272,"end":280,"id":46},{"text":"and","start":281,"end":284,"id":47},{"text":"labels","start":285,"end":291,"id":48},{"text":".","start":291,"end":292,"id":49}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"The transitions between these decisional states correspond to events that lead to a change of decision.","_input_hash":-752918440,"_task_hash":-1884347967,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"transitions","start":4,"end":15,"id":1},{"text":"between","start":16,"end":23,"id":2},{"text":"these","start":24,"end":29,"id":3},{"text":"decisional","start":30,"end":40,"id":4},{"text":"states","start":41,"end":47,"id":5},{"text":"correspond","start":48,"end":58,"id":6},{"text":"to","start":59,"end":61,"id":7},{"text":"events","start":62,"end":68,"id":8},{"text":"that","start":69,"end":73,"id":9},{"text":"lead","start":74,"end":78,"id":10},{"text":"to","start":79,"end":81,"id":11},{"text":"a","start":82,"end":83,"id":12},{"text":"change","start":84,"end":90,"id":13},{"text":"of","start":91,"end":93,"id":14},{"text":"decision","start":94,"end":102,"id":15},{"text":".","start":102,"end":103,"id":16}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"We demonstrate the application of our methodology in the context of decentralized anomaly detection in the Abilene backbone network.","_input_hash":178100919,"_task_hash":-249685443,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"demonstrate","start":3,"end":14,"id":1},{"text":"the","start":15,"end":18,"id":2},{"text":"application","start":19,"end":30,"id":3},{"text":"of","start":31,"end":33,"id":4},{"text":"our","start":34,"end":37,"id":5},{"text":"methodology","start":38,"end":49,"id":6},{"text":"in","start":50,"end":52,"id":7},{"text":"the","start":53,"end":56,"id":8},{"text":"context","start":57,"end":64,"id":9},{"text":"of","start":65,"end":67,"id":10},{"text":"decentralized","start":68,"end":81,"id":11},{"text":"anomaly","start":82,"end":89,"id":12},{"text":"detection","start":90,"end":99,"id":13},{"text":"in","start":100,"end":102,"id":14},{"text":"the","start":103,"end":106,"id":15},{"text":"Abilene","start":107,"end":114,"id":16},{"text":"backbone","start":115,"end":123,"id":17},{"text":"network","start":124,"end":131,"id":18},{"text":".","start":131,"end":132,"id":19}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"The performance for learning drifting concepts of one of the presented algorithms is analysed and compared with the Baldi-Chauvin algorithm in the same situations.","_input_hash":1678323752,"_task_hash":1819638822,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"performance","start":4,"end":15,"id":1},{"text":"for","start":16,"end":19,"id":2},{"text":"learning","start":20,"end":28,"id":3},{"text":"drifting","start":29,"end":37,"id":4},{"text":"concepts","start":38,"end":46,"id":5},{"text":"of","start":47,"end":49,"id":6},{"text":"one","start":50,"end":53,"id":7},{"text":"of","start":54,"end":56,"id":8},{"text":"the","start":57,"end":60,"id":9},{"text":"presented","start":61,"end":70,"id":10},{"text":"algorithms","start":71,"end":81,"id":11},{"text":"is","start":82,"end":84,"id":12},{"text":"analysed","start":85,"end":93,"id":13},{"text":"and","start":94,"end":97,"id":14},{"text":"compared","start":98,"end":106,"id":15},{"text":"with","start":107,"end":111,"id":16},{"text":"the","start":112,"end":115,"id":17},{"text":"Baldi","start":116,"end":121,"id":18},{"text":"-","start":121,"end":122,"id":19},{"text":"Chauvin","start":122,"end":129,"id":20},{"text":"algorithm","start":130,"end":139,"id":21},{"text":"in","start":140,"end":142,"id":22},{"text":"the","start":143,"end":146,"id":23},{"text":"same","start":147,"end":151,"id":24},{"text":"situations","start":152,"end":162,"id":25},{"text":".","start":162,"end":163,"id":26}],"spans":[{"token_start":18,"token_end":20,"start":116,"end":129,"text":"Baldi-Chauvin","label":"ALGO","source":"./algo_model5","input_hash":1678323752,"answer":"accept"}],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"This work is inspired by the concept of finite-time learning with known accuracy and confidence developed in statistical learning theory.","_input_hash":-619534829,"_task_hash":1297335163,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"work","start":5,"end":9,"id":1},{"text":"is","start":10,"end":12,"id":2},{"text":"inspired","start":13,"end":21,"id":3},{"text":"by","start":22,"end":24,"id":4},{"text":"the","start":25,"end":28,"id":5},{"text":"concept","start":29,"end":36,"id":6},{"text":"of","start":37,"end":39,"id":7},{"text":"finite","start":40,"end":46,"id":8},{"text":"-","start":46,"end":47,"id":9},{"text":"time","start":47,"end":51,"id":10},{"text":"learning","start":52,"end":60,"id":11},{"text":"with","start":61,"end":65,"id":12},{"text":"known","start":66,"end":71,"id":13},{"text":"accuracy","start":72,"end":80,"id":14},{"text":"and","start":81,"end":84,"id":15},{"text":"confidence","start":85,"end":95,"id":16},{"text":"developed","start":96,"end":105,"id":17},{"text":"in","start":106,"end":108,"id":18},{"text":"statistical","start":109,"end":120,"id":19},{"text":"learning","start":121,"end":129,"id":20},{"text":"theory","start":130,"end":136,"id":21},{"text":".","start":136,"end":137,"id":22}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"The network performance is determined by calculating the mean square error of the network prediction.","_input_hash":1246371537,"_task_hash":-64210889,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"network","start":4,"end":11,"id":1},{"text":"performance","start":12,"end":23,"id":2},{"text":"is","start":24,"end":26,"id":3},{"text":"determined","start":27,"end":37,"id":4},{"text":"by","start":38,"end":40,"id":5},{"text":"calculating","start":41,"end":52,"id":6},{"text":"the","start":53,"end":56,"id":7},{"text":"mean","start":57,"end":61,"id":8},{"text":"square","start":62,"end":68,"id":9},{"text":"error","start":69,"end":74,"id":10},{"text":"of","start":75,"end":77,"id":11},{"text":"the","start":78,"end":81,"id":12},{"text":"network","start":82,"end":89,"id":13},{"text":"prediction","start":90,"end":100,"id":14},{"text":".","start":100,"end":101,"id":15}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"Given a corpus of documents, a posterior inference algorithm finds an approximation to a posterior distribution over trees, topics and allocations of words to levels of the tree.","_input_hash":-1107644007,"_task_hash":564508626,"tokens":[{"text":"Given","start":0,"end":5,"id":0},{"text":"a","start":6,"end":7,"id":1},{"text":"corpus","start":8,"end":14,"id":2},{"text":"of","start":15,"end":17,"id":3},{"text":"documents","start":18,"end":27,"id":4},{"text":",","start":27,"end":28,"id":5},{"text":"a","start":29,"end":30,"id":6},{"text":"posterior","start":31,"end":40,"id":7},{"text":"inference","start":41,"end":50,"id":8},{"text":"algorithm","start":51,"end":60,"id":9},{"text":"finds","start":61,"end":66,"id":10},{"text":"an","start":67,"end":69,"id":11},{"text":"approximation","start":70,"end":83,"id":12},{"text":"to","start":84,"end":86,"id":13},{"text":"a","start":87,"end":88,"id":14},{"text":"posterior","start":89,"end":98,"id":15},{"text":"distribution","start":99,"end":111,"id":16},{"text":"over","start":112,"end":116,"id":17},{"text":"trees","start":117,"end":122,"id":18},{"text":",","start":122,"end":123,"id":19},{"text":"topics","start":124,"end":130,"id":20},{"text":"and","start":131,"end":134,"id":21},{"text":"allocations","start":135,"end":146,"id":22},{"text":"of","start":147,"end":149,"id":23},{"text":"words","start":150,"end":155,"id":24},{"text":"to","start":156,"end":158,"id":25},{"text":"levels","start":159,"end":165,"id":26},{"text":"of","start":166,"end":168,"id":27},{"text":"the","start":169,"end":172,"id":28},{"text":"tree","start":173,"end":177,"id":29},{"text":".","start":177,"end":178,"id":30}],"spans":[{"start":31,"end":50,"token_start":7,"token_end":8,"label":"ALGO","answer":"accept"}],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"We propose a scalable algorithmic framework, with which SLM posteriors over full, high-resolution images can be approximated for the first time, solving a variational optimization problem which is convex iff posterior mode finding is convex.","_input_hash":-1049062672,"_task_hash":-1627809004,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"propose","start":3,"end":10,"id":1},{"text":"a","start":11,"end":12,"id":2},{"text":"scalable","start":13,"end":21,"id":3},{"text":"algorithmic","start":22,"end":33,"id":4},{"text":"framework","start":34,"end":43,"id":5},{"text":",","start":43,"end":44,"id":6},{"text":"with","start":45,"end":49,"id":7},{"text":"which","start":50,"end":55,"id":8},{"text":"SLM","start":56,"end":59,"id":9},{"text":"posteriors","start":60,"end":70,"id":10},{"text":"over","start":71,"end":75,"id":11},{"text":"full","start":76,"end":80,"id":12},{"text":",","start":80,"end":81,"id":13},{"text":"high","start":82,"end":86,"id":14},{"text":"-","start":86,"end":87,"id":15},{"text":"resolution","start":87,"end":97,"id":16},{"text":"images","start":98,"end":104,"id":17},{"text":"can","start":105,"end":108,"id":18},{"text":"be","start":109,"end":111,"id":19},{"text":"approximated","start":112,"end":124,"id":20},{"text":"for","start":125,"end":128,"id":21},{"text":"the","start":129,"end":132,"id":22},{"text":"first","start":133,"end":138,"id":23},{"text":"time","start":139,"end":143,"id":24},{"text":",","start":143,"end":144,"id":25},{"text":"solving","start":145,"end":152,"id":26},{"text":"a","start":153,"end":154,"id":27},{"text":"variational","start":155,"end":166,"id":28},{"text":"optimization","start":167,"end":179,"id":29},{"text":"problem","start":180,"end":187,"id":30},{"text":"which","start":188,"end":193,"id":31},{"text":"is","start":194,"end":196,"id":32},{"text":"convex","start":197,"end":203,"id":33},{"text":"iff","start":204,"end":207,"id":34},{"text":"posterior","start":208,"end":217,"id":35},{"text":"mode","start":218,"end":222,"id":36},{"text":"finding","start":223,"end":230,"id":37},{"text":"is","start":231,"end":233,"id":38},{"text":"convex","start":234,"end":240,"id":39},{"text":".","start":240,"end":241,"id":40}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"Moreover, this causes LLE to converge to a linear projection of the input, as opposed to its non-linear embedding goal.","_input_hash":1855610389,"_task_hash":1935788299,"tokens":[{"text":"Moreover","start":0,"end":8,"id":0},{"text":",","start":8,"end":9,"id":1},{"text":"this","start":10,"end":14,"id":2},{"text":"causes","start":15,"end":21,"id":3},{"text":"LLE","start":22,"end":25,"id":4},{"text":"to","start":26,"end":28,"id":5},{"text":"converge","start":29,"end":37,"id":6},{"text":"to","start":38,"end":40,"id":7},{"text":"a","start":41,"end":42,"id":8},{"text":"linear","start":43,"end":49,"id":9},{"text":"projection","start":50,"end":60,"id":10},{"text":"of","start":61,"end":63,"id":11},{"text":"the","start":64,"end":67,"id":12},{"text":"input","start":68,"end":73,"id":13},{"text":",","start":73,"end":74,"id":14},{"text":"as","start":75,"end":77,"id":15},{"text":"opposed","start":78,"end":85,"id":16},{"text":"to","start":86,"end":88,"id":17},{"text":"its","start":89,"end":92,"id":18},{"text":"non","start":93,"end":96,"id":19},{"text":"-","start":96,"end":97,"id":20},{"text":"linear","start":97,"end":103,"id":21},{"text":"embedding","start":104,"end":113,"id":22},{"text":"goal","start":114,"end":118,"id":23},{"text":".","start":118,"end":119,"id":24}],"spans":[{"token_start":4,"token_end":4,"start":22,"end":25,"text":"LLE","label":"ALGO","source":"./algo_model5","input_hash":1855610389,"answer":"accept"}],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"In this paper, we study the problem of embedding arbitrary metric spaces into a Euclidean space with the goal to improve the accuracy of the NN classifier.","_input_hash":1951559990,"_task_hash":-1755695736,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"this","start":3,"end":7,"id":1},{"text":"paper","start":8,"end":13,"id":2},{"text":",","start":13,"end":14,"id":3},{"text":"we","start":15,"end":17,"id":4},{"text":"study","start":18,"end":23,"id":5},{"text":"the","start":24,"end":27,"id":6},{"text":"problem","start":28,"end":35,"id":7},{"text":"of","start":36,"end":38,"id":8},{"text":"embedding","start":39,"end":48,"id":9},{"text":"arbitrary","start":49,"end":58,"id":10},{"text":"metric","start":59,"end":65,"id":11},{"text":"spaces","start":66,"end":72,"id":12},{"text":"into","start":73,"end":77,"id":13},{"text":"a","start":78,"end":79,"id":14},{"text":"Euclidean","start":80,"end":89,"id":15},{"text":"space","start":90,"end":95,"id":16},{"text":"with","start":96,"end":100,"id":17},{"text":"the","start":101,"end":104,"id":18},{"text":"goal","start":105,"end":109,"id":19},{"text":"to","start":110,"end":112,"id":20},{"text":"improve","start":113,"end":120,"id":21},{"text":"the","start":121,"end":124,"id":22},{"text":"accuracy","start":125,"end":133,"id":23},{"text":"of","start":134,"end":136,"id":24},{"text":"the","start":137,"end":140,"id":25},{"text":"NN","start":141,"end":143,"id":26},{"text":"classifier","start":144,"end":154,"id":27},{"text":".","start":154,"end":155,"id":28}],"spans":[{"start":141,"end":154,"token_start":26,"token_end":27,"label":"ALGO","answer":"accept"}],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"This thesis responds to the challenges of using a large number, such as thousands, of features in regression and classification problems.","_input_hash":179630665,"_task_hash":-187982617,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"thesis","start":5,"end":11,"id":1},{"text":"responds","start":12,"end":20,"id":2},{"text":"to","start":21,"end":23,"id":3},{"text":"the","start":24,"end":27,"id":4},{"text":"challenges","start":28,"end":38,"id":5},{"text":"of","start":39,"end":41,"id":6},{"text":"using","start":42,"end":47,"id":7},{"text":"a","start":48,"end":49,"id":8},{"text":"large","start":50,"end":55,"id":9},{"text":"number","start":56,"end":62,"id":10},{"text":",","start":62,"end":63,"id":11},{"text":"such","start":64,"end":68,"id":12},{"text":"as","start":69,"end":71,"id":13},{"text":"thousands","start":72,"end":81,"id":14},{"text":",","start":81,"end":82,"id":15},{"text":"of","start":83,"end":85,"id":16},{"text":"features","start":86,"end":94,"id":17},{"text":"in","start":95,"end":97,"id":18},{"text":"regression","start":98,"end":108,"id":19},{"text":"and","start":109,"end":112,"id":20},{"text":"classification","start":113,"end":127,"id":21},{"text":"problems","start":128,"end":136,"id":22},{"text":".","start":136,"end":137,"id":23}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"It is however unclear if these heuristics can be derived from a more general principle facilitating generalization to new problem settings.","_input_hash":401064236,"_task_hash":1982098538,"tokens":[{"text":"It","start":0,"end":2,"id":0},{"text":"is","start":3,"end":5,"id":1},{"text":"however","start":6,"end":13,"id":2},{"text":"unclear","start":14,"end":21,"id":3},{"text":"if","start":22,"end":24,"id":4},{"text":"these","start":25,"end":30,"id":5},{"text":"heuristics","start":31,"end":41,"id":6},{"text":"can","start":42,"end":45,"id":7},{"text":"be","start":46,"end":48,"id":8},{"text":"derived","start":49,"end":56,"id":9},{"text":"from","start":57,"end":61,"id":10},{"text":"a","start":62,"end":63,"id":11},{"text":"more","start":64,"end":68,"id":12},{"text":"general","start":69,"end":76,"id":13},{"text":"principle","start":77,"end":86,"id":14},{"text":"facilitating","start":87,"end":99,"id":15},{"text":"generalization","start":100,"end":114,"id":16},{"text":"to","start":115,"end":117,"id":17},{"text":"new","start":118,"end":121,"id":18},{"text":"problem","start":122,"end":129,"id":19},{"text":"settings","start":130,"end":138,"id":20},{"text":".","start":138,"end":139,"id":21}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"We present numerical examples demonstrating both the perturbation and linear projection problems, and the improved outputs using the low-dimensional neighborhood representation.","_input_hash":1302518157,"_task_hash":-1290208708,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"present","start":3,"end":10,"id":1},{"text":"numerical","start":11,"end":20,"id":2},{"text":"examples","start":21,"end":29,"id":3},{"text":"demonstrating","start":30,"end":43,"id":4},{"text":"both","start":44,"end":48,"id":5},{"text":"the","start":49,"end":52,"id":6},{"text":"perturbation","start":53,"end":65,"id":7},{"text":"and","start":66,"end":69,"id":8},{"text":"linear","start":70,"end":76,"id":9},{"text":"projection","start":77,"end":87,"id":10},{"text":"problems","start":88,"end":96,"id":11},{"text":",","start":96,"end":97,"id":12},{"text":"and","start":98,"end":101,"id":13},{"text":"the","start":102,"end":105,"id":14},{"text":"improved","start":106,"end":114,"id":15},{"text":"outputs","start":115,"end":122,"id":16},{"text":"using","start":123,"end":128,"id":17},{"text":"the","start":129,"end":132,"id":18},{"text":"low","start":133,"end":136,"id":19},{"text":"-","start":136,"end":137,"id":20},{"text":"dimensional","start":137,"end":148,"id":21},{"text":"neighborhood","start":149,"end":161,"id":22},{"text":"representation","start":162,"end":176,"id":23},{"text":".","start":176,"end":177,"id":24}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"The number of compressed parameters may have converged before considering the highest possible order.","_input_hash":291336523,"_task_hash":-1464749194,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"number","start":4,"end":10,"id":1},{"text":"of","start":11,"end":13,"id":2},{"text":"compressed","start":14,"end":24,"id":3},{"text":"parameters","start":25,"end":35,"id":4},{"text":"may","start":36,"end":39,"id":5},{"text":"have","start":40,"end":44,"id":6},{"text":"converged","start":45,"end":54,"id":7},{"text":"before","start":55,"end":61,"id":8},{"text":"considering","start":62,"end":73,"id":9},{"text":"the","start":74,"end":77,"id":10},{"text":"highest","start":78,"end":85,"id":11},{"text":"possible","start":86,"end":94,"id":12},{"text":"order","start":95,"end":100,"id":13},{"text":".","start":100,"end":101,"id":14}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"Dendrograms used in data analysis are ultrametric spaces, hence objects of nonarchimedean geometry.","_input_hash":-1804434462,"_task_hash":1288390984,"tokens":[{"text":"Dendrograms","start":0,"end":11,"id":0},{"text":"used","start":12,"end":16,"id":1},{"text":"in","start":17,"end":19,"id":2},{"text":"data","start":20,"end":24,"id":3},{"text":"analysis","start":25,"end":33,"id":4},{"text":"are","start":34,"end":37,"id":5},{"text":"ultrametric","start":38,"end":49,"id":6},{"text":"spaces","start":50,"end":56,"id":7},{"text":",","start":56,"end":57,"id":8},{"text":"hence","start":58,"end":63,"id":9},{"text":"objects","start":64,"end":71,"id":10},{"text":"of","start":72,"end":74,"id":11},{"text":"nonarchimedean","start":75,"end":89,"id":12},{"text":"geometry","start":90,"end":98,"id":13},{"text":".","start":98,"end":99,"id":14}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"A key component involves the node mean squared error for a quantity we refer to as a maximal subtree.","_input_hash":-1629405522,"_task_hash":-1010331814,"tokens":[{"text":"A","start":0,"end":1,"id":0},{"text":"key","start":2,"end":5,"id":1},{"text":"component","start":6,"end":15,"id":2},{"text":"involves","start":16,"end":24,"id":3},{"text":"the","start":25,"end":28,"id":4},{"text":"node","start":29,"end":33,"id":5},{"text":"mean","start":34,"end":38,"id":6},{"text":"squared","start":39,"end":46,"id":7},{"text":"error","start":47,"end":52,"id":8},{"text":"for","start":53,"end":56,"id":9},{"text":"a","start":57,"end":58,"id":10},{"text":"quantity","start":59,"end":67,"id":11},{"text":"we","start":68,"end":70,"id":12},{"text":"refer","start":71,"end":76,"id":13},{"text":"to","start":77,"end":79,"id":14},{"text":"as","start":80,"end":82,"id":15},{"text":"a","start":83,"end":84,"id":16},{"text":"maximal","start":85,"end":92,"id":17},{"text":"subtree","start":93,"end":100,"id":18},{"text":".","start":100,"end":101,"id":19}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"The utility function encodes some a priori knowledge external to the system, it quantifies how bad it is to make mistakes.","_input_hash":651532569,"_task_hash":-1655137091,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"utility","start":4,"end":11,"id":1},{"text":"function","start":12,"end":20,"id":2},{"text":"encodes","start":21,"end":28,"id":3},{"text":"some","start":29,"end":33,"id":4},{"text":"a","start":34,"end":35,"id":5},{"text":"priori","start":36,"end":42,"id":6},{"text":"knowledge","start":43,"end":52,"id":7},{"text":"external","start":53,"end":61,"id":8},{"text":"to","start":62,"end":64,"id":9},{"text":"the","start":65,"end":68,"id":10},{"text":"system","start":69,"end":75,"id":11},{"text":",","start":75,"end":76,"id":12},{"text":"it","start":77,"end":79,"id":13},{"text":"quantifies","start":80,"end":90,"id":14},{"text":"how","start":91,"end":94,"id":15},{"text":"bad","start":95,"end":98,"id":16},{"text":"it","start":99,"end":101,"id":17},{"text":"is","start":102,"end":104,"id":18},{"text":"to","start":105,"end":107,"id":19},{"text":"make","start":108,"end":112,"id":20},{"text":"mistakes","start":113,"end":121,"id":21},{"text":".","start":121,"end":122,"id":22}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"Our methodology provides new insight into similarities and differences between sparse reconstruction and approximate Bayesian inference, and has important implications for compressive sensing of real-world images.","_input_hash":85836900,"_task_hash":1595739516,"tokens":[{"text":"Our","start":0,"end":3,"id":0},{"text":"methodology","start":4,"end":15,"id":1},{"text":"provides","start":16,"end":24,"id":2},{"text":"new","start":25,"end":28,"id":3},{"text":"insight","start":29,"end":36,"id":4},{"text":"into","start":37,"end":41,"id":5},{"text":"similarities","start":42,"end":54,"id":6},{"text":"and","start":55,"end":58,"id":7},{"text":"differences","start":59,"end":70,"id":8},{"text":"between","start":71,"end":78,"id":9},{"text":"sparse","start":79,"end":85,"id":10},{"text":"reconstruction","start":86,"end":100,"id":11},{"text":"and","start":101,"end":104,"id":12},{"text":"approximate","start":105,"end":116,"id":13},{"text":"Bayesian","start":117,"end":125,"id":14},{"text":"inference","start":126,"end":135,"id":15},{"text":",","start":135,"end":136,"id":16},{"text":"and","start":137,"end":140,"id":17},{"text":"has","start":141,"end":144,"id":18},{"text":"important","start":145,"end":154,"id":19},{"text":"implications","start":155,"end":167,"id":20},{"text":"for","start":168,"end":171,"id":21},{"text":"compressive","start":172,"end":183,"id":22},{"text":"sensing","start":184,"end":191,"id":23},{"text":"of","start":192,"end":194,"id":24},{"text":"real","start":195,"end":199,"id":25},{"text":"-","start":199,"end":200,"id":26},{"text":"world","start":200,"end":205,"id":27},{"text":"images","start":206,"end":212,"id":28},{"text":".","start":212,"end":213,"id":29}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"We present the Procrustes measure, a novel measure based on Procrustes rotation that enables quantitative comparison of the output of manifold-based embedding algorithms (such as LLE (Roweis and Saul, 2000) and Isomap (Tenenbaum et al, 2000)).","_input_hash":1911758319,"_task_hash":-781398468,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"present","start":3,"end":10,"id":1},{"text":"the","start":11,"end":14,"id":2},{"text":"Procrustes","start":15,"end":25,"id":3},{"text":"measure","start":26,"end":33,"id":4},{"text":",","start":33,"end":34,"id":5},{"text":"a","start":35,"end":36,"id":6},{"text":"novel","start":37,"end":42,"id":7},{"text":"measure","start":43,"end":50,"id":8},{"text":"based","start":51,"end":56,"id":9},{"text":"on","start":57,"end":59,"id":10},{"text":"Procrustes","start":60,"end":70,"id":11},{"text":"rotation","start":71,"end":79,"id":12},{"text":"that","start":80,"end":84,"id":13},{"text":"enables","start":85,"end":92,"id":14},{"text":"quantitative","start":93,"end":105,"id":15},{"text":"comparison","start":106,"end":116,"id":16},{"text":"of","start":117,"end":119,"id":17},{"text":"the","start":120,"end":123,"id":18},{"text":"output","start":124,"end":130,"id":19},{"text":"of","start":131,"end":133,"id":20},{"text":"manifold","start":134,"end":142,"id":21},{"text":"-","start":142,"end":143,"id":22},{"text":"based","start":143,"end":148,"id":23},{"text":"embedding","start":149,"end":158,"id":24},{"text":"algorithms","start":159,"end":169,"id":25},{"text":"(","start":170,"end":171,"id":26},{"text":"such","start":171,"end":175,"id":27},{"text":"as","start":176,"end":178,"id":28},{"text":"LLE","start":179,"end":182,"id":29},{"text":"(","start":183,"end":184,"id":30},{"text":"Roweis","start":184,"end":190,"id":31},{"text":"and","start":191,"end":194,"id":32},{"text":"Saul","start":195,"end":199,"id":33},{"text":",","start":199,"end":200,"id":34},{"text":"2000","start":201,"end":205,"id":35},{"text":")","start":205,"end":206,"id":36},{"text":"and","start":207,"end":210,"id":37},{"text":"Isomap","start":211,"end":217,"id":38},{"text":"(","start":218,"end":219,"id":39},{"text":"Tenenbaum","start":219,"end":228,"id":40},{"text":"et","start":229,"end":231,"id":41},{"text":"al","start":232,"end":234,"id":42},{"text":",","start":234,"end":235,"id":43},{"text":"2000","start":236,"end":240,"id":44},{"text":")","start":240,"end":241,"id":45},{"text":")","start":241,"end":242,"id":46},{"text":".","start":242,"end":243,"id":47}],"spans":[{"token_start":21,"token_end":24,"start":134,"end":158,"text":"manifold-based embedding","label":"ALGO","source":"./algo_model5","input_hash":1911758319,"answer":"accept"},{"token_start":29,"token_end":29,"start":179,"end":182,"text":"LLE","label":"ALGO","source":"./algo_model5","input_hash":1911758319,"answer":"accept"}],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"It first aims at confirming, known but sparse, advice for using random forests and at proposing some complementary remarks for both standard problems as well as high dimensional ones for which the number of variables hugely exceeds the sample size.","_input_hash":649819897,"_task_hash":-848801109,"tokens":[{"text":"It","start":0,"end":2,"id":0},{"text":"first","start":3,"end":8,"id":1},{"text":"aims","start":9,"end":13,"id":2},{"text":"at","start":14,"end":16,"id":3},{"text":"confirming","start":17,"end":27,"id":4},{"text":",","start":27,"end":28,"id":5},{"text":"known","start":29,"end":34,"id":6},{"text":"but","start":35,"end":38,"id":7},{"text":"sparse","start":39,"end":45,"id":8},{"text":",","start":45,"end":46,"id":9},{"text":"advice","start":47,"end":53,"id":10},{"text":"for","start":54,"end":57,"id":11},{"text":"using","start":58,"end":63,"id":12},{"text":"random","start":64,"end":70,"id":13},{"text":"forests","start":71,"end":78,"id":14},{"text":"and","start":79,"end":82,"id":15},{"text":"at","start":83,"end":85,"id":16},{"text":"proposing","start":86,"end":95,"id":17},{"text":"some","start":96,"end":100,"id":18},{"text":"complementary","start":101,"end":114,"id":19},{"text":"remarks","start":115,"end":122,"id":20},{"text":"for","start":123,"end":126,"id":21},{"text":"both","start":127,"end":131,"id":22},{"text":"standard","start":132,"end":140,"id":23},{"text":"problems","start":141,"end":149,"id":24},{"text":"as","start":150,"end":152,"id":25},{"text":"well","start":153,"end":157,"id":26},{"text":"as","start":158,"end":160,"id":27},{"text":"high","start":161,"end":165,"id":28},{"text":"dimensional","start":166,"end":177,"id":29},{"text":"ones","start":178,"end":182,"id":30},{"text":"for","start":183,"end":186,"id":31},{"text":"which","start":187,"end":192,"id":32},{"text":"the","start":193,"end":196,"id":33},{"text":"number","start":197,"end":203,"id":34},{"text":"of","start":204,"end":206,"id":35},{"text":"variables","start":207,"end":216,"id":36},{"text":"hugely","start":217,"end":223,"id":37},{"text":"exceeds","start":224,"end":231,"id":38},{"text":"the","start":232,"end":235,"id":39},{"text":"sample","start":236,"end":242,"id":40},{"text":"size","start":243,"end":247,"id":41},{"text":".","start":247,"end":248,"id":42}],"spans":[{"start":64,"end":78,"token_start":13,"token_end":14,"label":"ALGO","answer":"accept"}],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"On synthetic data, we compare our algorithm on the two estimation tasks to the other existing methods.","_input_hash":517898554,"_task_hash":-1080598850,"tokens":[{"text":"On","start":0,"end":2,"id":0},{"text":"synthetic","start":3,"end":12,"id":1},{"text":"data","start":13,"end":17,"id":2},{"text":",","start":17,"end":18,"id":3},{"text":"we","start":19,"end":21,"id":4},{"text":"compare","start":22,"end":29,"id":5},{"text":"our","start":30,"end":33,"id":6},{"text":"algorithm","start":34,"end":43,"id":7},{"text":"on","start":44,"end":46,"id":8},{"text":"the","start":47,"end":50,"id":9},{"text":"two","start":51,"end":54,"id":10},{"text":"estimation","start":55,"end":65,"id":11},{"text":"tasks","start":66,"end":71,"id":12},{"text":"to","start":72,"end":74,"id":13},{"text":"the","start":75,"end":78,"id":14},{"text":"other","start":79,"end":84,"id":15},{"text":"existing","start":85,"end":93,"id":16},{"text":"methods","start":94,"end":101,"id":17},{"text":".","start":101,"end":102,"id":18}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"In this paper, we present the classification constrained dimensionality reduction (CCDR) algorithm to account for label information.","_input_hash":-837527872,"_task_hash":1727578501,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"this","start":3,"end":7,"id":1},{"text":"paper","start":8,"end":13,"id":2},{"text":",","start":13,"end":14,"id":3},{"text":"we","start":15,"end":17,"id":4},{"text":"present","start":18,"end":25,"id":5},{"text":"the","start":26,"end":29,"id":6},{"text":"classification","start":30,"end":44,"id":7},{"text":"constrained","start":45,"end":56,"id":8},{"text":"dimensionality","start":57,"end":71,"id":9},{"text":"reduction","start":72,"end":81,"id":10},{"text":"(","start":82,"end":83,"id":11},{"text":"CCDR","start":83,"end":87,"id":12},{"text":")","start":87,"end":88,"id":13},{"text":"algorithm","start":89,"end":98,"id":14},{"text":"to","start":99,"end":101,"id":15},{"text":"account","start":102,"end":109,"id":16},{"text":"for","start":110,"end":113,"id":17},{"text":"label","start":114,"end":119,"id":18},{"text":"information","start":120,"end":131,"id":19},{"text":".","start":131,"end":132,"id":20}],"spans":[{"start":30,"end":56,"token_start":7,"token_end":8,"label":"ALGO","answer":"accept"},{"start":57,"end":81,"token_start":9,"token_end":10,"label":"ALGO","answer":"accept"}],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"This provides an aid in diagnosing similar forms of cancer, as well as a means for variable selection in exploratory flow cytometric research.","_input_hash":1247429372,"_task_hash":-623487225,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"provides","start":5,"end":13,"id":1},{"text":"an","start":14,"end":16,"id":2},{"text":"aid","start":17,"end":20,"id":3},{"text":"in","start":21,"end":23,"id":4},{"text":"diagnosing","start":24,"end":34,"id":5},{"text":"similar","start":35,"end":42,"id":6},{"text":"forms","start":43,"end":48,"id":7},{"text":"of","start":49,"end":51,"id":8},{"text":"cancer","start":52,"end":58,"id":9},{"text":",","start":58,"end":59,"id":10},{"text":"as","start":60,"end":62,"id":11},{"text":"well","start":63,"end":67,"id":12},{"text":"as","start":68,"end":70,"id":13},{"text":"a","start":71,"end":72,"id":14},{"text":"means","start":73,"end":78,"id":15},{"text":"for","start":79,"end":82,"id":16},{"text":"variable","start":83,"end":91,"id":17},{"text":"selection","start":92,"end":101,"id":18},{"text":"in","start":102,"end":104,"id":19},{"text":"exploratory","start":105,"end":116,"id":20},{"text":"flow","start":117,"end":121,"id":21},{"text":"cytometric","start":122,"end":132,"id":22},{"text":"research","start":133,"end":141,"id":23},{"text":".","start":141,"end":142,"id":24}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"For this purpose, we reformulate the problem in the sparse inverse covariance (concentration) domain and solve the global eigenvalue problem using a sequence of local eigenvalue problems in each of the cliques of the decomposable graph.","_input_hash":-886167127,"_task_hash":-2112496241,"tokens":[{"text":"For","start":0,"end":3,"id":0},{"text":"this","start":4,"end":8,"id":1},{"text":"purpose","start":9,"end":16,"id":2},{"text":",","start":16,"end":17,"id":3},{"text":"we","start":18,"end":20,"id":4},{"text":"reformulate","start":21,"end":32,"id":5},{"text":"the","start":33,"end":36,"id":6},{"text":"problem","start":37,"end":44,"id":7},{"text":"in","start":45,"end":47,"id":8},{"text":"the","start":48,"end":51,"id":9},{"text":"sparse","start":52,"end":58,"id":10},{"text":"inverse","start":59,"end":66,"id":11},{"text":"covariance","start":67,"end":77,"id":12},{"text":"(","start":78,"end":79,"id":13},{"text":"concentration","start":79,"end":92,"id":14},{"text":")","start":92,"end":93,"id":15},{"text":"domain","start":94,"end":100,"id":16},{"text":"and","start":101,"end":104,"id":17},{"text":"solve","start":105,"end":110,"id":18},{"text":"the","start":111,"end":114,"id":19},{"text":"global","start":115,"end":121,"id":20},{"text":"eigenvalue","start":122,"end":132,"id":21},{"text":"problem","start":133,"end":140,"id":22},{"text":"using","start":141,"end":146,"id":23},{"text":"a","start":147,"end":148,"id":24},{"text":"sequence","start":149,"end":157,"id":25},{"text":"of","start":158,"end":160,"id":26},{"text":"local","start":161,"end":166,"id":27},{"text":"eigenvalue","start":167,"end":177,"id":28},{"text":"problems","start":178,"end":186,"id":29},{"text":"in","start":187,"end":189,"id":30},{"text":"each","start":190,"end":194,"id":31},{"text":"of","start":195,"end":197,"id":32},{"text":"the","start":198,"end":201,"id":33},{"text":"cliques","start":202,"end":209,"id":34},{"text":"of","start":210,"end":212,"id":35},{"text":"the","start":213,"end":216,"id":36},{"text":"decomposable","start":217,"end":229,"id":37},{"text":"graph","start":230,"end":235,"id":38},{"text":".","start":235,"end":236,"id":39}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"persistence and functoriality.","_input_hash":690850410,"_task_hash":2015507077,"tokens":[{"text":"persistence","start":0,"end":11,"id":0},{"text":"and","start":12,"end":15,"id":1},{"text":"functoriality","start":16,"end":29,"id":2},{"text":".","start":29,"end":30,"id":3}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"This article proposes a method for finding global maxima if the joint distribution is modeled by a kernel density estimation.","_input_hash":232411880,"_task_hash":735084196,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"article","start":5,"end":12,"id":1},{"text":"proposes","start":13,"end":21,"id":2},{"text":"a","start":22,"end":23,"id":3},{"text":"method","start":24,"end":30,"id":4},{"text":"for","start":31,"end":34,"id":5},{"text":"finding","start":35,"end":42,"id":6},{"text":"global","start":43,"end":49,"id":7},{"text":"maxima","start":50,"end":56,"id":8},{"text":"if","start":57,"end":59,"id":9},{"text":"the","start":60,"end":63,"id":10},{"text":"joint","start":64,"end":69,"id":11},{"text":"distribution","start":70,"end":82,"id":12},{"text":"is","start":83,"end":85,"id":13},{"text":"modeled","start":86,"end":93,"id":14},{"text":"by","start":94,"end":96,"id":15},{"text":"a","start":97,"end":98,"id":16},{"text":"kernel","start":99,"end":105,"id":17},{"text":"density","start":106,"end":113,"id":18},{"text":"estimation","start":114,"end":124,"id":19},{"text":".","start":124,"end":125,"id":20}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"Missing data is treated as Missing At Random mechanism by implementing maximum likelihood algorithm.","_input_hash":-445726575,"_task_hash":-452880343,"tokens":[{"text":"Missing","start":0,"end":7,"id":0},{"text":"data","start":8,"end":12,"id":1},{"text":"is","start":13,"end":15,"id":2},{"text":"treated","start":16,"end":23,"id":3},{"text":"as","start":24,"end":26,"id":4},{"text":"Missing","start":27,"end":34,"id":5},{"text":"At","start":35,"end":37,"id":6},{"text":"Random","start":38,"end":44,"id":7},{"text":"mechanism","start":45,"end":54,"id":8},{"text":"by","start":55,"end":57,"id":9},{"text":"implementing","start":58,"end":70,"id":10},{"text":"maximum","start":71,"end":78,"id":11},{"text":"likelihood","start":79,"end":89,"id":12},{"text":"algorithm","start":90,"end":99,"id":13},{"text":".","start":99,"end":100,"id":14}],"spans":[{"start":71,"end":89,"token_start":11,"token_end":12,"label":"ALGO","answer":"accept"}],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"We propose a solution by appealing to the framework of regularization in a reproducing kernel Hilbert space and prove a representer-like theorem for NN classification.","_input_hash":88466947,"_task_hash":1788782057,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"propose","start":3,"end":10,"id":1},{"text":"a","start":11,"end":12,"id":2},{"text":"solution","start":13,"end":21,"id":3},{"text":"by","start":22,"end":24,"id":4},{"text":"appealing","start":25,"end":34,"id":5},{"text":"to","start":35,"end":37,"id":6},{"text":"the","start":38,"end":41,"id":7},{"text":"framework","start":42,"end":51,"id":8},{"text":"of","start":52,"end":54,"id":9},{"text":"regularization","start":55,"end":69,"id":10},{"text":"in","start":70,"end":72,"id":11},{"text":"a","start":73,"end":74,"id":12},{"text":"reproducing","start":75,"end":86,"id":13},{"text":"kernel","start":87,"end":93,"id":14},{"text":"Hilbert","start":94,"end":101,"id":15},{"text":"space","start":102,"end":107,"id":16},{"text":"and","start":108,"end":111,"id":17},{"text":"prove","start":112,"end":117,"id":18},{"text":"a","start":118,"end":119,"id":19},{"text":"representer","start":120,"end":131,"id":20},{"text":"-","start":131,"end":132,"id":21},{"text":"like","start":132,"end":136,"id":22},{"text":"theorem","start":137,"end":144,"id":23},{"text":"for","start":145,"end":148,"id":24},{"text":"NN","start":149,"end":151,"id":25},{"text":"classification","start":152,"end":166,"id":26},{"text":".","start":166,"end":167,"id":27}],"spans":[{"start":87,"end":107,"token_start":14,"token_end":16,"label":"ALGO","answer":"accept"},{"start":149,"end":166,"token_start":25,"token_end":26,"label":"ALGO","answer":"accept"}],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"This paper examines from an experimental perspective random forests, the increasingly used statistical method for classification and regression problems introduced by Leo Breiman in 2001.","_input_hash":-308113262,"_task_hash":-1920693465,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"paper","start":5,"end":10,"id":1},{"text":"examines","start":11,"end":19,"id":2},{"text":"from","start":20,"end":24,"id":3},{"text":"an","start":25,"end":27,"id":4},{"text":"experimental","start":28,"end":40,"id":5},{"text":"perspective","start":41,"end":52,"id":6},{"text":"random","start":53,"end":59,"id":7},{"text":"forests","start":60,"end":67,"id":8},{"text":",","start":67,"end":68,"id":9},{"text":"the","start":69,"end":72,"id":10},{"text":"increasingly","start":73,"end":85,"id":11},{"text":"used","start":86,"end":90,"id":12},{"text":"statistical","start":91,"end":102,"id":13},{"text":"method","start":103,"end":109,"id":14},{"text":"for","start":110,"end":113,"id":15},{"text":"classification","start":114,"end":128,"id":16},{"text":"and","start":129,"end":132,"id":17},{"text":"regression","start":133,"end":143,"id":18},{"text":"problems","start":144,"end":152,"id":19},{"text":"introduced","start":153,"end":163,"id":20},{"text":"by","start":164,"end":166,"id":21},{"text":"Leo","start":167,"end":170,"id":22},{"text":"Breiman","start":171,"end":178,"id":23},{"text":"in","start":179,"end":181,"id":24},{"text":"2001","start":182,"end":186,"id":25},{"text":".","start":186,"end":187,"id":26}],"spans":[{"start":53,"end":67,"token_start":7,"token_end":8,"label":"ALGO","answer":"accept"}],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"We present a simple proof for the oracle inequality for the excess risk of structural risk minimizers using a lasso type penalty.","_input_hash":1863823134,"_task_hash":1510549650,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"present","start":3,"end":10,"id":1},{"text":"a","start":11,"end":12,"id":2},{"text":"simple","start":13,"end":19,"id":3},{"text":"proof","start":20,"end":25,"id":4},{"text":"for","start":26,"end":29,"id":5},{"text":"the","start":30,"end":33,"id":6},{"text":"oracle","start":34,"end":40,"id":7},{"text":"inequality","start":41,"end":51,"id":8},{"text":"for","start":52,"end":55,"id":9},{"text":"the","start":56,"end":59,"id":10},{"text":"excess","start":60,"end":66,"id":11},{"text":"risk","start":67,"end":71,"id":12},{"text":"of","start":72,"end":74,"id":13},{"text":"structural","start":75,"end":85,"id":14},{"text":"risk","start":86,"end":90,"id":15},{"text":"minimizers","start":91,"end":101,"id":16},{"text":"using","start":102,"end":107,"id":17},{"text":"a","start":108,"end":109,"id":18},{"text":"lasso","start":110,"end":115,"id":19},{"text":"type","start":116,"end":120,"id":20},{"text":"penalty","start":121,"end":128,"id":21},{"text":".","start":128,"end":129,"id":22}],"spans":[{"token_start":19,"token_end":19,"start":110,"end":115,"text":"lasso","label":"ALGO","source":"./algo_model5","input_hash":1863823134,"answer":"accept"}],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"In the currently popular field of network modeling, relatively little work has taken uncertainty of data seriously in the Bayesian sense, and component models have been introduced to the field only recently, by treating each node as a bag of out-going links.","_input_hash":-2045461051,"_task_hash":1359431544,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"the","start":3,"end":6,"id":1},{"text":"currently","start":7,"end":16,"id":2},{"text":"popular","start":17,"end":24,"id":3},{"text":"field","start":25,"end":30,"id":4},{"text":"of","start":31,"end":33,"id":5},{"text":"network","start":34,"end":41,"id":6},{"text":"modeling","start":42,"end":50,"id":7},{"text":",","start":50,"end":51,"id":8},{"text":"relatively","start":52,"end":62,"id":9},{"text":"little","start":63,"end":69,"id":10},{"text":"work","start":70,"end":74,"id":11},{"text":"has","start":75,"end":78,"id":12},{"text":"taken","start":79,"end":84,"id":13},{"text":"uncertainty","start":85,"end":96,"id":14},{"text":"of","start":97,"end":99,"id":15},{"text":"data","start":100,"end":104,"id":16},{"text":"seriously","start":105,"end":114,"id":17},{"text":"in","start":115,"end":117,"id":18},{"text":"the","start":118,"end":121,"id":19},{"text":"Bayesian","start":122,"end":130,"id":20},{"text":"sense","start":131,"end":136,"id":21},{"text":",","start":136,"end":137,"id":22},{"text":"and","start":138,"end":141,"id":23},{"text":"component","start":142,"end":151,"id":24},{"text":"models","start":152,"end":158,"id":25},{"text":"have","start":159,"end":163,"id":26},{"text":"been","start":164,"end":168,"id":27},{"text":"introduced","start":169,"end":179,"id":28},{"text":"to","start":180,"end":182,"id":29},{"text":"the","start":183,"end":186,"id":30},{"text":"field","start":187,"end":192,"id":31},{"text":"only","start":193,"end":197,"id":32},{"text":"recently","start":198,"end":206,"id":33},{"text":",","start":206,"end":207,"id":34},{"text":"by","start":208,"end":210,"id":35},{"text":"treating","start":211,"end":219,"id":36},{"text":"each","start":220,"end":224,"id":37},{"text":"node","start":225,"end":229,"id":38},{"text":"as","start":230,"end":232,"id":39},{"text":"a","start":233,"end":234,"id":40},{"text":"bag","start":235,"end":238,"id":41},{"text":"of","start":239,"end":241,"id":42},{"text":"out","start":242,"end":245,"id":43},{"text":"-","start":245,"end":246,"id":44},{"text":"going","start":246,"end":251,"id":45},{"text":"links","start":252,"end":257,"id":46},{"text":".","start":257,"end":258,"id":47}],"spans":[{"start":34,"end":41,"token_start":6,"token_end":6,"label":"ALGO","answer":"accept"},{"start":142,"end":151,"token_start":24,"token_end":24,"label":"ALGO","answer":"accept"}],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"These are defined as the internal states of a system that lead to the same decision, based on a user-provided utility or pay-off function.","_input_hash":-64737787,"_task_hash":-325842168,"tokens":[{"text":"These","start":0,"end":5,"id":0},{"text":"are","start":6,"end":9,"id":1},{"text":"defined","start":10,"end":17,"id":2},{"text":"as","start":18,"end":20,"id":3},{"text":"the","start":21,"end":24,"id":4},{"text":"internal","start":25,"end":33,"id":5},{"text":"states","start":34,"end":40,"id":6},{"text":"of","start":41,"end":43,"id":7},{"text":"a","start":44,"end":45,"id":8},{"text":"system","start":46,"end":52,"id":9},{"text":"that","start":53,"end":57,"id":10},{"text":"lead","start":58,"end":62,"id":11},{"text":"to","start":63,"end":65,"id":12},{"text":"the","start":66,"end":69,"id":13},{"text":"same","start":70,"end":74,"id":14},{"text":"decision","start":75,"end":83,"id":15},{"text":",","start":83,"end":84,"id":16},{"text":"based","start":85,"end":90,"id":17},{"text":"on","start":91,"end":93,"id":18},{"text":"a","start":94,"end":95,"id":19},{"text":"user","start":96,"end":100,"id":20},{"text":"-","start":100,"end":101,"id":21},{"text":"provided","start":101,"end":109,"id":22},{"text":"utility","start":110,"end":117,"id":23},{"text":"or","start":118,"end":120,"id":24},{"text":"pay","start":121,"end":124,"id":25},{"text":"-","start":124,"end":125,"id":26},{"text":"off","start":125,"end":128,"id":27},{"text":"function","start":129,"end":137,"id":28},{"text":".","start":137,"end":138,"id":29}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"For labeled data, we introduce a method that improves the embedding during the training phase using the out-of-sample extension.","_input_hash":1108733217,"_task_hash":992685221,"tokens":[{"text":"For","start":0,"end":3,"id":0},{"text":"labeled","start":4,"end":11,"id":1},{"text":"data","start":12,"end":16,"id":2},{"text":",","start":16,"end":17,"id":3},{"text":"we","start":18,"end":20,"id":4},{"text":"introduce","start":21,"end":30,"id":5},{"text":"a","start":31,"end":32,"id":6},{"text":"method","start":33,"end":39,"id":7},{"text":"that","start":40,"end":44,"id":8},{"text":"improves","start":45,"end":53,"id":9},{"text":"the","start":54,"end":57,"id":10},{"text":"embedding","start":58,"end":67,"id":11},{"text":"during","start":68,"end":74,"id":12},{"text":"the","start":75,"end":78,"id":13},{"text":"training","start":79,"end":87,"id":14},{"text":"phase","start":88,"end":93,"id":15},{"text":"using","start":94,"end":99,"id":16},{"text":"the","start":100,"end":103,"id":17},{"text":"out","start":104,"end":107,"id":18},{"text":"-","start":107,"end":108,"id":19},{"text":"of","start":108,"end":110,"id":20},{"text":"-","start":110,"end":111,"id":21},{"text":"sample","start":111,"end":117,"id":22},{"text":"extension","start":118,"end":127,"id":23},{"text":".","start":127,"end":128,"id":24}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"The results hold universally for any optimization problem on a bounded domain and establish a connection between simulated annealing and up-to-date theory of convergence of Markov chain Monte Carlo methods on continuous domains.","_input_hash":-1859510931,"_task_hash":1327250912,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"results","start":4,"end":11,"id":1},{"text":"hold","start":12,"end":16,"id":2},{"text":"universally","start":17,"end":28,"id":3},{"text":"for","start":29,"end":32,"id":4},{"text":"any","start":33,"end":36,"id":5},{"text":"optimization","start":37,"end":49,"id":6},{"text":"problem","start":50,"end":57,"id":7},{"text":"on","start":58,"end":60,"id":8},{"text":"a","start":61,"end":62,"id":9},{"text":"bounded","start":63,"end":70,"id":10},{"text":"domain","start":71,"end":77,"id":11},{"text":"and","start":78,"end":81,"id":12},{"text":"establish","start":82,"end":91,"id":13},{"text":"a","start":92,"end":93,"id":14},{"text":"connection","start":94,"end":104,"id":15},{"text":"between","start":105,"end":112,"id":16},{"text":"simulated","start":113,"end":122,"id":17},{"text":"annealing","start":123,"end":132,"id":18},{"text":"and","start":133,"end":136,"id":19},{"text":"up","start":137,"end":139,"id":20},{"text":"-","start":139,"end":140,"id":21},{"text":"to","start":140,"end":142,"id":22},{"text":"-","start":142,"end":143,"id":23},{"text":"date","start":143,"end":147,"id":24},{"text":"theory","start":148,"end":154,"id":25},{"text":"of","start":155,"end":157,"id":26},{"text":"convergence","start":158,"end":169,"id":27},{"text":"of","start":170,"end":172,"id":28},{"text":"Markov","start":173,"end":179,"id":29},{"text":"chain","start":180,"end":185,"id":30},{"text":"Monte","start":186,"end":191,"id":31},{"text":"Carlo","start":192,"end":197,"id":32},{"text":"methods","start":198,"end":205,"id":33},{"text":"on","start":206,"end":208,"id":34},{"text":"continuous","start":209,"end":219,"id":35},{"text":"domains","start":220,"end":227,"id":36},{"text":".","start":227,"end":228,"id":37}],"spans":[{"token_start":17,"token_end":18,"start":113,"end":132,"text":"simulated annealing","label":"ALGO","source":"./algo_model5","input_hash":-1859510931,"answer":"accept"},{"token_start":29,"token_end":32,"start":173,"end":197,"text":"Markov chain Monte Carlo","label":"ALGO","source":"./algo_model5","input_hash":-1859510931,"answer":"accept"}],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"We construct a framework for studying clustering algorithms, which includes two key ideas:","_input_hash":748612535,"_task_hash":-1230786697,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"construct","start":3,"end":12,"id":1},{"text":"a","start":13,"end":14,"id":2},{"text":"framework","start":15,"end":24,"id":3},{"text":"for","start":25,"end":28,"id":4},{"text":"studying","start":29,"end":37,"id":5},{"text":"clustering","start":38,"end":48,"id":6},{"text":"algorithms","start":49,"end":59,"id":7},{"text":",","start":59,"end":60,"id":8},{"text":"which","start":61,"end":66,"id":9},{"text":"includes","start":67,"end":75,"id":10},{"text":"two","start":76,"end":79,"id":11},{"text":"key","start":80,"end":83,"id":12},{"text":"ideas","start":84,"end":89,"id":13},{"text":":","start":89,"end":90,"id":14}],"spans":[{"token_start":6,"token_end":6,"start":38,"end":48,"text":"clustering","label":"ALGO","source":"./algo_model5","input_hash":748612535,"answer":"accept"}],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"We present some evidence to show that BKNN still significantly underestimates model uncertainty.","_input_hash":-437162734,"_task_hash":-314923861,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"present","start":3,"end":10,"id":1},{"text":"some","start":11,"end":15,"id":2},{"text":"evidence","start":16,"end":24,"id":3},{"text":"to","start":25,"end":27,"id":4},{"text":"show","start":28,"end":32,"id":5},{"text":"that","start":33,"end":37,"id":6},{"text":"BKNN","start":38,"end":42,"id":7},{"text":"still","start":43,"end":48,"id":8},{"text":"significantly","start":49,"end":62,"id":9},{"text":"underestimates","start":63,"end":77,"id":10},{"text":"model","start":78,"end":83,"id":11},{"text":"uncertainty","start":84,"end":95,"id":12},{"text":".","start":95,"end":96,"id":13}],"spans":[{"token_start":7,"token_end":7,"start":38,"end":42,"text":"BKNN","label":"ALGO","source":"./algo_model5","input_hash":-437162734,"answer":"accept"}],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"In a complex systems perspective, the decisional states are thus the \"emerging\" patterns corresponding to the utility function.","_input_hash":1018471510,"_task_hash":1819015503,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"a","start":3,"end":4,"id":1},{"text":"complex","start":5,"end":12,"id":2},{"text":"systems","start":13,"end":20,"id":3},{"text":"perspective","start":21,"end":32,"id":4},{"text":",","start":32,"end":33,"id":5},{"text":"the","start":34,"end":37,"id":6},{"text":"decisional","start":38,"end":48,"id":7},{"text":"states","start":49,"end":55,"id":8},{"text":"are","start":56,"end":59,"id":9},{"text":"thus","start":60,"end":64,"id":10},{"text":"the","start":65,"end":68,"id":11},{"text":"\"","start":69,"end":70,"id":12},{"text":"emerging","start":70,"end":78,"id":13},{"text":"\"","start":78,"end":79,"id":14},{"text":"patterns","start":80,"end":88,"id":15},{"text":"corresponding","start":89,"end":102,"id":16},{"text":"to","start":103,"end":105,"id":17},{"text":"the","start":106,"end":109,"id":18},{"text":"utility","start":110,"end":117,"id":19},{"text":"function","start":118,"end":126,"id":20},{"text":".","start":126,"end":127,"id":21}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"Finally we review briefly the construction of Support Vector Machines and show how to derive generalization bounds for them, measuring the complexity either through the number of support vectors or through the value of the transductive or inductive margin.","_input_hash":1340001408,"_task_hash":-1861263665,"tokens":[{"text":"Finally","start":0,"end":7,"id":0},{"text":"we","start":8,"end":10,"id":1},{"text":"review","start":11,"end":17,"id":2},{"text":"briefly","start":18,"end":25,"id":3},{"text":"the","start":26,"end":29,"id":4},{"text":"construction","start":30,"end":42,"id":5},{"text":"of","start":43,"end":45,"id":6},{"text":"Support","start":46,"end":53,"id":7},{"text":"Vector","start":54,"end":60,"id":8},{"text":"Machines","start":61,"end":69,"id":9},{"text":"and","start":70,"end":73,"id":10},{"text":"show","start":74,"end":78,"id":11},{"text":"how","start":79,"end":82,"id":12},{"text":"to","start":83,"end":85,"id":13},{"text":"derive","start":86,"end":92,"id":14},{"text":"generalization","start":93,"end":107,"id":15},{"text":"bounds","start":108,"end":114,"id":16},{"text":"for","start":115,"end":118,"id":17},{"text":"them","start":119,"end":123,"id":18},{"text":",","start":123,"end":124,"id":19},{"text":"measuring","start":125,"end":134,"id":20},{"text":"the","start":135,"end":138,"id":21},{"text":"complexity","start":139,"end":149,"id":22},{"text":"either","start":150,"end":156,"id":23},{"text":"through","start":157,"end":164,"id":24},{"text":"the","start":165,"end":168,"id":25},{"text":"number","start":169,"end":175,"id":26},{"text":"of","start":176,"end":178,"id":27},{"text":"support","start":179,"end":186,"id":28},{"text":"vectors","start":187,"end":194,"id":29},{"text":"or","start":195,"end":197,"id":30},{"text":"through","start":198,"end":205,"id":31},{"text":"the","start":206,"end":209,"id":32},{"text":"value","start":210,"end":215,"id":33},{"text":"of","start":216,"end":218,"id":34},{"text":"the","start":219,"end":222,"id":35},{"text":"transductive","start":223,"end":235,"id":36},{"text":"or","start":236,"end":238,"id":37},{"text":"inductive","start":239,"end":248,"id":38},{"text":"margin","start":249,"end":255,"id":39},{"text":".","start":255,"end":256,"id":40}],"spans":[{"token_start":7,"token_end":9,"start":46,"end":69,"text":"Support Vector Machines","label":"ALGO","source":"./algo_model5","input_hash":1340001408,"answer":"accept"}],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"While frequentist methods have yielded online filtering and prediction techniques, most Bayesian papers have focused on the retrospective segmentation problem.","_input_hash":-1258359525,"_task_hash":121497497,"tokens":[{"text":"While","start":0,"end":5,"id":0},{"text":"frequentist","start":6,"end":17,"id":1},{"text":"methods","start":18,"end":25,"id":2},{"text":"have","start":26,"end":30,"id":3},{"text":"yielded","start":31,"end":38,"id":4},{"text":"online","start":39,"end":45,"id":5},{"text":"filtering","start":46,"end":55,"id":6},{"text":"and","start":56,"end":59,"id":7},{"text":"prediction","start":60,"end":70,"id":8},{"text":"techniques","start":71,"end":81,"id":9},{"text":",","start":81,"end":82,"id":10},{"text":"most","start":83,"end":87,"id":11},{"text":"Bayesian","start":88,"end":96,"id":12},{"text":"papers","start":97,"end":103,"id":13},{"text":"have","start":104,"end":108,"id":14},{"text":"focused","start":109,"end":116,"id":15},{"text":"on","start":117,"end":119,"id":16},{"text":"the","start":120,"end":123,"id":17},{"text":"retrospective","start":124,"end":137,"id":18},{"text":"segmentation","start":138,"end":150,"id":19},{"text":"problem","start":151,"end":158,"id":20},{"text":".","start":158,"end":159,"id":21}],"spans":[{"token_start":1,"token_end":1,"start":6,"end":17,"text":"frequentist","label":"ALGO","source":"./algo_model5","input_hash":-1258359525,"answer":"accept"}],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"For unlabeled data, we introduce a method of embedding a new point as preprocessing to a classifier.","_input_hash":1850829439,"_task_hash":963755807,"tokens":[{"text":"For","start":0,"end":3,"id":0},{"text":"unlabeled","start":4,"end":13,"id":1},{"text":"data","start":14,"end":18,"id":2},{"text":",","start":18,"end":19,"id":3},{"text":"we","start":20,"end":22,"id":4},{"text":"introduce","start":23,"end":32,"id":5},{"text":"a","start":33,"end":34,"id":6},{"text":"method","start":35,"end":41,"id":7},{"text":"of","start":42,"end":44,"id":8},{"text":"embedding","start":45,"end":54,"id":9},{"text":"a","start":55,"end":56,"id":10},{"text":"new","start":57,"end":60,"id":11},{"text":"point","start":61,"end":66,"id":12},{"text":"as","start":67,"end":69,"id":13},{"text":"preprocessing","start":70,"end":83,"id":14},{"text":"to","start":84,"end":86,"id":15},{"text":"a","start":87,"end":88,"id":16},{"text":"classifier","start":89,"end":99,"id":17},{"text":".","start":99,"end":100,"id":18}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"We apply this compression method to logistic sequence prediction models and logistic classification models.","_input_hash":-1491980493,"_task_hash":-1777951365,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"apply","start":3,"end":8,"id":1},{"text":"this","start":9,"end":13,"id":2},{"text":"compression","start":14,"end":25,"id":3},{"text":"method","start":26,"end":32,"id":4},{"text":"to","start":33,"end":35,"id":5},{"text":"logistic","start":36,"end":44,"id":6},{"text":"sequence","start":45,"end":53,"id":7},{"text":"prediction","start":54,"end":64,"id":8},{"text":"models","start":65,"end":71,"id":9},{"text":"and","start":72,"end":75,"id":10},{"text":"logistic","start":76,"end":84,"id":11},{"text":"classification","start":85,"end":99,"id":12},{"text":"models","start":100,"end":106,"id":13},{"text":".","start":106,"end":107,"id":14}],"spans":[{"token_start":6,"token_end":8,"start":36,"end":64,"text":"logistic sequence prediction","label":"ALGO","source":"./algo_model5","input_hash":-1491980493,"answer":"accept"},{"token_start":11,"token_end":12,"start":76,"end":99,"text":"logistic classification","label":"ALGO","source":"./algo_model5","input_hash":-1491980493,"answer":"accept"}],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"For graphs generated from a stochastic algorithm designed to model community structure, the optimal information theoretic partition and the optimal min-cut partition are shown to be the same with high probability.","_input_hash":1434805796,"_task_hash":829451928,"tokens":[{"text":"For","start":0,"end":3,"id":0},{"text":"graphs","start":4,"end":10,"id":1},{"text":"generated","start":11,"end":20,"id":2},{"text":"from","start":21,"end":25,"id":3},{"text":"a","start":26,"end":27,"id":4},{"text":"stochastic","start":28,"end":38,"id":5},{"text":"algorithm","start":39,"end":48,"id":6},{"text":"designed","start":49,"end":57,"id":7},{"text":"to","start":58,"end":60,"id":8},{"text":"model","start":61,"end":66,"id":9},{"text":"community","start":67,"end":76,"id":10},{"text":"structure","start":77,"end":86,"id":11},{"text":",","start":86,"end":87,"id":12},{"text":"the","start":88,"end":91,"id":13},{"text":"optimal","start":92,"end":99,"id":14},{"text":"information","start":100,"end":111,"id":15},{"text":"theoretic","start":112,"end":121,"id":16},{"text":"partition","start":122,"end":131,"id":17},{"text":"and","start":132,"end":135,"id":18},{"text":"the","start":136,"end":139,"id":19},{"text":"optimal","start":140,"end":147,"id":20},{"text":"min","start":148,"end":151,"id":21},{"text":"-","start":151,"end":152,"id":22},{"text":"cut","start":152,"end":155,"id":23},{"text":"partition","start":156,"end":165,"id":24},{"text":"are","start":166,"end":169,"id":25},{"text":"shown","start":170,"end":175,"id":26},{"text":"to","start":176,"end":178,"id":27},{"text":"be","start":179,"end":181,"id":28},{"text":"the","start":182,"end":185,"id":29},{"text":"same","start":186,"end":190,"id":30},{"text":"with","start":191,"end":195,"id":31},{"text":"high","start":196,"end":200,"id":32},{"text":"probability","start":201,"end":212,"id":33},{"text":".","start":212,"end":213,"id":34}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"The local linear embedding algorithm (LLE) is a non-linear dimension-reducing technique, widely used due to its computational simplicity and intuitive approach.","_input_hash":-1398033812,"_task_hash":-1622484973,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"local","start":4,"end":9,"id":1},{"text":"linear","start":10,"end":16,"id":2},{"text":"embedding","start":17,"end":26,"id":3},{"text":"algorithm","start":27,"end":36,"id":4},{"text":"(","start":37,"end":38,"id":5},{"text":"LLE","start":38,"end":41,"id":6},{"text":")","start":41,"end":42,"id":7},{"text":"is","start":43,"end":45,"id":8},{"text":"a","start":46,"end":47,"id":9},{"text":"non","start":48,"end":51,"id":10},{"text":"-","start":51,"end":52,"id":11},{"text":"linear","start":52,"end":58,"id":12},{"text":"dimension","start":59,"end":68,"id":13},{"text":"-","start":68,"end":69,"id":14},{"text":"reducing","start":69,"end":77,"id":15},{"text":"technique","start":78,"end":87,"id":16},{"text":",","start":87,"end":88,"id":17},{"text":"widely","start":89,"end":95,"id":18},{"text":"used","start":96,"end":100,"id":19},{"text":"due","start":101,"end":104,"id":20},{"text":"to","start":105,"end":107,"id":21},{"text":"its","start":108,"end":111,"id":22},{"text":"computational","start":112,"end":125,"id":23},{"text":"simplicity","start":126,"end":136,"id":24},{"text":"and","start":137,"end":140,"id":25},{"text":"intuitive","start":141,"end":150,"id":26},{"text":"approach","start":151,"end":159,"id":27},{"text":".","start":159,"end":160,"id":28}],"spans":[{"token_start":1,"token_end":3,"start":4,"end":26,"text":"local linear embedding","label":"ALGO","source":"./algo_model5","input_hash":-1398033812,"answer":"accept"}],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"Using the Kullback-Leibler divergence as a measure of generalisation error we draw learning curves in simplified situations.","_input_hash":1258501792,"_task_hash":-828622494,"tokens":[{"text":"Using","start":0,"end":5,"id":0},{"text":"the","start":6,"end":9,"id":1},{"text":"Kullback","start":10,"end":18,"id":2},{"text":"-","start":18,"end":19,"id":3},{"text":"Leibler","start":19,"end":26,"id":4},{"text":"divergence","start":27,"end":37,"id":5},{"text":"as","start":38,"end":40,"id":6},{"text":"a","start":41,"end":42,"id":7},{"text":"measure","start":43,"end":50,"id":8},{"text":"of","start":51,"end":53,"id":9},{"text":"generalisation","start":54,"end":68,"id":10},{"text":"error","start":69,"end":74,"id":11},{"text":"we","start":75,"end":77,"id":12},{"text":"draw","start":78,"end":82,"id":13},{"text":"learning","start":83,"end":91,"id":14},{"text":"curves","start":92,"end":98,"id":15},{"text":"in","start":99,"end":101,"id":16},{"text":"simplified","start":102,"end":112,"id":17},{"text":"situations","start":113,"end":123,"id":18},{"text":".","start":123,"end":124,"id":19}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"Asymptotic null distributions under null hypothesis are derived, and consistency against fixed and local alternatives is assessed.","_input_hash":1669956228,"_task_hash":967005207,"tokens":[{"text":"Asymptotic","start":0,"end":10,"id":0},{"text":"null","start":11,"end":15,"id":1},{"text":"distributions","start":16,"end":29,"id":2},{"text":"under","start":30,"end":35,"id":3},{"text":"null","start":36,"end":40,"id":4},{"text":"hypothesis","start":41,"end":51,"id":5},{"text":"are","start":52,"end":55,"id":6},{"text":"derived","start":56,"end":63,"id":7},{"text":",","start":63,"end":64,"id":8},{"text":"and","start":65,"end":68,"id":9},{"text":"consistency","start":69,"end":80,"id":10},{"text":"against","start":81,"end":88,"id":11},{"text":"fixed","start":89,"end":94,"id":12},{"text":"and","start":95,"end":98,"id":13},{"text":"local","start":99,"end":104,"id":14},{"text":"alternatives","start":105,"end":117,"id":15},{"text":"is","start":118,"end":120,"id":16},{"text":"assessed","start":121,"end":129,"id":17},{"text":".","start":129,"end":130,"id":18}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"We illustrate this modularity by demonstrating the algorithm on three different real-world data sets.","_input_hash":-995745589,"_task_hash":-439661184,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"illustrate","start":3,"end":13,"id":1},{"text":"this","start":14,"end":18,"id":2},{"text":"modularity","start":19,"end":29,"id":3},{"text":"by","start":30,"end":32,"id":4},{"text":"demonstrating","start":33,"end":46,"id":5},{"text":"the","start":47,"end":50,"id":6},{"text":"algorithm","start":51,"end":60,"id":7},{"text":"on","start":61,"end":63,"id":8},{"text":"three","start":64,"end":69,"id":9},{"text":"different","start":70,"end":79,"id":10},{"text":"real","start":80,"end":84,"id":11},{"text":"-","start":84,"end":85,"id":12},{"text":"world","start":85,"end":90,"id":13},{"text":"data","start":91,"end":95,"id":14},{"text":"sets","start":96,"end":100,"id":15},{"text":".","start":100,"end":101,"id":16}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"Specifically, we develop a James-Stein-type shrinkage estimator, resulting in a procedure that is highly efficient statistically as well as computationally.","_input_hash":-358073182,"_task_hash":1281824272,"tokens":[{"text":"Specifically","start":0,"end":12,"id":0},{"text":",","start":12,"end":13,"id":1},{"text":"we","start":14,"end":16,"id":2},{"text":"develop","start":17,"end":24,"id":3},{"text":"a","start":25,"end":26,"id":4},{"text":"James","start":27,"end":32,"id":5},{"text":"-","start":32,"end":33,"id":6},{"text":"Stein","start":33,"end":38,"id":7},{"text":"-","start":38,"end":39,"id":8},{"text":"type","start":39,"end":43,"id":9},{"text":"shrinkage","start":44,"end":53,"id":10},{"text":"estimator","start":54,"end":63,"id":11},{"text":",","start":63,"end":64,"id":12},{"text":"resulting","start":65,"end":74,"id":13},{"text":"in","start":75,"end":77,"id":14},{"text":"a","start":78,"end":79,"id":15},{"text":"procedure","start":80,"end":89,"id":16},{"text":"that","start":90,"end":94,"id":17},{"text":"is","start":95,"end":97,"id":18},{"text":"highly","start":98,"end":104,"id":19},{"text":"efficient","start":105,"end":114,"id":20},{"text":"statistically","start":115,"end":128,"id":21},{"text":"as","start":129,"end":131,"id":22},{"text":"well","start":132,"end":136,"id":23},{"text":"as","start":137,"end":139,"id":24},{"text":"computationally","start":140,"end":155,"id":25},{"text":".","start":155,"end":156,"id":26}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"The runtime for Kernel Partial Least Squares (KPLS) to compute the fit is quadratic in the number of examples.","_input_hash":-1440221630,"_task_hash":925807616,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"runtime","start":4,"end":11,"id":1},{"text":"for","start":12,"end":15,"id":2},{"text":"Kernel","start":16,"end":22,"id":3},{"text":"Partial","start":23,"end":30,"id":4},{"text":"Least","start":31,"end":36,"id":5},{"text":"Squares","start":37,"end":44,"id":6},{"text":"(","start":45,"end":46,"id":7},{"text":"KPLS","start":46,"end":50,"id":8},{"text":")","start":50,"end":51,"id":9},{"text":"to","start":52,"end":54,"id":10},{"text":"compute","start":55,"end":62,"id":11},{"text":"the","start":63,"end":66,"id":12},{"text":"fit","start":67,"end":70,"id":13},{"text":"is","start":71,"end":73,"id":14},{"text":"quadratic","start":74,"end":83,"id":15},{"text":"in","start":84,"end":86,"id":16},{"text":"the","start":87,"end":90,"id":17},{"text":"number","start":91,"end":97,"id":18},{"text":"of","start":98,"end":100,"id":19},{"text":"examples","start":101,"end":109,"id":20},{"text":".","start":109,"end":110,"id":21}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"We propose a new sparsity-smoothness penalty for high-dimensional generalized additive models.","_input_hash":1320253224,"_task_hash":1340210176,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"propose","start":3,"end":10,"id":1},{"text":"a","start":11,"end":12,"id":2},{"text":"new","start":13,"end":16,"id":3},{"text":"sparsity","start":17,"end":25,"id":4},{"text":"-","start":25,"end":26,"id":5},{"text":"smoothness","start":26,"end":36,"id":6},{"text":"penalty","start":37,"end":44,"id":7},{"text":"for","start":45,"end":48,"id":8},{"text":"high","start":49,"end":53,"id":9},{"text":"-","start":53,"end":54,"id":10},{"text":"dimensional","start":54,"end":65,"id":11},{"text":"generalized","start":66,"end":77,"id":12},{"text":"additive","start":78,"end":86,"id":13},{"text":"models","start":87,"end":93,"id":14},{"text":".","start":93,"end":94,"id":15}],"spans":[{"start":49,"end":86,"token_start":9,"token_end":13,"label":"ALGO","answer":"accept"}],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"We illustrate the approach by analyzing E. coli gene expression data and computing an entropy-based gene-association network from gene expression data.","_input_hash":-1542246212,"_task_hash":1128590524,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"illustrate","start":3,"end":13,"id":1},{"text":"the","start":14,"end":17,"id":2},{"text":"approach","start":18,"end":26,"id":3},{"text":"by","start":27,"end":29,"id":4},{"text":"analyzing","start":30,"end":39,"id":5},{"text":"E.","start":40,"end":42,"id":6},{"text":"coli","start":43,"end":47,"id":7},{"text":"gene","start":48,"end":52,"id":8},{"text":"expression","start":53,"end":63,"id":9},{"text":"data","start":64,"end":68,"id":10},{"text":"and","start":69,"end":72,"id":11},{"text":"computing","start":73,"end":82,"id":12},{"text":"an","start":83,"end":85,"id":13},{"text":"entropy","start":86,"end":93,"id":14},{"text":"-","start":93,"end":94,"id":15},{"text":"based","start":94,"end":99,"id":16},{"text":"gene","start":100,"end":104,"id":17},{"text":"-","start":104,"end":105,"id":18},{"text":"association","start":105,"end":116,"id":19},{"text":"network","start":117,"end":124,"id":20},{"text":"from","start":125,"end":129,"id":21},{"text":"gene","start":130,"end":134,"id":22},{"text":"expression","start":135,"end":145,"id":23},{"text":"data","start":146,"end":150,"id":24},{"text":".","start":150,"end":151,"id":25}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"We analyze the performance of a class of manifold-learning algorithms that find their output by minimizing a quadratic form under some normalization constraints.","_input_hash":1489635198,"_task_hash":1221246853,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"analyze","start":3,"end":10,"id":1},{"text":"the","start":11,"end":14,"id":2},{"text":"performance","start":15,"end":26,"id":3},{"text":"of","start":27,"end":29,"id":4},{"text":"a","start":30,"end":31,"id":5},{"text":"class","start":32,"end":37,"id":6},{"text":"of","start":38,"end":40,"id":7},{"text":"manifold","start":41,"end":49,"id":8},{"text":"-","start":49,"end":50,"id":9},{"text":"learning","start":50,"end":58,"id":10},{"text":"algorithms","start":59,"end":69,"id":11},{"text":"that","start":70,"end":74,"id":12},{"text":"find","start":75,"end":79,"id":13},{"text":"their","start":80,"end":85,"id":14},{"text":"output","start":86,"end":92,"id":15},{"text":"by","start":93,"end":95,"id":16},{"text":"minimizing","start":96,"end":106,"id":17},{"text":"a","start":107,"end":108,"id":18},{"text":"quadratic","start":109,"end":118,"id":19},{"text":"form","start":119,"end":123,"id":20},{"text":"under","start":124,"end":129,"id":21},{"text":"some","start":130,"end":134,"id":22},{"text":"normalization","start":135,"end":148,"id":23},{"text":"constraints","start":149,"end":160,"id":24},{"text":".","start":160,"end":161,"id":25}],"spans":[{"token_start":8,"token_end":10,"start":41,"end":58,"text":"manifold-learning","label":"ALGO","source":"./algo_model5","input_hash":1489635198,"answer":"accept"}],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"A simple and computationally efficient scheme for tree-structured vector quantization is presented.","_input_hash":240911377,"_task_hash":689423020,"tokens":[{"text":"A","start":0,"end":1,"id":0},{"text":"simple","start":2,"end":8,"id":1},{"text":"and","start":9,"end":12,"id":2},{"text":"computationally","start":13,"end":28,"id":3},{"text":"efficient","start":29,"end":38,"id":4},{"text":"scheme","start":39,"end":45,"id":5},{"text":"for","start":46,"end":49,"id":6},{"text":"tree","start":50,"end":54,"id":7},{"text":"-","start":54,"end":55,"id":8},{"text":"structured","start":55,"end":65,"id":9},{"text":"vector","start":66,"end":72,"id":10},{"text":"quantization","start":73,"end":85,"id":11},{"text":"is","start":86,"end":88,"id":12},{"text":"presented","start":89,"end":98,"id":13},{"text":".","start":98,"end":99,"id":14}],"spans":[],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"The first encodes the idea that the output of a clustering scheme should carry a multiresolution structure, the second the idea that one should be able to compare the results of clustering algorithms as one varies the data set, for example by adding points or by applying functions to it.","_input_hash":-595449267,"_task_hash":1798432803,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"first","start":4,"end":9,"id":1},{"text":"encodes","start":10,"end":17,"id":2},{"text":"the","start":18,"end":21,"id":3},{"text":"idea","start":22,"end":26,"id":4},{"text":"that","start":27,"end":31,"id":5},{"text":"the","start":32,"end":35,"id":6},{"text":"output","start":36,"end":42,"id":7},{"text":"of","start":43,"end":45,"id":8},{"text":"a","start":46,"end":47,"id":9},{"text":"clustering","start":48,"end":58,"id":10},{"text":"scheme","start":59,"end":65,"id":11},{"text":"should","start":66,"end":72,"id":12},{"text":"carry","start":73,"end":78,"id":13},{"text":"a","start":79,"end":80,"id":14},{"text":"multiresolution","start":81,"end":96,"id":15},{"text":"structure","start":97,"end":106,"id":16},{"text":",","start":106,"end":107,"id":17},{"text":"the","start":108,"end":111,"id":18},{"text":"second","start":112,"end":118,"id":19},{"text":"the","start":119,"end":122,"id":20},{"text":"idea","start":123,"end":127,"id":21},{"text":"that","start":128,"end":132,"id":22},{"text":"one","start":133,"end":136,"id":23},{"text":"should","start":137,"end":143,"id":24},{"text":"be","start":144,"end":146,"id":25},{"text":"able","start":147,"end":151,"id":26},{"text":"to","start":152,"end":154,"id":27},{"text":"compare","start":155,"end":162,"id":28},{"text":"the","start":163,"end":166,"id":29},{"text":"results","start":167,"end":174,"id":30},{"text":"of","start":175,"end":177,"id":31},{"text":"clustering","start":178,"end":188,"id":32},{"text":"algorithms","start":189,"end":199,"id":33},{"text":"as","start":200,"end":202,"id":34},{"text":"one","start":203,"end":206,"id":35},{"text":"varies","start":207,"end":213,"id":36},{"text":"the","start":214,"end":217,"id":37},{"text":"data","start":218,"end":222,"id":38},{"text":"set","start":223,"end":226,"id":39},{"text":",","start":226,"end":227,"id":40},{"text":"for","start":228,"end":231,"id":41},{"text":"example","start":232,"end":239,"id":42},{"text":"by","start":240,"end":242,"id":43},{"text":"adding","start":243,"end":249,"id":44},{"text":"points","start":250,"end":256,"id":45},{"text":"or","start":257,"end":259,"id":46},{"text":"by","start":260,"end":262,"id":47},{"text":"applying","start":263,"end":271,"id":48},{"text":"functions","start":272,"end":281,"id":49},{"text":"to","start":282,"end":284,"id":50},{"text":"it","start":285,"end":287,"id":51},{"text":".","start":287,"end":288,"id":52}],"spans":[{"token_start":32,"token_end":32,"start":178,"end":188,"text":"clustering","label":"ALGO","source":"./algo_model5","input_hash":-595449267,"answer":"accept"}],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
{"text":"Genetic algorithm is implemented for network optimization and estimating the missing data.","_input_hash":-1636818188,"_task_hash":1089166523,"tokens":[{"text":"Genetic","start":0,"end":7,"id":0},{"text":"algorithm","start":8,"end":17,"id":1},{"text":"is","start":18,"end":20,"id":2},{"text":"implemented","start":21,"end":32,"id":3},{"text":"for","start":33,"end":36,"id":4},{"text":"network","start":37,"end":44,"id":5},{"text":"optimization","start":45,"end":57,"id":6},{"text":"and","start":58,"end":61,"id":7},{"text":"estimating","start":62,"end":72,"id":8},{"text":"the","start":73,"end":76,"id":9},{"text":"missing","start":77,"end":84,"id":10},{"text":"data","start":85,"end":89,"id":11},{"text":".","start":89,"end":90,"id":12}],"spans":[{"token_start":0,"token_end":0,"start":0,"end":7,"text":"Genetic","label":"ALGO","source":"./algo_model5","input_hash":-1636818188,"answer":"accept"}],"_session_id":null,"_view_id":"ner_manual","answer":"accept"}
