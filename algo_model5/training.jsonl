{"text":"The Minimum Description Length (MDL) principle states that the optimal model for a given data set is that which compresses it best.","_input_hash":-1032037555,"_task_hash":83716215,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"Minimum","start":4,"end":11,"id":1},{"text":"Description","start":12,"end":23,"id":2},{"text":"Length","start":24,"end":30,"id":3},{"text":"(","start":31,"end":32,"id":4},{"text":"MDL","start":32,"end":35,"id":5},{"text":")","start":35,"end":36,"id":6},{"text":"principle","start":37,"end":46,"id":7},{"text":"states","start":47,"end":53,"id":8},{"text":"that","start":54,"end":58,"id":9},{"text":"the","start":59,"end":62,"id":10},{"text":"optimal","start":63,"end":70,"id":11},{"text":"model","start":71,"end":76,"id":12},{"text":"for","start":77,"end":80,"id":13},{"text":"a","start":81,"end":82,"id":14},{"text":"given","start":83,"end":88,"id":15},{"text":"data","start":89,"end":93,"id":16},{"text":"set","start":94,"end":97,"id":17},{"text":"is","start":98,"end":100,"id":18},{"text":"that","start":101,"end":105,"id":19},{"text":"which","start":106,"end":111,"id":20},{"text":"compresses","start":112,"end":122,"id":21},{"text":"it","start":123,"end":125,"id":22},{"text":"best","start":126,"end":130,"id":23},{"text":".","start":130,"end":131,"id":24}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Using the moment generating function technique, we further provide a lower bound for $k$ using pairwise $L_2$ distances in the space of points to be projected and pairwise $L_1$ distances in the space of the projected points.","_input_hash":823121753,"_task_hash":-1897836433,"tokens":[{"text":"Using","start":0,"end":5,"id":0},{"text":"the","start":6,"end":9,"id":1},{"text":"moment","start":10,"end":16,"id":2},{"text":"generating","start":17,"end":27,"id":3},{"text":"function","start":28,"end":36,"id":4},{"text":"technique","start":37,"end":46,"id":5},{"text":",","start":46,"end":47,"id":6},{"text":"we","start":48,"end":50,"id":7},{"text":"further","start":51,"end":58,"id":8},{"text":"provide","start":59,"end":66,"id":9},{"text":"a","start":67,"end":68,"id":10},{"text":"lower","start":69,"end":74,"id":11},{"text":"bound","start":75,"end":80,"id":12},{"text":"for","start":81,"end":84,"id":13},{"text":"$","start":85,"end":86,"id":14},{"text":"k$","start":86,"end":88,"id":15},{"text":"using","start":89,"end":94,"id":16},{"text":"pairwise","start":95,"end":103,"id":17},{"text":"$","start":104,"end":105,"id":18},{"text":"L_2","start":105,"end":108,"id":19},{"text":"$","start":108,"end":109,"id":20},{"text":"distances","start":110,"end":119,"id":21},{"text":"in","start":120,"end":122,"id":22},{"text":"the","start":123,"end":126,"id":23},{"text":"space","start":127,"end":132,"id":24},{"text":"of","start":133,"end":135,"id":25},{"text":"points","start":136,"end":142,"id":26},{"text":"to","start":143,"end":145,"id":27},{"text":"be","start":146,"end":148,"id":28},{"text":"projected","start":149,"end":158,"id":29},{"text":"and","start":159,"end":162,"id":30},{"text":"pairwise","start":163,"end":171,"id":31},{"text":"$","start":172,"end":173,"id":32},{"text":"L_1","start":173,"end":176,"id":33},{"text":"$","start":176,"end":177,"id":34},{"text":"distances","start":178,"end":187,"id":35},{"text":"in","start":188,"end":190,"id":36},{"text":"the","start":191,"end":194,"id":37},{"text":"space","start":195,"end":200,"id":38},{"text":"of","start":201,"end":203,"id":39},{"text":"the","start":204,"end":207,"id":40},{"text":"projected","start":208,"end":217,"id":41},{"text":"points","start":218,"end":224,"id":42},{"text":".","start":224,"end":225,"id":43}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"We illustrate the topic presentations on corpora of scientific abstracts and news articles.","_input_hash":180928036,"_task_hash":-1441474605,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"illustrate","start":3,"end":13,"id":1},{"text":"the","start":14,"end":17,"id":2},{"text":"topic","start":18,"end":23,"id":3},{"text":"presentations","start":24,"end":37,"id":4},{"text":"on","start":38,"end":40,"id":5},{"text":"corpora","start":41,"end":48,"id":6},{"text":"of","start":49,"end":51,"id":7},{"text":"scientific","start":52,"end":62,"id":8},{"text":"abstracts","start":63,"end":72,"id":9},{"text":"and","start":73,"end":76,"id":10},{"text":"news","start":77,"end":81,"id":11},{"text":"articles","start":82,"end":90,"id":12},{"text":".","start":90,"end":91,"id":13}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Network models have been popular for modeling and representing complex relationships and dependencies between observed variables.","_input_hash":699820024,"_task_hash":-1654158537,"tokens":[{"text":"Network","start":0,"end":7,"id":0},{"text":"models","start":8,"end":14,"id":1},{"text":"have","start":15,"end":19,"id":2},{"text":"been","start":20,"end":24,"id":3},{"text":"popular","start":25,"end":32,"id":4},{"text":"for","start":33,"end":36,"id":5},{"text":"modeling","start":37,"end":45,"id":6},{"text":"and","start":46,"end":49,"id":7},{"text":"representing","start":50,"end":62,"id":8},{"text":"complex","start":63,"end":70,"id":9},{"text":"relationships","start":71,"end":84,"id":10},{"text":"and","start":85,"end":88,"id":11},{"text":"dependencies","start":89,"end":101,"id":12},{"text":"between","start":102,"end":109,"id":13},{"text":"observed","start":110,"end":118,"id":14},{"text":"variables","start":119,"end":128,"id":15},{"text":".","start":128,"end":129,"id":16}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":0,"end":7,"token_start":0,"token_end":0,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"In this article, we have proposed several approaches for post processing a large ensemble of prediction models or rules.","_input_hash":686250237,"_task_hash":-1278394865,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"this","start":3,"end":7,"id":1},{"text":"article","start":8,"end":15,"id":2},{"text":",","start":15,"end":16,"id":3},{"text":"we","start":17,"end":19,"id":4},{"text":"have","start":20,"end":24,"id":5},{"text":"proposed","start":25,"end":33,"id":6},{"text":"several","start":34,"end":41,"id":7},{"text":"approaches","start":42,"end":52,"id":8},{"text":"for","start":53,"end":56,"id":9},{"text":"post","start":57,"end":61,"id":10},{"text":"processing","start":62,"end":72,"id":11},{"text":"a","start":73,"end":74,"id":12},{"text":"large","start":75,"end":80,"id":13},{"text":"ensemble","start":81,"end":89,"id":14},{"text":"of","start":90,"end":92,"id":15},{"text":"prediction","start":93,"end":103,"id":16},{"text":"models","start":104,"end":110,"id":17},{"text":"or","start":111,"end":113,"id":18},{"text":"rules","start":114,"end":119,"id":19},{"text":".","start":119,"end":120,"id":20}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":81,"end":103,"token_start":14,"token_end":16,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We show that if 0<p <= 1, then both all underlying subspaces and the best l0 subspace can be precisely recovered by lp minimization with overwhelming probability.","_input_hash":-1173300096,"_task_hash":-972853506,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"show","start":3,"end":7,"id":1},{"text":"that","start":8,"end":12,"id":2},{"text":"if","start":13,"end":15,"id":3},{"text":"0","start":16,"end":17,"id":4},{"text":"<","start":17,"end":18,"id":5},{"text":"p","start":18,"end":19,"id":6},{"text":"<","start":20,"end":21,"id":7},{"text":"=","start":21,"end":22,"id":8},{"text":"1","start":23,"end":24,"id":9},{"text":",","start":24,"end":25,"id":10},{"text":"then","start":26,"end":30,"id":11},{"text":"both","start":31,"end":35,"id":12},{"text":"all","start":36,"end":39,"id":13},{"text":"underlying","start":40,"end":50,"id":14},{"text":"subspaces","start":51,"end":60,"id":15},{"text":"and","start":61,"end":64,"id":16},{"text":"the","start":65,"end":68,"id":17},{"text":"best","start":69,"end":73,"id":18},{"text":"l0","start":74,"end":76,"id":19},{"text":"subspace","start":77,"end":85,"id":20},{"text":"can","start":86,"end":89,"id":21},{"text":"be","start":90,"end":92,"id":22},{"text":"precisely","start":93,"end":102,"id":23},{"text":"recovered","start":103,"end":112,"id":24},{"text":"by","start":113,"end":115,"id":25},{"text":"lp","start":116,"end":118,"id":26},{"text":"minimization","start":119,"end":131,"id":27},{"text":"with","start":132,"end":136,"id":28},{"text":"overwhelming","start":137,"end":149,"id":29},{"text":"probability","start":150,"end":161,"id":30},{"text":".","start":161,"end":162,"id":31}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"The goal is to select the convex combination of kernel matrices that best explains the data and by doing so improve the generalisation on unseen data.","_input_hash":-855762694,"_task_hash":-79795164,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"goal","start":4,"end":8,"id":1},{"text":"is","start":9,"end":11,"id":2},{"text":"to","start":12,"end":14,"id":3},{"text":"select","start":15,"end":21,"id":4},{"text":"the","start":22,"end":25,"id":5},{"text":"convex","start":26,"end":32,"id":6},{"text":"combination","start":33,"end":44,"id":7},{"text":"of","start":45,"end":47,"id":8},{"text":"kernel","start":48,"end":54,"id":9},{"text":"matrices","start":55,"end":63,"id":10},{"text":"that","start":64,"end":68,"id":11},{"text":"best","start":69,"end":73,"id":12},{"text":"explains","start":74,"end":82,"id":13},{"text":"the","start":83,"end":86,"id":14},{"text":"data","start":87,"end":91,"id":15},{"text":"and","start":92,"end":95,"id":16},{"text":"by","start":96,"end":98,"id":17},{"text":"doing","start":99,"end":104,"id":18},{"text":"so","start":105,"end":107,"id":19},{"text":"improve","start":108,"end":115,"id":20},{"text":"the","start":116,"end":119,"id":21},{"text":"generalisation","start":120,"end":134,"id":22},{"text":"on","start":135,"end":137,"id":23},{"text":"unseen","start":138,"end":144,"id":24},{"text":"data","start":145,"end":149,"id":25},{"text":".","start":149,"end":150,"id":26}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"We also provide formal evidence that a modified Bayesian information criterion (BIC) can be used to efficiently determine the number of iterations in S-OMP.","_input_hash":471017088,"_task_hash":413395569,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"also","start":3,"end":7,"id":1},{"text":"provide","start":8,"end":15,"id":2},{"text":"formal","start":16,"end":22,"id":3},{"text":"evidence","start":23,"end":31,"id":4},{"text":"that","start":32,"end":36,"id":5},{"text":"a","start":37,"end":38,"id":6},{"text":"modified","start":39,"end":47,"id":7},{"text":"Bayesian","start":48,"end":56,"id":8},{"text":"information","start":57,"end":68,"id":9},{"text":"criterion","start":69,"end":78,"id":10},{"text":"(","start":79,"end":80,"id":11},{"text":"BIC","start":80,"end":83,"id":12},{"text":")","start":83,"end":84,"id":13},{"text":"can","start":85,"end":88,"id":14},{"text":"be","start":89,"end":91,"id":15},{"text":"used","start":92,"end":96,"id":16},{"text":"to","start":97,"end":99,"id":17},{"text":"efficiently","start":100,"end":111,"id":18},{"text":"determine","start":112,"end":121,"id":19},{"text":"the","start":122,"end":125,"id":20},{"text":"number","start":126,"end":132,"id":21},{"text":"of","start":133,"end":135,"id":22},{"text":"iterations","start":136,"end":146,"id":23},{"text":"in","start":147,"end":149,"id":24},{"text":"S","start":150,"end":151,"id":25},{"text":"-","start":151,"end":152,"id":26},{"text":"OMP","start":152,"end":155,"id":27},{"text":".","start":155,"end":156,"id":28}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":48,"end":78,"token_start":8,"token_end":10,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We prove consistency of the method for directed and undirected graphs.","_input_hash":755208218,"_task_hash":-783732396,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"prove","start":3,"end":8,"id":1},{"text":"consistency","start":9,"end":20,"id":2},{"text":"of","start":21,"end":23,"id":3},{"text":"the","start":24,"end":27,"id":4},{"text":"method","start":28,"end":34,"id":5},{"text":"for","start":35,"end":38,"id":6},{"text":"directed","start":39,"end":47,"id":7},{"text":"and","start":48,"end":51,"id":8},{"text":"undirected","start":52,"end":62,"id":9},{"text":"graphs","start":63,"end":69,"id":10},{"text":".","start":69,"end":70,"id":11}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Orthogonal matching pursuit;","_input_hash":106142429,"_task_hash":-1009870301,"tokens":[{"text":"Orthogonal","start":0,"end":10,"id":0},{"text":"matching","start":11,"end":19,"id":1},{"text":"pursuit","start":20,"end":27,"id":2},{"text":";","start":27,"end":28,"id":3}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":0,"end":27,"token_start":0,"token_end":2,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"The GPLVM consists of a set of points in a low dimensional latent space, and a stochastic map to the observed space.","_input_hash":867554865,"_task_hash":582911741,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"GPLVM","start":4,"end":9,"id":1},{"text":"consists","start":10,"end":18,"id":2},{"text":"of","start":19,"end":21,"id":3},{"text":"a","start":22,"end":23,"id":4},{"text":"set","start":24,"end":27,"id":5},{"text":"of","start":28,"end":30,"id":6},{"text":"points","start":31,"end":37,"id":7},{"text":"in","start":38,"end":40,"id":8},{"text":"a","start":41,"end":42,"id":9},{"text":"low","start":43,"end":46,"id":10},{"text":"dimensional","start":47,"end":58,"id":11},{"text":"latent","start":59,"end":65,"id":12},{"text":"space","start":66,"end":71,"id":13},{"text":",","start":71,"end":72,"id":14},{"text":"and","start":73,"end":76,"id":15},{"text":"a","start":77,"end":78,"id":16},{"text":"stochastic","start":79,"end":89,"id":17},{"text":"map","start":90,"end":93,"id":18},{"text":"to","start":94,"end":96,"id":19},{"text":"the","start":97,"end":100,"id":20},{"text":"observed","start":101,"end":109,"id":21},{"text":"space","start":110,"end":115,"id":22},{"text":".","start":115,"end":116,"id":23}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We propose to investigate test statistics for testing homogeneity in reproducing kernel Hilbert spaces.","_input_hash":-1337820549,"_task_hash":-994165809,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"propose","start":3,"end":10,"id":1},{"text":"to","start":11,"end":13,"id":2},{"text":"investigate","start":14,"end":25,"id":3},{"text":"test","start":26,"end":30,"id":4},{"text":"statistics","start":31,"end":41,"id":5},{"text":"for","start":42,"end":45,"id":6},{"text":"testing","start":46,"end":53,"id":7},{"text":"homogeneity","start":54,"end":65,"id":8},{"text":"in","start":66,"end":68,"id":9},{"text":"reproducing","start":69,"end":80,"id":10},{"text":"kernel","start":81,"end":87,"id":11},{"text":"Hilbert","start":88,"end":95,"id":12},{"text":"spaces","start":96,"end":102,"id":13},{"text":".","start":102,"end":103,"id":14}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":81,"end":102,"token_start":11,"token_end":13,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Support vector machines (SVMs) are special kernel based methods and belong to the most successful learning methods since more than a decade.","_input_hash":-1389063560,"_task_hash":1964778525,"tokens":[{"text":"Support","start":0,"end":7,"id":0},{"text":"vector","start":8,"end":14,"id":1},{"text":"machines","start":15,"end":23,"id":2},{"text":"(","start":24,"end":25,"id":3},{"text":"SVMs","start":25,"end":29,"id":4},{"text":")","start":29,"end":30,"id":5},{"text":"are","start":31,"end":34,"id":6},{"text":"special","start":35,"end":42,"id":7},{"text":"kernel","start":43,"end":49,"id":8},{"text":"based","start":50,"end":55,"id":9},{"text":"methods","start":56,"end":63,"id":10},{"text":"and","start":64,"end":67,"id":11},{"text":"belong","start":68,"end":74,"id":12},{"text":"to","start":75,"end":77,"id":13},{"text":"the","start":78,"end":81,"id":14},{"text":"most","start":82,"end":86,"id":15},{"text":"successful","start":87,"end":97,"id":16},{"text":"learning","start":98,"end":106,"id":17},{"text":"methods","start":107,"end":114,"id":18},{"text":"since","start":115,"end":120,"id":19},{"text":"more","start":121,"end":125,"id":20},{"text":"than","start":126,"end":130,"id":21},{"text":"a","start":131,"end":132,"id":22},{"text":"decade","start":133,"end":139,"id":23},{"text":".","start":139,"end":140,"id":24}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":0,"end":23,"token_start":0,"token_end":2,"label":"ALGO","answer":"accept"},{"start":25,"end":29,"token_start":4,"token_end":4,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We construct a framework for studying clustering algorithms, which includes two key ideas:","_input_hash":748612535,"_task_hash":1075513076,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"construct","start":3,"end":12,"id":1},{"text":"a","start":13,"end":14,"id":2},{"text":"framework","start":15,"end":24,"id":3},{"text":"for","start":25,"end":28,"id":4},{"text":"studying","start":29,"end":37,"id":5},{"text":"clustering","start":38,"end":48,"id":6},{"text":"algorithms","start":49,"end":59,"id":7},{"text":",","start":59,"end":60,"id":8},{"text":"which","start":61,"end":66,"id":9},{"text":"includes","start":67,"end":75,"id":10},{"text":"two","start":76,"end":79,"id":11},{"text":"key","start":80,"end":83,"id":12},{"text":"ideas","start":84,"end":89,"id":13},{"text":":","start":89,"end":90,"id":14}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":38,"end":48,"token_start":6,"token_end":6,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"The ambition of the present work is to give an insight into covariance kernels that are well suited for building additive Kriging models and to describe some properties of the resulting models.","_input_hash":-658205527,"_task_hash":-407053411,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"ambition","start":4,"end":12,"id":1},{"text":"of","start":13,"end":15,"id":2},{"text":"the","start":16,"end":19,"id":3},{"text":"present","start":20,"end":27,"id":4},{"text":"work","start":28,"end":32,"id":5},{"text":"is","start":33,"end":35,"id":6},{"text":"to","start":36,"end":38,"id":7},{"text":"give","start":39,"end":43,"id":8},{"text":"an","start":44,"end":46,"id":9},{"text":"insight","start":47,"end":54,"id":10},{"text":"into","start":55,"end":59,"id":11},{"text":"covariance","start":60,"end":70,"id":12},{"text":"kernels","start":71,"end":78,"id":13},{"text":"that","start":79,"end":83,"id":14},{"text":"are","start":84,"end":87,"id":15},{"text":"well","start":88,"end":92,"id":16},{"text":"suited","start":93,"end":99,"id":17},{"text":"for","start":100,"end":103,"id":18},{"text":"building","start":104,"end":112,"id":19},{"text":"additive","start":113,"end":121,"id":20},{"text":"Kriging","start":122,"end":129,"id":21},{"text":"models","start":130,"end":136,"id":22},{"text":"and","start":137,"end":140,"id":23},{"text":"to","start":141,"end":143,"id":24},{"text":"describe","start":144,"end":152,"id":25},{"text":"some","start":153,"end":157,"id":26},{"text":"properties","start":158,"end":168,"id":27},{"text":"of","start":169,"end":171,"id":28},{"text":"the","start":172,"end":175,"id":29},{"text":"resulting","start":176,"end":185,"id":30},{"text":"models","start":186,"end":192,"id":31},{"text":".","start":192,"end":193,"id":32}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":122,"end":129,"token_start":21,"token_end":21,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"These could be as simple as monotone functions of individual predictor variables.","_input_hash":-446331697,"_task_hash":-781571654,"tokens":[{"text":"These","start":0,"end":5,"id":0},{"text":"could","start":6,"end":11,"id":1},{"text":"be","start":12,"end":14,"id":2},{"text":"as","start":15,"end":17,"id":3},{"text":"simple","start":18,"end":24,"id":4},{"text":"as","start":25,"end":27,"id":5},{"text":"monotone","start":28,"end":36,"id":6},{"text":"functions","start":37,"end":46,"id":7},{"text":"of","start":47,"end":49,"id":8},{"text":"individual","start":50,"end":60,"id":9},{"text":"predictor","start":61,"end":70,"id":10},{"text":"variables","start":71,"end":80,"id":11},{"text":".","start":80,"end":81,"id":12}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We formulate the assumptions in a Bayesian model which provides the factors, and apply the model to two data analysis tasks, in neuroimaging and chemical systems biology.","_input_hash":1125839678,"_task_hash":1154376637,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"formulate","start":3,"end":12,"id":1},{"text":"the","start":13,"end":16,"id":2},{"text":"assumptions","start":17,"end":28,"id":3},{"text":"in","start":29,"end":31,"id":4},{"text":"a","start":32,"end":33,"id":5},{"text":"Bayesian","start":34,"end":42,"id":6},{"text":"model","start":43,"end":48,"id":7},{"text":"which","start":49,"end":54,"id":8},{"text":"provides","start":55,"end":63,"id":9},{"text":"the","start":64,"end":67,"id":10},{"text":"factors","start":68,"end":75,"id":11},{"text":",","start":75,"end":76,"id":12},{"text":"and","start":77,"end":80,"id":13},{"text":"apply","start":81,"end":86,"id":14},{"text":"the","start":87,"end":90,"id":15},{"text":"model","start":91,"end":96,"id":16},{"text":"to","start":97,"end":99,"id":17},{"text":"two","start":100,"end":103,"id":18},{"text":"data","start":104,"end":108,"id":19},{"text":"analysis","start":109,"end":117,"id":20},{"text":"tasks","start":118,"end":123,"id":21},{"text":",","start":123,"end":124,"id":22},{"text":"in","start":125,"end":127,"id":23},{"text":"neuroimaging","start":128,"end":140,"id":24},{"text":"and","start":141,"end":144,"id":25},{"text":"chemical","start":145,"end":153,"id":26},{"text":"systems","start":154,"end":161,"id":27},{"text":"biology","start":162,"end":169,"id":28},{"text":".","start":169,"end":170,"id":29}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"The empirical performance is demonstrated on a wide array of datasets.","_input_hash":-180388319,"_task_hash":1952199026,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"empirical","start":4,"end":13,"id":1},{"text":"performance","start":14,"end":25,"id":2},{"text":"is","start":26,"end":28,"id":3},{"text":"demonstrated","start":29,"end":41,"id":4},{"text":"on","start":42,"end":44,"id":5},{"text":"a","start":45,"end":46,"id":6},{"text":"wide","start":47,"end":51,"id":7},{"text":"array","start":52,"end":57,"id":8},{"text":"of","start":58,"end":60,"id":9},{"text":"datasets","start":61,"end":69,"id":10},{"text":".","start":69,"end":70,"id":11}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Experiments with simulated data and microarray data indicate that the methods are a practical alternative to Gaussian graphical models.","_input_hash":-1805975362,"_task_hash":-92630751,"tokens":[{"text":"Experiments","start":0,"end":11,"id":0},{"text":"with","start":12,"end":16,"id":1},{"text":"simulated","start":17,"end":26,"id":2},{"text":"data","start":27,"end":31,"id":3},{"text":"and","start":32,"end":35,"id":4},{"text":"microarray","start":36,"end":46,"id":5},{"text":"data","start":47,"end":51,"id":6},{"text":"indicate","start":52,"end":60,"id":7},{"text":"that","start":61,"end":65,"id":8},{"text":"the","start":66,"end":69,"id":9},{"text":"methods","start":70,"end":77,"id":10},{"text":"are","start":78,"end":81,"id":11},{"text":"a","start":82,"end":83,"id":12},{"text":"practical","start":84,"end":93,"id":13},{"text":"alternative","start":94,"end":105,"id":14},{"text":"to","start":106,"end":108,"id":15},{"text":"Gaussian","start":109,"end":117,"id":16},{"text":"graphical","start":118,"end":127,"id":17},{"text":"models","start":128,"end":134,"id":18},{"text":".","start":134,"end":135,"id":19}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":109,"end":127,"token_start":16,"token_end":17,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We exploit the prior information in these models in order to distribute its computation.","_input_hash":1119992861,"_task_hash":333611047,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"exploit","start":3,"end":10,"id":1},{"text":"the","start":11,"end":14,"id":2},{"text":"prior","start":15,"end":20,"id":3},{"text":"information","start":21,"end":32,"id":4},{"text":"in","start":33,"end":35,"id":5},{"text":"these","start":36,"end":41,"id":6},{"text":"models","start":42,"end":48,"id":7},{"text":"in","start":49,"end":51,"id":8},{"text":"order","start":52,"end":57,"id":9},{"text":"to","start":58,"end":60,"id":10},{"text":"distribute","start":61,"end":71,"id":11},{"text":"its","start":72,"end":75,"id":12},{"text":"computation","start":76,"end":87,"id":13},{"text":".","start":87,"end":88,"id":14}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"as the correlation threshold decreases the number of discoveries increases abruptly.","_input_hash":-1364482929,"_task_hash":1952799445,"tokens":[{"text":"as","start":0,"end":2,"id":0},{"text":"the","start":3,"end":6,"id":1},{"text":"correlation","start":7,"end":18,"id":2},{"text":"threshold","start":19,"end":28,"id":3},{"text":"decreases","start":29,"end":38,"id":4},{"text":"the","start":39,"end":42,"id":5},{"text":"number","start":43,"end":49,"id":6},{"text":"of","start":50,"end":52,"id":7},{"text":"discoveries","start":53,"end":64,"id":8},{"text":"increases","start":65,"end":74,"id":9},{"text":"abruptly","start":75,"end":83,"id":10},{"text":".","start":83,"end":84,"id":11}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"This paper treats the problem of screening for variables with high correlations in high dimensional data in which there can be many fewer samples than variables.","_input_hash":-42169086,"_task_hash":-1630042804,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"paper","start":5,"end":10,"id":1},{"text":"treats","start":11,"end":17,"id":2},{"text":"the","start":18,"end":21,"id":3},{"text":"problem","start":22,"end":29,"id":4},{"text":"of","start":30,"end":32,"id":5},{"text":"screening","start":33,"end":42,"id":6},{"text":"for","start":43,"end":46,"id":7},{"text":"variables","start":47,"end":56,"id":8},{"text":"with","start":57,"end":61,"id":9},{"text":"high","start":62,"end":66,"id":10},{"text":"correlations","start":67,"end":79,"id":11},{"text":"in","start":80,"end":82,"id":12},{"text":"high","start":83,"end":87,"id":13},{"text":"dimensional","start":88,"end":99,"id":14},{"text":"data","start":100,"end":104,"id":15},{"text":"in","start":105,"end":107,"id":16},{"text":"which","start":108,"end":113,"id":17},{"text":"there","start":114,"end":119,"id":18},{"text":"can","start":120,"end":123,"id":19},{"text":"be","start":124,"end":126,"id":20},{"text":"many","start":127,"end":131,"id":21},{"text":"fewer","start":132,"end":137,"id":22},{"text":"samples","start":138,"end":145,"id":23},{"text":"than","start":146,"end":150,"id":24},{"text":"variables","start":151,"end":160,"id":25},{"text":".","start":160,"end":161,"id":26}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"As an alternative, we propose a provably convergent EP algorithm (PC-EP).","_input_hash":138256094,"_task_hash":2068621303,"tokens":[{"text":"As","start":0,"end":2,"id":0},{"text":"an","start":3,"end":5,"id":1},{"text":"alternative","start":6,"end":17,"id":2},{"text":",","start":17,"end":18,"id":3},{"text":"we","start":19,"end":21,"id":4},{"text":"propose","start":22,"end":29,"id":5},{"text":"a","start":30,"end":31,"id":6},{"text":"provably","start":32,"end":40,"id":7},{"text":"convergent","start":41,"end":51,"id":8},{"text":"EP","start":52,"end":54,"id":9},{"text":"algorithm","start":55,"end":64,"id":10},{"text":"(","start":65,"end":66,"id":11},{"text":"PC","start":66,"end":68,"id":12},{"text":"-","start":68,"end":69,"id":13},{"text":"EP","start":69,"end":71,"id":14},{"text":")","start":71,"end":72,"id":15},{"text":".","start":72,"end":73,"id":16}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":32,"end":54,"token_start":7,"token_end":9,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"A simulated example of this algorithm is proposed on a dictionary obtained using LARS, for the problem of selection of the regularization parameter of the LASSO.","_input_hash":-339762701,"_task_hash":-685929669,"tokens":[{"text":"A","start":0,"end":1,"id":0},{"text":"simulated","start":2,"end":11,"id":1},{"text":"example","start":12,"end":19,"id":2},{"text":"of","start":20,"end":22,"id":3},{"text":"this","start":23,"end":27,"id":4},{"text":"algorithm","start":28,"end":37,"id":5},{"text":"is","start":38,"end":40,"id":6},{"text":"proposed","start":41,"end":49,"id":7},{"text":"on","start":50,"end":52,"id":8},{"text":"a","start":53,"end":54,"id":9},{"text":"dictionary","start":55,"end":65,"id":10},{"text":"obtained","start":66,"end":74,"id":11},{"text":"using","start":75,"end":80,"id":12},{"text":"LARS","start":81,"end":85,"id":13},{"text":",","start":85,"end":86,"id":14},{"text":"for","start":87,"end":90,"id":15},{"text":"the","start":91,"end":94,"id":16},{"text":"problem","start":95,"end":102,"id":17},{"text":"of","start":103,"end":105,"id":18},{"text":"selection","start":106,"end":115,"id":19},{"text":"of","start":116,"end":118,"id":20},{"text":"the","start":119,"end":122,"id":21},{"text":"regularization","start":123,"end":137,"id":22},{"text":"parameter","start":138,"end":147,"id":23},{"text":"of","start":148,"end":150,"id":24},{"text":"the","start":151,"end":154,"id":25},{"text":"LASSO","start":155,"end":160,"id":26},{"text":".","start":160,"end":161,"id":27}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":155,"end":160,"token_start":26,"token_end":26,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"The framework proposes a general objective function suitable for regression, robust regression and classification that is lower bound of the marginal likelihood and contains many regularised risk approaches as special cases.","_input_hash":2032760909,"_task_hash":1529639250,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"framework","start":4,"end":13,"id":1},{"text":"proposes","start":14,"end":22,"id":2},{"text":"a","start":23,"end":24,"id":3},{"text":"general","start":25,"end":32,"id":4},{"text":"objective","start":33,"end":42,"id":5},{"text":"function","start":43,"end":51,"id":6},{"text":"suitable","start":52,"end":60,"id":7},{"text":"for","start":61,"end":64,"id":8},{"text":"regression","start":65,"end":75,"id":9},{"text":",","start":75,"end":76,"id":10},{"text":"robust","start":77,"end":83,"id":11},{"text":"regression","start":84,"end":94,"id":12},{"text":"and","start":95,"end":98,"id":13},{"text":"classification","start":99,"end":113,"id":14},{"text":"that","start":114,"end":118,"id":15},{"text":"is","start":119,"end":121,"id":16},{"text":"lower","start":122,"end":127,"id":17},{"text":"bound","start":128,"end":133,"id":18},{"text":"of","start":134,"end":136,"id":19},{"text":"the","start":137,"end":140,"id":20},{"text":"marginal","start":141,"end":149,"id":21},{"text":"likelihood","start":150,"end":160,"id":22},{"text":"and","start":161,"end":164,"id":23},{"text":"contains","start":165,"end":173,"id":24},{"text":"many","start":174,"end":178,"id":25},{"text":"regularised","start":179,"end":190,"id":26},{"text":"risk","start":191,"end":195,"id":27},{"text":"approaches","start":196,"end":206,"id":28},{"text":"as","start":207,"end":209,"id":29},{"text":"special","start":210,"end":217,"id":30},{"text":"cases","start":218,"end":223,"id":31},{"text":".","start":223,"end":224,"id":32}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We prove conditions for the asymptotic unbiasedness of the DP-GLM regression mean function estimate.","_input_hash":-190543059,"_task_hash":-988772315,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"prove","start":3,"end":8,"id":1},{"text":"conditions","start":9,"end":19,"id":2},{"text":"for","start":20,"end":23,"id":3},{"text":"the","start":24,"end":27,"id":4},{"text":"asymptotic","start":28,"end":38,"id":5},{"text":"unbiasedness","start":39,"end":51,"id":6},{"text":"of","start":52,"end":54,"id":7},{"text":"the","start":55,"end":58,"id":8},{"text":"DP","start":59,"end":61,"id":9},{"text":"-","start":61,"end":62,"id":10},{"text":"GLM","start":62,"end":65,"id":11},{"text":"regression","start":66,"end":76,"id":12},{"text":"mean","start":77,"end":81,"id":13},{"text":"function","start":82,"end":90,"id":14},{"text":"estimate","start":91,"end":99,"id":15},{"text":".","start":99,"end":100,"id":16}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We describe a computationally efficient implementation of this generalization that uses low-rank matrix factorization techniques.","_input_hash":-314672089,"_task_hash":-2145536370,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"describe","start":3,"end":11,"id":1},{"text":"a","start":12,"end":13,"id":2},{"text":"computationally","start":14,"end":29,"id":3},{"text":"efficient","start":30,"end":39,"id":4},{"text":"implementation","start":40,"end":54,"id":5},{"text":"of","start":55,"end":57,"id":6},{"text":"this","start":58,"end":62,"id":7},{"text":"generalization","start":63,"end":77,"id":8},{"text":"that","start":78,"end":82,"id":9},{"text":"uses","start":83,"end":87,"id":10},{"text":"low","start":88,"end":91,"id":11},{"text":"-","start":91,"end":92,"id":12},{"text":"rank","start":92,"end":96,"id":13},{"text":"matrix","start":97,"end":103,"id":14},{"text":"factorization","start":104,"end":117,"id":15},{"text":"techniques","start":118,"end":128,"id":16},{"text":".","start":128,"end":129,"id":17}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"In this context the current paper makes two contributions.","_input_hash":-142099518,"_task_hash":1989121230,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"this","start":3,"end":7,"id":1},{"text":"context","start":8,"end":15,"id":2},{"text":"the","start":16,"end":19,"id":3},{"text":"current","start":20,"end":27,"id":4},{"text":"paper","start":28,"end":33,"id":5},{"text":"makes","start":34,"end":39,"id":6},{"text":"two","start":40,"end":43,"id":7},{"text":"contributions","start":44,"end":57,"id":8},{"text":".","start":57,"end":58,"id":9}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We empirically analyze the properties of the DP-GLM and why it provides better results than existing Dirichlet process mixture regression models.","_input_hash":1933387715,"_task_hash":1188952563,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"empirically","start":3,"end":14,"id":1},{"text":"analyze","start":15,"end":22,"id":2},{"text":"the","start":23,"end":26,"id":3},{"text":"properties","start":27,"end":37,"id":4},{"text":"of","start":38,"end":40,"id":5},{"text":"the","start":41,"end":44,"id":6},{"text":"DP","start":45,"end":47,"id":7},{"text":"-","start":47,"end":48,"id":8},{"text":"GLM","start":48,"end":51,"id":9},{"text":"and","start":52,"end":55,"id":10},{"text":"why","start":56,"end":59,"id":11},{"text":"it","start":60,"end":62,"id":12},{"text":"provides","start":63,"end":71,"id":13},{"text":"better","start":72,"end":78,"id":14},{"text":"results","start":79,"end":86,"id":15},{"text":"than","start":87,"end":91,"id":16},{"text":"existing","start":92,"end":100,"id":17},{"text":"Dirichlet","start":101,"end":110,"id":18},{"text":"process","start":111,"end":118,"id":19},{"text":"mixture","start":119,"end":126,"id":20},{"text":"regression","start":127,"end":137,"id":21},{"text":"models","start":138,"end":144,"id":22},{"text":".","start":144,"end":145,"id":23}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"The iterative algorithm, KNIFE, alternates between finding the coefficients of the original problem and finding the feature weights through kernel linearization.","_input_hash":131542730,"_task_hash":-1141397284,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"iterative","start":4,"end":13,"id":1},{"text":"algorithm","start":14,"end":23,"id":2},{"text":",","start":23,"end":24,"id":3},{"text":"KNIFE","start":25,"end":30,"id":4},{"text":",","start":30,"end":31,"id":5},{"text":"alternates","start":32,"end":42,"id":6},{"text":"between","start":43,"end":50,"id":7},{"text":"finding","start":51,"end":58,"id":8},{"text":"the","start":59,"end":62,"id":9},{"text":"coefficients","start":63,"end":75,"id":10},{"text":"of","start":76,"end":78,"id":11},{"text":"the","start":79,"end":82,"id":12},{"text":"original","start":83,"end":91,"id":13},{"text":"problem","start":92,"end":99,"id":14},{"text":"and","start":100,"end":103,"id":15},{"text":"finding","start":104,"end":111,"id":16},{"text":"the","start":112,"end":115,"id":17},{"text":"feature","start":116,"end":123,"id":18},{"text":"weights","start":124,"end":131,"id":19},{"text":"through","start":132,"end":139,"id":20},{"text":"kernel","start":140,"end":146,"id":21},{"text":"linearization","start":147,"end":160,"id":22},{"text":".","start":160,"end":161,"id":23}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":25,"end":30,"token_start":4,"token_end":4,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"This construction is equivalent to imposing a product of heavy-tailed process priors over function space.","_input_hash":-432579211,"_task_hash":-1950498594,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"construction","start":5,"end":17,"id":1},{"text":"is","start":18,"end":20,"id":2},{"text":"equivalent","start":21,"end":31,"id":3},{"text":"to","start":32,"end":34,"id":4},{"text":"imposing","start":35,"end":43,"id":5},{"text":"a","start":44,"end":45,"id":6},{"text":"product","start":46,"end":53,"id":7},{"text":"of","start":54,"end":56,"id":8},{"text":"heavy","start":57,"end":62,"id":9},{"text":"-","start":62,"end":63,"id":10},{"text":"tailed","start":63,"end":69,"id":11},{"text":"process","start":70,"end":77,"id":12},{"text":"priors","start":78,"end":84,"id":13},{"text":"over","start":85,"end":89,"id":14},{"text":"function","start":90,"end":98,"id":15},{"text":"space","start":99,"end":104,"id":16},{"text":".","start":104,"end":105,"id":17}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"Our scheme consists of a two step alternating procedure of active set update rules and hyperparameter optimization based upon marginal likelihood maximization.","_input_hash":1648894686,"_task_hash":-96640925,"tokens":[{"text":"Our","start":0,"end":3,"id":0},{"text":"scheme","start":4,"end":10,"id":1},{"text":"consists","start":11,"end":19,"id":2},{"text":"of","start":20,"end":22,"id":3},{"text":"a","start":23,"end":24,"id":4},{"text":"two","start":25,"end":28,"id":5},{"text":"step","start":29,"end":33,"id":6},{"text":"alternating","start":34,"end":45,"id":7},{"text":"procedure","start":46,"end":55,"id":8},{"text":"of","start":56,"end":58,"id":9},{"text":"active","start":59,"end":65,"id":10},{"text":"set","start":66,"end":69,"id":11},{"text":"update","start":70,"end":76,"id":12},{"text":"rules","start":77,"end":82,"id":13},{"text":"and","start":83,"end":86,"id":14},{"text":"hyperparameter","start":87,"end":101,"id":15},{"text":"optimization","start":102,"end":114,"id":16},{"text":"based","start":115,"end":120,"id":17},{"text":"upon","start":121,"end":125,"id":18},{"text":"marginal","start":126,"end":134,"id":19},{"text":"likelihood","start":135,"end":145,"id":20},{"text":"maximization","start":146,"end":158,"id":21},{"text":".","start":158,"end":159,"id":22}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"A novel method for deflationary ICA, referred to as RobustICA, is put forward in this paper.","_input_hash":-1464039596,"_task_hash":1293334276,"tokens":[{"text":"A","start":0,"end":1,"id":0},{"text":"novel","start":2,"end":7,"id":1},{"text":"method","start":8,"end":14,"id":2},{"text":"for","start":15,"end":18,"id":3},{"text":"deflationary","start":19,"end":31,"id":4},{"text":"ICA","start":32,"end":35,"id":5},{"text":",","start":35,"end":36,"id":6},{"text":"referred","start":37,"end":45,"id":7},{"text":"to","start":46,"end":48,"id":8},{"text":"as","start":49,"end":51,"id":9},{"text":"RobustICA","start":52,"end":61,"id":10},{"text":",","start":61,"end":62,"id":11},{"text":"is","start":63,"end":65,"id":12},{"text":"put","start":66,"end":69,"id":13},{"text":"forward","start":70,"end":77,"id":14},{"text":"in","start":78,"end":80,"id":15},{"text":"this","start":81,"end":85,"id":16},{"text":"paper","start":86,"end":91,"id":17},{"text":".","start":91,"end":92,"id":18}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":52,"end":61,"token_start":10,"token_end":10,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Motivated by the relative scarcity of formal tools for SBL in complex-valued models, this paper proposes a GSM model - the Bessel K model - that induces concave penalty functions for the estimation of complex sparse signals.","_input_hash":1959789610,"_task_hash":2053271298,"tokens":[{"text":"Motivated","start":0,"end":9,"id":0},{"text":"by","start":10,"end":12,"id":1},{"text":"the","start":13,"end":16,"id":2},{"text":"relative","start":17,"end":25,"id":3},{"text":"scarcity","start":26,"end":34,"id":4},{"text":"of","start":35,"end":37,"id":5},{"text":"formal","start":38,"end":44,"id":6},{"text":"tools","start":45,"end":50,"id":7},{"text":"for","start":51,"end":54,"id":8},{"text":"SBL","start":55,"end":58,"id":9},{"text":"in","start":59,"end":61,"id":10},{"text":"complex","start":62,"end":69,"id":11},{"text":"-","start":69,"end":70,"id":12},{"text":"valued","start":70,"end":76,"id":13},{"text":"models","start":77,"end":83,"id":14},{"text":",","start":83,"end":84,"id":15},{"text":"this","start":85,"end":89,"id":16},{"text":"paper","start":90,"end":95,"id":17},{"text":"proposes","start":96,"end":104,"id":18},{"text":"a","start":105,"end":106,"id":19},{"text":"GSM","start":107,"end":110,"id":20},{"text":"model","start":111,"end":116,"id":21},{"text":"-","start":117,"end":118,"id":22},{"text":"the","start":119,"end":122,"id":23},{"text":"Bessel","start":123,"end":129,"id":24},{"text":"K","start":130,"end":131,"id":25},{"text":"model","start":132,"end":137,"id":26},{"text":"-","start":138,"end":139,"id":27},{"text":"that","start":140,"end":144,"id":28},{"text":"induces","start":145,"end":152,"id":29},{"text":"concave","start":153,"end":160,"id":30},{"text":"penalty","start":161,"end":168,"id":31},{"text":"functions","start":169,"end":178,"id":32},{"text":"for","start":179,"end":182,"id":33},{"text":"the","start":183,"end":186,"id":34},{"text":"estimation","start":187,"end":197,"id":35},{"text":"of","start":198,"end":200,"id":36},{"text":"complex","start":201,"end":208,"id":37},{"text":"sparse","start":209,"end":215,"id":38},{"text":"signals","start":216,"end":223,"id":39},{"text":".","start":223,"end":224,"id":40}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":107,"end":110,"token_start":20,"token_end":20,"label":"ALGO","answer":"accept"},{"start":123,"end":131,"token_start":24,"token_end":25,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"The performance of the Lasso is well understood under the assumptions of the standard linear model with homoscedastic noise.","_input_hash":1187249395,"_task_hash":-1407800836,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"performance","start":4,"end":15,"id":1},{"text":"of","start":16,"end":18,"id":2},{"text":"the","start":19,"end":22,"id":3},{"text":"Lasso","start":23,"end":28,"id":4},{"text":"is","start":29,"end":31,"id":5},{"text":"well","start":32,"end":36,"id":6},{"text":"understood","start":37,"end":47,"id":7},{"text":"under","start":48,"end":53,"id":8},{"text":"the","start":54,"end":57,"id":9},{"text":"assumptions","start":58,"end":69,"id":10},{"text":"of","start":70,"end":72,"id":11},{"text":"the","start":73,"end":76,"id":12},{"text":"standard","start":77,"end":85,"id":13},{"text":"linear","start":86,"end":92,"id":14},{"text":"model","start":93,"end":98,"id":15},{"text":"with","start":99,"end":103,"id":16},{"text":"homoscedastic","start":104,"end":117,"id":17},{"text":"noise","start":118,"end":123,"id":18},{"text":".","start":123,"end":124,"id":19}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":23,"end":28,"token_start":4,"token_end":4,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Unlike other lp minimization problems, this minimization is non-convex for all p>0 and thus requires different methods for its analysis.","_input_hash":460153296,"_task_hash":1612676887,"tokens":[{"text":"Unlike","start":0,"end":6,"id":0},{"text":"other","start":7,"end":12,"id":1},{"text":"lp","start":13,"end":15,"id":2},{"text":"minimization","start":16,"end":28,"id":3},{"text":"problems","start":29,"end":37,"id":4},{"text":",","start":37,"end":38,"id":5},{"text":"this","start":39,"end":43,"id":6},{"text":"minimization","start":44,"end":56,"id":7},{"text":"is","start":57,"end":59,"id":8},{"text":"non","start":60,"end":63,"id":9},{"text":"-","start":63,"end":64,"id":10},{"text":"convex","start":64,"end":70,"id":11},{"text":"for","start":71,"end":74,"id":12},{"text":"all","start":75,"end":78,"id":13},{"text":"p>0","start":79,"end":82,"id":14},{"text":"and","start":83,"end":86,"id":15},{"text":"thus","start":87,"end":91,"id":16},{"text":"requires","start":92,"end":100,"id":17},{"text":"different","start":101,"end":110,"id":18},{"text":"methods","start":111,"end":118,"id":19},{"text":"for","start":119,"end":122,"id":20},{"text":"its","start":123,"end":126,"id":21},{"text":"analysis","start":127,"end":135,"id":22},{"text":".","start":135,"end":136,"id":23}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"Many real-world problems encountered in several disciplines deal with the modeling of time-series containing different underlying dynamical regimes, for which probabilistic approaches are very often employed.","_input_hash":1110788271,"_task_hash":-857560844,"tokens":[{"text":"Many","start":0,"end":4,"id":0},{"text":"real","start":5,"end":9,"id":1},{"text":"-","start":9,"end":10,"id":2},{"text":"world","start":10,"end":15,"id":3},{"text":"problems","start":16,"end":24,"id":4},{"text":"encountered","start":25,"end":36,"id":5},{"text":"in","start":37,"end":39,"id":6},{"text":"several","start":40,"end":47,"id":7},{"text":"disciplines","start":48,"end":59,"id":8},{"text":"deal","start":60,"end":64,"id":9},{"text":"with","start":65,"end":69,"id":10},{"text":"the","start":70,"end":73,"id":11},{"text":"modeling","start":74,"end":82,"id":12},{"text":"of","start":83,"end":85,"id":13},{"text":"time","start":86,"end":90,"id":14},{"text":"-","start":90,"end":91,"id":15},{"text":"series","start":91,"end":97,"id":16},{"text":"containing","start":98,"end":108,"id":17},{"text":"different","start":109,"end":118,"id":18},{"text":"underlying","start":119,"end":129,"id":19},{"text":"dynamical","start":130,"end":139,"id":20},{"text":"regimes","start":140,"end":147,"id":21},{"text":",","start":147,"end":148,"id":22},{"text":"for","start":149,"end":152,"id":23},{"text":"which","start":153,"end":158,"id":24},{"text":"probabilistic","start":159,"end":172,"id":25},{"text":"approaches","start":173,"end":183,"id":26},{"text":"are","start":184,"end":187,"id":27},{"text":"very","start":188,"end":192,"id":28},{"text":"often","start":193,"end":198,"id":29},{"text":"employed","start":199,"end":207,"id":30},{"text":".","start":207,"end":208,"id":31}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":5,"end":15,"token_start":1,"token_end":3,"label":"ALGO","answer":"reject"},{"start":70,"end":73,"token_start":11,"token_end":11,"label":"ALGO","answer":"reject"},{"start":86,"end":97,"token_start":14,"token_end":16,"label":"ALGO","answer":"reject"}],"answer":"reject"}
{"text":"We study graph estimation and density estimation in high dimensions, using a family of density estimators based on forest structured undirected graphical models.","_input_hash":2048189827,"_task_hash":-533471290,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"study","start":3,"end":8,"id":1},{"text":"graph","start":9,"end":14,"id":2},{"text":"estimation","start":15,"end":25,"id":3},{"text":"and","start":26,"end":29,"id":4},{"text":"density","start":30,"end":37,"id":5},{"text":"estimation","start":38,"end":48,"id":6},{"text":"in","start":49,"end":51,"id":7},{"text":"high","start":52,"end":56,"id":8},{"text":"dimensions","start":57,"end":67,"id":9},{"text":",","start":67,"end":68,"id":10},{"text":"using","start":69,"end":74,"id":11},{"text":"a","start":75,"end":76,"id":12},{"text":"family","start":77,"end":83,"id":13},{"text":"of","start":84,"end":86,"id":14},{"text":"density","start":87,"end":94,"id":15},{"text":"estimators","start":95,"end":105,"id":16},{"text":"based","start":106,"end":111,"id":17},{"text":"on","start":112,"end":114,"id":18},{"text":"forest","start":115,"end":121,"id":19},{"text":"structured","start":122,"end":132,"id":20},{"text":"undirected","start":133,"end":143,"id":21},{"text":"graphical","start":144,"end":153,"id":22},{"text":"models","start":154,"end":160,"id":23},{"text":".","start":160,"end":161,"id":24}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":115,"end":153,"token_start":19,"token_end":22,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"2007b) - a covariance regularization framework that has received significant attention in the statistics community over the past few years.","_input_hash":-250094303,"_task_hash":-2065777967,"tokens":[{"text":"2007b","start":0,"end":5,"id":0},{"text":")","start":5,"end":6,"id":1},{"text":"-","start":7,"end":8,"id":2},{"text":"a","start":9,"end":10,"id":3},{"text":"covariance","start":11,"end":21,"id":4},{"text":"regularization","start":22,"end":36,"id":5},{"text":"framework","start":37,"end":46,"id":6},{"text":"that","start":47,"end":51,"id":7},{"text":"has","start":52,"end":55,"id":8},{"text":"received","start":56,"end":64,"id":9},{"text":"significant","start":65,"end":76,"id":10},{"text":"attention","start":77,"end":86,"id":11},{"text":"in","start":87,"end":89,"id":12},{"text":"the","start":90,"end":93,"id":13},{"text":"statistics","start":94,"end":104,"id":14},{"text":"community","start":105,"end":114,"id":15},{"text":"over","start":115,"end":119,"id":16},{"text":"the","start":120,"end":123,"id":17},{"text":"past","start":124,"end":128,"id":18},{"text":"few","start":129,"end":132,"id":19},{"text":"years","start":133,"end":138,"id":20},{"text":".","start":138,"end":139,"id":21}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We also consider the question of the ``best'' surrogate binary loss.","_input_hash":-726954419,"_task_hash":1390030707,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"also","start":3,"end":7,"id":1},{"text":"consider","start":8,"end":16,"id":2},{"text":"the","start":17,"end":20,"id":3},{"text":"question","start":21,"end":29,"id":4},{"text":"of","start":30,"end":32,"id":5},{"text":"the","start":33,"end":36,"id":6},{"text":"`","start":37,"end":38,"id":7},{"text":"`","start":38,"end":39,"id":8},{"text":"best","start":39,"end":43,"id":9},{"text":"'","start":43,"end":44,"id":10},{"text":"'","start":44,"end":45,"id":11},{"text":"surrogate","start":46,"end":55,"id":12},{"text":"binary","start":56,"end":62,"id":13},{"text":"loss","start":63,"end":67,"id":14},{"text":".","start":67,"end":68,"id":15}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"In this contribution we analyze the performance of a particular method -- online centroid anomaly detection -- in the presence of adversarial noise.","_input_hash":1971747661,"_task_hash":-1706592860,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"this","start":3,"end":7,"id":1},{"text":"contribution","start":8,"end":20,"id":2},{"text":"we","start":21,"end":23,"id":3},{"text":"analyze","start":24,"end":31,"id":4},{"text":"the","start":32,"end":35,"id":5},{"text":"performance","start":36,"end":47,"id":6},{"text":"of","start":48,"end":50,"id":7},{"text":"a","start":51,"end":52,"id":8},{"text":"particular","start":53,"end":63,"id":9},{"text":"method","start":64,"end":70,"id":10},{"text":"--","start":71,"end":73,"id":11},{"text":"online","start":74,"end":80,"id":12},{"text":"centroid","start":81,"end":89,"id":13},{"text":"anomaly","start":90,"end":97,"id":14},{"text":"detection","start":98,"end":107,"id":15},{"text":"--","start":108,"end":110,"id":16},{"text":"in","start":111,"end":113,"id":17},{"text":"the","start":114,"end":117,"id":18},{"text":"presence","start":118,"end":126,"id":19},{"text":"of","start":127,"end":129,"id":20},{"text":"adversarial","start":130,"end":141,"id":21},{"text":"noise","start":142,"end":147,"id":22},{"text":".","start":147,"end":148,"id":23}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":74,"end":107,"token_start":12,"token_end":15,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"The assumption is that the variance is piecewise constant in time, and we want to estimate change times and the variance values within the segments.","_input_hash":325873404,"_task_hash":-1962895239,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"assumption","start":4,"end":14,"id":1},{"text":"is","start":15,"end":17,"id":2},{"text":"that","start":18,"end":22,"id":3},{"text":"the","start":23,"end":26,"id":4},{"text":"variance","start":27,"end":35,"id":5},{"text":"is","start":36,"end":38,"id":6},{"text":"piecewise","start":39,"end":48,"id":7},{"text":"constant","start":49,"end":57,"id":8},{"text":"in","start":58,"end":60,"id":9},{"text":"time","start":61,"end":65,"id":10},{"text":",","start":65,"end":66,"id":11},{"text":"and","start":67,"end":70,"id":12},{"text":"we","start":71,"end":73,"id":13},{"text":"want","start":74,"end":78,"id":14},{"text":"to","start":79,"end":81,"id":15},{"text":"estimate","start":82,"end":90,"id":16},{"text":"change","start":91,"end":97,"id":17},{"text":"times","start":98,"end":103,"id":18},{"text":"and","start":104,"end":107,"id":19},{"text":"the","start":108,"end":111,"id":20},{"text":"variance","start":112,"end":120,"id":21},{"text":"values","start":121,"end":127,"id":22},{"text":"within","start":128,"end":134,"id":23},{"text":"the","start":135,"end":138,"id":24},{"text":"segments","start":139,"end":147,"id":25},{"text":".","start":147,"end":148,"id":26}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We present a method to estimate block membership of nodes in a random graph generated by a stochastic blockmodel.","_input_hash":-557509397,"_task_hash":997473775,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"present","start":3,"end":10,"id":1},{"text":"a","start":11,"end":12,"id":2},{"text":"method","start":13,"end":19,"id":3},{"text":"to","start":20,"end":22,"id":4},{"text":"estimate","start":23,"end":31,"id":5},{"text":"block","start":32,"end":37,"id":6},{"text":"membership","start":38,"end":48,"id":7},{"text":"of","start":49,"end":51,"id":8},{"text":"nodes","start":52,"end":57,"id":9},{"text":"in","start":58,"end":60,"id":10},{"text":"a","start":61,"end":62,"id":11},{"text":"random","start":63,"end":69,"id":12},{"text":"graph","start":70,"end":75,"id":13},{"text":"generated","start":76,"end":85,"id":14},{"text":"by","start":86,"end":88,"id":15},{"text":"a","start":89,"end":90,"id":16},{"text":"stochastic","start":91,"end":101,"id":17},{"text":"blockmodel","start":102,"end":112,"id":18},{"text":".","start":112,"end":113,"id":19}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"A Garrote-style convex penalty for trees ensembles, in particular Random Forests, is proposed.","_input_hash":782473757,"_task_hash":2107645131,"tokens":[{"text":"A","start":0,"end":1,"id":0},{"text":"Garrote","start":2,"end":9,"id":1},{"text":"-","start":9,"end":10,"id":2},{"text":"style","start":10,"end":15,"id":3},{"text":"convex","start":16,"end":22,"id":4},{"text":"penalty","start":23,"end":30,"id":5},{"text":"for","start":31,"end":34,"id":6},{"text":"trees","start":35,"end":40,"id":7},{"text":"ensembles","start":41,"end":50,"id":8},{"text":",","start":50,"end":51,"id":9},{"text":"in","start":52,"end":54,"id":10},{"text":"particular","start":55,"end":65,"id":11},{"text":"Random","start":66,"end":72,"id":12},{"text":"Forests","start":73,"end":80,"id":13},{"text":",","start":80,"end":81,"id":14},{"text":"is","start":82,"end":84,"id":15},{"text":"proposed","start":85,"end":93,"id":16},{"text":".","start":93,"end":94,"id":17}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":35,"end":50,"token_start":7,"token_end":8,"label":"ALGO","answer":"accept"},{"start":66,"end":80,"token_start":12,"token_end":13,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"The resulting algorithms are efficient, and perform well in simulations under stochastic and adversarial inputs.","_input_hash":-843765577,"_task_hash":-1363796091,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"resulting","start":4,"end":13,"id":1},{"text":"algorithms","start":14,"end":24,"id":2},{"text":"are","start":25,"end":28,"id":3},{"text":"efficient","start":29,"end":38,"id":4},{"text":",","start":38,"end":39,"id":5},{"text":"and","start":40,"end":43,"id":6},{"text":"perform","start":44,"end":51,"id":7},{"text":"well","start":52,"end":56,"id":8},{"text":"in","start":57,"end":59,"id":9},{"text":"simulations","start":60,"end":71,"id":10},{"text":"under","start":72,"end":77,"id":11},{"text":"stochastic","start":78,"end":88,"id":12},{"text":"and","start":89,"end":92,"id":13},{"text":"adversarial","start":93,"end":104,"id":14},{"text":"inputs","start":105,"end":111,"id":15},{"text":".","start":111,"end":112,"id":16}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":4,"end":13,"token_start":1,"token_end":1,"label":"ALGO","answer":"reject"},{"start":93,"end":104,"token_start":14,"token_end":14,"label":"ALGO","answer":"reject"}],"answer":"reject"}
{"text":"A larger decay rate gives a faster convergence rate.","_input_hash":-705196724,"_task_hash":532706899,"tokens":[{"text":"A","start":0,"end":1,"id":0},{"text":"larger","start":2,"end":8,"id":1},{"text":"decay","start":9,"end":14,"id":2},{"text":"rate","start":15,"end":19,"id":3},{"text":"gives","start":20,"end":25,"id":4},{"text":"a","start":26,"end":27,"id":5},{"text":"faster","start":28,"end":34,"id":6},{"text":"convergence","start":35,"end":46,"id":7},{"text":"rate","start":47,"end":51,"id":8},{"text":".","start":51,"end":52,"id":9}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"The main drawback of current methods of finding sparse solutions is their computational complexity.","_input_hash":1917160618,"_task_hash":-1522427682,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"main","start":4,"end":8,"id":1},{"text":"drawback","start":9,"end":17,"id":2},{"text":"of","start":18,"end":20,"id":3},{"text":"current","start":21,"end":28,"id":4},{"text":"methods","start":29,"end":36,"id":5},{"text":"of","start":37,"end":39,"id":6},{"text":"finding","start":40,"end":47,"id":7},{"text":"sparse","start":48,"end":54,"id":8},{"text":"solutions","start":55,"end":64,"id":9},{"text":"is","start":65,"end":67,"id":10},{"text":"their","start":68,"end":73,"id":11},{"text":"computational","start":74,"end":87,"id":12},{"text":"complexity","start":88,"end":98,"id":13},{"text":".","start":98,"end":99,"id":14}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Our experiments reveal that our approach outperforms state-of-the-art competitors, often significantly so, for large problems.","_input_hash":1060404976,"_task_hash":82362045,"tokens":[{"text":"Our","start":0,"end":3,"id":0},{"text":"experiments","start":4,"end":15,"id":1},{"text":"reveal","start":16,"end":22,"id":2},{"text":"that","start":23,"end":27,"id":3},{"text":"our","start":28,"end":31,"id":4},{"text":"approach","start":32,"end":40,"id":5},{"text":"outperforms","start":41,"end":52,"id":6},{"text":"state","start":53,"end":58,"id":7},{"text":"-","start":58,"end":59,"id":8},{"text":"of","start":59,"end":61,"id":9},{"text":"-","start":61,"end":62,"id":10},{"text":"the","start":62,"end":65,"id":11},{"text":"-","start":65,"end":66,"id":12},{"text":"art","start":66,"end":69,"id":13},{"text":"competitors","start":70,"end":81,"id":14},{"text":",","start":81,"end":82,"id":15},{"text":"often","start":83,"end":88,"id":16},{"text":"significantly","start":89,"end":102,"id":17},{"text":"so","start":103,"end":105,"id":18},{"text":",","start":105,"end":106,"id":19},{"text":"for","start":107,"end":110,"id":20},{"text":"large","start":111,"end":116,"id":21},{"text":"problems","start":117,"end":125,"id":22},{"text":".","start":125,"end":126,"id":23}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"This is useful because while importance values from random forests are used to screen variables, for example they are used to filter high throughput genomic data in Bioinformatics, very little theory exists about their properties.","_input_hash":1109894544,"_task_hash":-1965616499,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"is","start":5,"end":7,"id":1},{"text":"useful","start":8,"end":14,"id":2},{"text":"because","start":15,"end":22,"id":3},{"text":"while","start":23,"end":28,"id":4},{"text":"importance","start":29,"end":39,"id":5},{"text":"values","start":40,"end":46,"id":6},{"text":"from","start":47,"end":51,"id":7},{"text":"random","start":52,"end":58,"id":8},{"text":"forests","start":59,"end":66,"id":9},{"text":"are","start":67,"end":70,"id":10},{"text":"used","start":71,"end":75,"id":11},{"text":"to","start":76,"end":78,"id":12},{"text":"screen","start":79,"end":85,"id":13},{"text":"variables","start":86,"end":95,"id":14},{"text":",","start":95,"end":96,"id":15},{"text":"for","start":97,"end":100,"id":16},{"text":"example","start":101,"end":108,"id":17},{"text":"they","start":109,"end":113,"id":18},{"text":"are","start":114,"end":117,"id":19},{"text":"used","start":118,"end":122,"id":20},{"text":"to","start":123,"end":125,"id":21},{"text":"filter","start":126,"end":132,"id":22},{"text":"high","start":133,"end":137,"id":23},{"text":"throughput","start":138,"end":148,"id":24},{"text":"genomic","start":149,"end":156,"id":25},{"text":"data","start":157,"end":161,"id":26},{"text":"in","start":162,"end":164,"id":27},{"text":"Bioinformatics","start":165,"end":179,"id":28},{"text":",","start":179,"end":180,"id":29},{"text":"very","start":181,"end":185,"id":30},{"text":"little","start":186,"end":192,"id":31},{"text":"theory","start":193,"end":199,"id":32},{"text":"exists","start":200,"end":206,"id":33},{"text":"about","start":207,"end":212,"id":34},{"text":"their","start":213,"end":218,"id":35},{"text":"properties","start":219,"end":229,"id":36},{"text":".","start":229,"end":230,"id":37}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":52,"end":66,"token_start":8,"token_end":9,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We propose a novel algorithm to solve the expectation propagation relaxation of Bayesian inference for continuous-variable graphical models.","_input_hash":1160818325,"_task_hash":-512326557,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"propose","start":3,"end":10,"id":1},{"text":"a","start":11,"end":12,"id":2},{"text":"novel","start":13,"end":18,"id":3},{"text":"algorithm","start":19,"end":28,"id":4},{"text":"to","start":29,"end":31,"id":5},{"text":"solve","start":32,"end":37,"id":6},{"text":"the","start":38,"end":41,"id":7},{"text":"expectation","start":42,"end":53,"id":8},{"text":"propagation","start":54,"end":65,"id":9},{"text":"relaxation","start":66,"end":76,"id":10},{"text":"of","start":77,"end":79,"id":11},{"text":"Bayesian","start":80,"end":88,"id":12},{"text":"inference","start":89,"end":98,"id":13},{"text":"for","start":99,"end":102,"id":14},{"text":"continuous","start":103,"end":113,"id":15},{"text":"-","start":113,"end":114,"id":16},{"text":"variable","start":114,"end":122,"id":17},{"text":"graphical","start":123,"end":132,"id":18},{"text":"models","start":133,"end":139,"id":19},{"text":".","start":139,"end":140,"id":20}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":42,"end":65,"token_start":8,"token_end":9,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"The correlation screening approach bears tremendous dividends in terms of the type and strength of the asymptotic results that can be obtained.","_input_hash":872468230,"_task_hash":1512423520,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"correlation","start":4,"end":15,"id":1},{"text":"screening","start":16,"end":25,"id":2},{"text":"approach","start":26,"end":34,"id":3},{"text":"bears","start":35,"end":40,"id":4},{"text":"tremendous","start":41,"end":51,"id":5},{"text":"dividends","start":52,"end":61,"id":6},{"text":"in","start":62,"end":64,"id":7},{"text":"terms","start":65,"end":70,"id":8},{"text":"of","start":71,"end":73,"id":9},{"text":"the","start":74,"end":77,"id":10},{"text":"type","start":78,"end":82,"id":11},{"text":"and","start":83,"end":86,"id":12},{"text":"strength","start":87,"end":95,"id":13},{"text":"of","start":96,"end":98,"id":14},{"text":"the","start":99,"end":102,"id":15},{"text":"asymptotic","start":103,"end":113,"id":16},{"text":"results","start":114,"end":121,"id":17},{"text":"that","start":122,"end":126,"id":18},{"text":"can","start":127,"end":130,"id":19},{"text":"be","start":131,"end":133,"id":20},{"text":"obtained","start":134,"end":142,"id":21},{"text":".","start":142,"end":143,"id":22}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"A variational inference algorithm is derived for regression and binary classification.","_input_hash":537592127,"_task_hash":-1722145332,"tokens":[{"text":"A","start":0,"end":1,"id":0},{"text":"variational","start":2,"end":13,"id":1},{"text":"inference","start":14,"end":23,"id":2},{"text":"algorithm","start":24,"end":33,"id":3},{"text":"is","start":34,"end":36,"id":4},{"text":"derived","start":37,"end":44,"id":5},{"text":"for","start":45,"end":48,"id":6},{"text":"regression","start":49,"end":59,"id":7},{"text":"and","start":60,"end":63,"id":8},{"text":"binary","start":64,"end":70,"id":9},{"text":"classification","start":71,"end":85,"id":10},{"text":".","start":85,"end":86,"id":11}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"In contrast to the dominant approach to modeling whole images focusing on Markov random fields, we formulate our model in terms of a directed graphical model.","_input_hash":-1547175166,"_task_hash":2033391237,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"contrast","start":3,"end":11,"id":1},{"text":"to","start":12,"end":14,"id":2},{"text":"the","start":15,"end":18,"id":3},{"text":"dominant","start":19,"end":27,"id":4},{"text":"approach","start":28,"end":36,"id":5},{"text":"to","start":37,"end":39,"id":6},{"text":"modeling","start":40,"end":48,"id":7},{"text":"whole","start":49,"end":54,"id":8},{"text":"images","start":55,"end":61,"id":9},{"text":"focusing","start":62,"end":70,"id":10},{"text":"on","start":71,"end":73,"id":11},{"text":"Markov","start":74,"end":80,"id":12},{"text":"random","start":81,"end":87,"id":13},{"text":"fields","start":88,"end":94,"id":14},{"text":",","start":94,"end":95,"id":15},{"text":"we","start":96,"end":98,"id":16},{"text":"formulate","start":99,"end":108,"id":17},{"text":"our","start":109,"end":112,"id":18},{"text":"model","start":113,"end":118,"id":19},{"text":"in","start":119,"end":121,"id":20},{"text":"terms","start":122,"end":127,"id":21},{"text":"of","start":128,"end":130,"id":22},{"text":"a","start":131,"end":132,"id":23},{"text":"directed","start":133,"end":141,"id":24},{"text":"graphical","start":142,"end":151,"id":25},{"text":"model","start":152,"end":157,"id":26},{"text":".","start":157,"end":158,"id":27}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":74,"end":94,"token_start":12,"token_end":14,"label":"ALGO","answer":"accept"},{"start":133,"end":151,"token_start":24,"token_end":25,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We describe a simple and efficient procedure for approximating the L\\'evy measure of a $\\text{Gamma}(\\alpha,1)$ random variable.","_input_hash":1880885262,"_task_hash":-409737409,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"describe","start":3,"end":11,"id":1},{"text":"a","start":12,"end":13,"id":2},{"text":"simple","start":14,"end":20,"id":3},{"text":"and","start":21,"end":24,"id":4},{"text":"efficient","start":25,"end":34,"id":5},{"text":"procedure","start":35,"end":44,"id":6},{"text":"for","start":45,"end":48,"id":7},{"text":"approximating","start":49,"end":62,"id":8},{"text":"the","start":63,"end":66,"id":9},{"text":"L\\'evy","start":67,"end":73,"id":10},{"text":"measure","start":74,"end":81,"id":11},{"text":"of","start":82,"end":84,"id":12},{"text":"a","start":85,"end":86,"id":13},{"text":"$","start":87,"end":88,"id":14},{"text":"\\text{Gamma}(\\alpha,1)$","start":88,"end":111,"id":15},{"text":"random","start":112,"end":118,"id":16},{"text":"variable","start":119,"end":127,"id":17},{"text":".","start":127,"end":128,"id":18}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"This property may be used, for example, to solve the underdetermined Blind Source Separation (BSS) problem, or to find sparse representation of a signal in an `overcomplete' dictionary of primitive elements (i.e., the so-called atomic decomposition).","_input_hash":-2083551708,"_task_hash":-75038150,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"property","start":5,"end":13,"id":1},{"text":"may","start":14,"end":17,"id":2},{"text":"be","start":18,"end":20,"id":3},{"text":"used","start":21,"end":25,"id":4},{"text":",","start":25,"end":26,"id":5},{"text":"for","start":27,"end":30,"id":6},{"text":"example","start":31,"end":38,"id":7},{"text":",","start":38,"end":39,"id":8},{"text":"to","start":40,"end":42,"id":9},{"text":"solve","start":43,"end":48,"id":10},{"text":"the","start":49,"end":52,"id":11},{"text":"underdetermined","start":53,"end":68,"id":12},{"text":"Blind","start":69,"end":74,"id":13},{"text":"Source","start":75,"end":81,"id":14},{"text":"Separation","start":82,"end":92,"id":15},{"text":"(","start":93,"end":94,"id":16},{"text":"BSS","start":94,"end":97,"id":17},{"text":")","start":97,"end":98,"id":18},{"text":"problem","start":99,"end":106,"id":19},{"text":",","start":106,"end":107,"id":20},{"text":"or","start":108,"end":110,"id":21},{"text":"to","start":111,"end":113,"id":22},{"text":"find","start":114,"end":118,"id":23},{"text":"sparse","start":119,"end":125,"id":24},{"text":"representation","start":126,"end":140,"id":25},{"text":"of","start":141,"end":143,"id":26},{"text":"a","start":144,"end":145,"id":27},{"text":"signal","start":146,"end":152,"id":28},{"text":"in","start":153,"end":155,"id":29},{"text":"an","start":156,"end":158,"id":30},{"text":"`","start":159,"end":160,"id":31},{"text":"overcomplete","start":160,"end":172,"id":32},{"text":"'","start":172,"end":173,"id":33},{"text":"dictionary","start":174,"end":184,"id":34},{"text":"of","start":185,"end":187,"id":35},{"text":"primitive","start":188,"end":197,"id":36},{"text":"elements","start":198,"end":206,"id":37},{"text":"(","start":207,"end":208,"id":38},{"text":"i.e.","start":208,"end":212,"id":39},{"text":",","start":212,"end":213,"id":40},{"text":"the","start":214,"end":217,"id":41},{"text":"so","start":218,"end":220,"id":42},{"text":"-","start":220,"end":221,"id":43},{"text":"called","start":221,"end":227,"id":44},{"text":"atomic","start":228,"end":234,"id":45},{"text":"decomposition","start":235,"end":248,"id":46},{"text":")","start":248,"end":249,"id":47},{"text":".","start":249,"end":250,"id":48}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"In our method, a solution is computed by iteratively performing dependence estimation and maximization:","_input_hash":-1841444250,"_task_hash":773563512,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"our","start":3,"end":6,"id":1},{"text":"method","start":7,"end":13,"id":2},{"text":",","start":13,"end":14,"id":3},{"text":"a","start":15,"end":16,"id":4},{"text":"solution","start":17,"end":25,"id":5},{"text":"is","start":26,"end":28,"id":6},{"text":"computed","start":29,"end":37,"id":7},{"text":"by","start":38,"end":40,"id":8},{"text":"iteratively","start":41,"end":52,"id":9},{"text":"performing","start":53,"end":63,"id":10},{"text":"dependence","start":64,"end":74,"id":11},{"text":"estimation","start":75,"end":85,"id":12},{"text":"and","start":86,"end":89,"id":13},{"text":"maximization","start":90,"end":102,"id":14},{"text":":","start":102,"end":103,"id":15}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Given a corpus of documents, a posterior inference algorithm finds an approximation to a posterior distribution over trees, topics and allocations of words to levels of the tree.","_input_hash":-1107644007,"_task_hash":1722989748,"tokens":[{"text":"Given","start":0,"end":5,"id":0},{"text":"a","start":6,"end":7,"id":1},{"text":"corpus","start":8,"end":14,"id":2},{"text":"of","start":15,"end":17,"id":3},{"text":"documents","start":18,"end":27,"id":4},{"text":",","start":27,"end":28,"id":5},{"text":"a","start":29,"end":30,"id":6},{"text":"posterior","start":31,"end":40,"id":7},{"text":"inference","start":41,"end":50,"id":8},{"text":"algorithm","start":51,"end":60,"id":9},{"text":"finds","start":61,"end":66,"id":10},{"text":"an","start":67,"end":69,"id":11},{"text":"approximation","start":70,"end":83,"id":12},{"text":"to","start":84,"end":86,"id":13},{"text":"a","start":87,"end":88,"id":14},{"text":"posterior","start":89,"end":98,"id":15},{"text":"distribution","start":99,"end":111,"id":16},{"text":"over","start":112,"end":116,"id":17},{"text":"trees","start":117,"end":122,"id":18},{"text":",","start":122,"end":123,"id":19},{"text":"topics","start":124,"end":130,"id":20},{"text":"and","start":131,"end":134,"id":21},{"text":"allocations","start":135,"end":146,"id":22},{"text":"of","start":147,"end":149,"id":23},{"text":"words","start":150,"end":155,"id":24},{"text":"to","start":156,"end":158,"id":25},{"text":"levels","start":159,"end":165,"id":26},{"text":"of","start":166,"end":168,"id":27},{"text":"the","start":169,"end":172,"id":28},{"text":"tree","start":173,"end":177,"id":29},{"text":".","start":177,"end":178,"id":30}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We describe many vantage points on the Baire metric and its use in clustering data, or its use in preprocessing and structuring data in order to support search and retrieval operations.","_input_hash":57737664,"_task_hash":321858652,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"describe","start":3,"end":11,"id":1},{"text":"many","start":12,"end":16,"id":2},{"text":"vantage","start":17,"end":24,"id":3},{"text":"points","start":25,"end":31,"id":4},{"text":"on","start":32,"end":34,"id":5},{"text":"the","start":35,"end":38,"id":6},{"text":"Baire","start":39,"end":44,"id":7},{"text":"metric","start":45,"end":51,"id":8},{"text":"and","start":52,"end":55,"id":9},{"text":"its","start":56,"end":59,"id":10},{"text":"use","start":60,"end":63,"id":11},{"text":"in","start":64,"end":66,"id":12},{"text":"clustering","start":67,"end":77,"id":13},{"text":"data","start":78,"end":82,"id":14},{"text":",","start":82,"end":83,"id":15},{"text":"or","start":84,"end":86,"id":16},{"text":"its","start":87,"end":90,"id":17},{"text":"use","start":91,"end":94,"id":18},{"text":"in","start":95,"end":97,"id":19},{"text":"preprocessing","start":98,"end":111,"id":20},{"text":"and","start":112,"end":115,"id":21},{"text":"structuring","start":116,"end":127,"id":22},{"text":"data","start":128,"end":132,"id":23},{"text":"in","start":133,"end":135,"id":24},{"text":"order","start":136,"end":141,"id":25},{"text":"to","start":142,"end":144,"id":26},{"text":"support","start":145,"end":152,"id":27},{"text":"search","start":153,"end":159,"id":28},{"text":"and","start":160,"end":163,"id":29},{"text":"retrieval","start":164,"end":173,"id":30},{"text":"operations","start":174,"end":184,"id":31},{"text":".","start":184,"end":185,"id":32}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"In particular, we present a bound on the excess risk incurred by the method.","_input_hash":791849239,"_task_hash":277318544,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"particular","start":3,"end":13,"id":1},{"text":",","start":13,"end":14,"id":2},{"text":"we","start":15,"end":17,"id":3},{"text":"present","start":18,"end":25,"id":4},{"text":"a","start":26,"end":27,"id":5},{"text":"bound","start":28,"end":33,"id":6},{"text":"on","start":34,"end":36,"id":7},{"text":"the","start":37,"end":40,"id":8},{"text":"excess","start":41,"end":47,"id":9},{"text":"risk","start":48,"end":52,"id":10},{"text":"incurred","start":53,"end":61,"id":11},{"text":"by","start":62,"end":64,"id":12},{"text":"the","start":65,"end":68,"id":13},{"text":"method","start":69,"end":75,"id":14},{"text":".","start":75,"end":76,"id":15}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"This flexible framework suggests that arbitrarily complex structures can be encoded with an intricate set of groups.","_input_hash":-36195648,"_task_hash":121122839,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"flexible","start":5,"end":13,"id":1},{"text":"framework","start":14,"end":23,"id":2},{"text":"suggests","start":24,"end":32,"id":3},{"text":"that","start":33,"end":37,"id":4},{"text":"arbitrarily","start":38,"end":49,"id":5},{"text":"complex","start":50,"end":57,"id":6},{"text":"structures","start":58,"end":68,"id":7},{"text":"can","start":69,"end":72,"id":8},{"text":"be","start":73,"end":75,"id":9},{"text":"encoded","start":76,"end":83,"id":10},{"text":"with","start":84,"end":88,"id":11},{"text":"an","start":89,"end":91,"id":12},{"text":"intricate","start":92,"end":101,"id":13},{"text":"set","start":102,"end":105,"id":14},{"text":"of","start":106,"end":108,"id":15},{"text":"groups","start":109,"end":115,"id":16},{"text":".","start":115,"end":116,"id":17}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"The second contribution of this paper is to design anytime robust policies for specific multi-armed bandit problems in which some restrictions are put on the set of possible distributions of the different arms.","_input_hash":1027711212,"_task_hash":-323374235,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"second","start":4,"end":10,"id":1},{"text":"contribution","start":11,"end":23,"id":2},{"text":"of","start":24,"end":26,"id":3},{"text":"this","start":27,"end":31,"id":4},{"text":"paper","start":32,"end":37,"id":5},{"text":"is","start":38,"end":40,"id":6},{"text":"to","start":41,"end":43,"id":7},{"text":"design","start":44,"end":50,"id":8},{"text":"anytime","start":51,"end":58,"id":9},{"text":"robust","start":59,"end":65,"id":10},{"text":"policies","start":66,"end":74,"id":11},{"text":"for","start":75,"end":78,"id":12},{"text":"specific","start":79,"end":87,"id":13},{"text":"multi","start":88,"end":93,"id":14},{"text":"-","start":93,"end":94,"id":15},{"text":"armed","start":94,"end":99,"id":16},{"text":"bandit","start":100,"end":106,"id":17},{"text":"problems","start":107,"end":115,"id":18},{"text":"in","start":116,"end":118,"id":19},{"text":"which","start":119,"end":124,"id":20},{"text":"some","start":125,"end":129,"id":21},{"text":"restrictions","start":130,"end":142,"id":22},{"text":"are","start":143,"end":146,"id":23},{"text":"put","start":147,"end":150,"id":24},{"text":"on","start":151,"end":153,"id":25},{"text":"the","start":154,"end":157,"id":26},{"text":"set","start":158,"end":161,"id":27},{"text":"of","start":162,"end":164,"id":28},{"text":"possible","start":165,"end":173,"id":29},{"text":"distributions","start":174,"end":187,"id":30},{"text":"of","start":188,"end":190,"id":31},{"text":"the","start":191,"end":194,"id":32},{"text":"different","start":195,"end":204,"id":33},{"text":"arms","start":205,"end":209,"id":34},{"text":".","start":209,"end":210,"id":35}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"We propose in this work a new family of kernels for variable-length time series.","_input_hash":-1062527547,"_task_hash":1737403691,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"propose","start":3,"end":10,"id":1},{"text":"in","start":11,"end":13,"id":2},{"text":"this","start":14,"end":18,"id":3},{"text":"work","start":19,"end":23,"id":4},{"text":"a","start":24,"end":25,"id":5},{"text":"new","start":26,"end":29,"id":6},{"text":"family","start":30,"end":36,"id":7},{"text":"of","start":37,"end":39,"id":8},{"text":"kernels","start":40,"end":47,"id":9},{"text":"for","start":48,"end":51,"id":10},{"text":"variable","start":52,"end":60,"id":11},{"text":"-","start":60,"end":61,"id":12},{"text":"length","start":61,"end":67,"id":13},{"text":"time","start":68,"end":72,"id":14},{"text":"series","start":73,"end":79,"id":15},{"text":".","start":79,"end":80,"id":16}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"A variational counterpart to least-trimmed squares regression is shown closely related to an L0-(pseudo)norm-regularized estimator, that encourages sparsity in a vector explicitly modeling the outliers.","_input_hash":-971639675,"_task_hash":979420203,"tokens":[{"text":"A","start":0,"end":1,"id":0},{"text":"variational","start":2,"end":13,"id":1},{"text":"counterpart","start":14,"end":25,"id":2},{"text":"to","start":26,"end":28,"id":3},{"text":"least","start":29,"end":34,"id":4},{"text":"-","start":34,"end":35,"id":5},{"text":"trimmed","start":35,"end":42,"id":6},{"text":"squares","start":43,"end":50,"id":7},{"text":"regression","start":51,"end":61,"id":8},{"text":"is","start":62,"end":64,"id":9},{"text":"shown","start":65,"end":70,"id":10},{"text":"closely","start":71,"end":78,"id":11},{"text":"related","start":79,"end":86,"id":12},{"text":"to","start":87,"end":89,"id":13},{"text":"an","start":90,"end":92,"id":14},{"text":"L0-(pseudo)norm","start":93,"end":108,"id":15},{"text":"-","start":108,"end":109,"id":16},{"text":"regularized","start":109,"end":120,"id":17},{"text":"estimator","start":121,"end":130,"id":18},{"text":",","start":130,"end":131,"id":19},{"text":"that","start":132,"end":136,"id":20},{"text":"encourages","start":137,"end":147,"id":21},{"text":"sparsity","start":148,"end":156,"id":22},{"text":"in","start":157,"end":159,"id":23},{"text":"a","start":160,"end":161,"id":24},{"text":"vector","start":162,"end":168,"id":25},{"text":"explicitly","start":169,"end":179,"id":26},{"text":"modeling","start":180,"end":188,"id":27},{"text":"the","start":189,"end":192,"id":28},{"text":"outliers","start":193,"end":201,"id":29},{"text":".","start":201,"end":202,"id":30}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":29,"end":61,"token_start":4,"token_end":8,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Our analysis addresses the following security-related issues:","_input_hash":-355262591,"_task_hash":-236809872,"tokens":[{"text":"Our","start":0,"end":3,"id":0},{"text":"analysis","start":4,"end":12,"id":1},{"text":"addresses","start":13,"end":22,"id":2},{"text":"the","start":23,"end":26,"id":3},{"text":"following","start":27,"end":36,"id":4},{"text":"security","start":37,"end":45,"id":5},{"text":"-","start":45,"end":46,"id":6},{"text":"related","start":46,"end":53,"id":7},{"text":"issues","start":54,"end":60,"id":8},{"text":":","start":60,"end":61,"id":9}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Dendrograms used in data analysis are ultrametric spaces, hence objects of nonarchimedean geometry.","_input_hash":-1804434462,"_task_hash":1288390984,"tokens":[{"text":"Dendrograms","start":0,"end":11,"id":0},{"text":"used","start":12,"end":16,"id":1},{"text":"in","start":17,"end":19,"id":2},{"text":"data","start":20,"end":24,"id":3},{"text":"analysis","start":25,"end":33,"id":4},{"text":"are","start":34,"end":37,"id":5},{"text":"ultrametric","start":38,"end":49,"id":6},{"text":"spaces","start":50,"end":56,"id":7},{"text":",","start":56,"end":57,"id":8},{"text":"hence","start":58,"end":63,"id":9},{"text":"objects","start":64,"end":71,"id":10},{"text":"of","start":72,"end":74,"id":11},{"text":"nonarchimedean","start":75,"end":89,"id":12},{"text":"geometry","start":90,"end":98,"id":13},{"text":".","start":98,"end":99,"id":14}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Furthermore, we derive an efficient and provably convergent optimisation algorithm.","_input_hash":-179135990,"_task_hash":429552575,"tokens":[{"text":"Furthermore","start":0,"end":11,"id":0},{"text":",","start":11,"end":12,"id":1},{"text":"we","start":13,"end":15,"id":2},{"text":"derive","start":16,"end":22,"id":3},{"text":"an","start":23,"end":25,"id":4},{"text":"efficient","start":26,"end":35,"id":5},{"text":"and","start":36,"end":39,"id":6},{"text":"provably","start":40,"end":48,"id":7},{"text":"convergent","start":49,"end":59,"id":8},{"text":"optimisation","start":60,"end":72,"id":9},{"text":"algorithm","start":73,"end":82,"id":10},{"text":".","start":82,"end":83,"id":11}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":49,"end":72,"token_start":8,"token_end":9,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"The source code and scripts are available from http://cogsys.imm.dtu.dk/slim/.","_input_hash":383922989,"_task_hash":1548251900,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"source","start":4,"end":10,"id":1},{"text":"code","start":11,"end":15,"id":2},{"text":"and","start":16,"end":19,"id":3},{"text":"scripts","start":20,"end":27,"id":4},{"text":"are","start":28,"end":31,"id":5},{"text":"available","start":32,"end":41,"id":6},{"text":"from","start":42,"end":46,"id":7},{"text":"http://cogsys.imm.dtu.dk/slim/.","start":47,"end":78,"id":8}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We provide an online algorithm with a complexity of O(FK) in time and memory for updates in the dictionary.","_input_hash":521561455,"_task_hash":2036901374,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"provide","start":3,"end":10,"id":1},{"text":"an","start":11,"end":13,"id":2},{"text":"online","start":14,"end":20,"id":3},{"text":"algorithm","start":21,"end":30,"id":4},{"text":"with","start":31,"end":35,"id":5},{"text":"a","start":36,"end":37,"id":6},{"text":"complexity","start":38,"end":48,"id":7},{"text":"of","start":49,"end":51,"id":8},{"text":"O(FK","start":52,"end":56,"id":9},{"text":")","start":56,"end":57,"id":10},{"text":"in","start":58,"end":60,"id":11},{"text":"time","start":61,"end":65,"id":12},{"text":"and","start":66,"end":69,"id":13},{"text":"memory","start":70,"end":76,"id":14},{"text":"for","start":77,"end":80,"id":15},{"text":"updates","start":81,"end":88,"id":16},{"text":"in","start":89,"end":91,"id":17},{"text":"the","start":92,"end":95,"id":18},{"text":"dictionary","start":96,"end":106,"id":19},{"text":".","start":106,"end":107,"id":20}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":14,"end":20,"token_start":3,"token_end":3,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"By marrying convergent EP ideas from (Opper&Winther 05) with covariance decoupling techniques (Wipf&Nagarajan 08, Nickisch&Seeger 09), it runs at least an order of magnitude faster than the most commonly used EP solver.","_input_hash":116563820,"_task_hash":2044065124,"tokens":[{"text":"By","start":0,"end":2,"id":0},{"text":"marrying","start":3,"end":11,"id":1},{"text":"convergent","start":12,"end":22,"id":2},{"text":"EP","start":23,"end":25,"id":3},{"text":"ideas","start":26,"end":31,"id":4},{"text":"from","start":32,"end":36,"id":5},{"text":"(","start":37,"end":38,"id":6},{"text":"Opper&Winther","start":38,"end":51,"id":7},{"text":"05","start":52,"end":54,"id":8},{"text":")","start":54,"end":55,"id":9},{"text":"with","start":56,"end":60,"id":10},{"text":"covariance","start":61,"end":71,"id":11},{"text":"decoupling","start":72,"end":82,"id":12},{"text":"techniques","start":83,"end":93,"id":13},{"text":"(","start":94,"end":95,"id":14},{"text":"Wipf&Nagarajan","start":95,"end":109,"id":15},{"text":"08","start":110,"end":112,"id":16},{"text":",","start":112,"end":113,"id":17},{"text":"Nickisch&Seeger","start":114,"end":129,"id":18},{"text":"09","start":130,"end":132,"id":19},{"text":")","start":132,"end":133,"id":20},{"text":",","start":133,"end":134,"id":21},{"text":"it","start":135,"end":137,"id":22},{"text":"runs","start":138,"end":142,"id":23},{"text":"at","start":143,"end":145,"id":24},{"text":"least","start":146,"end":151,"id":25},{"text":"an","start":152,"end":154,"id":26},{"text":"order","start":155,"end":160,"id":27},{"text":"of","start":161,"end":163,"id":28},{"text":"magnitude","start":164,"end":173,"id":29},{"text":"faster","start":174,"end":180,"id":30},{"text":"than","start":181,"end":185,"id":31},{"text":"the","start":186,"end":189,"id":32},{"text":"most","start":190,"end":194,"id":33},{"text":"commonly","start":195,"end":203,"id":34},{"text":"used","start":204,"end":208,"id":35},{"text":"EP","start":209,"end":211,"id":36},{"text":"solver","start":212,"end":218,"id":37},{"text":".","start":218,"end":219,"id":38}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"The key idea is to identify which variables are exogenous based on non-Gaussianity instead of estimating the entire structure of the model.","_input_hash":-2077789567,"_task_hash":-912162644,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"key","start":4,"end":7,"id":1},{"text":"idea","start":8,"end":12,"id":2},{"text":"is","start":13,"end":15,"id":3},{"text":"to","start":16,"end":18,"id":4},{"text":"identify","start":19,"end":27,"id":5},{"text":"which","start":28,"end":33,"id":6},{"text":"variables","start":34,"end":43,"id":7},{"text":"are","start":44,"end":47,"id":8},{"text":"exogenous","start":48,"end":57,"id":9},{"text":"based","start":58,"end":63,"id":10},{"text":"on","start":64,"end":66,"id":11},{"text":"non","start":67,"end":70,"id":12},{"text":"-","start":70,"end":71,"id":13},{"text":"Gaussianity","start":71,"end":82,"id":14},{"text":"instead","start":83,"end":90,"id":15},{"text":"of","start":91,"end":93,"id":16},{"text":"estimating","start":94,"end":104,"id":17},{"text":"the","start":105,"end":108,"id":18},{"text":"entire","start":109,"end":115,"id":19},{"text":"structure","start":116,"end":125,"id":20},{"text":"of","start":126,"end":128,"id":21},{"text":"the","start":129,"end":132,"id":22},{"text":"model","start":133,"end":138,"id":23},{"text":".","start":138,"end":139,"id":24}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We derive an approximate maximum-likelihood procedure for parameter estimation, which relies on variational methods to handle intractable posterior expectations.","_input_hash":-1403220553,"_task_hash":251824613,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"derive","start":3,"end":9,"id":1},{"text":"an","start":10,"end":12,"id":2},{"text":"approximate","start":13,"end":24,"id":3},{"text":"maximum","start":25,"end":32,"id":4},{"text":"-","start":32,"end":33,"id":5},{"text":"likelihood","start":33,"end":43,"id":6},{"text":"procedure","start":44,"end":53,"id":7},{"text":"for","start":54,"end":57,"id":8},{"text":"parameter","start":58,"end":67,"id":9},{"text":"estimation","start":68,"end":78,"id":10},{"text":",","start":78,"end":79,"id":11},{"text":"which","start":80,"end":85,"id":12},{"text":"relies","start":86,"end":92,"id":13},{"text":"on","start":93,"end":95,"id":14},{"text":"variational","start":96,"end":107,"id":15},{"text":"methods","start":108,"end":115,"id":16},{"text":"to","start":116,"end":118,"id":17},{"text":"handle","start":119,"end":125,"id":18},{"text":"intractable","start":126,"end":137,"id":19},{"text":"posterior","start":138,"end":147,"id":20},{"text":"expectations","start":148,"end":160,"id":21},{"text":".","start":160,"end":161,"id":22}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Both sets of results give insight into the consequences of choosing an increasingly complex set of groups for the procedure, as well as what happens when the set of groups cannot recover the true sparsity pattern.","_input_hash":-134084783,"_task_hash":-1272501679,"tokens":[{"text":"Both","start":0,"end":4,"id":0},{"text":"sets","start":5,"end":9,"id":1},{"text":"of","start":10,"end":12,"id":2},{"text":"results","start":13,"end":20,"id":3},{"text":"give","start":21,"end":25,"id":4},{"text":"insight","start":26,"end":33,"id":5},{"text":"into","start":34,"end":38,"id":6},{"text":"the","start":39,"end":42,"id":7},{"text":"consequences","start":43,"end":55,"id":8},{"text":"of","start":56,"end":58,"id":9},{"text":"choosing","start":59,"end":67,"id":10},{"text":"an","start":68,"end":70,"id":11},{"text":"increasingly","start":71,"end":83,"id":12},{"text":"complex","start":84,"end":91,"id":13},{"text":"set","start":92,"end":95,"id":14},{"text":"of","start":96,"end":98,"id":15},{"text":"groups","start":99,"end":105,"id":16},{"text":"for","start":106,"end":109,"id":17},{"text":"the","start":110,"end":113,"id":18},{"text":"procedure","start":114,"end":123,"id":19},{"text":",","start":123,"end":124,"id":20},{"text":"as","start":125,"end":127,"id":21},{"text":"well","start":128,"end":132,"id":22},{"text":"as","start":133,"end":135,"id":23},{"text":"what","start":136,"end":140,"id":24},{"text":"happens","start":141,"end":148,"id":25},{"text":"when","start":149,"end":153,"id":26},{"text":"the","start":154,"end":157,"id":27},{"text":"set","start":158,"end":161,"id":28},{"text":"of","start":162,"end":164,"id":29},{"text":"groups","start":165,"end":171,"id":30},{"text":"can","start":172,"end":175,"id":31},{"text":"not","start":175,"end":178,"id":32},{"text":"recover","start":179,"end":186,"id":33},{"text":"the","start":187,"end":190,"id":34},{"text":"true","start":191,"end":195,"id":35},{"text":"sparsity","start":196,"end":204,"id":36},{"text":"pattern","start":205,"end":212,"id":37},{"text":".","start":212,"end":213,"id":38}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Kolar et al (2010b) proposed a method based on kernel-smoothing l1-penalized logistic regression for estimating time-varying networks from nodal observations collected from a time-series of observational data.","_input_hash":-1486393567,"_task_hash":-1142128691,"tokens":[{"text":"Kolar","start":0,"end":5,"id":0},{"text":"et","start":6,"end":8,"id":1},{"text":"al","start":9,"end":11,"id":2},{"text":"(","start":12,"end":13,"id":3},{"text":"2010b","start":13,"end":18,"id":4},{"text":")","start":18,"end":19,"id":5},{"text":"proposed","start":20,"end":28,"id":6},{"text":"a","start":29,"end":30,"id":7},{"text":"method","start":31,"end":37,"id":8},{"text":"based","start":38,"end":43,"id":9},{"text":"on","start":44,"end":46,"id":10},{"text":"kernel","start":47,"end":53,"id":11},{"text":"-","start":53,"end":54,"id":12},{"text":"smoothing","start":54,"end":63,"id":13},{"text":"l1-penalized","start":64,"end":76,"id":14},{"text":"logistic","start":77,"end":85,"id":15},{"text":"regression","start":86,"end":96,"id":16},{"text":"for","start":97,"end":100,"id":17},{"text":"estimating","start":101,"end":111,"id":18},{"text":"time","start":112,"end":116,"id":19},{"text":"-","start":116,"end":117,"id":20},{"text":"varying","start":117,"end":124,"id":21},{"text":"networks","start":125,"end":133,"id":22},{"text":"from","start":134,"end":138,"id":23},{"text":"nodal","start":139,"end":144,"id":24},{"text":"observations","start":145,"end":157,"id":25},{"text":"collected","start":158,"end":167,"id":26},{"text":"from","start":168,"end":172,"id":27},{"text":"a","start":173,"end":174,"id":28},{"text":"time","start":175,"end":179,"id":29},{"text":"-","start":179,"end":180,"id":30},{"text":"series","start":180,"end":186,"id":31},{"text":"of","start":187,"end":189,"id":32},{"text":"observational","start":190,"end":203,"id":33},{"text":"data","start":204,"end":208,"id":34},{"text":".","start":208,"end":209,"id":35}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":47,"end":96,"token_start":11,"token_end":16,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"The most common way of introducing such problem-specific guidance in autoencoders has been through the incorporation of a parametric component that ties the latent representation to the label information.","_input_hash":707073491,"_task_hash":1320855624,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"most","start":4,"end":8,"id":1},{"text":"common","start":9,"end":15,"id":2},{"text":"way","start":16,"end":19,"id":3},{"text":"of","start":20,"end":22,"id":4},{"text":"introducing","start":23,"end":34,"id":5},{"text":"such","start":35,"end":39,"id":6},{"text":"problem","start":40,"end":47,"id":7},{"text":"-","start":47,"end":48,"id":8},{"text":"specific","start":48,"end":56,"id":9},{"text":"guidance","start":57,"end":65,"id":10},{"text":"in","start":66,"end":68,"id":11},{"text":"autoencoders","start":69,"end":81,"id":12},{"text":"has","start":82,"end":85,"id":13},{"text":"been","start":86,"end":90,"id":14},{"text":"through","start":91,"end":98,"id":15},{"text":"the","start":99,"end":102,"id":16},{"text":"incorporation","start":103,"end":116,"id":17},{"text":"of","start":117,"end":119,"id":18},{"text":"a","start":120,"end":121,"id":19},{"text":"parametric","start":122,"end":132,"id":20},{"text":"component","start":133,"end":142,"id":21},{"text":"that","start":143,"end":147,"id":22},{"text":"ties","start":148,"end":152,"id":23},{"text":"the","start":153,"end":156,"id":24},{"text":"latent","start":157,"end":163,"id":25},{"text":"representation","start":164,"end":178,"id":26},{"text":"to","start":179,"end":181,"id":27},{"text":"the","start":182,"end":185,"id":28},{"text":"label","start":186,"end":191,"id":29},{"text":"information","start":192,"end":203,"id":30},{"text":".","start":203,"end":204,"id":31}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":69,"end":81,"token_start":12,"token_end":12,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We detail optimal algorithms to predict sequences generated in this way.","_input_hash":1746763958,"_task_hash":1353998774,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"detail","start":3,"end":9,"id":1},{"text":"optimal","start":10,"end":17,"id":2},{"text":"algorithms","start":18,"end":28,"id":3},{"text":"to","start":29,"end":31,"id":4},{"text":"predict","start":32,"end":39,"id":5},{"text":"sequences","start":40,"end":49,"id":6},{"text":"generated","start":50,"end":59,"id":7},{"text":"in","start":60,"end":62,"id":8},{"text":"this","start":63,"end":67,"id":9},{"text":"way","start":68,"end":71,"id":10},{"text":".","start":71,"end":72,"id":11}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":10,"end":17,"token_start":2,"token_end":2,"label":"ALGO","answer":"reject"}],"answer":"reject"}
{"text":"LSIR learns the additive noise model through the minimization of an estimator of the squared-loss mutual information between inputs and residuals.","_input_hash":164197147,"_task_hash":-347441251,"tokens":[{"text":"LSIR","start":0,"end":4,"id":0},{"text":"learns","start":5,"end":11,"id":1},{"text":"the","start":12,"end":15,"id":2},{"text":"additive","start":16,"end":24,"id":3},{"text":"noise","start":25,"end":30,"id":4},{"text":"model","start":31,"end":36,"id":5},{"text":"through","start":37,"end":44,"id":6},{"text":"the","start":45,"end":48,"id":7},{"text":"minimization","start":49,"end":61,"id":8},{"text":"of","start":62,"end":64,"id":9},{"text":"an","start":65,"end":67,"id":10},{"text":"estimator","start":68,"end":77,"id":11},{"text":"of","start":78,"end":80,"id":12},{"text":"the","start":81,"end":84,"id":13},{"text":"squared","start":85,"end":92,"id":14},{"text":"-","start":92,"end":93,"id":15},{"text":"loss","start":93,"end":97,"id":16},{"text":"mutual","start":98,"end":104,"id":17},{"text":"information","start":105,"end":116,"id":18},{"text":"between","start":117,"end":124,"id":19},{"text":"inputs","start":125,"end":131,"id":20},{"text":"and","start":132,"end":135,"id":21},{"text":"residuals","start":136,"end":145,"id":22},{"text":".","start":145,"end":146,"id":23}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":16,"end":30,"token_start":3,"token_end":4,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"In contrast to most previous algorithms, our method is provably convergent.","_input_hash":-1707314796,"_task_hash":-710689949,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"contrast","start":3,"end":11,"id":1},{"text":"to","start":12,"end":14,"id":2},{"text":"most","start":15,"end":19,"id":3},{"text":"previous","start":20,"end":28,"id":4},{"text":"algorithms","start":29,"end":39,"id":5},{"text":",","start":39,"end":40,"id":6},{"text":"our","start":41,"end":44,"id":7},{"text":"method","start":45,"end":51,"id":8},{"text":"is","start":52,"end":54,"id":9},{"text":"provably","start":55,"end":63,"id":10},{"text":"convergent","start":64,"end":74,"id":11},{"text":".","start":74,"end":75,"id":12}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Vapnik-Chervonenkis (VC) dimension is a fundamental measure of the generalization capacity of learning algorithms.","_input_hash":-1301337095,"_task_hash":-455084202,"tokens":[{"text":"Vapnik","start":0,"end":6,"id":0},{"text":"-","start":6,"end":7,"id":1},{"text":"Chervonenkis","start":7,"end":19,"id":2},{"text":"(","start":20,"end":21,"id":3},{"text":"VC","start":21,"end":23,"id":4},{"text":")","start":23,"end":24,"id":5},{"text":"dimension","start":25,"end":34,"id":6},{"text":"is","start":35,"end":37,"id":7},{"text":"a","start":38,"end":39,"id":8},{"text":"fundamental","start":40,"end":51,"id":9},{"text":"measure","start":52,"end":59,"id":10},{"text":"of","start":60,"end":62,"id":11},{"text":"the","start":63,"end":66,"id":12},{"text":"generalization","start":67,"end":81,"id":13},{"text":"capacity","start":82,"end":90,"id":14},{"text":"of","start":91,"end":93,"id":15},{"text":"learning","start":94,"end":102,"id":16},{"text":"algorithms","start":103,"end":113,"id":17},{"text":".","start":113,"end":114,"id":18}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":0,"end":19,"token_start":0,"token_end":2,"label":"ALGO","answer":"reject"}],"answer":"reject"}
{"text":"This monograph deals with adaptive supervised classification, using tools borrowed from statistical mechanics and information theory, stemming from the PACBayesian approach pioneered by David McAllester and applied to a conception of statistical learning theory forged by Vladimir Vapnik.","_input_hash":291673665,"_task_hash":1261677885,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"monograph","start":5,"end":14,"id":1},{"text":"deals","start":15,"end":20,"id":2},{"text":"with","start":21,"end":25,"id":3},{"text":"adaptive","start":26,"end":34,"id":4},{"text":"supervised","start":35,"end":45,"id":5},{"text":"classification","start":46,"end":60,"id":6},{"text":",","start":60,"end":61,"id":7},{"text":"using","start":62,"end":67,"id":8},{"text":"tools","start":68,"end":73,"id":9},{"text":"borrowed","start":74,"end":82,"id":10},{"text":"from","start":83,"end":87,"id":11},{"text":"statistical","start":88,"end":99,"id":12},{"text":"mechanics","start":100,"end":109,"id":13},{"text":"and","start":110,"end":113,"id":14},{"text":"information","start":114,"end":125,"id":15},{"text":"theory","start":126,"end":132,"id":16},{"text":",","start":132,"end":133,"id":17},{"text":"stemming","start":134,"end":142,"id":18},{"text":"from","start":143,"end":147,"id":19},{"text":"the","start":148,"end":151,"id":20},{"text":"PACBayesian","start":152,"end":163,"id":21},{"text":"approach","start":164,"end":172,"id":22},{"text":"pioneered","start":173,"end":182,"id":23},{"text":"by","start":183,"end":185,"id":24},{"text":"David","start":186,"end":191,"id":25},{"text":"McAllester","start":192,"end":202,"id":26},{"text":"and","start":203,"end":206,"id":27},{"text":"applied","start":207,"end":214,"id":28},{"text":"to","start":215,"end":217,"id":29},{"text":"a","start":218,"end":219,"id":30},{"text":"conception","start":220,"end":230,"id":31},{"text":"of","start":231,"end":233,"id":32},{"text":"statistical","start":234,"end":245,"id":33},{"text":"learning","start":246,"end":254,"id":34},{"text":"theory","start":255,"end":261,"id":35},{"text":"forged","start":262,"end":268,"id":36},{"text":"by","start":269,"end":271,"id":37},{"text":"Vladimir","start":272,"end":280,"id":38},{"text":"Vapnik","start":281,"end":287,"id":39},{"text":".","start":287,"end":288,"id":40}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"This technical report derives the first two posterior moments of the maximum of two correlated Gaussian variables and the first two posterior moments of the two generating variables (corresponding to Gaussian approximations minimizing relative entropy).","_input_hash":-1369732246,"_task_hash":-1544135136,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"technical","start":5,"end":14,"id":1},{"text":"report","start":15,"end":21,"id":2},{"text":"derives","start":22,"end":29,"id":3},{"text":"the","start":30,"end":33,"id":4},{"text":"first","start":34,"end":39,"id":5},{"text":"two","start":40,"end":43,"id":6},{"text":"posterior","start":44,"end":53,"id":7},{"text":"moments","start":54,"end":61,"id":8},{"text":"of","start":62,"end":64,"id":9},{"text":"the","start":65,"end":68,"id":10},{"text":"maximum","start":69,"end":76,"id":11},{"text":"of","start":77,"end":79,"id":12},{"text":"two","start":80,"end":83,"id":13},{"text":"correlated","start":84,"end":94,"id":14},{"text":"Gaussian","start":95,"end":103,"id":15},{"text":"variables","start":104,"end":113,"id":16},{"text":"and","start":114,"end":117,"id":17},{"text":"the","start":118,"end":121,"id":18},{"text":"first","start":122,"end":127,"id":19},{"text":"two","start":128,"end":131,"id":20},{"text":"posterior","start":132,"end":141,"id":21},{"text":"moments","start":142,"end":149,"id":22},{"text":"of","start":150,"end":152,"id":23},{"text":"the","start":153,"end":156,"id":24},{"text":"two","start":157,"end":160,"id":25},{"text":"generating","start":161,"end":171,"id":26},{"text":"variables","start":172,"end":181,"id":27},{"text":"(","start":182,"end":183,"id":28},{"text":"corresponding","start":183,"end":196,"id":29},{"text":"to","start":197,"end":199,"id":30},{"text":"Gaussian","start":200,"end":208,"id":31},{"text":"approximations","start":209,"end":223,"id":32},{"text":"minimizing","start":224,"end":234,"id":33},{"text":"relative","start":235,"end":243,"id":34},{"text":"entropy","start":244,"end":251,"id":35},{"text":")","start":251,"end":252,"id":36},{"text":".","start":252,"end":253,"id":37}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Experiments with synthetic data indicate that when R-EP does not converge, the approximation generated by PC-EP is often better.","_input_hash":-1913743291,"_task_hash":753494534,"tokens":[{"text":"Experiments","start":0,"end":11,"id":0},{"text":"with","start":12,"end":16,"id":1},{"text":"synthetic","start":17,"end":26,"id":2},{"text":"data","start":27,"end":31,"id":3},{"text":"indicate","start":32,"end":40,"id":4},{"text":"that","start":41,"end":45,"id":5},{"text":"when","start":46,"end":50,"id":6},{"text":"R","start":51,"end":52,"id":7},{"text":"-","start":52,"end":53,"id":8},{"text":"EP","start":53,"end":55,"id":9},{"text":"does","start":56,"end":60,"id":10},{"text":"not","start":61,"end":64,"id":11},{"text":"converge","start":65,"end":73,"id":12},{"text":",","start":73,"end":74,"id":13},{"text":"the","start":75,"end":78,"id":14},{"text":"approximation","start":79,"end":92,"id":15},{"text":"generated","start":93,"end":102,"id":16},{"text":"by","start":103,"end":105,"id":17},{"text":"PC","start":106,"end":108,"id":18},{"text":"-","start":108,"end":109,"id":19},{"text":"EP","start":109,"end":111,"id":20},{"text":"is","start":112,"end":114,"id":21},{"text":"often","start":115,"end":120,"id":22},{"text":"better","start":121,"end":127,"id":23},{"text":".","start":127,"end":128,"id":24}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We investigate the role of the initialization for the stability of the k-means clustering algorithm.","_input_hash":-858948277,"_task_hash":1709937010,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"investigate","start":3,"end":14,"id":1},{"text":"the","start":15,"end":18,"id":2},{"text":"role","start":19,"end":23,"id":3},{"text":"of","start":24,"end":26,"id":4},{"text":"the","start":27,"end":30,"id":5},{"text":"initialization","start":31,"end":45,"id":6},{"text":"for","start":46,"end":49,"id":7},{"text":"the","start":50,"end":53,"id":8},{"text":"stability","start":54,"end":63,"id":9},{"text":"of","start":64,"end":66,"id":10},{"text":"the","start":67,"end":70,"id":11},{"text":"k","start":71,"end":72,"id":12},{"text":"-","start":72,"end":73,"id":13},{"text":"means","start":73,"end":78,"id":14},{"text":"clustering","start":79,"end":89,"id":15},{"text":"algorithm","start":90,"end":99,"id":16},{"text":".","start":99,"end":100,"id":17}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":71,"end":89,"token_start":12,"token_end":15,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"However, existing estimation methods are based on iterative search algorithms and may not converge to a correct solution in a finite number of steps.","_input_hash":-1868016986,"_task_hash":781167147,"tokens":[{"text":"However","start":0,"end":7,"id":0},{"text":",","start":7,"end":8,"id":1},{"text":"existing","start":9,"end":17,"id":2},{"text":"estimation","start":18,"end":28,"id":3},{"text":"methods","start":29,"end":36,"id":4},{"text":"are","start":37,"end":40,"id":5},{"text":"based","start":41,"end":46,"id":6},{"text":"on","start":47,"end":49,"id":7},{"text":"iterative","start":50,"end":59,"id":8},{"text":"search","start":60,"end":66,"id":9},{"text":"algorithms","start":67,"end":77,"id":10},{"text":"and","start":78,"end":81,"id":11},{"text":"may","start":82,"end":85,"id":12},{"text":"not","start":86,"end":89,"id":13},{"text":"converge","start":90,"end":98,"id":14},{"text":"to","start":99,"end":101,"id":15},{"text":"a","start":102,"end":103,"id":16},{"text":"correct","start":104,"end":111,"id":17},{"text":"solution","start":112,"end":120,"id":18},{"text":"in","start":121,"end":123,"id":19},{"text":"a","start":124,"end":125,"id":20},{"text":"finite","start":126,"end":132,"id":21},{"text":"number","start":133,"end":139,"id":22},{"text":"of","start":140,"end":142,"id":23},{"text":"steps","start":143,"end":148,"id":24},{"text":".","start":148,"end":149,"id":25}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Our procedure has a complexity linear, or close to linear, in the number of atoms, and allows the use of accelerated gradient techniques to solve the tree-structured sparse approximation problem at the same computational cost as traditional ones using the L1-norm.","_input_hash":-1480593943,"_task_hash":-1751024069,"tokens":[{"text":"Our","start":0,"end":3,"id":0},{"text":"procedure","start":4,"end":13,"id":1},{"text":"has","start":14,"end":17,"id":2},{"text":"a","start":18,"end":19,"id":3},{"text":"complexity","start":20,"end":30,"id":4},{"text":"linear","start":31,"end":37,"id":5},{"text":",","start":37,"end":38,"id":6},{"text":"or","start":39,"end":41,"id":7},{"text":"close","start":42,"end":47,"id":8},{"text":"to","start":48,"end":50,"id":9},{"text":"linear","start":51,"end":57,"id":10},{"text":",","start":57,"end":58,"id":11},{"text":"in","start":59,"end":61,"id":12},{"text":"the","start":62,"end":65,"id":13},{"text":"number","start":66,"end":72,"id":14},{"text":"of","start":73,"end":75,"id":15},{"text":"atoms","start":76,"end":81,"id":16},{"text":",","start":81,"end":82,"id":17},{"text":"and","start":83,"end":86,"id":18},{"text":"allows","start":87,"end":93,"id":19},{"text":"the","start":94,"end":97,"id":20},{"text":"use","start":98,"end":101,"id":21},{"text":"of","start":102,"end":104,"id":22},{"text":"accelerated","start":105,"end":116,"id":23},{"text":"gradient","start":117,"end":125,"id":24},{"text":"techniques","start":126,"end":136,"id":25},{"text":"to","start":137,"end":139,"id":26},{"text":"solve","start":140,"end":145,"id":27},{"text":"the","start":146,"end":149,"id":28},{"text":"tree","start":150,"end":154,"id":29},{"text":"-","start":154,"end":155,"id":30},{"text":"structured","start":155,"end":165,"id":31},{"text":"sparse","start":166,"end":172,"id":32},{"text":"approximation","start":173,"end":186,"id":33},{"text":"problem","start":187,"end":194,"id":34},{"text":"at","start":195,"end":197,"id":35},{"text":"the","start":198,"end":201,"id":36},{"text":"same","start":202,"end":206,"id":37},{"text":"computational","start":207,"end":220,"id":38},{"text":"cost","start":221,"end":225,"id":39},{"text":"as","start":226,"end":228,"id":40},{"text":"traditional","start":229,"end":240,"id":41},{"text":"ones","start":241,"end":245,"id":42},{"text":"using","start":246,"end":251,"id":43},{"text":"the","start":252,"end":255,"id":44},{"text":"L1-norm","start":256,"end":263,"id":45},{"text":".","start":263,"end":264,"id":46}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"We propose a computationally very efficient independent screening method for survival data which can be viewed as the natural survival equivalent of correlation screening.","_input_hash":-447712179,"_task_hash":114818922,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"propose","start":3,"end":10,"id":1},{"text":"a","start":11,"end":12,"id":2},{"text":"computationally","start":13,"end":28,"id":3},{"text":"very","start":29,"end":33,"id":4},{"text":"efficient","start":34,"end":43,"id":5},{"text":"independent","start":44,"end":55,"id":6},{"text":"screening","start":56,"end":65,"id":7},{"text":"method","start":66,"end":72,"id":8},{"text":"for","start":73,"end":76,"id":9},{"text":"survival","start":77,"end":85,"id":10},{"text":"data","start":86,"end":90,"id":11},{"text":"which","start":91,"end":96,"id":12},{"text":"can","start":97,"end":100,"id":13},{"text":"be","start":101,"end":103,"id":14},{"text":"viewed","start":104,"end":110,"id":15},{"text":"as","start":111,"end":113,"id":16},{"text":"the","start":114,"end":117,"id":17},{"text":"natural","start":118,"end":125,"id":18},{"text":"survival","start":126,"end":134,"id":19},{"text":"equivalent","start":135,"end":145,"id":20},{"text":"of","start":146,"end":148,"id":21},{"text":"correlation","start":149,"end":160,"id":22},{"text":"screening","start":161,"end":170,"id":23},{"text":".","start":170,"end":171,"id":24}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We show that this method outperforms the more standard use of $\\chi^2$ and likelihood ratio tests.","_input_hash":-131603167,"_task_hash":-657724470,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"show","start":3,"end":7,"id":1},{"text":"that","start":8,"end":12,"id":2},{"text":"this","start":13,"end":17,"id":3},{"text":"method","start":18,"end":24,"id":4},{"text":"outperforms","start":25,"end":36,"id":5},{"text":"the","start":37,"end":40,"id":6},{"text":"more","start":41,"end":45,"id":7},{"text":"standard","start":46,"end":54,"id":8},{"text":"use","start":55,"end":58,"id":9},{"text":"of","start":59,"end":61,"id":10},{"text":"$","start":62,"end":63,"id":11},{"text":"\\chi^2","start":63,"end":69,"id":12},{"text":"$","start":69,"end":70,"id":13},{"text":"and","start":71,"end":74,"id":14},{"text":"likelihood","start":75,"end":85,"id":15},{"text":"ratio","start":86,"end":91,"id":16},{"text":"tests","start":92,"end":97,"id":17},{"text":".","start":97,"end":98,"id":18}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We present some evidence to show that BKNN still significantly underestimates model uncertainty.","_input_hash":-437162734,"_task_hash":1533104581,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"present","start":3,"end":10,"id":1},{"text":"some","start":11,"end":15,"id":2},{"text":"evidence","start":16,"end":24,"id":3},{"text":"to","start":25,"end":27,"id":4},{"text":"show","start":28,"end":32,"id":5},{"text":"that","start":33,"end":37,"id":6},{"text":"BKNN","start":38,"end":42,"id":7},{"text":"still","start":43,"end":48,"id":8},{"text":"significantly","start":49,"end":62,"id":9},{"text":"underestimates","start":63,"end":77,"id":10},{"text":"model","start":78,"end":83,"id":11},{"text":"uncertainty","start":84,"end":95,"id":12},{"text":".","start":95,"end":96,"id":13}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":38,"end":42,"token_start":7,"token_end":7,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"In this work we extend the notion of additive noise models to these cases.","_input_hash":-1949067549,"_task_hash":-1271167792,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"this","start":3,"end":7,"id":1},{"text":"work","start":8,"end":12,"id":2},{"text":"we","start":13,"end":15,"id":3},{"text":"extend","start":16,"end":22,"id":4},{"text":"the","start":23,"end":26,"id":5},{"text":"notion","start":27,"end":33,"id":6},{"text":"of","start":34,"end":36,"id":7},{"text":"additive","start":37,"end":45,"id":8},{"text":"noise","start":46,"end":51,"id":9},{"text":"models","start":52,"end":58,"id":10},{"text":"to","start":59,"end":61,"id":11},{"text":"these","start":62,"end":67,"id":12},{"text":"cases","start":68,"end":73,"id":13},{"text":".","start":73,"end":74,"id":14}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":37,"end":51,"token_start":8,"token_end":9,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Inverse inference takes into account the multivariate information between voxels and is currently the only way to assess how precisely some cognitive information is encoded by the activity of neural populations within the whole brain.","_input_hash":-1368884971,"_task_hash":-1092114794,"tokens":[{"text":"Inverse","start":0,"end":7,"id":0},{"text":"inference","start":8,"end":17,"id":1},{"text":"takes","start":18,"end":23,"id":2},{"text":"into","start":24,"end":28,"id":3},{"text":"account","start":29,"end":36,"id":4},{"text":"the","start":37,"end":40,"id":5},{"text":"multivariate","start":41,"end":53,"id":6},{"text":"information","start":54,"end":65,"id":7},{"text":"between","start":66,"end":73,"id":8},{"text":"voxels","start":74,"end":80,"id":9},{"text":"and","start":81,"end":84,"id":10},{"text":"is","start":85,"end":87,"id":11},{"text":"currently","start":88,"end":97,"id":12},{"text":"the","start":98,"end":101,"id":13},{"text":"only","start":102,"end":106,"id":14},{"text":"way","start":107,"end":110,"id":15},{"text":"to","start":111,"end":113,"id":16},{"text":"assess","start":114,"end":120,"id":17},{"text":"how","start":121,"end":124,"id":18},{"text":"precisely","start":125,"end":134,"id":19},{"text":"some","start":135,"end":139,"id":20},{"text":"cognitive","start":140,"end":149,"id":21},{"text":"information","start":150,"end":161,"id":22},{"text":"is","start":162,"end":164,"id":23},{"text":"encoded","start":165,"end":172,"id":24},{"text":"by","start":173,"end":175,"id":25},{"text":"the","start":176,"end":179,"id":26},{"text":"activity","start":180,"end":188,"id":27},{"text":"of","start":189,"end":191,"id":28},{"text":"neural","start":192,"end":198,"id":29},{"text":"populations","start":199,"end":210,"id":30},{"text":"within","start":211,"end":217,"id":31},{"text":"the","start":218,"end":221,"id":32},{"text":"whole","start":222,"end":227,"id":33},{"text":"brain","start":228,"end":233,"id":34},{"text":".","start":233,"end":234,"id":35}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"the number of observations).","_input_hash":-1297366295,"_task_hash":-818741412,"tokens":[{"text":"the","start":0,"end":3,"id":0},{"text":"number","start":4,"end":10,"id":1},{"text":"of","start":11,"end":13,"id":2},{"text":"observations","start":14,"end":26,"id":3},{"text":")","start":26,"end":27,"id":4},{"text":".","start":27,"end":28,"id":5}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We compare approximation error to batch AdaBoost on synthetic datasets and generalization error on face datasets and the MNIST dataset.","_input_hash":1258227701,"_task_hash":-333339323,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"compare","start":3,"end":10,"id":1},{"text":"approximation","start":11,"end":24,"id":2},{"text":"error","start":25,"end":30,"id":3},{"text":"to","start":31,"end":33,"id":4},{"text":"batch","start":34,"end":39,"id":5},{"text":"AdaBoost","start":40,"end":48,"id":6},{"text":"on","start":49,"end":51,"id":7},{"text":"synthetic","start":52,"end":61,"id":8},{"text":"datasets","start":62,"end":70,"id":9},{"text":"and","start":71,"end":74,"id":10},{"text":"generalization","start":75,"end":89,"id":11},{"text":"error","start":90,"end":95,"id":12},{"text":"on","start":96,"end":98,"id":13},{"text":"face","start":99,"end":103,"id":14},{"text":"datasets","start":104,"end":112,"id":15},{"text":"and","start":113,"end":116,"id":16},{"text":"the","start":117,"end":120,"id":17},{"text":"MNIST","start":121,"end":126,"id":18},{"text":"dataset","start":127,"end":134,"id":19},{"text":".","start":134,"end":135,"id":20}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":40,"end":48,"token_start":6,"token_end":6,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"In particular, we give two sets of results: (","_input_hash":1088111616,"_task_hash":-781340643,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"particular","start":3,"end":13,"id":1},{"text":",","start":13,"end":14,"id":2},{"text":"we","start":15,"end":17,"id":3},{"text":"give","start":18,"end":22,"id":4},{"text":"two","start":23,"end":26,"id":5},{"text":"sets","start":27,"end":31,"id":6},{"text":"of","start":32,"end":34,"id":7},{"text":"results","start":35,"end":42,"id":8},{"text":":","start":42,"end":43,"id":9},{"text":"(","start":44,"end":45,"id":10}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Through numerical studies, we illustrate the adequacy of the asymptotic theory for finite-sample inference.","_input_hash":-82152914,"_task_hash":-83550846,"tokens":[{"text":"Through","start":0,"end":7,"id":0},{"text":"numerical","start":8,"end":17,"id":1},{"text":"studies","start":18,"end":25,"id":2},{"text":",","start":25,"end":26,"id":3},{"text":"we","start":27,"end":29,"id":4},{"text":"illustrate","start":30,"end":40,"id":5},{"text":"the","start":41,"end":44,"id":6},{"text":"adequacy","start":45,"end":53,"id":7},{"text":"of","start":54,"end":56,"id":8},{"text":"the","start":57,"end":60,"id":9},{"text":"asymptotic","start":61,"end":71,"id":10},{"text":"theory","start":72,"end":78,"id":11},{"text":"for","start":79,"end":82,"id":12},{"text":"finite","start":83,"end":89,"id":13},{"text":"-","start":89,"end":90,"id":14},{"text":"sample","start":90,"end":96,"id":15},{"text":"inference","start":97,"end":106,"id":16},{"text":".","start":106,"end":107,"id":17}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"In data sets with many more features than observations, independent screening based on all univariate regression models leads to a computationally convenient variable selection method.","_input_hash":-1017300188,"_task_hash":-86584680,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"data","start":3,"end":7,"id":1},{"text":"sets","start":8,"end":12,"id":2},{"text":"with","start":13,"end":17,"id":3},{"text":"many","start":18,"end":22,"id":4},{"text":"more","start":23,"end":27,"id":5},{"text":"features","start":28,"end":36,"id":6},{"text":"than","start":37,"end":41,"id":7},{"text":"observations","start":42,"end":54,"id":8},{"text":",","start":54,"end":55,"id":9},{"text":"independent","start":56,"end":67,"id":10},{"text":"screening","start":68,"end":77,"id":11},{"text":"based","start":78,"end":83,"id":12},{"text":"on","start":84,"end":86,"id":13},{"text":"all","start":87,"end":90,"id":14},{"text":"univariate","start":91,"end":101,"id":15},{"text":"regression","start":102,"end":112,"id":16},{"text":"models","start":113,"end":119,"id":17},{"text":"leads","start":120,"end":125,"id":18},{"text":"to","start":126,"end":128,"id":19},{"text":"a","start":129,"end":130,"id":20},{"text":"computationally","start":131,"end":146,"id":21},{"text":"convenient","start":147,"end":157,"id":22},{"text":"variable","start":158,"end":166,"id":23},{"text":"selection","start":167,"end":176,"id":24},{"text":"method","start":177,"end":183,"id":25},{"text":".","start":183,"end":184,"id":26}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":91,"end":112,"token_start":15,"token_end":16,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Changepoints are abrupt variations in the generative parameters of a data sequence.","_input_hash":1519314402,"_task_hash":-980663315,"tokens":[{"text":"Changepoints","start":0,"end":12,"id":0},{"text":"are","start":13,"end":16,"id":1},{"text":"abrupt","start":17,"end":23,"id":2},{"text":"variations","start":24,"end":34,"id":3},{"text":"in","start":35,"end":37,"id":4},{"text":"the","start":38,"end":41,"id":5},{"text":"generative","start":42,"end":52,"id":6},{"text":"parameters","start":53,"end":63,"id":7},{"text":"of","start":64,"end":66,"id":8},{"text":"a","start":67,"end":68,"id":9},{"text":"data","start":69,"end":73,"id":10},{"text":"sequence","start":74,"end":82,"id":11},{"text":".","start":82,"end":83,"id":12}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"non-linear dependence on observed variables, called SNIM (Sparse Non-linear Identifiable Multivariate modeling) and allowing for correlations between latent variables, called CSLIM (Correlated SLIM), for the temporal and/or spatial data.","_input_hash":-1646286336,"_task_hash":-73871217,"tokens":[{"text":"non","start":0,"end":3,"id":0},{"text":"-","start":3,"end":4,"id":1},{"text":"linear","start":4,"end":10,"id":2},{"text":"dependence","start":11,"end":21,"id":3},{"text":"on","start":22,"end":24,"id":4},{"text":"observed","start":25,"end":33,"id":5},{"text":"variables","start":34,"end":43,"id":6},{"text":",","start":43,"end":44,"id":7},{"text":"called","start":45,"end":51,"id":8},{"text":"SNIM","start":52,"end":56,"id":9},{"text":"(","start":57,"end":58,"id":10},{"text":"Sparse","start":58,"end":64,"id":11},{"text":"Non","start":65,"end":68,"id":12},{"text":"-","start":68,"end":69,"id":13},{"text":"linear","start":69,"end":75,"id":14},{"text":"Identifiable","start":76,"end":88,"id":15},{"text":"Multivariate","start":89,"end":101,"id":16},{"text":"modeling","start":102,"end":110,"id":17},{"text":")","start":110,"end":111,"id":18},{"text":"and","start":112,"end":115,"id":19},{"text":"allowing","start":116,"end":124,"id":20},{"text":"for","start":125,"end":128,"id":21},{"text":"correlations","start":129,"end":141,"id":22},{"text":"between","start":142,"end":149,"id":23},{"text":"latent","start":150,"end":156,"id":24},{"text":"variables","start":157,"end":166,"id":25},{"text":",","start":166,"end":167,"id":26},{"text":"called","start":168,"end":174,"id":27},{"text":"CSLIM","start":175,"end":180,"id":28},{"text":"(","start":181,"end":182,"id":29},{"text":"Correlated","start":182,"end":192,"id":30},{"text":"SLIM","start":193,"end":197,"id":31},{"text":")","start":197,"end":198,"id":32},{"text":",","start":198,"end":199,"id":33},{"text":"for","start":200,"end":203,"id":34},{"text":"the","start":204,"end":207,"id":35},{"text":"temporal","start":208,"end":216,"id":36},{"text":"and/or","start":217,"end":223,"id":37},{"text":"spatial","start":224,"end":231,"id":38},{"text":"data","start":232,"end":236,"id":39},{"text":".","start":236,"end":237,"id":40}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":58,"end":101,"token_start":11,"token_end":16,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"The corresponding cluster centroids are then computed in order to initialize the generalized Lloyd's algorithm, also known as $K$-means, which allows to circumvent initialization problems.","_input_hash":-1615555195,"_task_hash":-761468587,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"corresponding","start":4,"end":17,"id":1},{"text":"cluster","start":18,"end":25,"id":2},{"text":"centroids","start":26,"end":35,"id":3},{"text":"are","start":36,"end":39,"id":4},{"text":"then","start":40,"end":44,"id":5},{"text":"computed","start":45,"end":53,"id":6},{"text":"in","start":54,"end":56,"id":7},{"text":"order","start":57,"end":62,"id":8},{"text":"to","start":63,"end":65,"id":9},{"text":"initialize","start":66,"end":76,"id":10},{"text":"the","start":77,"end":80,"id":11},{"text":"generalized","start":81,"end":92,"id":12},{"text":"Lloyd","start":93,"end":98,"id":13},{"text":"'s","start":98,"end":100,"id":14},{"text":"algorithm","start":101,"end":110,"id":15},{"text":",","start":110,"end":111,"id":16},{"text":"also","start":112,"end":116,"id":17},{"text":"known","start":117,"end":122,"id":18},{"text":"as","start":123,"end":125,"id":19},{"text":"$","start":126,"end":127,"id":20},{"text":"K$-means","start":127,"end":135,"id":21},{"text":",","start":135,"end":136,"id":22},{"text":"which","start":137,"end":142,"id":23},{"text":"allows","start":143,"end":149,"id":24},{"text":"to","start":150,"end":152,"id":25},{"text":"circumvent","start":153,"end":163,"id":26},{"text":"initialization","start":164,"end":178,"id":27},{"text":"problems","start":179,"end":187,"id":28},{"text":".","start":187,"end":188,"id":29}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":93,"end":100,"token_start":13,"token_end":14,"label":"ALGO","answer":"accept"},{"start":126,"end":135,"token_start":20,"token_end":21,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"no explicit parametric model is needed for these quantities.","_input_hash":-113288791,"_task_hash":-1766393390,"tokens":[{"text":"no","start":0,"end":2,"id":0},{"text":"explicit","start":3,"end":11,"id":1},{"text":"parametric","start":12,"end":22,"id":2},{"text":"model","start":23,"end":28,"id":3},{"text":"is","start":29,"end":31,"id":4},{"text":"needed","start":32,"end":38,"id":5},{"text":"for","start":39,"end":42,"id":6},{"text":"these","start":43,"end":48,"id":7},{"text":"quantities","start":49,"end":59,"id":8},{"text":".","start":59,"end":60,"id":9}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"This is achieved using a recently introduced tree-structured sparse regularization norm, which has proven useful in several applications.","_input_hash":7370038,"_task_hash":-31705055,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"is","start":5,"end":7,"id":1},{"text":"achieved","start":8,"end":16,"id":2},{"text":"using","start":17,"end":22,"id":3},{"text":"a","start":23,"end":24,"id":4},{"text":"recently","start":25,"end":33,"id":5},{"text":"introduced","start":34,"end":44,"id":6},{"text":"tree","start":45,"end":49,"id":7},{"text":"-","start":49,"end":50,"id":8},{"text":"structured","start":50,"end":60,"id":9},{"text":"sparse","start":61,"end":67,"id":10},{"text":"regularization","start":68,"end":82,"id":11},{"text":"norm","start":83,"end":87,"id":12},{"text":",","start":87,"end":88,"id":13},{"text":"which","start":89,"end":94,"id":14},{"text":"has","start":95,"end":98,"id":15},{"text":"proven","start":99,"end":105,"id":16},{"text":"useful","start":106,"end":112,"id":17},{"text":"in","start":113,"end":115,"id":18},{"text":"several","start":116,"end":123,"id":19},{"text":"applications","start":124,"end":136,"id":20},{"text":".","start":136,"end":137,"id":21}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":116,"end":123,"token_start":19,"token_end":19,"label":"ALGO","answer":"reject"}],"answer":"reject"}
{"text":"this is a common setup in recent bioinformatics experiments, of which we analyze metabolite profiles in different conditions (disease vs. control and treatment vs. untreated) in different tissues (views).","_input_hash":1779557843,"_task_hash":-1576667569,"tokens":[{"text":"this","start":0,"end":4,"id":0},{"text":"is","start":5,"end":7,"id":1},{"text":"a","start":8,"end":9,"id":2},{"text":"common","start":10,"end":16,"id":3},{"text":"setup","start":17,"end":22,"id":4},{"text":"in","start":23,"end":25,"id":5},{"text":"recent","start":26,"end":32,"id":6},{"text":"bioinformatics","start":33,"end":47,"id":7},{"text":"experiments","start":48,"end":59,"id":8},{"text":",","start":59,"end":60,"id":9},{"text":"of","start":61,"end":63,"id":10},{"text":"which","start":64,"end":69,"id":11},{"text":"we","start":70,"end":72,"id":12},{"text":"analyze","start":73,"end":80,"id":13},{"text":"metabolite","start":81,"end":91,"id":14},{"text":"profiles","start":92,"end":100,"id":15},{"text":"in","start":101,"end":103,"id":16},{"text":"different","start":104,"end":113,"id":17},{"text":"conditions","start":114,"end":124,"id":18},{"text":"(","start":125,"end":126,"id":19},{"text":"disease","start":126,"end":133,"id":20},{"text":"vs.","start":134,"end":137,"id":21},{"text":"control","start":138,"end":145,"id":22},{"text":"and","start":146,"end":149,"id":23},{"text":"treatment","start":150,"end":159,"id":24},{"text":"vs.","start":160,"end":163,"id":25},{"text":"untreated","start":164,"end":173,"id":26},{"text":")","start":173,"end":174,"id":27},{"text":"in","start":175,"end":177,"id":28},{"text":"different","start":178,"end":187,"id":29},{"text":"tissues","start":188,"end":195,"id":30},{"text":"(","start":196,"end":197,"id":31},{"text":"views","start":197,"end":202,"id":32},{"text":")","start":202,"end":203,"id":33},{"text":".","start":203,"end":204,"id":34}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"This method allows for joint selection of predictors in sparse regression, allowing for complex structured sparsity over the predictors encoded as a set of groups.","_input_hash":1908988755,"_task_hash":1585229606,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"method","start":5,"end":11,"id":1},{"text":"allows","start":12,"end":18,"id":2},{"text":"for","start":19,"end":22,"id":3},{"text":"joint","start":23,"end":28,"id":4},{"text":"selection","start":29,"end":38,"id":5},{"text":"of","start":39,"end":41,"id":6},{"text":"predictors","start":42,"end":52,"id":7},{"text":"in","start":53,"end":55,"id":8},{"text":"sparse","start":56,"end":62,"id":9},{"text":"regression","start":63,"end":73,"id":10},{"text":",","start":73,"end":74,"id":11},{"text":"allowing","start":75,"end":83,"id":12},{"text":"for","start":84,"end":87,"id":13},{"text":"complex","start":88,"end":95,"id":14},{"text":"structured","start":96,"end":106,"id":15},{"text":"sparsity","start":107,"end":115,"id":16},{"text":"over","start":116,"end":120,"id":17},{"text":"the","start":121,"end":124,"id":18},{"text":"predictors","start":125,"end":135,"id":19},{"text":"encoded","start":136,"end":143,"id":20},{"text":"as","start":144,"end":146,"id":21},{"text":"a","start":147,"end":148,"id":22},{"text":"set","start":149,"end":152,"id":23},{"text":"of","start":153,"end":155,"id":24},{"text":"groups","start":156,"end":162,"id":25},{"text":".","start":162,"end":163,"id":26}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":56,"end":73,"token_start":9,"token_end":10,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"First, we introduce a general definition of dual certificate, which we then use to develop a unified theory of sparse recovery analysis for convex programming.","_input_hash":-1114798812,"_task_hash":1725021970,"tokens":[{"text":"First","start":0,"end":5,"id":0},{"text":",","start":5,"end":6,"id":1},{"text":"we","start":7,"end":9,"id":2},{"text":"introduce","start":10,"end":19,"id":3},{"text":"a","start":20,"end":21,"id":4},{"text":"general","start":22,"end":29,"id":5},{"text":"definition","start":30,"end":40,"id":6},{"text":"of","start":41,"end":43,"id":7},{"text":"dual","start":44,"end":48,"id":8},{"text":"certificate","start":49,"end":60,"id":9},{"text":",","start":60,"end":61,"id":10},{"text":"which","start":62,"end":67,"id":11},{"text":"we","start":68,"end":70,"id":12},{"text":"then","start":71,"end":75,"id":13},{"text":"use","start":76,"end":79,"id":14},{"text":"to","start":80,"end":82,"id":15},{"text":"develop","start":83,"end":90,"id":16},{"text":"a","start":91,"end":92,"id":17},{"text":"unified","start":93,"end":100,"id":18},{"text":"theory","start":101,"end":107,"id":19},{"text":"of","start":108,"end":110,"id":20},{"text":"sparse","start":111,"end":117,"id":21},{"text":"recovery","start":118,"end":126,"id":22},{"text":"analysis","start":127,"end":135,"id":23},{"text":"for","start":136,"end":139,"id":24},{"text":"convex","start":140,"end":146,"id":25},{"text":"programming","start":147,"end":158,"id":26},{"text":".","start":158,"end":159,"id":27}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"This yields a parsimonious function fit, which lends itself easily to visualization and interpretation.","_input_hash":257907864,"_task_hash":1998647606,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"yields","start":5,"end":11,"id":1},{"text":"a","start":12,"end":13,"id":2},{"text":"parsimonious","start":14,"end":26,"id":3},{"text":"function","start":27,"end":35,"id":4},{"text":"fit","start":36,"end":39,"id":5},{"text":",","start":39,"end":40,"id":6},{"text":"which","start":41,"end":46,"id":7},{"text":"lends","start":47,"end":52,"id":8},{"text":"itself","start":53,"end":59,"id":9},{"text":"easily","start":60,"end":66,"id":10},{"text":"to","start":67,"end":69,"id":11},{"text":"visualization","start":70,"end":83,"id":12},{"text":"and","start":84,"end":87,"id":13},{"text":"interpretation","start":88,"end":102,"id":14},{"text":".","start":102,"end":103,"id":15}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"In this paper, we study the problem of embedding arbitrary metric spaces into a Euclidean space with the goal to improve the accuracy of the NN classifier.","_input_hash":1951559990,"_task_hash":-1755695736,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"this","start":3,"end":7,"id":1},{"text":"paper","start":8,"end":13,"id":2},{"text":",","start":13,"end":14,"id":3},{"text":"we","start":15,"end":17,"id":4},{"text":"study","start":18,"end":23,"id":5},{"text":"the","start":24,"end":27,"id":6},{"text":"problem","start":28,"end":35,"id":7},{"text":"of","start":36,"end":38,"id":8},{"text":"embedding","start":39,"end":48,"id":9},{"text":"arbitrary","start":49,"end":58,"id":10},{"text":"metric","start":59,"end":65,"id":11},{"text":"spaces","start":66,"end":72,"id":12},{"text":"into","start":73,"end":77,"id":13},{"text":"a","start":78,"end":79,"id":14},{"text":"Euclidean","start":80,"end":89,"id":15},{"text":"space","start":90,"end":95,"id":16},{"text":"with","start":96,"end":100,"id":17},{"text":"the","start":101,"end":104,"id":18},{"text":"goal","start":105,"end":109,"id":19},{"text":"to","start":110,"end":112,"id":20},{"text":"improve","start":113,"end":120,"id":21},{"text":"the","start":121,"end":124,"id":22},{"text":"accuracy","start":125,"end":133,"id":23},{"text":"of","start":134,"end":136,"id":24},{"text":"the","start":137,"end":140,"id":25},{"text":"NN","start":141,"end":143,"id":26},{"text":"classifier","start":144,"end":154,"id":27},{"text":".","start":154,"end":155,"id":28}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":141,"end":154,"token_start":26,"token_end":27,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We propose a restricted collapsed draw (RCD) sampler, a general Markov chain Monte Carlo sampler of simultaneous draws from a hierarchical Chinese restaurant process (HCRP) with restriction.","_input_hash":-94334322,"_task_hash":-777254972,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"propose","start":3,"end":10,"id":1},{"text":"a","start":11,"end":12,"id":2},{"text":"restricted","start":13,"end":23,"id":3},{"text":"collapsed","start":24,"end":33,"id":4},{"text":"draw","start":34,"end":38,"id":5},{"text":"(","start":39,"end":40,"id":6},{"text":"RCD","start":40,"end":43,"id":7},{"text":")","start":43,"end":44,"id":8},{"text":"sampler","start":45,"end":52,"id":9},{"text":",","start":52,"end":53,"id":10},{"text":"a","start":54,"end":55,"id":11},{"text":"general","start":56,"end":63,"id":12},{"text":"Markov","start":64,"end":70,"id":13},{"text":"chain","start":71,"end":76,"id":14},{"text":"Monte","start":77,"end":82,"id":15},{"text":"Carlo","start":83,"end":88,"id":16},{"text":"sampler","start":89,"end":96,"id":17},{"text":"of","start":97,"end":99,"id":18},{"text":"simultaneous","start":100,"end":112,"id":19},{"text":"draws","start":113,"end":118,"id":20},{"text":"from","start":119,"end":123,"id":21},{"text":"a","start":124,"end":125,"id":22},{"text":"hierarchical","start":126,"end":138,"id":23},{"text":"Chinese","start":139,"end":146,"id":24},{"text":"restaurant","start":147,"end":157,"id":25},{"text":"process","start":158,"end":165,"id":26},{"text":"(","start":166,"end":167,"id":27},{"text":"HCRP","start":167,"end":171,"id":28},{"text":")","start":171,"end":172,"id":29},{"text":"with","start":173,"end":177,"id":30},{"text":"restriction","start":178,"end":189,"id":31},{"text":".","start":189,"end":190,"id":32}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":64,"end":88,"token_start":13,"token_end":16,"label":"ALGO","answer":"accept"},{"start":126,"end":157,"token_start":23,"token_end":25,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Prediction problems motivate this research:","_input_hash":298200160,"_task_hash":1586548048,"tokens":[{"text":"Prediction","start":0,"end":10,"id":0},{"text":"problems","start":11,"end":19,"id":1},{"text":"motivate","start":20,"end":28,"id":2},{"text":"this","start":29,"end":33,"id":3},{"text":"research","start":34,"end":42,"id":4},{"text":":","start":42,"end":43,"id":5}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"They are simple, computationally fast and scalable, interpretable, and admit nonparametric priors.","_input_hash":438256361,"_task_hash":333327110,"tokens":[{"text":"They","start":0,"end":4,"id":0},{"text":"are","start":5,"end":8,"id":1},{"text":"simple","start":9,"end":15,"id":2},{"text":",","start":15,"end":16,"id":3},{"text":"computationally","start":17,"end":32,"id":4},{"text":"fast","start":33,"end":37,"id":5},{"text":"and","start":38,"end":41,"id":6},{"text":"scalable","start":42,"end":50,"id":7},{"text":",","start":50,"end":51,"id":8},{"text":"interpretable","start":52,"end":65,"id":9},{"text":",","start":65,"end":66,"id":10},{"text":"and","start":67,"end":70,"id":11},{"text":"admit","start":71,"end":76,"id":12},{"text":"nonparametric","start":77,"end":90,"id":13},{"text":"priors","start":91,"end":97,"id":14},{"text":".","start":97,"end":98,"id":15}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Sure screening;","_input_hash":169778553,"_task_hash":-1113817156,"tokens":[{"text":"Sure","start":0,"end":4,"id":0},{"text":"screening","start":5,"end":14,"id":1},{"text":";","start":14,"end":15,"id":2}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"2006), but differs substantially in inference, Bayesian network structure learning and model comparison.","_input_hash":2064840711,"_task_hash":652101488,"tokens":[{"text":"2006","start":0,"end":4,"id":0},{"text":")","start":4,"end":5,"id":1},{"text":",","start":5,"end":6,"id":2},{"text":"but","start":7,"end":10,"id":3},{"text":"differs","start":11,"end":18,"id":4},{"text":"substantially","start":19,"end":32,"id":5},{"text":"in","start":33,"end":35,"id":6},{"text":"inference","start":36,"end":45,"id":7},{"text":",","start":45,"end":46,"id":8},{"text":"Bayesian","start":47,"end":55,"id":9},{"text":"network","start":56,"end":63,"id":10},{"text":"structure","start":64,"end":73,"id":11},{"text":"learning","start":74,"end":82,"id":12},{"text":"and","start":83,"end":86,"id":13},{"text":"model","start":87,"end":92,"id":14},{"text":"comparison","start":93,"end":103,"id":15},{"text":".","start":103,"end":104,"id":16}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"The abstract learning space, where all agents are located, is constructed here using a fully connected model that couples all agents with random strength values.","_input_hash":810703736,"_task_hash":-178055020,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"abstract","start":4,"end":12,"id":1},{"text":"learning","start":13,"end":21,"id":2},{"text":"space","start":22,"end":27,"id":3},{"text":",","start":27,"end":28,"id":4},{"text":"where","start":29,"end":34,"id":5},{"text":"all","start":35,"end":38,"id":6},{"text":"agents","start":39,"end":45,"id":7},{"text":"are","start":46,"end":49,"id":8},{"text":"located","start":50,"end":57,"id":9},{"text":",","start":57,"end":58,"id":10},{"text":"is","start":59,"end":61,"id":11},{"text":"constructed","start":62,"end":73,"id":12},{"text":"here","start":74,"end":78,"id":13},{"text":"using","start":79,"end":84,"id":14},{"text":"a","start":85,"end":86,"id":15},{"text":"fully","start":87,"end":92,"id":16},{"text":"connected","start":93,"end":102,"id":17},{"text":"model","start":103,"end":108,"id":18},{"text":"that","start":109,"end":113,"id":19},{"text":"couples","start":114,"end":121,"id":20},{"text":"all","start":122,"end":125,"id":21},{"text":"agents","start":126,"end":132,"id":22},{"text":"with","start":133,"end":137,"id":23},{"text":"random","start":138,"end":144,"id":24},{"text":"strength","start":145,"end":153,"id":25},{"text":"values","start":154,"end":160,"id":26},{"text":".","start":160,"end":161,"id":27}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We show how this stochastic process can be used as a prior distribution in a Bayesian nonparametric model of document collections.","_input_hash":-1051689315,"_task_hash":-214433426,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"show","start":3,"end":7,"id":1},{"text":"how","start":8,"end":11,"id":2},{"text":"this","start":12,"end":16,"id":3},{"text":"stochastic","start":17,"end":27,"id":4},{"text":"process","start":28,"end":35,"id":5},{"text":"can","start":36,"end":39,"id":6},{"text":"be","start":40,"end":42,"id":7},{"text":"used","start":43,"end":47,"id":8},{"text":"as","start":48,"end":50,"id":9},{"text":"a","start":51,"end":52,"id":10},{"text":"prior","start":53,"end":58,"id":11},{"text":"distribution","start":59,"end":71,"id":12},{"text":"in","start":72,"end":74,"id":13},{"text":"a","start":75,"end":76,"id":14},{"text":"Bayesian","start":77,"end":85,"id":15},{"text":"nonparametric","start":86,"end":99,"id":16},{"text":"model","start":100,"end":105,"id":17},{"text":"of","start":106,"end":108,"id":18},{"text":"document","start":109,"end":117,"id":19},{"text":"collections","start":118,"end":129,"id":20},{"text":".","start":129,"end":130,"id":21}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":77,"end":99,"token_start":15,"token_end":16,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Deflation-based implementations, such as the popular one-unit FastICA algorithm and its variants, extract the independent components one after another.","_input_hash":-170851408,"_task_hash":-1224616534,"tokens":[{"text":"Deflation","start":0,"end":9,"id":0},{"text":"-","start":9,"end":10,"id":1},{"text":"based","start":10,"end":15,"id":2},{"text":"implementations","start":16,"end":31,"id":3},{"text":",","start":31,"end":32,"id":4},{"text":"such","start":33,"end":37,"id":5},{"text":"as","start":38,"end":40,"id":6},{"text":"the","start":41,"end":44,"id":7},{"text":"popular","start":45,"end":52,"id":8},{"text":"one","start":53,"end":56,"id":9},{"text":"-","start":56,"end":57,"id":10},{"text":"unit","start":57,"end":61,"id":11},{"text":"FastICA","start":62,"end":69,"id":12},{"text":"algorithm","start":70,"end":79,"id":13},{"text":"and","start":80,"end":83,"id":14},{"text":"its","start":84,"end":87,"id":15},{"text":"variants","start":88,"end":96,"id":16},{"text":",","start":96,"end":97,"id":17},{"text":"extract","start":98,"end":105,"id":18},{"text":"the","start":106,"end":109,"id":19},{"text":"independent","start":110,"end":121,"id":20},{"text":"components","start":122,"end":132,"id":21},{"text":"one","start":133,"end":136,"id":22},{"text":"after","start":137,"end":142,"id":23},{"text":"another","start":143,"end":150,"id":24},{"text":".","start":150,"end":151,"id":25}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":62,"end":69,"token_start":12,"token_end":12,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Comparing the performance of IDE(s) with one of the most successful method to date, which is based on Linear Programming (LP), an improvement in speed of about two to three orders of magnitude is observed.","_input_hash":106666573,"_task_hash":-1969435842,"tokens":[{"text":"Comparing","start":0,"end":9,"id":0},{"text":"the","start":10,"end":13,"id":1},{"text":"performance","start":14,"end":25,"id":2},{"text":"of","start":26,"end":28,"id":3},{"text":"IDE(s","start":29,"end":34,"id":4},{"text":")","start":34,"end":35,"id":5},{"text":"with","start":36,"end":40,"id":6},{"text":"one","start":41,"end":44,"id":7},{"text":"of","start":45,"end":47,"id":8},{"text":"the","start":48,"end":51,"id":9},{"text":"most","start":52,"end":56,"id":10},{"text":"successful","start":57,"end":67,"id":11},{"text":"method","start":68,"end":74,"id":12},{"text":"to","start":75,"end":77,"id":13},{"text":"date","start":78,"end":82,"id":14},{"text":",","start":82,"end":83,"id":15},{"text":"which","start":84,"end":89,"id":16},{"text":"is","start":90,"end":92,"id":17},{"text":"based","start":93,"end":98,"id":18},{"text":"on","start":99,"end":101,"id":19},{"text":"Linear","start":102,"end":108,"id":20},{"text":"Programming","start":109,"end":120,"id":21},{"text":"(","start":121,"end":122,"id":22},{"text":"LP","start":122,"end":124,"id":23},{"text":")","start":124,"end":125,"id":24},{"text":",","start":125,"end":126,"id":25},{"text":"an","start":127,"end":129,"id":26},{"text":"improvement","start":130,"end":141,"id":27},{"text":"in","start":142,"end":144,"id":28},{"text":"speed","start":145,"end":150,"id":29},{"text":"of","start":151,"end":153,"id":30},{"text":"about","start":154,"end":159,"id":31},{"text":"two","start":160,"end":163,"id":32},{"text":"to","start":164,"end":166,"id":33},{"text":"three","start":167,"end":172,"id":34},{"text":"orders","start":173,"end":179,"id":35},{"text":"of","start":180,"end":182,"id":36},{"text":"magnitude","start":183,"end":192,"id":37},{"text":"is","start":193,"end":195,"id":38},{"text":"observed","start":196,"end":204,"id":39},{"text":".","start":204,"end":205,"id":40}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"S-OMP can be applied to problems with hundreds of thousands of variables and once the number of variables is reduced to a manageable size, a more computationally demanding procedure can be used to identify the relevant variables for each of the regression outputs.","_input_hash":-1310735266,"_task_hash":-65509265,"tokens":[{"text":"S","start":0,"end":1,"id":0},{"text":"-","start":1,"end":2,"id":1},{"text":"OMP","start":2,"end":5,"id":2},{"text":"can","start":6,"end":9,"id":3},{"text":"be","start":10,"end":12,"id":4},{"text":"applied","start":13,"end":20,"id":5},{"text":"to","start":21,"end":23,"id":6},{"text":"problems","start":24,"end":32,"id":7},{"text":"with","start":33,"end":37,"id":8},{"text":"hundreds","start":38,"end":46,"id":9},{"text":"of","start":47,"end":49,"id":10},{"text":"thousands","start":50,"end":59,"id":11},{"text":"of","start":60,"end":62,"id":12},{"text":"variables","start":63,"end":72,"id":13},{"text":"and","start":73,"end":76,"id":14},{"text":"once","start":77,"end":81,"id":15},{"text":"the","start":82,"end":85,"id":16},{"text":"number","start":86,"end":92,"id":17},{"text":"of","start":93,"end":95,"id":18},{"text":"variables","start":96,"end":105,"id":19},{"text":"is","start":106,"end":108,"id":20},{"text":"reduced","start":109,"end":116,"id":21},{"text":"to","start":117,"end":119,"id":22},{"text":"a","start":120,"end":121,"id":23},{"text":"manageable","start":122,"end":132,"id":24},{"text":"size","start":133,"end":137,"id":25},{"text":",","start":137,"end":138,"id":26},{"text":"a","start":139,"end":140,"id":27},{"text":"more","start":141,"end":145,"id":28},{"text":"computationally","start":146,"end":161,"id":29},{"text":"demanding","start":162,"end":171,"id":30},{"text":"procedure","start":172,"end":181,"id":31},{"text":"can","start":182,"end":185,"id":32},{"text":"be","start":186,"end":188,"id":33},{"text":"used","start":189,"end":193,"id":34},{"text":"to","start":194,"end":196,"id":35},{"text":"identify","start":197,"end":205,"id":36},{"text":"the","start":206,"end":209,"id":37},{"text":"relevant","start":210,"end":218,"id":38},{"text":"variables","start":219,"end":228,"id":39},{"text":"for","start":229,"end":232,"id":40},{"text":"each","start":233,"end":237,"id":41},{"text":"of","start":238,"end":240,"id":42},{"text":"the","start":241,"end":244,"id":43},{"text":"regression","start":245,"end":255,"id":44},{"text":"outputs","start":256,"end":263,"id":45},{"text":".","start":263,"end":264,"id":46}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Our bounds show that whereas a poisoning attack can be effectively staged in the unconstrained case, it can be made arbitrarily difficult (a strict upper bound on the attacker's gain) if external constraints are properly used.","_input_hash":-693715575,"_task_hash":-2075658,"tokens":[{"text":"Our","start":0,"end":3,"id":0},{"text":"bounds","start":4,"end":10,"id":1},{"text":"show","start":11,"end":15,"id":2},{"text":"that","start":16,"end":20,"id":3},{"text":"whereas","start":21,"end":28,"id":4},{"text":"a","start":29,"end":30,"id":5},{"text":"poisoning","start":31,"end":40,"id":6},{"text":"attack","start":41,"end":47,"id":7},{"text":"can","start":48,"end":51,"id":8},{"text":"be","start":52,"end":54,"id":9},{"text":"effectively","start":55,"end":66,"id":10},{"text":"staged","start":67,"end":73,"id":11},{"text":"in","start":74,"end":76,"id":12},{"text":"the","start":77,"end":80,"id":13},{"text":"unconstrained","start":81,"end":94,"id":14},{"text":"case","start":95,"end":99,"id":15},{"text":",","start":99,"end":100,"id":16},{"text":"it","start":101,"end":103,"id":17},{"text":"can","start":104,"end":107,"id":18},{"text":"be","start":108,"end":110,"id":19},{"text":"made","start":111,"end":115,"id":20},{"text":"arbitrarily","start":116,"end":127,"id":21},{"text":"difficult","start":128,"end":137,"id":22},{"text":"(","start":138,"end":139,"id":23},{"text":"a","start":139,"end":140,"id":24},{"text":"strict","start":141,"end":147,"id":25},{"text":"upper","start":148,"end":153,"id":26},{"text":"bound","start":154,"end":159,"id":27},{"text":"on","start":160,"end":162,"id":28},{"text":"the","start":163,"end":166,"id":29},{"text":"attacker","start":167,"end":175,"id":30},{"text":"'s","start":175,"end":177,"id":31},{"text":"gain","start":178,"end":182,"id":32},{"text":")","start":182,"end":183,"id":33},{"text":"if","start":184,"end":186,"id":34},{"text":"external","start":187,"end":195,"id":35},{"text":"constraints","start":196,"end":207,"id":36},{"text":"are","start":208,"end":211,"id":37},{"text":"properly","start":212,"end":220,"id":38},{"text":"used","start":221,"end":225,"id":39},{"text":".","start":225,"end":226,"id":40}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"It is however unclear if these heuristics can be derived from a more general principle facilitating generalization to new problem settings.","_input_hash":401064236,"_task_hash":1982098538,"tokens":[{"text":"It","start":0,"end":2,"id":0},{"text":"is","start":3,"end":5,"id":1},{"text":"however","start":6,"end":13,"id":2},{"text":"unclear","start":14,"end":21,"id":3},{"text":"if","start":22,"end":24,"id":4},{"text":"these","start":25,"end":30,"id":5},{"text":"heuristics","start":31,"end":41,"id":6},{"text":"can","start":42,"end":45,"id":7},{"text":"be","start":46,"end":48,"id":8},{"text":"derived","start":49,"end":56,"id":9},{"text":"from","start":57,"end":61,"id":10},{"text":"a","start":62,"end":63,"id":11},{"text":"more","start":64,"end":68,"id":12},{"text":"general","start":69,"end":76,"id":13},{"text":"principle","start":77,"end":86,"id":14},{"text":"facilitating","start":87,"end":99,"id":15},{"text":"generalization","start":100,"end":114,"id":16},{"text":"to","start":115,"end":117,"id":17},{"text":"new","start":118,"end":121,"id":18},{"text":"problem","start":122,"end":129,"id":19},{"text":"settings","start":130,"end":138,"id":20},{"text":".","start":138,"end":139,"id":21}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"First, it was found that, contrary to the popular expectation that the bistatic ATR performance might be substantially worse than the monostatic ATR performance, the bistatic ATR performed fairly well (though not better than the monostatic ATR).","_input_hash":503128034,"_task_hash":1598640678,"tokens":[{"text":"First","start":0,"end":5,"id":0},{"text":",","start":5,"end":6,"id":1},{"text":"it","start":7,"end":9,"id":2},{"text":"was","start":10,"end":13,"id":3},{"text":"found","start":14,"end":19,"id":4},{"text":"that","start":20,"end":24,"id":5},{"text":",","start":24,"end":25,"id":6},{"text":"contrary","start":26,"end":34,"id":7},{"text":"to","start":35,"end":37,"id":8},{"text":"the","start":38,"end":41,"id":9},{"text":"popular","start":42,"end":49,"id":10},{"text":"expectation","start":50,"end":61,"id":11},{"text":"that","start":62,"end":66,"id":12},{"text":"the","start":67,"end":70,"id":13},{"text":"bistatic","start":71,"end":79,"id":14},{"text":"ATR","start":80,"end":83,"id":15},{"text":"performance","start":84,"end":95,"id":16},{"text":"might","start":96,"end":101,"id":17},{"text":"be","start":102,"end":104,"id":18},{"text":"substantially","start":105,"end":118,"id":19},{"text":"worse","start":119,"end":124,"id":20},{"text":"than","start":125,"end":129,"id":21},{"text":"the","start":130,"end":133,"id":22},{"text":"monostatic","start":134,"end":144,"id":23},{"text":"ATR","start":145,"end":148,"id":24},{"text":"performance","start":149,"end":160,"id":25},{"text":",","start":160,"end":161,"id":26},{"text":"the","start":162,"end":165,"id":27},{"text":"bistatic","start":166,"end":174,"id":28},{"text":"ATR","start":175,"end":178,"id":29},{"text":"performed","start":179,"end":188,"id":30},{"text":"fairly","start":189,"end":195,"id":31},{"text":"well","start":196,"end":200,"id":32},{"text":"(","start":201,"end":202,"id":33},{"text":"though","start":202,"end":208,"id":34},{"text":"not","start":209,"end":212,"id":35},{"text":"better","start":213,"end":219,"id":36},{"text":"than","start":220,"end":224,"id":37},{"text":"the","start":225,"end":228,"id":38},{"text":"monostatic","start":229,"end":239,"id":39},{"text":"ATR","start":240,"end":243,"id":40},{"text":")","start":243,"end":244,"id":41},{"text":".","start":244,"end":245,"id":42}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"10] proposed a technique for estimating the VC dimension empirically.","_input_hash":2033264152,"_task_hash":1565538650,"tokens":[{"text":"10","start":0,"end":2,"id":0},{"text":"]","start":2,"end":3,"id":1},{"text":"proposed","start":4,"end":12,"id":2},{"text":"a","start":13,"end":14,"id":3},{"text":"technique","start":15,"end":24,"id":4},{"text":"for","start":25,"end":28,"id":5},{"text":"estimating","start":29,"end":39,"id":6},{"text":"the","start":40,"end":43,"id":7},{"text":"VC","start":44,"end":46,"id":8},{"text":"dimension","start":47,"end":56,"id":9},{"text":"empirically","start":57,"end":68,"id":10},{"text":".","start":68,"end":69,"id":11}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":44,"end":46,"token_start":8,"token_end":8,"label":"ALGO","answer":"reject"},{"start":47,"end":56,"token_start":9,"token_end":9,"label":"ALGO","answer":"reject"}],"answer":"reject"}
{"text":"Because of the lack of data, experiments were carried out on simulated data.","_input_hash":739962983,"_task_hash":2072858796,"tokens":[{"text":"Because","start":0,"end":7,"id":0},{"text":"of","start":8,"end":10,"id":1},{"text":"the","start":11,"end":14,"id":2},{"text":"lack","start":15,"end":19,"id":3},{"text":"of","start":20,"end":22,"id":4},{"text":"data","start":23,"end":27,"id":5},{"text":",","start":27,"end":28,"id":6},{"text":"experiments","start":29,"end":40,"id":7},{"text":"were","start":41,"end":45,"id":8},{"text":"carried","start":46,"end":53,"id":9},{"text":"out","start":54,"end":57,"id":10},{"text":"on","start":58,"end":60,"id":11},{"text":"simulated","start":61,"end":70,"id":12},{"text":"data","start":71,"end":75,"id":13},{"text":".","start":75,"end":76,"id":14}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We investigate the learning rate of multiple kernel leaning (MKL) with elastic-net regularization, which consists of an $\\ell_1$-regularizer for inducing the sparsity and an $\\ell_2$-regularizer for controlling the smoothness.","_input_hash":607177753,"_task_hash":2036359344,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"investigate","start":3,"end":14,"id":1},{"text":"the","start":15,"end":18,"id":2},{"text":"learning","start":19,"end":27,"id":3},{"text":"rate","start":28,"end":32,"id":4},{"text":"of","start":33,"end":35,"id":5},{"text":"multiple","start":36,"end":44,"id":6},{"text":"kernel","start":45,"end":51,"id":7},{"text":"leaning","start":52,"end":59,"id":8},{"text":"(","start":60,"end":61,"id":9},{"text":"MKL","start":61,"end":64,"id":10},{"text":")","start":64,"end":65,"id":11},{"text":"with","start":66,"end":70,"id":12},{"text":"elastic","start":71,"end":78,"id":13},{"text":"-","start":78,"end":79,"id":14},{"text":"net","start":79,"end":82,"id":15},{"text":"regularization","start":83,"end":97,"id":16},{"text":",","start":97,"end":98,"id":17},{"text":"which","start":99,"end":104,"id":18},{"text":"consists","start":105,"end":113,"id":19},{"text":"of","start":114,"end":116,"id":20},{"text":"an","start":117,"end":119,"id":21},{"text":"$","start":120,"end":121,"id":22},{"text":"\\ell_1$-regularizer","start":121,"end":140,"id":23},{"text":"for","start":141,"end":144,"id":24},{"text":"inducing","start":145,"end":153,"id":25},{"text":"the","start":154,"end":157,"id":26},{"text":"sparsity","start":158,"end":166,"id":27},{"text":"and","start":167,"end":170,"id":28},{"text":"an","start":171,"end":173,"id":29},{"text":"$","start":174,"end":175,"id":30},{"text":"\\ell_2$-regularizer","start":175,"end":194,"id":31},{"text":"for","start":195,"end":198,"id":32},{"text":"controlling","start":199,"end":210,"id":33},{"text":"the","start":211,"end":214,"id":34},{"text":"smoothness","start":215,"end":225,"id":35},{"text":".","start":225,"end":226,"id":36}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":36,"end":59,"token_start":6,"token_end":8,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"By predicting some cognitive variables related to brain activation maps, this approach aims at decoding brain activity.","_input_hash":281246858,"_task_hash":-1647603544,"tokens":[{"text":"By","start":0,"end":2,"id":0},{"text":"predicting","start":3,"end":13,"id":1},{"text":"some","start":14,"end":18,"id":2},{"text":"cognitive","start":19,"end":28,"id":3},{"text":"variables","start":29,"end":38,"id":4},{"text":"related","start":39,"end":46,"id":5},{"text":"to","start":47,"end":49,"id":6},{"text":"brain","start":50,"end":55,"id":7},{"text":"activation","start":56,"end":66,"id":8},{"text":"maps","start":67,"end":71,"id":9},{"text":",","start":71,"end":72,"id":10},{"text":"this","start":73,"end":77,"id":11},{"text":"approach","start":78,"end":86,"id":12},{"text":"aims","start":87,"end":91,"id":13},{"text":"at","start":92,"end":94,"id":14},{"text":"decoding","start":95,"end":103,"id":15},{"text":"brain","start":104,"end":109,"id":16},{"text":"activity","start":110,"end":118,"id":17},{"text":".","start":118,"end":119,"id":18}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Metrics used for measuring similarity between multi-dimensional data points are based on symmetrical divergences.","_input_hash":-834676812,"_task_hash":921118485,"tokens":[{"text":"Metrics","start":0,"end":7,"id":0},{"text":"used","start":8,"end":12,"id":1},{"text":"for","start":13,"end":16,"id":2},{"text":"measuring","start":17,"end":26,"id":3},{"text":"similarity","start":27,"end":37,"id":4},{"text":"between","start":38,"end":45,"id":5},{"text":"multi","start":46,"end":51,"id":6},{"text":"-","start":51,"end":52,"id":7},{"text":"dimensional","start":52,"end":63,"id":8},{"text":"data","start":64,"end":68,"id":9},{"text":"points","start":69,"end":75,"id":10},{"text":"are","start":76,"end":79,"id":11},{"text":"based","start":80,"end":85,"id":12},{"text":"on","start":86,"end":88,"id":13},{"text":"symmetrical","start":89,"end":100,"id":14},{"text":"divergences","start":101,"end":112,"id":15},{"text":".","start":112,"end":113,"id":16}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"One is when high dimensional measurements are available, for example, gene expression data produced by microarray techniques.","_input_hash":492717479,"_task_hash":1185154125,"tokens":[{"text":"One","start":0,"end":3,"id":0},{"text":"is","start":4,"end":6,"id":1},{"text":"when","start":7,"end":11,"id":2},{"text":"high","start":12,"end":16,"id":3},{"text":"dimensional","start":17,"end":28,"id":4},{"text":"measurements","start":29,"end":41,"id":5},{"text":"are","start":42,"end":45,"id":6},{"text":"available","start":46,"end":55,"id":7},{"text":",","start":55,"end":56,"id":8},{"text":"for","start":57,"end":60,"id":9},{"text":"example","start":61,"end":68,"id":10},{"text":",","start":68,"end":69,"id":11},{"text":"gene","start":70,"end":74,"id":12},{"text":"expression","start":75,"end":85,"id":13},{"text":"data","start":86,"end":90,"id":14},{"text":"produced","start":91,"end":99,"id":15},{"text":"by","start":100,"end":102,"id":16},{"text":"microarray","start":103,"end":113,"id":17},{"text":"techniques","start":114,"end":124,"id":18},{"text":".","start":124,"end":125,"id":19}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"In this paper we give a high-level overview about the existing literature on clustering stability.","_input_hash":-1013745153,"_task_hash":-1312563659,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"this","start":3,"end":7,"id":1},{"text":"paper","start":8,"end":13,"id":2},{"text":"we","start":14,"end":16,"id":3},{"text":"give","start":17,"end":21,"id":4},{"text":"a","start":22,"end":23,"id":5},{"text":"high","start":24,"end":28,"id":6},{"text":"-","start":28,"end":29,"id":7},{"text":"level","start":29,"end":34,"id":8},{"text":"overview","start":35,"end":43,"id":9},{"text":"about","start":44,"end":49,"id":10},{"text":"the","start":50,"end":53,"id":11},{"text":"existing","start":54,"end":62,"id":12},{"text":"literature","start":63,"end":73,"id":13},{"text":"on","start":74,"end":76,"id":14},{"text":"clustering","start":77,"end":87,"id":15},{"text":"stability","start":88,"end":97,"id":16},{"text":".","start":97,"end":98,"id":17}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"The utility of the model and algorithms is demonstrated on synthetic and real world data, both continuous and binary.","_input_hash":-2137203936,"_task_hash":1252463359,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"utility","start":4,"end":11,"id":1},{"text":"of","start":12,"end":14,"id":2},{"text":"the","start":15,"end":18,"id":3},{"text":"model","start":19,"end":24,"id":4},{"text":"and","start":25,"end":28,"id":5},{"text":"algorithms","start":29,"end":39,"id":6},{"text":"is","start":40,"end":42,"id":7},{"text":"demonstrated","start":43,"end":55,"id":8},{"text":"on","start":56,"end":58,"id":9},{"text":"synthetic","start":59,"end":68,"id":10},{"text":"and","start":69,"end":72,"id":11},{"text":"real","start":73,"end":77,"id":12},{"text":"world","start":78,"end":83,"id":13},{"text":"data","start":84,"end":88,"id":14},{"text":",","start":88,"end":89,"id":15},{"text":"both","start":90,"end":94,"id":16},{"text":"continuous","start":95,"end":105,"id":17},{"text":"and","start":106,"end":109,"id":18},{"text":"binary","start":110,"end":116,"id":19},{"text":".","start":116,"end":117,"id":20}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"The method is compatible with any dissimilarity measure, making it amenable to situations in which the data are not embedded in an underlying feature space or in which using a non-Euclidean metric is desirable.","_input_hash":-792706932,"_task_hash":1584051152,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"method","start":4,"end":10,"id":1},{"text":"is","start":11,"end":13,"id":2},{"text":"compatible","start":14,"end":24,"id":3},{"text":"with","start":25,"end":29,"id":4},{"text":"any","start":30,"end":33,"id":5},{"text":"dissimilarity","start":34,"end":47,"id":6},{"text":"measure","start":48,"end":55,"id":7},{"text":",","start":55,"end":56,"id":8},{"text":"making","start":57,"end":63,"id":9},{"text":"it","start":64,"end":66,"id":10},{"text":"amenable","start":67,"end":75,"id":11},{"text":"to","start":76,"end":78,"id":12},{"text":"situations","start":79,"end":89,"id":13},{"text":"in","start":90,"end":92,"id":14},{"text":"which","start":93,"end":98,"id":15},{"text":"the","start":99,"end":102,"id":16},{"text":"data","start":103,"end":107,"id":17},{"text":"are","start":108,"end":111,"id":18},{"text":"not","start":112,"end":115,"id":19},{"text":"embedded","start":116,"end":124,"id":20},{"text":"in","start":125,"end":127,"id":21},{"text":"an","start":128,"end":130,"id":22},{"text":"underlying","start":131,"end":141,"id":23},{"text":"feature","start":142,"end":149,"id":24},{"text":"space","start":150,"end":155,"id":25},{"text":"or","start":156,"end":158,"id":26},{"text":"in","start":159,"end":161,"id":27},{"text":"which","start":162,"end":167,"id":28},{"text":"using","start":168,"end":173,"id":29},{"text":"a","start":174,"end":175,"id":30},{"text":"non","start":176,"end":179,"id":31},{"text":"-","start":179,"end":180,"id":32},{"text":"Euclidean","start":180,"end":189,"id":33},{"text":"metric","start":190,"end":196,"id":34},{"text":"is","start":197,"end":199,"id":35},{"text":"desirable","start":200,"end":209,"id":36},{"text":".","start":209,"end":210,"id":37}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"given a multivariate time series x, we consider the likelihood function p_{\\theta}(x) of different parameters \\theta in the VAR model as features to describe x. To compare two time series x and x', we form the product of their features p_{\\theta}(x) p_{\\theta}(x') which is integrated out w.r.t \\theta using a matrix normal-inverse Wishart prior.","_input_hash":1870249199,"_task_hash":-31472660,"tokens":[{"text":"given","start":0,"end":5,"id":0},{"text":"a","start":6,"end":7,"id":1},{"text":"multivariate","start":8,"end":20,"id":2},{"text":"time","start":21,"end":25,"id":3},{"text":"series","start":26,"end":32,"id":4},{"text":"x","start":33,"end":34,"id":5},{"text":",","start":34,"end":35,"id":6},{"text":"we","start":36,"end":38,"id":7},{"text":"consider","start":39,"end":47,"id":8},{"text":"the","start":48,"end":51,"id":9},{"text":"likelihood","start":52,"end":62,"id":10},{"text":"function","start":63,"end":71,"id":11},{"text":"p_{\\theta}(x","start":72,"end":84,"id":12},{"text":")","start":84,"end":85,"id":13},{"text":"of","start":86,"end":88,"id":14},{"text":"different","start":89,"end":98,"id":15},{"text":"parameters","start":99,"end":109,"id":16},{"text":"\\theta","start":110,"end":116,"id":17},{"text":"in","start":117,"end":119,"id":18},{"text":"the","start":120,"end":123,"id":19},{"text":"VAR","start":124,"end":127,"id":20},{"text":"model","start":128,"end":133,"id":21},{"text":"as","start":134,"end":136,"id":22},{"text":"features","start":137,"end":145,"id":23},{"text":"to","start":146,"end":148,"id":24},{"text":"describe","start":149,"end":157,"id":25},{"text":"x.","start":158,"end":160,"id":26},{"text":"To","start":161,"end":163,"id":27},{"text":"compare","start":164,"end":171,"id":28},{"text":"two","start":172,"end":175,"id":29},{"text":"time","start":176,"end":180,"id":30},{"text":"series","start":181,"end":187,"id":31},{"text":"x","start":188,"end":189,"id":32},{"text":"and","start":190,"end":193,"id":33},{"text":"x","start":194,"end":195,"id":34},{"text":"'","start":195,"end":196,"id":35},{"text":",","start":196,"end":197,"id":36},{"text":"we","start":198,"end":200,"id":37},{"text":"form","start":201,"end":205,"id":38},{"text":"the","start":206,"end":209,"id":39},{"text":"product","start":210,"end":217,"id":40},{"text":"of","start":218,"end":220,"id":41},{"text":"their","start":221,"end":226,"id":42},{"text":"features","start":227,"end":235,"id":43},{"text":"p_{\\theta}(x","start":236,"end":248,"id":44},{"text":")","start":248,"end":249,"id":45},{"text":"p_{\\theta}(x","start":250,"end":262,"id":46},{"text":"'","start":262,"end":263,"id":47},{"text":")","start":263,"end":264,"id":48},{"text":"which","start":265,"end":270,"id":49},{"text":"is","start":271,"end":273,"id":50},{"text":"integrated","start":274,"end":284,"id":51},{"text":"out","start":285,"end":288,"id":52},{"text":"w.r.t","start":289,"end":294,"id":53},{"text":"\\theta","start":295,"end":301,"id":54},{"text":"using","start":302,"end":307,"id":55},{"text":"a","start":308,"end":309,"id":56},{"text":"matrix","start":310,"end":316,"id":57},{"text":"normal","start":317,"end":323,"id":58},{"text":"-","start":323,"end":324,"id":59},{"text":"inverse","start":324,"end":331,"id":60},{"text":"Wishart","start":332,"end":339,"id":61},{"text":"prior","start":340,"end":345,"id":62},{"text":".","start":345,"end":346,"id":63}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":124,"end":127,"token_start":20,"token_end":20,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"The bound is characterized by the decay rate of the eigenvalues of the associated kernels.","_input_hash":-1303486549,"_task_hash":1801891478,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"bound","start":4,"end":9,"id":1},{"text":"is","start":10,"end":12,"id":2},{"text":"characterized","start":13,"end":26,"id":3},{"text":"by","start":27,"end":29,"id":4},{"text":"the","start":30,"end":33,"id":5},{"text":"decay","start":34,"end":39,"id":6},{"text":"rate","start":40,"end":44,"id":7},{"text":"of","start":45,"end":47,"id":8},{"text":"the","start":48,"end":51,"id":9},{"text":"eigenvalues","start":52,"end":63,"id":10},{"text":"of","start":64,"end":66,"id":11},{"text":"the","start":67,"end":70,"id":12},{"text":"associated","start":71,"end":81,"id":13},{"text":"kernels","start":82,"end":89,"id":14},{"text":".","start":89,"end":90,"id":15}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"We assume that the weak hypotheses were selected beforehand, and only their weights are updated during online boosting.","_input_hash":366728403,"_task_hash":-219993,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"assume","start":3,"end":9,"id":1},{"text":"that","start":10,"end":14,"id":2},{"text":"the","start":15,"end":18,"id":3},{"text":"weak","start":19,"end":23,"id":4},{"text":"hypotheses","start":24,"end":34,"id":5},{"text":"were","start":35,"end":39,"id":6},{"text":"selected","start":40,"end":48,"id":7},{"text":"beforehand","start":49,"end":59,"id":8},{"text":",","start":59,"end":60,"id":9},{"text":"and","start":61,"end":64,"id":10},{"text":"only","start":65,"end":69,"id":11},{"text":"their","start":70,"end":75,"id":12},{"text":"weights","start":76,"end":83,"id":13},{"text":"are","start":84,"end":87,"id":14},{"text":"updated","start":88,"end":95,"id":15},{"text":"during","start":96,"end":102,"id":16},{"text":"online","start":103,"end":109,"id":17},{"text":"boosting","start":110,"end":118,"id":18},{"text":".","start":118,"end":119,"id":19}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"This polynomial rooting can be performed algebraically, and thus at low cost, at each iteration.","_input_hash":2146141936,"_task_hash":94118176,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"polynomial","start":5,"end":15,"id":1},{"text":"rooting","start":16,"end":23,"id":2},{"text":"can","start":24,"end":27,"id":3},{"text":"be","start":28,"end":30,"id":4},{"text":"performed","start":31,"end":40,"id":5},{"text":"algebraically","start":41,"end":54,"id":6},{"text":",","start":54,"end":55,"id":7},{"text":"and","start":56,"end":59,"id":8},{"text":"thus","start":60,"end":64,"id":9},{"text":"at","start":65,"end":67,"id":10},{"text":"low","start":68,"end":71,"id":11},{"text":"cost","start":72,"end":76,"id":12},{"text":",","start":76,"end":77,"id":13},{"text":"at","start":78,"end":80,"id":14},{"text":"each","start":81,"end":85,"id":15},{"text":"iteration","start":86,"end":95,"id":16},{"text":".","start":95,"end":96,"id":17}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"In addition, we present several new models and inference routines, which are naturally derived within this unified viewpoint.","_input_hash":-741579447,"_task_hash":-1403401569,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"addition","start":3,"end":11,"id":1},{"text":",","start":11,"end":12,"id":2},{"text":"we","start":13,"end":15,"id":3},{"text":"present","start":16,"end":23,"id":4},{"text":"several","start":24,"end":31,"id":5},{"text":"new","start":32,"end":35,"id":6},{"text":"models","start":36,"end":42,"id":7},{"text":"and","start":43,"end":46,"id":8},{"text":"inference","start":47,"end":56,"id":9},{"text":"routines","start":57,"end":65,"id":10},{"text":",","start":65,"end":66,"id":11},{"text":"which","start":67,"end":72,"id":12},{"text":"are","start":73,"end":76,"id":13},{"text":"naturally","start":77,"end":86,"id":14},{"text":"derived","start":87,"end":94,"id":15},{"text":"within","start":95,"end":101,"id":16},{"text":"this","start":102,"end":106,"id":17},{"text":"unified","start":107,"end":114,"id":18},{"text":"viewpoint","start":115,"end":124,"id":19},{"text":".","start":124,"end":125,"id":20}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":32,"end":35,"token_start":6,"token_end":6,"label":"ALGO","answer":"reject"},{"start":47,"end":56,"token_start":9,"token_end":9,"label":"ALGO","answer":"reject"}],"answer":"reject"}
{"text":"We describe a method for inferring linear causal relations among multi-dimensional variables.","_input_hash":254992690,"_task_hash":-1840526543,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"describe","start":3,"end":11,"id":1},{"text":"a","start":12,"end":13,"id":2},{"text":"method","start":14,"end":20,"id":3},{"text":"for","start":21,"end":24,"id":4},{"text":"inferring","start":25,"end":34,"id":5},{"text":"linear","start":35,"end":41,"id":6},{"text":"causal","start":42,"end":48,"id":7},{"text":"relations","start":49,"end":58,"id":8},{"text":"among","start":59,"end":64,"id":9},{"text":"multi","start":65,"end":70,"id":10},{"text":"-","start":70,"end":71,"id":11},{"text":"dimensional","start":71,"end":82,"id":12},{"text":"variables","start":83,"end":92,"id":13},{"text":".","start":92,"end":93,"id":14}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"Part of the attraction is the variable selection effect:","_input_hash":-1062851974,"_task_hash":1647961294,"tokens":[{"text":"Part","start":0,"end":4,"id":0},{"text":"of","start":5,"end":7,"id":1},{"text":"the","start":8,"end":11,"id":2},{"text":"attraction","start":12,"end":22,"id":3},{"text":"is","start":23,"end":25,"id":4},{"text":"the","start":26,"end":29,"id":5},{"text":"variable","start":30,"end":38,"id":6},{"text":"selection","start":39,"end":48,"id":7},{"text":"effect","start":49,"end":55,"id":8},{"text":":","start":55,"end":56,"id":9}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"As an example, we consider the unsupervised learning problem of finding the subspace on which several probability distributions agree.","_input_hash":-2086363081,"_task_hash":-1727266520,"tokens":[{"text":"As","start":0,"end":2,"id":0},{"text":"an","start":3,"end":5,"id":1},{"text":"example","start":6,"end":13,"id":2},{"text":",","start":13,"end":14,"id":3},{"text":"we","start":15,"end":17,"id":4},{"text":"consider","start":18,"end":26,"id":5},{"text":"the","start":27,"end":30,"id":6},{"text":"unsupervised","start":31,"end":43,"id":7},{"text":"learning","start":44,"end":52,"id":8},{"text":"problem","start":53,"end":60,"id":9},{"text":"of","start":61,"end":63,"id":10},{"text":"finding","start":64,"end":71,"id":11},{"text":"the","start":72,"end":75,"id":12},{"text":"subspace","start":76,"end":84,"id":13},{"text":"on","start":85,"end":87,"id":14},{"text":"which","start":88,"end":93,"id":15},{"text":"several","start":94,"end":101,"id":16},{"text":"probability","start":102,"end":113,"id":17},{"text":"distributions","start":114,"end":127,"id":18},{"text":"agree","start":128,"end":133,"id":19},{"text":".","start":133,"end":134,"id":20}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":31,"end":52,"token_start":7,"token_end":8,"label":"ALGO","answer":"reject"}],"answer":"reject"}
{"text":"Our extensive experiments show that our approach can compete with state-of-the-art classification techniques with reasonable time complexity.","_input_hash":1799416593,"_task_hash":1580386376,"tokens":[{"text":"Our","start":0,"end":3,"id":0},{"text":"extensive","start":4,"end":13,"id":1},{"text":"experiments","start":14,"end":25,"id":2},{"text":"show","start":26,"end":30,"id":3},{"text":"that","start":31,"end":35,"id":4},{"text":"our","start":36,"end":39,"id":5},{"text":"approach","start":40,"end":48,"id":6},{"text":"can","start":49,"end":52,"id":7},{"text":"compete","start":53,"end":60,"id":8},{"text":"with","start":61,"end":65,"id":9},{"text":"state","start":66,"end":71,"id":10},{"text":"-","start":71,"end":72,"id":11},{"text":"of","start":72,"end":74,"id":12},{"text":"-","start":74,"end":75,"id":13},{"text":"the","start":75,"end":78,"id":14},{"text":"-","start":78,"end":79,"id":15},{"text":"art","start":79,"end":82,"id":16},{"text":"classification","start":83,"end":97,"id":17},{"text":"techniques","start":98,"end":108,"id":18},{"text":"with","start":109,"end":113,"id":19},{"text":"reasonable","start":114,"end":124,"id":20},{"text":"time","start":125,"end":129,"id":21},{"text":"complexity","start":130,"end":140,"id":22},{"text":".","start":140,"end":141,"id":23}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"When many of the features are irrelevant, kernel methods such as the support vector machine and kernel ridge regression can sometimes perform poorly.","_input_hash":1399878476,"_task_hash":-684973354,"tokens":[{"text":"When","start":0,"end":4,"id":0},{"text":"many","start":5,"end":9,"id":1},{"text":"of","start":10,"end":12,"id":2},{"text":"the","start":13,"end":16,"id":3},{"text":"features","start":17,"end":25,"id":4},{"text":"are","start":26,"end":29,"id":5},{"text":"irrelevant","start":30,"end":40,"id":6},{"text":",","start":40,"end":41,"id":7},{"text":"kernel","start":42,"end":48,"id":8},{"text":"methods","start":49,"end":56,"id":9},{"text":"such","start":57,"end":61,"id":10},{"text":"as","start":62,"end":64,"id":11},{"text":"the","start":65,"end":68,"id":12},{"text":"support","start":69,"end":76,"id":13},{"text":"vector","start":77,"end":83,"id":14},{"text":"machine","start":84,"end":91,"id":15},{"text":"and","start":92,"end":95,"id":16},{"text":"kernel","start":96,"end":102,"id":17},{"text":"ridge","start":103,"end":108,"id":18},{"text":"regression","start":109,"end":119,"id":19},{"text":"can","start":120,"end":123,"id":20},{"text":"sometimes","start":124,"end":133,"id":21},{"text":"perform","start":134,"end":141,"id":22},{"text":"poorly","start":142,"end":148,"id":23},{"text":".","start":148,"end":149,"id":24}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":42,"end":48,"token_start":8,"token_end":8,"label":"ALGO","answer":"accept"},{"start":69,"end":91,"token_start":13,"token_end":15,"label":"ALGO","answer":"accept"},{"start":96,"end":119,"token_start":17,"token_end":19,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Source code publicly available at http://cogsys.imm.dtu.dk/passgp.","_input_hash":270022569,"_task_hash":1487129882,"tokens":[{"text":"Source","start":0,"end":6,"id":0},{"text":"code","start":7,"end":11,"id":1},{"text":"publicly","start":12,"end":20,"id":2},{"text":"available","start":21,"end":30,"id":3},{"text":"at","start":31,"end":33,"id":4},{"text":"http://cogsys.imm.dtu.dk/passgp","start":34,"end":65,"id":5},{"text":".","start":65,"end":66,"id":6}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"The problem of supervised classification (or discrimination) with functional data is considered, with a special interest on the popular k-nearest neighbors (k-NN) classifier.","_input_hash":1446338438,"_task_hash":-1203685157,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"problem","start":4,"end":11,"id":1},{"text":"of","start":12,"end":14,"id":2},{"text":"supervised","start":15,"end":25,"id":3},{"text":"classification","start":26,"end":40,"id":4},{"text":"(","start":41,"end":42,"id":5},{"text":"or","start":42,"end":44,"id":6},{"text":"discrimination","start":45,"end":59,"id":7},{"text":")","start":59,"end":60,"id":8},{"text":"with","start":61,"end":65,"id":9},{"text":"functional","start":66,"end":76,"id":10},{"text":"data","start":77,"end":81,"id":11},{"text":"is","start":82,"end":84,"id":12},{"text":"considered","start":85,"end":95,"id":13},{"text":",","start":95,"end":96,"id":14},{"text":"with","start":97,"end":101,"id":15},{"text":"a","start":102,"end":103,"id":16},{"text":"special","start":104,"end":111,"id":17},{"text":"interest","start":112,"end":120,"id":18},{"text":"on","start":121,"end":123,"id":19},{"text":"the","start":124,"end":127,"id":20},{"text":"popular","start":128,"end":135,"id":21},{"text":"k","start":136,"end":137,"id":22},{"text":"-","start":137,"end":138,"id":23},{"text":"nearest","start":138,"end":145,"id":24},{"text":"neighbors","start":146,"end":155,"id":25},{"text":"(","start":156,"end":157,"id":26},{"text":"k","start":157,"end":158,"id":27},{"text":"-","start":158,"end":159,"id":28},{"text":"NN","start":159,"end":161,"id":29},{"text":")","start":161,"end":162,"id":30},{"text":"classifier","start":163,"end":173,"id":31},{"text":".","start":173,"end":174,"id":32}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":136,"end":155,"token_start":22,"token_end":25,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Our implementation is highly modular so that the algorithm may be applied to a variety of types of data.","_input_hash":-1056800728,"_task_hash":-1521443138,"tokens":[{"text":"Our","start":0,"end":3,"id":0},{"text":"implementation","start":4,"end":18,"id":1},{"text":"is","start":19,"end":21,"id":2},{"text":"highly","start":22,"end":28,"id":3},{"text":"modular","start":29,"end":36,"id":4},{"text":"so","start":37,"end":39,"id":5},{"text":"that","start":40,"end":44,"id":6},{"text":"the","start":45,"end":48,"id":7},{"text":"algorithm","start":49,"end":58,"id":8},{"text":"may","start":59,"end":62,"id":9},{"text":"be","start":63,"end":65,"id":10},{"text":"applied","start":66,"end":73,"id":11},{"text":"to","start":74,"end":76,"id":12},{"text":"a","start":77,"end":78,"id":13},{"text":"variety","start":79,"end":86,"id":14},{"text":"of","start":87,"end":89,"id":15},{"text":"types","start":90,"end":95,"id":16},{"text":"of","start":96,"end":98,"id":17},{"text":"data","start":99,"end":103,"id":18},{"text":".","start":103,"end":104,"id":19}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Using this example, we describe that sometimes a causal hypothesis must be rejected because P(effect|cause) and P(cause) share algorithmic information (which is untypical if they are chosen independently).","_input_hash":1988888483,"_task_hash":-2104558794,"tokens":[{"text":"Using","start":0,"end":5,"id":0},{"text":"this","start":6,"end":10,"id":1},{"text":"example","start":11,"end":18,"id":2},{"text":",","start":18,"end":19,"id":3},{"text":"we","start":20,"end":22,"id":4},{"text":"describe","start":23,"end":31,"id":5},{"text":"that","start":32,"end":36,"id":6},{"text":"sometimes","start":37,"end":46,"id":7},{"text":"a","start":47,"end":48,"id":8},{"text":"causal","start":49,"end":55,"id":9},{"text":"hypothesis","start":56,"end":66,"id":10},{"text":"must","start":67,"end":71,"id":11},{"text":"be","start":72,"end":74,"id":12},{"text":"rejected","start":75,"end":83,"id":13},{"text":"because","start":84,"end":91,"id":14},{"text":"P(effect|cause","start":92,"end":106,"id":15},{"text":")","start":106,"end":107,"id":16},{"text":"and","start":108,"end":111,"id":17},{"text":"P(cause","start":112,"end":119,"id":18},{"text":")","start":119,"end":120,"id":19},{"text":"share","start":121,"end":126,"id":20},{"text":"algorithmic","start":127,"end":138,"id":21},{"text":"information","start":139,"end":150,"id":22},{"text":"(","start":151,"end":152,"id":23},{"text":"which","start":152,"end":157,"id":24},{"text":"is","start":158,"end":160,"id":25},{"text":"untypical","start":161,"end":170,"id":26},{"text":"if","start":171,"end":173,"id":27},{"text":"they","start":174,"end":178,"id":28},{"text":"are","start":179,"end":182,"id":29},{"text":"chosen","start":183,"end":189,"id":30},{"text":"independently","start":190,"end":203,"id":31},{"text":")","start":203,"end":204,"id":32},{"text":".","start":204,"end":205,"id":33}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"We give improved constants for data dependent and variance sensitive confidence bounds, called empirical Bernstein bounds, and extend these inequalities to hold uniformly over classes of functionswhose growth function is polynomial in the sample size n. The bounds lead us to consider sample variance penalization, a novel learning method which takes into account the empirical variance of the loss function.","_input_hash":-1242792690,"_task_hash":-1953945584,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"give","start":3,"end":7,"id":1},{"text":"improved","start":8,"end":16,"id":2},{"text":"constants","start":17,"end":26,"id":3},{"text":"for","start":27,"end":30,"id":4},{"text":"data","start":31,"end":35,"id":5},{"text":"dependent","start":36,"end":45,"id":6},{"text":"and","start":46,"end":49,"id":7},{"text":"variance","start":50,"end":58,"id":8},{"text":"sensitive","start":59,"end":68,"id":9},{"text":"confidence","start":69,"end":79,"id":10},{"text":"bounds","start":80,"end":86,"id":11},{"text":",","start":86,"end":87,"id":12},{"text":"called","start":88,"end":94,"id":13},{"text":"empirical","start":95,"end":104,"id":14},{"text":"Bernstein","start":105,"end":114,"id":15},{"text":"bounds","start":115,"end":121,"id":16},{"text":",","start":121,"end":122,"id":17},{"text":"and","start":123,"end":126,"id":18},{"text":"extend","start":127,"end":133,"id":19},{"text":"these","start":134,"end":139,"id":20},{"text":"inequalities","start":140,"end":152,"id":21},{"text":"to","start":153,"end":155,"id":22},{"text":"hold","start":156,"end":160,"id":23},{"text":"uniformly","start":161,"end":170,"id":24},{"text":"over","start":171,"end":175,"id":25},{"text":"classes","start":176,"end":183,"id":26},{"text":"of","start":184,"end":186,"id":27},{"text":"functionswhose","start":187,"end":201,"id":28},{"text":"growth","start":202,"end":208,"id":29},{"text":"function","start":209,"end":217,"id":30},{"text":"is","start":218,"end":220,"id":31},{"text":"polynomial","start":221,"end":231,"id":32},{"text":"in","start":232,"end":234,"id":33},{"text":"the","start":235,"end":238,"id":34},{"text":"sample","start":239,"end":245,"id":35},{"text":"size","start":246,"end":250,"id":36},{"text":"n.","start":251,"end":253,"id":37},{"text":"The","start":254,"end":257,"id":38},{"text":"bounds","start":258,"end":264,"id":39},{"text":"lead","start":265,"end":269,"id":40},{"text":"us","start":270,"end":272,"id":41},{"text":"to","start":273,"end":275,"id":42},{"text":"consider","start":276,"end":284,"id":43},{"text":"sample","start":285,"end":291,"id":44},{"text":"variance","start":292,"end":300,"id":45},{"text":"penalization","start":301,"end":313,"id":46},{"text":",","start":313,"end":314,"id":47},{"text":"a","start":315,"end":316,"id":48},{"text":"novel","start":317,"end":322,"id":49},{"text":"learning","start":323,"end":331,"id":50},{"text":"method","start":332,"end":338,"id":51},{"text":"which","start":339,"end":344,"id":52},{"text":"takes","start":345,"end":350,"id":53},{"text":"into","start":351,"end":355,"id":54},{"text":"account","start":356,"end":363,"id":55},{"text":"the","start":364,"end":367,"id":56},{"text":"empirical","start":368,"end":377,"id":57},{"text":"variance","start":378,"end":386,"id":58},{"text":"of","start":387,"end":389,"id":59},{"text":"the","start":390,"end":393,"id":60},{"text":"loss","start":394,"end":398,"id":61},{"text":"function","start":399,"end":407,"id":62},{"text":".","start":407,"end":408,"id":63}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"But the calculations can be carried out in a finite-dimensional space.","_input_hash":-1790706726,"_task_hash":-1095628019,"tokens":[{"text":"But","start":0,"end":3,"id":0},{"text":"the","start":4,"end":7,"id":1},{"text":"calculations","start":8,"end":20,"id":2},{"text":"can","start":21,"end":24,"id":3},{"text":"be","start":25,"end":27,"id":4},{"text":"carried","start":28,"end":35,"id":5},{"text":"out","start":36,"end":39,"id":6},{"text":"in","start":40,"end":42,"id":7},{"text":"a","start":43,"end":44,"id":8},{"text":"finite","start":45,"end":51,"id":9},{"text":"-","start":51,"end":52,"id":10},{"text":"dimensional","start":52,"end":63,"id":11},{"text":"space","start":64,"end":69,"id":12},{"text":".","start":69,"end":70,"id":13}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"As a mechanism for treating multidimensional data, including very high dimensional data, we use random projections.","_input_hash":272323236,"_task_hash":-145864374,"tokens":[{"text":"As","start":0,"end":2,"id":0},{"text":"a","start":3,"end":4,"id":1},{"text":"mechanism","start":5,"end":14,"id":2},{"text":"for","start":15,"end":18,"id":3},{"text":"treating","start":19,"end":27,"id":4},{"text":"multidimensional","start":28,"end":44,"id":5},{"text":"data","start":45,"end":49,"id":6},{"text":",","start":49,"end":50,"id":7},{"text":"including","start":51,"end":60,"id":8},{"text":"very","start":61,"end":65,"id":9},{"text":"high","start":66,"end":70,"id":10},{"text":"dimensional","start":71,"end":82,"id":11},{"text":"data","start":83,"end":87,"id":12},{"text":",","start":87,"end":88,"id":13},{"text":"we","start":89,"end":91,"id":14},{"text":"use","start":92,"end":95,"id":15},{"text":"random","start":96,"end":102,"id":16},{"text":"projections","start":103,"end":114,"id":17},{"text":".","start":114,"end":115,"id":18}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"In particular, by means of a detailed example, we provide some characterization of the properties of ERG models, and, in particular, of certain behaviors of ERG models known as degeneracy.","_input_hash":1206154583,"_task_hash":-1997537402,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"particular","start":3,"end":13,"id":1},{"text":",","start":13,"end":14,"id":2},{"text":"by","start":15,"end":17,"id":3},{"text":"means","start":18,"end":23,"id":4},{"text":"of","start":24,"end":26,"id":5},{"text":"a","start":27,"end":28,"id":6},{"text":"detailed","start":29,"end":37,"id":7},{"text":"example","start":38,"end":45,"id":8},{"text":",","start":45,"end":46,"id":9},{"text":"we","start":47,"end":49,"id":10},{"text":"provide","start":50,"end":57,"id":11},{"text":"some","start":58,"end":62,"id":12},{"text":"characterization","start":63,"end":79,"id":13},{"text":"of","start":80,"end":82,"id":14},{"text":"the","start":83,"end":86,"id":15},{"text":"properties","start":87,"end":97,"id":16},{"text":"of","start":98,"end":100,"id":17},{"text":"ERG","start":101,"end":104,"id":18},{"text":"models","start":105,"end":111,"id":19},{"text":",","start":111,"end":112,"id":20},{"text":"and","start":113,"end":116,"id":21},{"text":",","start":116,"end":117,"id":22},{"text":"in","start":118,"end":120,"id":23},{"text":"particular","start":121,"end":131,"id":24},{"text":",","start":131,"end":132,"id":25},{"text":"of","start":133,"end":135,"id":26},{"text":"certain","start":136,"end":143,"id":27},{"text":"behaviors","start":144,"end":153,"id":28},{"text":"of","start":154,"end":156,"id":29},{"text":"ERG","start":157,"end":160,"id":30},{"text":"models","start":161,"end":167,"id":31},{"text":"known","start":168,"end":173,"id":32},{"text":"as","start":174,"end":176,"id":33},{"text":"degeneracy","start":177,"end":187,"id":34},{"text":".","start":187,"end":188,"id":35}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":101,"end":104,"token_start":18,"token_end":18,"label":"ALGO","answer":"accept"},{"start":157,"end":160,"token_start":30,"token_end":30,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"These are defined as sums of Euclidean norms on certain subsets of variables, extending the usual $\\ell_1$-norm and the group $\\ell_1$-norm by allowing the subsets to overlap.","_input_hash":-1642135944,"_task_hash":1807253447,"tokens":[{"text":"These","start":0,"end":5,"id":0},{"text":"are","start":6,"end":9,"id":1},{"text":"defined","start":10,"end":17,"id":2},{"text":"as","start":18,"end":20,"id":3},{"text":"sums","start":21,"end":25,"id":4},{"text":"of","start":26,"end":28,"id":5},{"text":"Euclidean","start":29,"end":38,"id":6},{"text":"norms","start":39,"end":44,"id":7},{"text":"on","start":45,"end":47,"id":8},{"text":"certain","start":48,"end":55,"id":9},{"text":"subsets","start":56,"end":63,"id":10},{"text":"of","start":64,"end":66,"id":11},{"text":"variables","start":67,"end":76,"id":12},{"text":",","start":76,"end":77,"id":13},{"text":"extending","start":78,"end":87,"id":14},{"text":"the","start":88,"end":91,"id":15},{"text":"usual","start":92,"end":97,"id":16},{"text":"$","start":98,"end":99,"id":17},{"text":"\\ell_1$-norm","start":99,"end":111,"id":18},{"text":"and","start":112,"end":115,"id":19},{"text":"the","start":116,"end":119,"id":20},{"text":"group","start":120,"end":125,"id":21},{"text":"$","start":126,"end":127,"id":22},{"text":"\\ell_1$-norm","start":127,"end":139,"id":23},{"text":"by","start":140,"end":142,"id":24},{"text":"allowing","start":143,"end":151,"id":25},{"text":"the","start":152,"end":155,"id":26},{"text":"subsets","start":156,"end":163,"id":27},{"text":"to","start":164,"end":166,"id":28},{"text":"overlap","start":167,"end":174,"id":29},{"text":".","start":174,"end":175,"id":30}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"For computational or other reasons, people may select only a small subset of features when modelling such data, by looking at how relevant the features are to predicting the response, based on some measure such as correlation with the response in the training data.","_input_hash":-901972611,"_task_hash":240848819,"tokens":[{"text":"For","start":0,"end":3,"id":0},{"text":"computational","start":4,"end":17,"id":1},{"text":"or","start":18,"end":20,"id":2},{"text":"other","start":21,"end":26,"id":3},{"text":"reasons","start":27,"end":34,"id":4},{"text":",","start":34,"end":35,"id":5},{"text":"people","start":36,"end":42,"id":6},{"text":"may","start":43,"end":46,"id":7},{"text":"select","start":47,"end":53,"id":8},{"text":"only","start":54,"end":58,"id":9},{"text":"a","start":59,"end":60,"id":10},{"text":"small","start":61,"end":66,"id":11},{"text":"subset","start":67,"end":73,"id":12},{"text":"of","start":74,"end":76,"id":13},{"text":"features","start":77,"end":85,"id":14},{"text":"when","start":86,"end":90,"id":15},{"text":"modelling","start":91,"end":100,"id":16},{"text":"such","start":101,"end":105,"id":17},{"text":"data","start":106,"end":110,"id":18},{"text":",","start":110,"end":111,"id":19},{"text":"by","start":112,"end":114,"id":20},{"text":"looking","start":115,"end":122,"id":21},{"text":"at","start":123,"end":125,"id":22},{"text":"how","start":126,"end":129,"id":23},{"text":"relevant","start":130,"end":138,"id":24},{"text":"the","start":139,"end":142,"id":25},{"text":"features","start":143,"end":151,"id":26},{"text":"are","start":152,"end":155,"id":27},{"text":"to","start":156,"end":158,"id":28},{"text":"predicting","start":159,"end":169,"id":29},{"text":"the","start":170,"end":173,"id":30},{"text":"response","start":174,"end":182,"id":31},{"text":",","start":182,"end":183,"id":32},{"text":"based","start":184,"end":189,"id":33},{"text":"on","start":190,"end":192,"id":34},{"text":"some","start":193,"end":197,"id":35},{"text":"measure","start":198,"end":205,"id":36},{"text":"such","start":206,"end":210,"id":37},{"text":"as","start":211,"end":213,"id":38},{"text":"correlation","start":214,"end":225,"id":39},{"text":"with","start":226,"end":230,"id":40},{"text":"the","start":231,"end":234,"id":41},{"text":"response","start":235,"end":243,"id":42},{"text":"in","start":244,"end":246,"id":43},{"text":"the","start":247,"end":250,"id":44},{"text":"training","start":251,"end":259,"id":45},{"text":"data","start":260,"end":264,"id":46},{"text":".","start":264,"end":265,"id":47}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We generate a set of biased sequences by applying a finite state automata with a specified number, $m$, of states to the set of all binary sequences.","_input_hash":1039097680,"_task_hash":1277283597,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"generate","start":3,"end":11,"id":1},{"text":"a","start":12,"end":13,"id":2},{"text":"set","start":14,"end":17,"id":3},{"text":"of","start":18,"end":20,"id":4},{"text":"biased","start":21,"end":27,"id":5},{"text":"sequences","start":28,"end":37,"id":6},{"text":"by","start":38,"end":40,"id":7},{"text":"applying","start":41,"end":49,"id":8},{"text":"a","start":50,"end":51,"id":9},{"text":"finite","start":52,"end":58,"id":10},{"text":"state","start":59,"end":64,"id":11},{"text":"automata","start":65,"end":73,"id":12},{"text":"with","start":74,"end":78,"id":13},{"text":"a","start":79,"end":80,"id":14},{"text":"specified","start":81,"end":90,"id":15},{"text":"number","start":91,"end":97,"id":16},{"text":",","start":97,"end":98,"id":17},{"text":"$","start":99,"end":100,"id":18},{"text":"m$","start":100,"end":102,"id":19},{"text":",","start":102,"end":103,"id":20},{"text":"of","start":104,"end":106,"id":21},{"text":"states","start":107,"end":113,"id":22},{"text":"to","start":114,"end":116,"id":23},{"text":"the","start":117,"end":120,"id":24},{"text":"set","start":121,"end":124,"id":25},{"text":"of","start":125,"end":127,"id":26},{"text":"all","start":128,"end":131,"id":27},{"text":"binary","start":132,"end":138,"id":28},{"text":"sequences","start":139,"end":148,"id":29},{"text":".","start":148,"end":149,"id":30}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"Therefore, the probability of selecting proper class for a given input data, can be estimated even without the prior knowledge of its affiliation.","_input_hash":432004175,"_task_hash":-455672980,"tokens":[{"text":"Therefore","start":0,"end":9,"id":0},{"text":",","start":9,"end":10,"id":1},{"text":"the","start":11,"end":14,"id":2},{"text":"probability","start":15,"end":26,"id":3},{"text":"of","start":27,"end":29,"id":4},{"text":"selecting","start":30,"end":39,"id":5},{"text":"proper","start":40,"end":46,"id":6},{"text":"class","start":47,"end":52,"id":7},{"text":"for","start":53,"end":56,"id":8},{"text":"a","start":57,"end":58,"id":9},{"text":"given","start":59,"end":64,"id":10},{"text":"input","start":65,"end":70,"id":11},{"text":"data","start":71,"end":75,"id":12},{"text":",","start":75,"end":76,"id":13},{"text":"can","start":77,"end":80,"id":14},{"text":"be","start":81,"end":83,"id":15},{"text":"estimated","start":84,"end":93,"id":16},{"text":"even","start":94,"end":98,"id":17},{"text":"without","start":99,"end":106,"id":18},{"text":"the","start":107,"end":110,"id":19},{"text":"prior","start":111,"end":116,"id":20},{"text":"knowledge","start":117,"end":126,"id":21},{"text":"of","start":127,"end":129,"id":22},{"text":"its","start":130,"end":133,"id":23},{"text":"affiliation","start":134,"end":145,"id":24},{"text":".","start":145,"end":146,"id":25}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"The penalty selects functional groups of nodes in the trees.","_input_hash":-1197160411,"_task_hash":-168394650,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"penalty","start":4,"end":11,"id":1},{"text":"selects","start":12,"end":19,"id":2},{"text":"functional","start":20,"end":30,"id":3},{"text":"groups","start":31,"end":37,"id":4},{"text":"of","start":38,"end":40,"id":5},{"text":"nodes","start":41,"end":46,"id":6},{"text":"in","start":47,"end":49,"id":7},{"text":"the","start":50,"end":53,"id":8},{"text":"trees","start":54,"end":59,"id":9},{"text":".","start":59,"end":60,"id":10}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We illustrate the benefits of sLDA versus modern regularized regression, as well as versus an unsupervised LDA analysis followed by a separate regression.","_input_hash":1070269309,"_task_hash":2017059354,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"illustrate","start":3,"end":13,"id":1},{"text":"the","start":14,"end":17,"id":2},{"text":"benefits","start":18,"end":26,"id":3},{"text":"of","start":27,"end":29,"id":4},{"text":"sLDA","start":30,"end":34,"id":5},{"text":"versus","start":35,"end":41,"id":6},{"text":"modern","start":42,"end":48,"id":7},{"text":"regularized","start":49,"end":60,"id":8},{"text":"regression","start":61,"end":71,"id":9},{"text":",","start":71,"end":72,"id":10},{"text":"as","start":73,"end":75,"id":11},{"text":"well","start":76,"end":80,"id":12},{"text":"as","start":81,"end":83,"id":13},{"text":"versus","start":84,"end":90,"id":14},{"text":"an","start":91,"end":93,"id":15},{"text":"unsupervised","start":94,"end":106,"id":16},{"text":"LDA","start":107,"end":110,"id":17},{"text":"analysis","start":111,"end":119,"id":18},{"text":"followed","start":120,"end":128,"id":19},{"text":"by","start":129,"end":131,"id":20},{"text":"a","start":132,"end":133,"id":21},{"text":"separate","start":134,"end":142,"id":22},{"text":"regression","start":143,"end":153,"id":23},{"text":".","start":153,"end":154,"id":24}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Many nonparametric regressors were recently shown to converge at rates that depend only on the intrinsic dimension of data.","_input_hash":-1700816254,"_task_hash":-427869412,"tokens":[{"text":"Many","start":0,"end":4,"id":0},{"text":"nonparametric","start":5,"end":18,"id":1},{"text":"regressors","start":19,"end":29,"id":2},{"text":"were","start":30,"end":34,"id":3},{"text":"recently","start":35,"end":43,"id":4},{"text":"shown","start":44,"end":49,"id":5},{"text":"to","start":50,"end":52,"id":6},{"text":"converge","start":53,"end":61,"id":7},{"text":"at","start":62,"end":64,"id":8},{"text":"rates","start":65,"end":70,"id":9},{"text":"that","start":71,"end":75,"id":10},{"text":"depend","start":76,"end":82,"id":11},{"text":"only","start":83,"end":87,"id":12},{"text":"on","start":88,"end":90,"id":13},{"text":"the","start":91,"end":94,"id":14},{"text":"intrinsic","start":95,"end":104,"id":15},{"text":"dimension","start":105,"end":114,"id":16},{"text":"of","start":115,"end":117,"id":17},{"text":"data","start":118,"end":122,"id":18},{"text":".","start":122,"end":123,"id":19}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":5,"end":29,"token_start":1,"token_end":2,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"This thesis responds to the challenges of using a large number, such as thousands, of features in regression and classification problems.","_input_hash":179630665,"_task_hash":-187982617,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"thesis","start":5,"end":11,"id":1},{"text":"responds","start":12,"end":20,"id":2},{"text":"to","start":21,"end":23,"id":3},{"text":"the","start":24,"end":27,"id":4},{"text":"challenges","start":28,"end":38,"id":5},{"text":"of","start":39,"end":41,"id":6},{"text":"using","start":42,"end":47,"id":7},{"text":"a","start":48,"end":49,"id":8},{"text":"large","start":50,"end":55,"id":9},{"text":"number","start":56,"end":62,"id":10},{"text":",","start":62,"end":63,"id":11},{"text":"such","start":64,"end":68,"id":12},{"text":"as","start":69,"end":71,"id":13},{"text":"thousands","start":72,"end":81,"id":14},{"text":",","start":81,"end":82,"id":15},{"text":"of","start":83,"end":85,"id":16},{"text":"features","start":86,"end":94,"id":17},{"text":"in","start":95,"end":97,"id":18},{"text":"regression","start":98,"end":108,"id":19},{"text":"and","start":109,"end":112,"id":20},{"text":"classification","start":113,"end":127,"id":21},{"text":"problems","start":128,"end":136,"id":22},{"text":".","start":136,"end":137,"id":23}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Experimental results over reverberant synthetic mixtures and live recordings of speech data show the effectiveness of the proposed approach.","_input_hash":-1693532875,"_task_hash":1109455853,"tokens":[{"text":"Experimental","start":0,"end":12,"id":0},{"text":"results","start":13,"end":20,"id":1},{"text":"over","start":21,"end":25,"id":2},{"text":"reverberant","start":26,"end":37,"id":3},{"text":"synthetic","start":38,"end":47,"id":4},{"text":"mixtures","start":48,"end":56,"id":5},{"text":"and","start":57,"end":60,"id":6},{"text":"live","start":61,"end":65,"id":7},{"text":"recordings","start":66,"end":76,"id":8},{"text":"of","start":77,"end":79,"id":9},{"text":"speech","start":80,"end":86,"id":10},{"text":"data","start":87,"end":91,"id":11},{"text":"show","start":92,"end":96,"id":12},{"text":"the","start":97,"end":100,"id":13},{"text":"effectiveness","start":101,"end":114,"id":14},{"text":"of","start":115,"end":117,"id":15},{"text":"the","start":118,"end":121,"id":16},{"text":"proposed","start":122,"end":130,"id":17},{"text":"approach","start":131,"end":139,"id":18},{"text":".","start":139,"end":140,"id":19}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We first explore the relationship between the groups defining the norm and the resulting nonzero patterns, providing both forward and backward algorithms to go back and forth from groups to patterns.","_input_hash":-234136492,"_task_hash":-295065051,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"first","start":3,"end":8,"id":1},{"text":"explore","start":9,"end":16,"id":2},{"text":"the","start":17,"end":20,"id":3},{"text":"relationship","start":21,"end":33,"id":4},{"text":"between","start":34,"end":41,"id":5},{"text":"the","start":42,"end":45,"id":6},{"text":"groups","start":46,"end":52,"id":7},{"text":"defining","start":53,"end":61,"id":8},{"text":"the","start":62,"end":65,"id":9},{"text":"norm","start":66,"end":70,"id":10},{"text":"and","start":71,"end":74,"id":11},{"text":"the","start":75,"end":78,"id":12},{"text":"resulting","start":79,"end":88,"id":13},{"text":"nonzero","start":89,"end":96,"id":14},{"text":"patterns","start":97,"end":105,"id":15},{"text":",","start":105,"end":106,"id":16},{"text":"providing","start":107,"end":116,"id":17},{"text":"both","start":117,"end":121,"id":18},{"text":"forward","start":122,"end":129,"id":19},{"text":"and","start":130,"end":133,"id":20},{"text":"backward","start":134,"end":142,"id":21},{"text":"algorithms","start":143,"end":153,"id":22},{"text":"to","start":154,"end":156,"id":23},{"text":"go","start":157,"end":159,"id":24},{"text":"back","start":160,"end":164,"id":25},{"text":"and","start":165,"end":168,"id":26},{"text":"forth","start":169,"end":174,"id":27},{"text":"from","start":175,"end":179,"id":28},{"text":"groups","start":180,"end":186,"id":29},{"text":"to","start":187,"end":189,"id":30},{"text":"patterns","start":190,"end":198,"id":31},{"text":".","start":198,"end":199,"id":32}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"In the last few years, due to the growing ubiquity of unlabeled data, much effort has been spent by the machine learning community to develop better understanding and improve the quality of classifiers exploiting unlabeled data.","_input_hash":1675284532,"_task_hash":-2132619793,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"the","start":3,"end":6,"id":1},{"text":"last","start":7,"end":11,"id":2},{"text":"few","start":12,"end":15,"id":3},{"text":"years","start":16,"end":21,"id":4},{"text":",","start":21,"end":22,"id":5},{"text":"due","start":23,"end":26,"id":6},{"text":"to","start":27,"end":29,"id":7},{"text":"the","start":30,"end":33,"id":8},{"text":"growing","start":34,"end":41,"id":9},{"text":"ubiquity","start":42,"end":50,"id":10},{"text":"of","start":51,"end":53,"id":11},{"text":"unlabeled","start":54,"end":63,"id":12},{"text":"data","start":64,"end":68,"id":13},{"text":",","start":68,"end":69,"id":14},{"text":"much","start":70,"end":74,"id":15},{"text":"effort","start":75,"end":81,"id":16},{"text":"has","start":82,"end":85,"id":17},{"text":"been","start":86,"end":90,"id":18},{"text":"spent","start":91,"end":96,"id":19},{"text":"by","start":97,"end":99,"id":20},{"text":"the","start":100,"end":103,"id":21},{"text":"machine","start":104,"end":111,"id":22},{"text":"learning","start":112,"end":120,"id":23},{"text":"community","start":121,"end":130,"id":24},{"text":"to","start":131,"end":133,"id":25},{"text":"develop","start":134,"end":141,"id":26},{"text":"better","start":142,"end":148,"id":27},{"text":"understanding","start":149,"end":162,"id":28},{"text":"and","start":163,"end":166,"id":29},{"text":"improve","start":167,"end":174,"id":30},{"text":"the","start":175,"end":178,"id":31},{"text":"quality","start":179,"end":186,"id":32},{"text":"of","start":187,"end":189,"id":33},{"text":"classifiers","start":190,"end":201,"id":34},{"text":"exploiting","start":202,"end":212,"id":35},{"text":"unlabeled","start":213,"end":222,"id":36},{"text":"data","start":223,"end":227,"id":37},{"text":".","start":227,"end":228,"id":38}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Best previously known training algorithms achieve the same efficiency only for restricted special cases, whereas the proposed approach allows any real valued utility scores in the training data.","_input_hash":-1787796891,"_task_hash":-1934592570,"tokens":[{"text":"Best","start":0,"end":4,"id":0},{"text":"previously","start":5,"end":15,"id":1},{"text":"known","start":16,"end":21,"id":2},{"text":"training","start":22,"end":30,"id":3},{"text":"algorithms","start":31,"end":41,"id":4},{"text":"achieve","start":42,"end":49,"id":5},{"text":"the","start":50,"end":53,"id":6},{"text":"same","start":54,"end":58,"id":7},{"text":"efficiency","start":59,"end":69,"id":8},{"text":"only","start":70,"end":74,"id":9},{"text":"for","start":75,"end":78,"id":10},{"text":"restricted","start":79,"end":89,"id":11},{"text":"special","start":90,"end":97,"id":12},{"text":"cases","start":98,"end":103,"id":13},{"text":",","start":103,"end":104,"id":14},{"text":"whereas","start":105,"end":112,"id":15},{"text":"the","start":113,"end":116,"id":16},{"text":"proposed","start":117,"end":125,"id":17},{"text":"approach","start":126,"end":134,"id":18},{"text":"allows","start":135,"end":141,"id":19},{"text":"any","start":142,"end":145,"id":20},{"text":"real","start":146,"end":150,"id":21},{"text":"valued","start":151,"end":157,"id":22},{"text":"utility","start":158,"end":165,"id":23},{"text":"scores","start":166,"end":172,"id":24},{"text":"in","start":173,"end":175,"id":25},{"text":"the","start":176,"end":179,"id":26},{"text":"training","start":180,"end":188,"id":27},{"text":"data","start":189,"end":193,"id":28},{"text":".","start":193,"end":194,"id":29}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"However, the number of observation required for building an emulator becomes unrealistic when using classical covariance kernels when the dimension of input increases.","_input_hash":1843379829,"_task_hash":843947419,"tokens":[{"text":"However","start":0,"end":7,"id":0},{"text":",","start":7,"end":8,"id":1},{"text":"the","start":9,"end":12,"id":2},{"text":"number","start":13,"end":19,"id":3},{"text":"of","start":20,"end":22,"id":4},{"text":"observation","start":23,"end":34,"id":5},{"text":"required","start":35,"end":43,"id":6},{"text":"for","start":44,"end":47,"id":7},{"text":"building","start":48,"end":56,"id":8},{"text":"an","start":57,"end":59,"id":9},{"text":"emulator","start":60,"end":68,"id":10},{"text":"becomes","start":69,"end":76,"id":11},{"text":"unrealistic","start":77,"end":88,"id":12},{"text":"when","start":89,"end":93,"id":13},{"text":"using","start":94,"end":99,"id":14},{"text":"classical","start":100,"end":109,"id":15},{"text":"covariance","start":110,"end":120,"id":16},{"text":"kernels","start":121,"end":128,"id":17},{"text":"when","start":129,"end":133,"id":18},{"text":"the","start":134,"end":137,"id":19},{"text":"dimension","start":138,"end":147,"id":20},{"text":"of","start":148,"end":150,"id":21},{"text":"input","start":151,"end":156,"id":22},{"text":"increases","start":157,"end":166,"id":23},{"text":".","start":166,"end":167,"id":24}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We assume data independently sampled from a mixture distribution on the unit ball of the D-dimensional Euclidean space with K+1 components:","_input_hash":279266441,"_task_hash":-368282575,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"assume","start":3,"end":9,"id":1},{"text":"data","start":10,"end":14,"id":2},{"text":"independently","start":15,"end":28,"id":3},{"text":"sampled","start":29,"end":36,"id":4},{"text":"from","start":37,"end":41,"id":5},{"text":"a","start":42,"end":43,"id":6},{"text":"mixture","start":44,"end":51,"id":7},{"text":"distribution","start":52,"end":64,"id":8},{"text":"on","start":65,"end":67,"id":9},{"text":"the","start":68,"end":71,"id":10},{"text":"unit","start":72,"end":76,"id":11},{"text":"ball","start":77,"end":81,"id":12},{"text":"of","start":82,"end":84,"id":13},{"text":"the","start":85,"end":88,"id":14},{"text":"D","start":89,"end":90,"id":15},{"text":"-","start":90,"end":91,"id":16},{"text":"dimensional","start":91,"end":102,"id":17},{"text":"Euclidean","start":103,"end":112,"id":18},{"text":"space","start":113,"end":118,"id":19},{"text":"with","start":119,"end":123,"id":20},{"text":"K+1","start":124,"end":127,"id":21},{"text":"components","start":128,"end":138,"id":22},{"text":":","start":138,"end":139,"id":23}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"While no global \"uniform\" winner emerges from such comparisons, the overall performance of the k-NN method, together with its sound intuitive motivation and relative simplicity, suggests that it could represent a reasonable benchmark for the classification problem with functional data.","_input_hash":624564835,"_task_hash":-431428739,"tokens":[{"text":"While","start":0,"end":5,"id":0},{"text":"no","start":6,"end":8,"id":1},{"text":"global","start":9,"end":15,"id":2},{"text":"\"","start":16,"end":17,"id":3},{"text":"uniform","start":17,"end":24,"id":4},{"text":"\"","start":24,"end":25,"id":5},{"text":"winner","start":26,"end":32,"id":6},{"text":"emerges","start":33,"end":40,"id":7},{"text":"from","start":41,"end":45,"id":8},{"text":"such","start":46,"end":50,"id":9},{"text":"comparisons","start":51,"end":62,"id":10},{"text":",","start":62,"end":63,"id":11},{"text":"the","start":64,"end":67,"id":12},{"text":"overall","start":68,"end":75,"id":13},{"text":"performance","start":76,"end":87,"id":14},{"text":"of","start":88,"end":90,"id":15},{"text":"the","start":91,"end":94,"id":16},{"text":"k","start":95,"end":96,"id":17},{"text":"-","start":96,"end":97,"id":18},{"text":"NN","start":97,"end":99,"id":19},{"text":"method","start":100,"end":106,"id":20},{"text":",","start":106,"end":107,"id":21},{"text":"together","start":108,"end":116,"id":22},{"text":"with","start":117,"end":121,"id":23},{"text":"its","start":122,"end":125,"id":24},{"text":"sound","start":126,"end":131,"id":25},{"text":"intuitive","start":132,"end":141,"id":26},{"text":"motivation","start":142,"end":152,"id":27},{"text":"and","start":153,"end":156,"id":28},{"text":"relative","start":157,"end":165,"id":29},{"text":"simplicity","start":166,"end":176,"id":30},{"text":",","start":176,"end":177,"id":31},{"text":"suggests","start":178,"end":186,"id":32},{"text":"that","start":187,"end":191,"id":33},{"text":"it","start":192,"end":194,"id":34},{"text":"could","start":195,"end":200,"id":35},{"text":"represent","start":201,"end":210,"id":36},{"text":"a","start":211,"end":212,"id":37},{"text":"reasonable","start":213,"end":223,"id":38},{"text":"benchmark","start":224,"end":233,"id":39},{"text":"for","start":234,"end":237,"id":40},{"text":"the","start":238,"end":241,"id":41},{"text":"classification","start":242,"end":256,"id":42},{"text":"problem","start":257,"end":264,"id":43},{"text":"with","start":265,"end":269,"id":44},{"text":"functional","start":270,"end":280,"id":45},{"text":"data","start":281,"end":285,"id":46},{"text":".","start":285,"end":286,"id":47}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":95,"end":99,"token_start":17,"token_end":19,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"This new theory includes many seemingly loosely related previous work as special cases;","_input_hash":383726469,"_task_hash":-1084951565,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"new","start":5,"end":8,"id":1},{"text":"theory","start":9,"end":15,"id":2},{"text":"includes","start":16,"end":24,"id":3},{"text":"many","start":25,"end":29,"id":4},{"text":"seemingly","start":30,"end":39,"id":5},{"text":"loosely","start":40,"end":47,"id":6},{"text":"related","start":48,"end":55,"id":7},{"text":"previous","start":56,"end":64,"id":8},{"text":"work","start":65,"end":69,"id":9},{"text":"as","start":70,"end":72,"id":10},{"text":"special","start":73,"end":80,"id":11},{"text":"cases","start":81,"end":86,"id":12},{"text":";","start":86,"end":87,"id":13}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We also describe how for a given limiting Laplacian operator desirable properties such as a convergent spectrum and sparseness can be achieved choosing the appropriate graph construction.","_input_hash":-1246146752,"_task_hash":2057370631,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"also","start":3,"end":7,"id":1},{"text":"describe","start":8,"end":16,"id":2},{"text":"how","start":17,"end":20,"id":3},{"text":"for","start":21,"end":24,"id":4},{"text":"a","start":25,"end":26,"id":5},{"text":"given","start":27,"end":32,"id":6},{"text":"limiting","start":33,"end":41,"id":7},{"text":"Laplacian","start":42,"end":51,"id":8},{"text":"operator","start":52,"end":60,"id":9},{"text":"desirable","start":61,"end":70,"id":10},{"text":"properties","start":71,"end":81,"id":11},{"text":"such","start":82,"end":86,"id":12},{"text":"as","start":87,"end":89,"id":13},{"text":"a","start":90,"end":91,"id":14},{"text":"convergent","start":92,"end":102,"id":15},{"text":"spectrum","start":103,"end":111,"id":16},{"text":"and","start":112,"end":115,"id":17},{"text":"sparseness","start":116,"end":126,"id":18},{"text":"can","start":127,"end":130,"id":19},{"text":"be","start":131,"end":133,"id":20},{"text":"achieved","start":134,"end":142,"id":21},{"text":"choosing","start":143,"end":151,"id":22},{"text":"the","start":152,"end":155,"id":23},{"text":"appropriate","start":156,"end":167,"id":24},{"text":"graph","start":168,"end":173,"id":25},{"text":"construction","start":174,"end":186,"id":26},{"text":".","start":186,"end":187,"id":27}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"A popular method for selecting the number of clusters is based on stability arguments:","_input_hash":-753137441,"_task_hash":-437085051,"tokens":[{"text":"A","start":0,"end":1,"id":0},{"text":"popular","start":2,"end":9,"id":1},{"text":"method","start":10,"end":16,"id":2},{"text":"for","start":17,"end":20,"id":3},{"text":"selecting","start":21,"end":30,"id":4},{"text":"the","start":31,"end":34,"id":5},{"text":"number","start":35,"end":41,"id":6},{"text":"of","start":42,"end":44,"id":7},{"text":"clusters","start":45,"end":53,"id":8},{"text":"is","start":54,"end":56,"id":9},{"text":"based","start":57,"end":62,"id":10},{"text":"on","start":63,"end":65,"id":11},{"text":"stability","start":66,"end":75,"id":12},{"text":"arguments","start":76,"end":85,"id":13},{"text":":","start":85,"end":86,"id":14}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"PC-EP is proved to minimize an energy function which, under some constraints, is bounded from below and whose stationary points coincide with the solution of R-EP.","_input_hash":873792897,"_task_hash":-32147804,"tokens":[{"text":"PC","start":0,"end":2,"id":0},{"text":"-","start":2,"end":3,"id":1},{"text":"EP","start":3,"end":5,"id":2},{"text":"is","start":6,"end":8,"id":3},{"text":"proved","start":9,"end":15,"id":4},{"text":"to","start":16,"end":18,"id":5},{"text":"minimize","start":19,"end":27,"id":6},{"text":"an","start":28,"end":30,"id":7},{"text":"energy","start":31,"end":37,"id":8},{"text":"function","start":38,"end":46,"id":9},{"text":"which","start":47,"end":52,"id":10},{"text":",","start":52,"end":53,"id":11},{"text":"under","start":54,"end":59,"id":12},{"text":"some","start":60,"end":64,"id":13},{"text":"constraints","start":65,"end":76,"id":14},{"text":",","start":76,"end":77,"id":15},{"text":"is","start":78,"end":80,"id":16},{"text":"bounded","start":81,"end":88,"id":17},{"text":"from","start":89,"end":93,"id":18},{"text":"below","start":94,"end":99,"id":19},{"text":"and","start":100,"end":103,"id":20},{"text":"whose","start":104,"end":109,"id":21},{"text":"stationary","start":110,"end":120,"id":22},{"text":"points","start":121,"end":127,"id":23},{"text":"coincide","start":128,"end":136,"id":24},{"text":"with","start":137,"end":141,"id":25},{"text":"the","start":142,"end":145,"id":26},{"text":"solution","start":146,"end":154,"id":27},{"text":"of","start":155,"end":157,"id":28},{"text":"R","start":158,"end":159,"id":29},{"text":"-","start":159,"end":160,"id":30},{"text":"EP","start":160,"end":162,"id":31},{"text":".","start":162,"end":163,"id":32}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"In this context, learning the dictionary amounts to solving a large-scale matrix factorization problem, which can be done efficiently with classical optimization tools.","_input_hash":1100707809,"_task_hash":1043151255,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"this","start":3,"end":7,"id":1},{"text":"context","start":8,"end":15,"id":2},{"text":",","start":15,"end":16,"id":3},{"text":"learning","start":17,"end":25,"id":4},{"text":"the","start":26,"end":29,"id":5},{"text":"dictionary","start":30,"end":40,"id":6},{"text":"amounts","start":41,"end":48,"id":7},{"text":"to","start":49,"end":51,"id":8},{"text":"solving","start":52,"end":59,"id":9},{"text":"a","start":60,"end":61,"id":10},{"text":"large","start":62,"end":67,"id":11},{"text":"-","start":67,"end":68,"id":12},{"text":"scale","start":68,"end":73,"id":13},{"text":"matrix","start":74,"end":80,"id":14},{"text":"factorization","start":81,"end":94,"id":15},{"text":"problem","start":95,"end":102,"id":16},{"text":",","start":102,"end":103,"id":17},{"text":"which","start":104,"end":109,"id":18},{"text":"can","start":110,"end":113,"id":19},{"text":"be","start":114,"end":116,"id":20},{"text":"done","start":117,"end":121,"id":21},{"text":"efficiently","start":122,"end":133,"id":22},{"text":"with","start":134,"end":138,"id":23},{"text":"classical","start":139,"end":148,"id":24},{"text":"optimization","start":149,"end":161,"id":25},{"text":"tools","start":162,"end":167,"id":26},{"text":".","start":167,"end":168,"id":27}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Recently there has been an increasing interest in methods that deal with multiple outputs.","_input_hash":922042019,"_task_hash":231578561,"tokens":[{"text":"Recently","start":0,"end":8,"id":0},{"text":"there","start":9,"end":14,"id":1},{"text":"has","start":15,"end":18,"id":2},{"text":"been","start":19,"end":23,"id":3},{"text":"an","start":24,"end":26,"id":4},{"text":"increasing","start":27,"end":37,"id":5},{"text":"interest","start":38,"end":46,"id":6},{"text":"in","start":47,"end":49,"id":7},{"text":"methods","start":50,"end":57,"id":8},{"text":"that","start":58,"end":62,"id":9},{"text":"deal","start":63,"end":67,"id":10},{"text":"with","start":68,"end":72,"id":11},{"text":"multiple","start":73,"end":81,"id":12},{"text":"outputs","start":82,"end":89,"id":13},{"text":".","start":89,"end":90,"id":14}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"We propose a solution by appealing to the framework of regularization in a reproducing kernel Hilbert space and prove a representer-like theorem for NN classification.","_input_hash":88466947,"_task_hash":432560752,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"propose","start":3,"end":10,"id":1},{"text":"a","start":11,"end":12,"id":2},{"text":"solution","start":13,"end":21,"id":3},{"text":"by","start":22,"end":24,"id":4},{"text":"appealing","start":25,"end":34,"id":5},{"text":"to","start":35,"end":37,"id":6},{"text":"the","start":38,"end":41,"id":7},{"text":"framework","start":42,"end":51,"id":8},{"text":"of","start":52,"end":54,"id":9},{"text":"regularization","start":55,"end":69,"id":10},{"text":"in","start":70,"end":72,"id":11},{"text":"a","start":73,"end":74,"id":12},{"text":"reproducing","start":75,"end":86,"id":13},{"text":"kernel","start":87,"end":93,"id":14},{"text":"Hilbert","start":94,"end":101,"id":15},{"text":"space","start":102,"end":107,"id":16},{"text":"and","start":108,"end":111,"id":17},{"text":"prove","start":112,"end":117,"id":18},{"text":"a","start":118,"end":119,"id":19},{"text":"representer","start":120,"end":131,"id":20},{"text":"-","start":131,"end":132,"id":21},{"text":"like","start":132,"end":136,"id":22},{"text":"theorem","start":137,"end":144,"id":23},{"text":"for","start":145,"end":148,"id":24},{"text":"NN","start":149,"end":151,"id":25},{"text":"classification","start":152,"end":166,"id":26},{"text":".","start":166,"end":167,"id":27}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":149,"end":166,"token_start":25,"token_end":26,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Asymptotic null distributions under null hypothesis are derived, and consistency against fixed and local alternatives is assessed.","_input_hash":1669956228,"_task_hash":967005207,"tokens":[{"text":"Asymptotic","start":0,"end":10,"id":0},{"text":"null","start":11,"end":15,"id":1},{"text":"distributions","start":16,"end":29,"id":2},{"text":"under","start":30,"end":35,"id":3},{"text":"null","start":36,"end":40,"id":4},{"text":"hypothesis","start":41,"end":51,"id":5},{"text":"are","start":52,"end":55,"id":6},{"text":"derived","start":56,"end":63,"id":7},{"text":",","start":63,"end":64,"id":8},{"text":"and","start":65,"end":68,"id":9},{"text":"consistency","start":69,"end":80,"id":10},{"text":"against","start":81,"end":88,"id":11},{"text":"fixed","start":89,"end":94,"id":12},{"text":"and","start":95,"end":98,"id":13},{"text":"local","start":99,"end":104,"id":14},{"text":"alternatives","start":105,"end":117,"id":15},{"text":"is","start":118,"end":120,"id":16},{"text":"assessed","start":121,"end":129,"id":17},{"text":".","start":129,"end":130,"id":18}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"In such frameworks, linear acyclic models are typically used to model the data-generating process of variables.","_input_hash":1940473842,"_task_hash":1627445776,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"such","start":3,"end":7,"id":1},{"text":"frameworks","start":8,"end":18,"id":2},{"text":",","start":18,"end":19,"id":3},{"text":"linear","start":20,"end":26,"id":4},{"text":"acyclic","start":27,"end":34,"id":5},{"text":"models","start":35,"end":41,"id":6},{"text":"are","start":42,"end":45,"id":7},{"text":"typically","start":46,"end":55,"id":8},{"text":"used","start":56,"end":60,"id":9},{"text":"to","start":61,"end":63,"id":10},{"text":"model","start":64,"end":69,"id":11},{"text":"the","start":70,"end":73,"id":12},{"text":"data","start":74,"end":78,"id":13},{"text":"-","start":78,"end":79,"id":14},{"text":"generating","start":79,"end":89,"id":15},{"text":"process","start":90,"end":97,"id":16},{"text":"of","start":98,"end":100,"id":17},{"text":"variables","start":101,"end":110,"id":18},{"text":".","start":110,"end":111,"id":19}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":20,"end":34,"token_start":4,"token_end":5,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Our method is constructed from an ensemble of interacting, learning agents, that acquire and process incoming information using various types, or different versions of machine learning algorithms.","_input_hash":2074196084,"_task_hash":-547819629,"tokens":[{"text":"Our","start":0,"end":3,"id":0},{"text":"method","start":4,"end":10,"id":1},{"text":"is","start":11,"end":13,"id":2},{"text":"constructed","start":14,"end":25,"id":3},{"text":"from","start":26,"end":30,"id":4},{"text":"an","start":31,"end":33,"id":5},{"text":"ensemble","start":34,"end":42,"id":6},{"text":"of","start":43,"end":45,"id":7},{"text":"interacting","start":46,"end":57,"id":8},{"text":",","start":57,"end":58,"id":9},{"text":"learning","start":59,"end":67,"id":10},{"text":"agents","start":68,"end":74,"id":11},{"text":",","start":74,"end":75,"id":12},{"text":"that","start":76,"end":80,"id":13},{"text":"acquire","start":81,"end":88,"id":14},{"text":"and","start":89,"end":92,"id":15},{"text":"process","start":93,"end":100,"id":16},{"text":"incoming","start":101,"end":109,"id":17},{"text":"information","start":110,"end":121,"id":18},{"text":"using","start":122,"end":127,"id":19},{"text":"various","start":128,"end":135,"id":20},{"text":"types","start":136,"end":141,"id":21},{"text":",","start":141,"end":142,"id":22},{"text":"or","start":143,"end":145,"id":23},{"text":"different","start":146,"end":155,"id":24},{"text":"versions","start":156,"end":164,"id":25},{"text":"of","start":165,"end":167,"id":26},{"text":"machine","start":168,"end":175,"id":27},{"text":"learning","start":176,"end":184,"id":28},{"text":"algorithms","start":185,"end":195,"id":29},{"text":".","start":195,"end":196,"id":30}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Graph-based approaches are popular tools for these problems.","_input_hash":1636215581,"_task_hash":-416079340,"tokens":[{"text":"Graph","start":0,"end":5,"id":0},{"text":"-","start":5,"end":6,"id":1},{"text":"based","start":6,"end":11,"id":2},{"text":"approaches","start":12,"end":22,"id":3},{"text":"are","start":23,"end":26,"id":4},{"text":"popular","start":27,"end":34,"id":5},{"text":"tools","start":35,"end":40,"id":6},{"text":"for","start":41,"end":44,"id":7},{"text":"these","start":45,"end":50,"id":8},{"text":"problems","start":51,"end":59,"id":9},{"text":".","start":59,"end":60,"id":10}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Concave regularization methods provide natural procedures for sparse recovery.","_input_hash":1752761472,"_task_hash":-1173486104,"tokens":[{"text":"Concave","start":0,"end":7,"id":0},{"text":"regularization","start":8,"end":22,"id":1},{"text":"methods","start":23,"end":30,"id":2},{"text":"provide","start":31,"end":38,"id":3},{"text":"natural","start":39,"end":46,"id":4},{"text":"procedures","start":47,"end":57,"id":5},{"text":"for","start":58,"end":61,"id":6},{"text":"sparse","start":62,"end":68,"id":7},{"text":"recovery","start":69,"end":77,"id":8},{"text":".","start":77,"end":78,"id":9}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":0,"end":22,"token_start":0,"token_end":1,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Finally, experimental evidence of the performance of the proposed approach on both artificial data and a speaker verification task is provided.","_input_hash":-1272997971,"_task_hash":799308236,"tokens":[{"text":"Finally","start":0,"end":7,"id":0},{"text":",","start":7,"end":8,"id":1},{"text":"experimental","start":9,"end":21,"id":2},{"text":"evidence","start":22,"end":30,"id":3},{"text":"of","start":31,"end":33,"id":4},{"text":"the","start":34,"end":37,"id":5},{"text":"performance","start":38,"end":49,"id":6},{"text":"of","start":50,"end":52,"id":7},{"text":"the","start":53,"end":56,"id":8},{"text":"proposed","start":57,"end":65,"id":9},{"text":"approach","start":66,"end":74,"id":10},{"text":"on","start":75,"end":77,"id":11},{"text":"both","start":78,"end":82,"id":12},{"text":"artificial","start":83,"end":93,"id":13},{"text":"data","start":94,"end":98,"id":14},{"text":"and","start":99,"end":102,"id":15},{"text":"a","start":103,"end":104,"id":16},{"text":"speaker","start":105,"end":112,"id":17},{"text":"verification","start":113,"end":125,"id":18},{"text":"task","start":126,"end":130,"id":19},{"text":"is","start":131,"end":133,"id":20},{"text":"provided","start":134,"end":142,"id":21},{"text":".","start":142,"end":143,"id":22}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Furthermore, we give the minimax learning rate on the ball characterized by lp-mixed-norm in the product space.","_input_hash":-1901652191,"_task_hash":-979709130,"tokens":[{"text":"Furthermore","start":0,"end":11,"id":0},{"text":",","start":11,"end":12,"id":1},{"text":"we","start":13,"end":15,"id":2},{"text":"give","start":16,"end":20,"id":3},{"text":"the","start":21,"end":24,"id":4},{"text":"minimax","start":25,"end":32,"id":5},{"text":"learning","start":33,"end":41,"id":6},{"text":"rate","start":42,"end":46,"id":7},{"text":"on","start":47,"end":49,"id":8},{"text":"the","start":50,"end":53,"id":9},{"text":"ball","start":54,"end":58,"id":10},{"text":"characterized","start":59,"end":72,"id":11},{"text":"by","start":73,"end":75,"id":12},{"text":"lp","start":76,"end":78,"id":13},{"text":"-","start":78,"end":79,"id":14},{"text":"mixed","start":79,"end":84,"id":15},{"text":"-","start":84,"end":85,"id":16},{"text":"norm","start":85,"end":89,"id":17},{"text":"in","start":90,"end":92,"id":18},{"text":"the","start":93,"end":96,"id":19},{"text":"product","start":97,"end":104,"id":20},{"text":"space","start":105,"end":110,"id":21},{"text":".","start":110,"end":111,"id":22}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"The implications are that certain moduli spaces known in algebraic geometry are $p$-adic parameter spaces of (families of) dendrograms, and stochastic classification can also be handled within this framework.","_input_hash":-1428625400,"_task_hash":2022874463,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"implications","start":4,"end":16,"id":1},{"text":"are","start":17,"end":20,"id":2},{"text":"that","start":21,"end":25,"id":3},{"text":"certain","start":26,"end":33,"id":4},{"text":"moduli","start":34,"end":40,"id":5},{"text":"spaces","start":41,"end":47,"id":6},{"text":"known","start":48,"end":53,"id":7},{"text":"in","start":54,"end":56,"id":8},{"text":"algebraic","start":57,"end":66,"id":9},{"text":"geometry","start":67,"end":75,"id":10},{"text":"are","start":76,"end":79,"id":11},{"text":"$","start":80,"end":81,"id":12},{"text":"p$-adic","start":81,"end":88,"id":13},{"text":"parameter","start":89,"end":98,"id":14},{"text":"spaces","start":99,"end":105,"id":15},{"text":"of","start":106,"end":108,"id":16},{"text":"(","start":109,"end":110,"id":17},{"text":"families","start":110,"end":118,"id":18},{"text":"of","start":119,"end":121,"id":19},{"text":")","start":121,"end":122,"id":20},{"text":"dendrograms","start":123,"end":134,"id":21},{"text":",","start":134,"end":135,"id":22},{"text":"and","start":136,"end":139,"id":23},{"text":"stochastic","start":140,"end":150,"id":24},{"text":"classification","start":151,"end":165,"id":25},{"text":"can","start":166,"end":169,"id":26},{"text":"also","start":170,"end":174,"id":27},{"text":"be","start":175,"end":177,"id":28},{"text":"handled","start":178,"end":185,"id":29},{"text":"within","start":186,"end":192,"id":30},{"text":"this","start":193,"end":197,"id":31},{"text":"framework","start":198,"end":207,"id":32},{"text":".","start":207,"end":208,"id":33}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":140,"end":165,"token_start":24,"token_end":25,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"The estimator includes as a special instance the algorithms proposed by Tipping and Faul [1] and by Babacan et al. [","_input_hash":598006809,"_task_hash":-1753613545,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"estimator","start":4,"end":13,"id":1},{"text":"includes","start":14,"end":22,"id":2},{"text":"as","start":23,"end":25,"id":3},{"text":"a","start":26,"end":27,"id":4},{"text":"special","start":28,"end":35,"id":5},{"text":"instance","start":36,"end":44,"id":6},{"text":"the","start":45,"end":48,"id":7},{"text":"algorithms","start":49,"end":59,"id":8},{"text":"proposed","start":60,"end":68,"id":9},{"text":"by","start":69,"end":71,"id":10},{"text":"Tipping","start":72,"end":79,"id":11},{"text":"and","start":80,"end":83,"id":12},{"text":"Faul","start":84,"end":88,"id":13},{"text":"[","start":89,"end":90,"id":14},{"text":"1","start":90,"end":91,"id":15},{"text":"]","start":91,"end":92,"id":16},{"text":"and","start":93,"end":96,"id":17},{"text":"by","start":97,"end":99,"id":18},{"text":"Babacan","start":100,"end":107,"id":19},{"text":"et","start":108,"end":110,"id":20},{"text":"al","start":111,"end":113,"id":21},{"text":".","start":113,"end":114,"id":22},{"text":"[","start":115,"end":116,"id":23}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"This way, our method is in the same spirit as faithfulness-based causal inference because it also rejects non-generic mutual adjustments among DAG-parameters.","_input_hash":1196395389,"_task_hash":-1337465027,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"way","start":5,"end":8,"id":1},{"text":",","start":8,"end":9,"id":2},{"text":"our","start":10,"end":13,"id":3},{"text":"method","start":14,"end":20,"id":4},{"text":"is","start":21,"end":23,"id":5},{"text":"in","start":24,"end":26,"id":6},{"text":"the","start":27,"end":30,"id":7},{"text":"same","start":31,"end":35,"id":8},{"text":"spirit","start":36,"end":42,"id":9},{"text":"as","start":43,"end":45,"id":10},{"text":"faithfulness","start":46,"end":58,"id":11},{"text":"-","start":58,"end":59,"id":12},{"text":"based","start":59,"end":64,"id":13},{"text":"causal","start":65,"end":71,"id":14},{"text":"inference","start":72,"end":81,"id":15},{"text":"because","start":82,"end":89,"id":16},{"text":"it","start":90,"end":92,"id":17},{"text":"also","start":93,"end":97,"id":18},{"text":"rejects","start":98,"end":105,"id":19},{"text":"non","start":106,"end":109,"id":20},{"text":"-","start":109,"end":110,"id":21},{"text":"generic","start":110,"end":117,"id":22},{"text":"mutual","start":118,"end":124,"id":23},{"text":"adjustments","start":125,"end":136,"id":24},{"text":"among","start":137,"end":142,"id":25},{"text":"DAG","start":143,"end":146,"id":26},{"text":"-","start":146,"end":147,"id":27},{"text":"parameters","start":147,"end":157,"id":28},{"text":".","start":157,"end":158,"id":29}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"To address this problem, different methods have been proposed, such as, among others, univariate feature selection, feature agglomeration and regularization techniques.","_input_hash":1300311174,"_task_hash":-1651083350,"tokens":[{"text":"To","start":0,"end":2,"id":0},{"text":"address","start":3,"end":10,"id":1},{"text":"this","start":11,"end":15,"id":2},{"text":"problem","start":16,"end":23,"id":3},{"text":",","start":23,"end":24,"id":4},{"text":"different","start":25,"end":34,"id":5},{"text":"methods","start":35,"end":42,"id":6},{"text":"have","start":43,"end":47,"id":7},{"text":"been","start":48,"end":52,"id":8},{"text":"proposed","start":53,"end":61,"id":9},{"text":",","start":61,"end":62,"id":10},{"text":"such","start":63,"end":67,"id":11},{"text":"as","start":68,"end":70,"id":12},{"text":",","start":70,"end":71,"id":13},{"text":"among","start":72,"end":77,"id":14},{"text":"others","start":78,"end":84,"id":15},{"text":",","start":84,"end":85,"id":16},{"text":"univariate","start":86,"end":96,"id":17},{"text":"feature","start":97,"end":104,"id":18},{"text":"selection","start":105,"end":114,"id":19},{"text":",","start":114,"end":115,"id":20},{"text":"feature","start":116,"end":123,"id":21},{"text":"agglomeration","start":124,"end":137,"id":22},{"text":"and","start":138,"end":141,"id":23},{"text":"regularization","start":142,"end":156,"id":24},{"text":"techniques","start":157,"end":167,"id":25},{"text":".","start":167,"end":168,"id":26}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We show that any distribution p on the set of binary vectors of length n can be arbitrarily well approximated by an RBM with k-1 hidden units, where k is the minimal number of pairs of binary vectors differing in only one entry such that their union contains the support set of p. In important cases this number is half of the cardinality of the support set of p. We construct a DBN with 2^n/2(n-b), b ~ log(n), hidden layers of width n that is capable of approximating any distribution on {0,1}^n arbitrarily well.","_input_hash":1763506409,"_task_hash":169123241,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"show","start":3,"end":7,"id":1},{"text":"that","start":8,"end":12,"id":2},{"text":"any","start":13,"end":16,"id":3},{"text":"distribution","start":17,"end":29,"id":4},{"text":"p","start":30,"end":31,"id":5},{"text":"on","start":32,"end":34,"id":6},{"text":"the","start":35,"end":38,"id":7},{"text":"set","start":39,"end":42,"id":8},{"text":"of","start":43,"end":45,"id":9},{"text":"binary","start":46,"end":52,"id":10},{"text":"vectors","start":53,"end":60,"id":11},{"text":"of","start":61,"end":63,"id":12},{"text":"length","start":64,"end":70,"id":13},{"text":"n","start":71,"end":72,"id":14},{"text":"can","start":73,"end":76,"id":15},{"text":"be","start":77,"end":79,"id":16},{"text":"arbitrarily","start":80,"end":91,"id":17},{"text":"well","start":92,"end":96,"id":18},{"text":"approximated","start":97,"end":109,"id":19},{"text":"by","start":110,"end":112,"id":20},{"text":"an","start":113,"end":115,"id":21},{"text":"RBM","start":116,"end":119,"id":22},{"text":"with","start":120,"end":124,"id":23},{"text":"k-1","start":125,"end":128,"id":24},{"text":"hidden","start":129,"end":135,"id":25},{"text":"units","start":136,"end":141,"id":26},{"text":",","start":141,"end":142,"id":27},{"text":"where","start":143,"end":148,"id":28},{"text":"k","start":149,"end":150,"id":29},{"text":"is","start":151,"end":153,"id":30},{"text":"the","start":154,"end":157,"id":31},{"text":"minimal","start":158,"end":165,"id":32},{"text":"number","start":166,"end":172,"id":33},{"text":"of","start":173,"end":175,"id":34},{"text":"pairs","start":176,"end":181,"id":35},{"text":"of","start":182,"end":184,"id":36},{"text":"binary","start":185,"end":191,"id":37},{"text":"vectors","start":192,"end":199,"id":38},{"text":"differing","start":200,"end":209,"id":39},{"text":"in","start":210,"end":212,"id":40},{"text":"only","start":213,"end":217,"id":41},{"text":"one","start":218,"end":221,"id":42},{"text":"entry","start":222,"end":227,"id":43},{"text":"such","start":228,"end":232,"id":44},{"text":"that","start":233,"end":237,"id":45},{"text":"their","start":238,"end":243,"id":46},{"text":"union","start":244,"end":249,"id":47},{"text":"contains","start":250,"end":258,"id":48},{"text":"the","start":259,"end":262,"id":49},{"text":"support","start":263,"end":270,"id":50},{"text":"set","start":271,"end":274,"id":51},{"text":"of","start":275,"end":277,"id":52},{"text":"p.","start":278,"end":280,"id":53},{"text":"In","start":281,"end":283,"id":54},{"text":"important","start":284,"end":293,"id":55},{"text":"cases","start":294,"end":299,"id":56},{"text":"this","start":300,"end":304,"id":57},{"text":"number","start":305,"end":311,"id":58},{"text":"is","start":312,"end":314,"id":59},{"text":"half","start":315,"end":319,"id":60},{"text":"of","start":320,"end":322,"id":61},{"text":"the","start":323,"end":326,"id":62},{"text":"cardinality","start":327,"end":338,"id":63},{"text":"of","start":339,"end":341,"id":64},{"text":"the","start":342,"end":345,"id":65},{"text":"support","start":346,"end":353,"id":66},{"text":"set","start":354,"end":357,"id":67},{"text":"of","start":358,"end":360,"id":68},{"text":"p.","start":361,"end":363,"id":69},{"text":"We","start":364,"end":366,"id":70},{"text":"construct","start":367,"end":376,"id":71},{"text":"a","start":377,"end":378,"id":72},{"text":"DBN","start":379,"end":382,"id":73},{"text":"with","start":383,"end":387,"id":74},{"text":"2^n/2(n","start":388,"end":395,"id":75},{"text":"-","start":395,"end":396,"id":76},{"text":"b","start":396,"end":397,"id":77},{"text":")","start":397,"end":398,"id":78},{"text":",","start":398,"end":399,"id":79},{"text":"b","start":400,"end":401,"id":80},{"text":"~","start":402,"end":403,"id":81},{"text":"log(n","start":404,"end":409,"id":82},{"text":")","start":409,"end":410,"id":83},{"text":",","start":410,"end":411,"id":84},{"text":"hidden","start":412,"end":418,"id":85},{"text":"layers","start":419,"end":425,"id":86},{"text":"of","start":426,"end":428,"id":87},{"text":"width","start":429,"end":434,"id":88},{"text":"n","start":435,"end":436,"id":89},{"text":"that","start":437,"end":441,"id":90},{"text":"is","start":442,"end":444,"id":91},{"text":"capable","start":445,"end":452,"id":92},{"text":"of","start":453,"end":455,"id":93},{"text":"approximating","start":456,"end":469,"id":94},{"text":"any","start":470,"end":473,"id":95},{"text":"distribution","start":474,"end":486,"id":96},{"text":"on","start":487,"end":489,"id":97},{"text":"{","start":490,"end":491,"id":98},{"text":"0,1}^n","start":491,"end":497,"id":99},{"text":"arbitrarily","start":498,"end":509,"id":100},{"text":"well","start":510,"end":514,"id":101},{"text":".","start":514,"end":515,"id":102}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Using convex analysis on the set of posterior probability measures, we show how to get local measures of the complexity of the classification model involving the relative entropy of posterior distributions with respect to Gibbs posterior measures.","_input_hash":-1112714564,"_task_hash":-1706041221,"tokens":[{"text":"Using","start":0,"end":5,"id":0},{"text":"convex","start":6,"end":12,"id":1},{"text":"analysis","start":13,"end":21,"id":2},{"text":"on","start":22,"end":24,"id":3},{"text":"the","start":25,"end":28,"id":4},{"text":"set","start":29,"end":32,"id":5},{"text":"of","start":33,"end":35,"id":6},{"text":"posterior","start":36,"end":45,"id":7},{"text":"probability","start":46,"end":57,"id":8},{"text":"measures","start":58,"end":66,"id":9},{"text":",","start":66,"end":67,"id":10},{"text":"we","start":68,"end":70,"id":11},{"text":"show","start":71,"end":75,"id":12},{"text":"how","start":76,"end":79,"id":13},{"text":"to","start":80,"end":82,"id":14},{"text":"get","start":83,"end":86,"id":15},{"text":"local","start":87,"end":92,"id":16},{"text":"measures","start":93,"end":101,"id":17},{"text":"of","start":102,"end":104,"id":18},{"text":"the","start":105,"end":108,"id":19},{"text":"complexity","start":109,"end":119,"id":20},{"text":"of","start":120,"end":122,"id":21},{"text":"the","start":123,"end":126,"id":22},{"text":"classification","start":127,"end":141,"id":23},{"text":"model","start":142,"end":147,"id":24},{"text":"involving","start":148,"end":157,"id":25},{"text":"the","start":158,"end":161,"id":26},{"text":"relative","start":162,"end":170,"id":27},{"text":"entropy","start":171,"end":178,"id":28},{"text":"of","start":179,"end":181,"id":29},{"text":"posterior","start":182,"end":191,"id":30},{"text":"distributions","start":192,"end":205,"id":31},{"text":"with","start":206,"end":210,"id":32},{"text":"respect","start":211,"end":218,"id":33},{"text":"to","start":219,"end":221,"id":34},{"text":"Gibbs","start":222,"end":227,"id":35},{"text":"posterior","start":228,"end":237,"id":36},{"text":"measures","start":238,"end":246,"id":37},{"text":".","start":246,"end":247,"id":38}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We show that this low-rank representation can be kernelized, has a closed-form solution, allows for separation of independent manifolds, and is robust to noise.","_input_hash":419765769,"_task_hash":-896925370,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"show","start":3,"end":7,"id":1},{"text":"that","start":8,"end":12,"id":2},{"text":"this","start":13,"end":17,"id":3},{"text":"low","start":18,"end":21,"id":4},{"text":"-","start":21,"end":22,"id":5},{"text":"rank","start":22,"end":26,"id":6},{"text":"representation","start":27,"end":41,"id":7},{"text":"can","start":42,"end":45,"id":8},{"text":"be","start":46,"end":48,"id":9},{"text":"kernelized","start":49,"end":59,"id":10},{"text":",","start":59,"end":60,"id":11},{"text":"has","start":61,"end":64,"id":12},{"text":"a","start":65,"end":66,"id":13},{"text":"closed","start":67,"end":73,"id":14},{"text":"-","start":73,"end":74,"id":15},{"text":"form","start":74,"end":78,"id":16},{"text":"solution","start":79,"end":87,"id":17},{"text":",","start":87,"end":88,"id":18},{"text":"allows","start":89,"end":95,"id":19},{"text":"for","start":96,"end":99,"id":20},{"text":"separation","start":100,"end":110,"id":21},{"text":"of","start":111,"end":113,"id":22},{"text":"independent","start":114,"end":125,"id":23},{"text":"manifolds","start":126,"end":135,"id":24},{"text":",","start":135,"end":136,"id":25},{"text":"and","start":137,"end":140,"id":26},{"text":"is","start":141,"end":143,"id":27},{"text":"robust","start":144,"end":150,"id":28},{"text":"to","start":151,"end":153,"id":29},{"text":"noise","start":154,"end":159,"id":30},{"text":".","start":159,"end":160,"id":31}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"A conceptual framework for cluster analysis from the viewpoint of p-adic geometry is introduced by describing the space of all dendrograms for n datapoints and relating it to the moduli space of p-adic Riemannian spheres with punctures using a method recently applied by Murtagh (2004b).","_input_hash":-1260630185,"_task_hash":-739351078,"tokens":[{"text":"A","start":0,"end":1,"id":0},{"text":"conceptual","start":2,"end":12,"id":1},{"text":"framework","start":13,"end":22,"id":2},{"text":"for","start":23,"end":26,"id":3},{"text":"cluster","start":27,"end":34,"id":4},{"text":"analysis","start":35,"end":43,"id":5},{"text":"from","start":44,"end":48,"id":6},{"text":"the","start":49,"end":52,"id":7},{"text":"viewpoint","start":53,"end":62,"id":8},{"text":"of","start":63,"end":65,"id":9},{"text":"p","start":66,"end":67,"id":10},{"text":"-","start":67,"end":68,"id":11},{"text":"adic","start":68,"end":72,"id":12},{"text":"geometry","start":73,"end":81,"id":13},{"text":"is","start":82,"end":84,"id":14},{"text":"introduced","start":85,"end":95,"id":15},{"text":"by","start":96,"end":98,"id":16},{"text":"describing","start":99,"end":109,"id":17},{"text":"the","start":110,"end":113,"id":18},{"text":"space","start":114,"end":119,"id":19},{"text":"of","start":120,"end":122,"id":20},{"text":"all","start":123,"end":126,"id":21},{"text":"dendrograms","start":127,"end":138,"id":22},{"text":"for","start":139,"end":142,"id":23},{"text":"n","start":143,"end":144,"id":24},{"text":"datapoints","start":145,"end":155,"id":25},{"text":"and","start":156,"end":159,"id":26},{"text":"relating","start":160,"end":168,"id":27},{"text":"it","start":169,"end":171,"id":28},{"text":"to","start":172,"end":174,"id":29},{"text":"the","start":175,"end":178,"id":30},{"text":"moduli","start":179,"end":185,"id":31},{"text":"space","start":186,"end":191,"id":32},{"text":"of","start":192,"end":194,"id":33},{"text":"p","start":195,"end":196,"id":34},{"text":"-","start":196,"end":197,"id":35},{"text":"adic","start":197,"end":201,"id":36},{"text":"Riemannian","start":202,"end":212,"id":37},{"text":"spheres","start":213,"end":220,"id":38},{"text":"with","start":221,"end":225,"id":39},{"text":"punctures","start":226,"end":235,"id":40},{"text":"using","start":236,"end":241,"id":41},{"text":"a","start":242,"end":243,"id":42},{"text":"method","start":244,"end":250,"id":43},{"text":"recently","start":251,"end":259,"id":44},{"text":"applied","start":260,"end":267,"id":45},{"text":"by","start":268,"end":270,"id":46},{"text":"Murtagh","start":271,"end":278,"id":47},{"text":"(","start":279,"end":280,"id":48},{"text":"2004b","start":280,"end":285,"id":49},{"text":")","start":285,"end":286,"id":50},{"text":".","start":286,"end":287,"id":51}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Unsupervised and semi-supervised experiments on synthetic and real data sets demonstrate the superiority of our method.","_input_hash":-273437635,"_task_hash":-290203467,"tokens":[{"text":"Unsupervised","start":0,"end":12,"id":0},{"text":"and","start":13,"end":16,"id":1},{"text":"semi","start":17,"end":21,"id":2},{"text":"-","start":21,"end":22,"id":3},{"text":"supervised","start":22,"end":32,"id":4},{"text":"experiments","start":33,"end":44,"id":5},{"text":"on","start":45,"end":47,"id":6},{"text":"synthetic","start":48,"end":57,"id":7},{"text":"and","start":58,"end":61,"id":8},{"text":"real","start":62,"end":66,"id":9},{"text":"data","start":67,"end":71,"id":10},{"text":"sets","start":72,"end":76,"id":11},{"text":"demonstrate","start":77,"end":88,"id":12},{"text":"the","start":89,"end":92,"id":13},{"text":"superiority","start":93,"end":104,"id":14},{"text":"of","start":105,"end":107,"id":15},{"text":"our","start":108,"end":111,"id":16},{"text":"method","start":112,"end":118,"id":17},{"text":".","start":118,"end":119,"id":18}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"In this paper, we will show that by detecting `active' components of the (potential) solution, i.e., those components having a considerable value, a framework for fast solution of the problem may be devised.","_input_hash":1565904891,"_task_hash":-529641857,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"this","start":3,"end":7,"id":1},{"text":"paper","start":8,"end":13,"id":2},{"text":",","start":13,"end":14,"id":3},{"text":"we","start":15,"end":17,"id":4},{"text":"will","start":18,"end":22,"id":5},{"text":"show","start":23,"end":27,"id":6},{"text":"that","start":28,"end":32,"id":7},{"text":"by","start":33,"end":35,"id":8},{"text":"detecting","start":36,"end":45,"id":9},{"text":"`","start":46,"end":47,"id":10},{"text":"active","start":47,"end":53,"id":11},{"text":"'","start":53,"end":54,"id":12},{"text":"components","start":55,"end":65,"id":13},{"text":"of","start":66,"end":68,"id":14},{"text":"the","start":69,"end":72,"id":15},{"text":"(","start":73,"end":74,"id":16},{"text":"potential","start":74,"end":83,"id":17},{"text":")","start":83,"end":84,"id":18},{"text":"solution","start":85,"end":93,"id":19},{"text":",","start":93,"end":94,"id":20},{"text":"i.e.","start":95,"end":99,"id":21},{"text":",","start":99,"end":100,"id":22},{"text":"those","start":101,"end":106,"id":23},{"text":"components","start":107,"end":117,"id":24},{"text":"having","start":118,"end":124,"id":25},{"text":"a","start":125,"end":126,"id":26},{"text":"considerable","start":127,"end":139,"id":27},{"text":"value","start":140,"end":145,"id":28},{"text":",","start":145,"end":146,"id":29},{"text":"a","start":147,"end":148,"id":30},{"text":"framework","start":149,"end":158,"id":31},{"text":"for","start":159,"end":162,"id":32},{"text":"fast","start":163,"end":167,"id":33},{"text":"solution","start":168,"end":176,"id":34},{"text":"of","start":177,"end":179,"id":35},{"text":"the","start":180,"end":183,"id":36},{"text":"problem","start":184,"end":191,"id":37},{"text":"may","start":192,"end":195,"id":38},{"text":"be","start":196,"end":198,"id":39},{"text":"devised","start":199,"end":206,"id":40},{"text":".","start":206,"end":207,"id":41}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Our criterion is stated in terms of a combinatorial parameter $\\VC({\\mathscr C}\\,{\\mathrm{mod}}\\,\\omega_1)$ which we call the VC dimension of $\\mathscr C$ modulo countable sets.","_input_hash":487615820,"_task_hash":2104426801,"tokens":[{"text":"Our","start":0,"end":3,"id":0},{"text":"criterion","start":4,"end":13,"id":1},{"text":"is","start":14,"end":16,"id":2},{"text":"stated","start":17,"end":23,"id":3},{"text":"in","start":24,"end":26,"id":4},{"text":"terms","start":27,"end":32,"id":5},{"text":"of","start":33,"end":35,"id":6},{"text":"a","start":36,"end":37,"id":7},{"text":"combinatorial","start":38,"end":51,"id":8},{"text":"parameter","start":52,"end":61,"id":9},{"text":"$","start":62,"end":63,"id":10},{"text":"\\VC({\\mathscr","start":63,"end":76,"id":11},{"text":"C}\\,{\\mathrm{mod}}\\,\\omega_1)$","start":77,"end":107,"id":12},{"text":"which","start":108,"end":113,"id":13},{"text":"we","start":114,"end":116,"id":14},{"text":"call","start":117,"end":121,"id":15},{"text":"the","start":122,"end":125,"id":16},{"text":"VC","start":126,"end":128,"id":17},{"text":"dimension","start":129,"end":138,"id":18},{"text":"of","start":139,"end":141,"id":19},{"text":"$","start":142,"end":143,"id":20},{"text":"\\mathscr","start":143,"end":151,"id":21},{"text":"C$","start":152,"end":154,"id":22},{"text":"modulo","start":155,"end":161,"id":23},{"text":"countable","start":162,"end":171,"id":24},{"text":"sets","start":172,"end":176,"id":25},{"text":".","start":176,"end":177,"id":26}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":77,"end":107,"token_start":12,"token_end":12,"label":"ALGO","answer":"reject"}],"answer":"reject"}
{"text":"We believe that our framework extends the applicability of Graphical Lasso to large-scale modern applications like bioinformatics, collaborative filtering and social networks, among others.","_input_hash":-578415509,"_task_hash":-1204025661,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"believe","start":3,"end":10,"id":1},{"text":"that","start":11,"end":15,"id":2},{"text":"our","start":16,"end":19,"id":3},{"text":"framework","start":20,"end":29,"id":4},{"text":"extends","start":30,"end":37,"id":5},{"text":"the","start":38,"end":41,"id":6},{"text":"applicability","start":42,"end":55,"id":7},{"text":"of","start":56,"end":58,"id":8},{"text":"Graphical","start":59,"end":68,"id":9},{"text":"Lasso","start":69,"end":74,"id":10},{"text":"to","start":75,"end":77,"id":11},{"text":"large","start":78,"end":83,"id":12},{"text":"-","start":83,"end":84,"id":13},{"text":"scale","start":84,"end":89,"id":14},{"text":"modern","start":90,"end":96,"id":15},{"text":"applications","start":97,"end":109,"id":16},{"text":"like","start":110,"end":114,"id":17},{"text":"bioinformatics","start":115,"end":129,"id":18},{"text":",","start":129,"end":130,"id":19},{"text":"collaborative","start":131,"end":144,"id":20},{"text":"filtering","start":145,"end":154,"id":21},{"text":"and","start":155,"end":158,"id":22},{"text":"social","start":159,"end":165,"id":23},{"text":"networks","start":166,"end":174,"id":24},{"text":",","start":174,"end":175,"id":25},{"text":"among","start":176,"end":181,"id":26},{"text":"others","start":182,"end":188,"id":27},{"text":".","start":188,"end":189,"id":28}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":59,"end":74,"token_start":9,"token_end":10,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We derive bounds on the effectiveness of a poisoning attack against centroid anomaly under different conditions:","_input_hash":-254283982,"_task_hash":-1123992711,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"derive","start":3,"end":9,"id":1},{"text":"bounds","start":10,"end":16,"id":2},{"text":"on","start":17,"end":19,"id":3},{"text":"the","start":20,"end":23,"id":4},{"text":"effectiveness","start":24,"end":37,"id":5},{"text":"of","start":38,"end":40,"id":6},{"text":"a","start":41,"end":42,"id":7},{"text":"poisoning","start":43,"end":52,"id":8},{"text":"attack","start":53,"end":59,"id":9},{"text":"against","start":60,"end":67,"id":10},{"text":"centroid","start":68,"end":76,"id":11},{"text":"anomaly","start":77,"end":84,"id":12},{"text":"under","start":85,"end":90,"id":13},{"text":"different","start":91,"end":100,"id":14},{"text":"conditions","start":101,"end":111,"id":15},{"text":":","start":111,"end":112,"id":16}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"This problem is relevant in machine learning, statistics and signal processing.","_input_hash":-381626183,"_task_hash":-1360300335,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"problem","start":5,"end":12,"id":1},{"text":"is","start":13,"end":15,"id":2},{"text":"relevant","start":16,"end":24,"id":3},{"text":"in","start":25,"end":27,"id":4},{"text":"machine","start":28,"end":35,"id":5},{"text":"learning","start":36,"end":44,"id":6},{"text":",","start":44,"end":45,"id":7},{"text":"statistics","start":46,"end":56,"id":8},{"text":"and","start":57,"end":60,"id":9},{"text":"signal","start":61,"end":67,"id":10},{"text":"processing","start":68,"end":78,"id":11},{"text":".","start":78,"end":79,"id":12}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"These are defined as the internal states of a system that lead to the same decision, based on a user-provided utility or pay-off function.","_input_hash":-64737787,"_task_hash":-325842168,"tokens":[{"text":"These","start":0,"end":5,"id":0},{"text":"are","start":6,"end":9,"id":1},{"text":"defined","start":10,"end":17,"id":2},{"text":"as","start":18,"end":20,"id":3},{"text":"the","start":21,"end":24,"id":4},{"text":"internal","start":25,"end":33,"id":5},{"text":"states","start":34,"end":40,"id":6},{"text":"of","start":41,"end":43,"id":7},{"text":"a","start":44,"end":45,"id":8},{"text":"system","start":46,"end":52,"id":9},{"text":"that","start":53,"end":57,"id":10},{"text":"lead","start":58,"end":62,"id":11},{"text":"to","start":63,"end":65,"id":12},{"text":"the","start":66,"end":69,"id":13},{"text":"same","start":70,"end":74,"id":14},{"text":"decision","start":75,"end":83,"id":15},{"text":",","start":83,"end":84,"id":16},{"text":"based","start":85,"end":90,"id":17},{"text":"on","start":91,"end":93,"id":18},{"text":"a","start":94,"end":95,"id":19},{"text":"user","start":96,"end":100,"id":20},{"text":"-","start":100,"end":101,"id":21},{"text":"provided","start":101,"end":109,"id":22},{"text":"utility","start":110,"end":117,"id":23},{"text":"or","start":118,"end":120,"id":24},{"text":"pay","start":121,"end":124,"id":25},{"text":"-","start":124,"end":125,"id":26},{"text":"off","start":125,"end":128,"id":27},{"text":"function","start":129,"end":137,"id":28},{"text":".","start":137,"end":138,"id":29}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"parsimonious models are obtained, which are very suitable for interpretation.","_input_hash":1352466466,"_task_hash":-1367422401,"tokens":[{"text":"parsimonious","start":0,"end":12,"id":0},{"text":"models","start":13,"end":19,"id":1},{"text":"are","start":20,"end":23,"id":2},{"text":"obtained","start":24,"end":32,"id":3},{"text":",","start":32,"end":33,"id":4},{"text":"which","start":34,"end":39,"id":5},{"text":"are","start":40,"end":43,"id":6},{"text":"very","start":44,"end":48,"id":7},{"text":"suitable","start":49,"end":57,"id":8},{"text":"for","start":58,"end":61,"id":9},{"text":"interpretation","start":62,"end":76,"id":10},{"text":".","start":76,"end":77,"id":11}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"In this paper we present two strategies to solve the primal LapSVM problem, in order to overcome some issues of the original dual formulation.","_input_hash":-233113015,"_task_hash":-1666527428,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"this","start":3,"end":7,"id":1},{"text":"paper","start":8,"end":13,"id":2},{"text":"we","start":14,"end":16,"id":3},{"text":"present","start":17,"end":24,"id":4},{"text":"two","start":25,"end":28,"id":5},{"text":"strategies","start":29,"end":39,"id":6},{"text":"to","start":40,"end":42,"id":7},{"text":"solve","start":43,"end":48,"id":8},{"text":"the","start":49,"end":52,"id":9},{"text":"primal","start":53,"end":59,"id":10},{"text":"LapSVM","start":60,"end":66,"id":11},{"text":"problem","start":67,"end":74,"id":12},{"text":",","start":74,"end":75,"id":13},{"text":"in","start":76,"end":78,"id":14},{"text":"order","start":79,"end":84,"id":15},{"text":"to","start":85,"end":87,"id":16},{"text":"overcome","start":88,"end":96,"id":17},{"text":"some","start":97,"end":101,"id":18},{"text":"issues","start":102,"end":108,"id":19},{"text":"of","start":109,"end":111,"id":20},{"text":"the","start":112,"end":115,"id":21},{"text":"original","start":116,"end":124,"id":22},{"text":"dual","start":125,"end":129,"id":23},{"text":"formulation","start":130,"end":141,"id":24},{"text":".","start":141,"end":142,"id":25}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":60,"end":66,"token_start":11,"token_end":11,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We then consider four specific covariance models, including a full-rank unconstrained model.","_input_hash":-2040857313,"_task_hash":-1757697241,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"then","start":3,"end":7,"id":1},{"text":"consider","start":8,"end":16,"id":2},{"text":"four","start":17,"end":21,"id":3},{"text":"specific","start":22,"end":30,"id":4},{"text":"covariance","start":31,"end":41,"id":5},{"text":"models","start":42,"end":48,"id":6},{"text":",","start":48,"end":49,"id":7},{"text":"including","start":50,"end":59,"id":8},{"text":"a","start":60,"end":61,"id":9},{"text":"full","start":62,"end":66,"id":10},{"text":"-","start":66,"end":67,"id":11},{"text":"rank","start":67,"end":71,"id":12},{"text":"unconstrained","start":72,"end":85,"id":13},{"text":"model","start":86,"end":91,"id":14},{"text":".","start":91,"end":92,"id":15}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":31,"end":41,"token_start":5,"token_end":5,"label":"ALGO","answer":"accept"},{"start":62,"end":85,"token_start":10,"token_end":13,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"This paper examines how the Lasso performs on a non-standard model that is motivated by medical imaging applications.","_input_hash":1109115263,"_task_hash":490492205,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"paper","start":5,"end":10,"id":1},{"text":"examines","start":11,"end":19,"id":2},{"text":"how","start":20,"end":23,"id":3},{"text":"the","start":24,"end":27,"id":4},{"text":"Lasso","start":28,"end":33,"id":5},{"text":"performs","start":34,"end":42,"id":6},{"text":"on","start":43,"end":45,"id":7},{"text":"a","start":46,"end":47,"id":8},{"text":"non","start":48,"end":51,"id":9},{"text":"-","start":51,"end":52,"id":10},{"text":"standard","start":52,"end":60,"id":11},{"text":"model","start":61,"end":66,"id":12},{"text":"that","start":67,"end":71,"id":13},{"text":"is","start":72,"end":74,"id":14},{"text":"motivated","start":75,"end":84,"id":15},{"text":"by","start":85,"end":87,"id":16},{"text":"medical","start":88,"end":95,"id":17},{"text":"imaging","start":96,"end":103,"id":18},{"text":"applications","start":104,"end":116,"id":19},{"text":".","start":116,"end":117,"id":20}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":28,"end":33,"token_start":5,"token_end":5,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Correlation screening suffers from a phase transition phenomenon:","_input_hash":1083128319,"_task_hash":630366386,"tokens":[{"text":"Correlation","start":0,"end":11,"id":0},{"text":"screening","start":12,"end":21,"id":1},{"text":"suffers","start":22,"end":29,"id":2},{"text":"from","start":30,"end":34,"id":3},{"text":"a","start":35,"end":36,"id":4},{"text":"phase","start":37,"end":42,"id":5},{"text":"transition","start":43,"end":53,"id":6},{"text":"phenomenon","start":54,"end":64,"id":7},{"text":":","start":64,"end":65,"id":8}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Taken as a whole, these results suggest that the Lasso is robust to the Poisson-like heteroscedastic noise.","_input_hash":1799510448,"_task_hash":997256920,"tokens":[{"text":"Taken","start":0,"end":5,"id":0},{"text":"as","start":6,"end":8,"id":1},{"text":"a","start":9,"end":10,"id":2},{"text":"whole","start":11,"end":16,"id":3},{"text":",","start":16,"end":17,"id":4},{"text":"these","start":18,"end":23,"id":5},{"text":"results","start":24,"end":31,"id":6},{"text":"suggest","start":32,"end":39,"id":7},{"text":"that","start":40,"end":44,"id":8},{"text":"the","start":45,"end":48,"id":9},{"text":"Lasso","start":49,"end":54,"id":10},{"text":"is","start":55,"end":57,"id":11},{"text":"robust","start":58,"end":64,"id":12},{"text":"to","start":65,"end":67,"id":13},{"text":"the","start":68,"end":71,"id":14},{"text":"Poisson","start":72,"end":79,"id":15},{"text":"-","start":79,"end":80,"id":16},{"text":"like","start":80,"end":84,"id":17},{"text":"heteroscedastic","start":85,"end":100,"id":18},{"text":"noise","start":101,"end":106,"id":19},{"text":".","start":106,"end":107,"id":20}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":49,"end":54,"token_start":10,"token_end":10,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"This model, often used in the field of statistics, is a simplified model that provides a laboratory for studying complex procedures.","_input_hash":-1960783815,"_task_hash":306878197,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"model","start":5,"end":10,"id":1},{"text":",","start":10,"end":11,"id":2},{"text":"often","start":12,"end":17,"id":3},{"text":"used","start":18,"end":22,"id":4},{"text":"in","start":23,"end":25,"id":5},{"text":"the","start":26,"end":29,"id":6},{"text":"field","start":30,"end":35,"id":7},{"text":"of","start":36,"end":38,"id":8},{"text":"statistics","start":39,"end":49,"id":9},{"text":",","start":49,"end":50,"id":10},{"text":"is","start":51,"end":53,"id":11},{"text":"a","start":54,"end":55,"id":12},{"text":"simplified","start":56,"end":66,"id":13},{"text":"model","start":67,"end":72,"id":14},{"text":"that","start":73,"end":77,"id":15},{"text":"provides","start":78,"end":86,"id":16},{"text":"a","start":87,"end":88,"id":17},{"text":"laboratory","start":89,"end":99,"id":18},{"text":"for","start":100,"end":103,"id":19},{"text":"studying","start":104,"end":112,"id":20},{"text":"complex","start":113,"end":120,"id":21},{"text":"procedures","start":121,"end":131,"id":22},{"text":".","start":131,"end":132,"id":23}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Variable selection for high-dimensional linear models has received a lot of attention lately, mostly in the context of l1-regularization.","_input_hash":412488295,"_task_hash":-303182233,"tokens":[{"text":"Variable","start":0,"end":8,"id":0},{"text":"selection","start":9,"end":18,"id":1},{"text":"for","start":19,"end":22,"id":2},{"text":"high","start":23,"end":27,"id":3},{"text":"-","start":27,"end":28,"id":4},{"text":"dimensional","start":28,"end":39,"id":5},{"text":"linear","start":40,"end":46,"id":6},{"text":"models","start":47,"end":53,"id":7},{"text":"has","start":54,"end":57,"id":8},{"text":"received","start":58,"end":66,"id":9},{"text":"a","start":67,"end":68,"id":10},{"text":"lot","start":69,"end":72,"id":11},{"text":"of","start":73,"end":75,"id":12},{"text":"attention","start":76,"end":85,"id":13},{"text":"lately","start":86,"end":92,"id":14},{"text":",","start":92,"end":93,"id":15},{"text":"mostly","start":94,"end":100,"id":16},{"text":"in","start":101,"end":103,"id":17},{"text":"the","start":104,"end":107,"id":18},{"text":"context","start":108,"end":115,"id":19},{"text":"of","start":116,"end":118,"id":20},{"text":"l1-regularization","start":119,"end":136,"id":21},{"text":".","start":136,"end":137,"id":22}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":23,"end":46,"token_start":3,"token_end":6,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We show experimental results with synthetic and real data, in particular, we show results in pollution prediction, school exams score prediction and gene expression data.","_input_hash":1904077874,"_task_hash":1745400184,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"show","start":3,"end":7,"id":1},{"text":"experimental","start":8,"end":20,"id":2},{"text":"results","start":21,"end":28,"id":3},{"text":"with","start":29,"end":33,"id":4},{"text":"synthetic","start":34,"end":43,"id":5},{"text":"and","start":44,"end":47,"id":6},{"text":"real","start":48,"end":52,"id":7},{"text":"data","start":53,"end":57,"id":8},{"text":",","start":57,"end":58,"id":9},{"text":"in","start":59,"end":61,"id":10},{"text":"particular","start":62,"end":72,"id":11},{"text":",","start":72,"end":73,"id":12},{"text":"we","start":74,"end":76,"id":13},{"text":"show","start":77,"end":81,"id":14},{"text":"results","start":82,"end":89,"id":15},{"text":"in","start":90,"end":92,"id":16},{"text":"pollution","start":93,"end":102,"id":17},{"text":"prediction","start":103,"end":113,"id":18},{"text":",","start":113,"end":114,"id":19},{"text":"school","start":115,"end":121,"id":20},{"text":"exams","start":122,"end":127,"id":21},{"text":"score","start":128,"end":133,"id":22},{"text":"prediction","start":134,"end":144,"id":23},{"text":"and","start":145,"end":148,"id":24},{"text":"gene","start":149,"end":153,"id":25},{"text":"expression","start":154,"end":164,"id":26},{"text":"data","start":165,"end":169,"id":27},{"text":".","start":169,"end":170,"id":28}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"This method embeds a dendrogram as a subtree into the Bruhat-Tits tree associated to the p-adic numbers, and goes back to Cornelissen et al. (","_input_hash":778461077,"_task_hash":-475077569,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"method","start":5,"end":11,"id":1},{"text":"embeds","start":12,"end":18,"id":2},{"text":"a","start":19,"end":20,"id":3},{"text":"dendrogram","start":21,"end":31,"id":4},{"text":"as","start":32,"end":34,"id":5},{"text":"a","start":35,"end":36,"id":6},{"text":"subtree","start":37,"end":44,"id":7},{"text":"into","start":45,"end":49,"id":8},{"text":"the","start":50,"end":53,"id":9},{"text":"Bruhat","start":54,"end":60,"id":10},{"text":"-","start":60,"end":61,"id":11},{"text":"Tits","start":61,"end":65,"id":12},{"text":"tree","start":66,"end":70,"id":13},{"text":"associated","start":71,"end":81,"id":14},{"text":"to","start":82,"end":84,"id":15},{"text":"the","start":85,"end":88,"id":16},{"text":"p","start":89,"end":90,"id":17},{"text":"-","start":90,"end":91,"id":18},{"text":"adic","start":91,"end":95,"id":19},{"text":"numbers","start":96,"end":103,"id":20},{"text":",","start":103,"end":104,"id":21},{"text":"and","start":105,"end":108,"id":22},{"text":"goes","start":109,"end":113,"id":23},{"text":"back","start":114,"end":118,"id":24},{"text":"to","start":119,"end":121,"id":25},{"text":"Cornelissen","start":122,"end":133,"id":26},{"text":"et","start":134,"end":136,"id":27},{"text":"al","start":137,"end":139,"id":28},{"text":".","start":139,"end":140,"id":29},{"text":"(","start":141,"end":142,"id":30}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":54,"end":70,"token_start":10,"token_end":13,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Our method finds significant $n$-grams related to a topic, which are then used to help understand and interpret the underlying distribution.","_input_hash":994805695,"_task_hash":1013407051,"tokens":[{"text":"Our","start":0,"end":3,"id":0},{"text":"method","start":4,"end":10,"id":1},{"text":"finds","start":11,"end":16,"id":2},{"text":"significant","start":17,"end":28,"id":3},{"text":"$","start":29,"end":30,"id":4},{"text":"n$-grams","start":30,"end":38,"id":5},{"text":"related","start":39,"end":46,"id":6},{"text":"to","start":47,"end":49,"id":7},{"text":"a","start":50,"end":51,"id":8},{"text":"topic","start":52,"end":57,"id":9},{"text":",","start":57,"end":58,"id":10},{"text":"which","start":59,"end":64,"id":11},{"text":"are","start":65,"end":68,"id":12},{"text":"then","start":69,"end":73,"id":13},{"text":"used","start":74,"end":78,"id":14},{"text":"to","start":79,"end":81,"id":15},{"text":"help","start":82,"end":86,"id":16},{"text":"understand","start":87,"end":97,"id":17},{"text":"and","start":98,"end":101,"id":18},{"text":"interpret","start":102,"end":111,"id":19},{"text":"the","start":112,"end":115,"id":20},{"text":"underlying","start":116,"end":126,"id":21},{"text":"distribution","start":127,"end":139,"id":22},{"text":".","start":139,"end":140,"id":23}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"This characterisation suggests new ways of ``surrogate tuning''.","_input_hash":1432929133,"_task_hash":665166806,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"characterisation","start":5,"end":21,"id":1},{"text":"suggests","start":22,"end":30,"id":2},{"text":"new","start":31,"end":34,"id":3},{"text":"ways","start":35,"end":39,"id":4},{"text":"of","start":40,"end":42,"id":5},{"text":"`","start":43,"end":44,"id":6},{"text":"`","start":44,"end":45,"id":7},{"text":"surrogate","start":45,"end":54,"id":8},{"text":"tuning","start":55,"end":61,"id":9},{"text":"'","start":61,"end":62,"id":10},{"text":"'","start":62,"end":63,"id":11},{"text":".","start":63,"end":64,"id":12}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Autoencoder neural network is implemented to estimate the missing data.","_input_hash":909308757,"_task_hash":1784407609,"tokens":[{"text":"Autoencoder","start":0,"end":11,"id":0},{"text":"neural","start":12,"end":18,"id":1},{"text":"network","start":19,"end":26,"id":2},{"text":"is","start":27,"end":29,"id":3},{"text":"implemented","start":30,"end":41,"id":4},{"text":"to","start":42,"end":44,"id":5},{"text":"estimate","start":45,"end":53,"id":6},{"text":"the","start":54,"end":57,"id":7},{"text":"missing","start":58,"end":65,"id":8},{"text":"data","start":66,"end":70,"id":9},{"text":".","start":70,"end":71,"id":10}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":0,"end":26,"token_start":0,"token_end":2,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Thus, our theoretical results overall suggest the use of elastic-net regularization in MKL.","_input_hash":1813690408,"_task_hash":1804686857,"tokens":[{"text":"Thus","start":0,"end":4,"id":0},{"text":",","start":4,"end":5,"id":1},{"text":"our","start":6,"end":9,"id":2},{"text":"theoretical","start":10,"end":21,"id":3},{"text":"results","start":22,"end":29,"id":4},{"text":"overall","start":30,"end":37,"id":5},{"text":"suggest","start":38,"end":45,"id":6},{"text":"the","start":46,"end":49,"id":7},{"text":"use","start":50,"end":53,"id":8},{"text":"of","start":54,"end":56,"id":9},{"text":"elastic","start":57,"end":64,"id":10},{"text":"-","start":64,"end":65,"id":11},{"text":"net","start":65,"end":68,"id":12},{"text":"regularization","start":69,"end":83,"id":13},{"text":"in","start":84,"end":86,"id":14},{"text":"MKL","start":87,"end":90,"id":15},{"text":".","start":90,"end":91,"id":16}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We present the discrete infinite logistic normal distribution (DILN), a Bayesian nonparametric prior for mixed membership models.","_input_hash":1720752320,"_task_hash":1345450229,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"present","start":3,"end":10,"id":1},{"text":"the","start":11,"end":14,"id":2},{"text":"discrete","start":15,"end":23,"id":3},{"text":"infinite","start":24,"end":32,"id":4},{"text":"logistic","start":33,"end":41,"id":5},{"text":"normal","start":42,"end":48,"id":6},{"text":"distribution","start":49,"end":61,"id":7},{"text":"(","start":62,"end":63,"id":8},{"text":"DILN","start":63,"end":67,"id":9},{"text":")","start":67,"end":68,"id":10},{"text":",","start":68,"end":69,"id":11},{"text":"a","start":70,"end":71,"id":12},{"text":"Bayesian","start":72,"end":80,"id":13},{"text":"nonparametric","start":81,"end":94,"id":14},{"text":"prior","start":95,"end":100,"id":15},{"text":"for","start":101,"end":104,"id":16},{"text":"mixed","start":105,"end":110,"id":17},{"text":"membership","start":111,"end":121,"id":18},{"text":"models","start":122,"end":128,"id":19},{"text":".","start":128,"end":129,"id":20}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":15,"end":61,"token_start":3,"token_end":7,"label":"ALGO","answer":"accept"},{"start":105,"end":121,"token_start":17,"token_end":18,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We provide yet another proof of the existence of calibrated forecasters;","_input_hash":-683543219,"_task_hash":762053067,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"provide","start":3,"end":10,"id":1},{"text":"yet","start":11,"end":14,"id":2},{"text":"another","start":15,"end":22,"id":3},{"text":"proof","start":23,"end":28,"id":4},{"text":"of","start":29,"end":31,"id":5},{"text":"the","start":32,"end":35,"id":6},{"text":"existence","start":36,"end":45,"id":7},{"text":"of","start":46,"end":48,"id":8},{"text":"calibrated","start":49,"end":59,"id":9},{"text":"forecasters","start":60,"end":71,"id":10},{"text":";","start":71,"end":72,"id":11}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"We propose a conceptually simple method, akin to bagging, to approach both inductive and transductive PU learning problems, by converting them into series of supervised binary classification problems discriminating the known positive examples from random subsamples of the unlabeled set.","_input_hash":574901897,"_task_hash":1544642611,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"propose","start":3,"end":10,"id":1},{"text":"a","start":11,"end":12,"id":2},{"text":"conceptually","start":13,"end":25,"id":3},{"text":"simple","start":26,"end":32,"id":4},{"text":"method","start":33,"end":39,"id":5},{"text":",","start":39,"end":40,"id":6},{"text":"akin","start":41,"end":45,"id":7},{"text":"to","start":46,"end":48,"id":8},{"text":"bagging","start":49,"end":56,"id":9},{"text":",","start":56,"end":57,"id":10},{"text":"to","start":58,"end":60,"id":11},{"text":"approach","start":61,"end":69,"id":12},{"text":"both","start":70,"end":74,"id":13},{"text":"inductive","start":75,"end":84,"id":14},{"text":"and","start":85,"end":88,"id":15},{"text":"transductive","start":89,"end":101,"id":16},{"text":"PU","start":102,"end":104,"id":17},{"text":"learning","start":105,"end":113,"id":18},{"text":"problems","start":114,"end":122,"id":19},{"text":",","start":122,"end":123,"id":20},{"text":"by","start":124,"end":126,"id":21},{"text":"converting","start":127,"end":137,"id":22},{"text":"them","start":138,"end":142,"id":23},{"text":"into","start":143,"end":147,"id":24},{"text":"series","start":148,"end":154,"id":25},{"text":"of","start":155,"end":157,"id":26},{"text":"supervised","start":158,"end":168,"id":27},{"text":"binary","start":169,"end":175,"id":28},{"text":"classification","start":176,"end":190,"id":29},{"text":"problems","start":191,"end":199,"id":30},{"text":"discriminating","start":200,"end":214,"id":31},{"text":"the","start":215,"end":218,"id":32},{"text":"known","start":219,"end":224,"id":33},{"text":"positive","start":225,"end":233,"id":34},{"text":"examples","start":234,"end":242,"id":35},{"text":"from","start":243,"end":247,"id":36},{"text":"random","start":248,"end":254,"id":37},{"text":"subsamples","start":255,"end":265,"id":38},{"text":"of","start":266,"end":268,"id":39},{"text":"the","start":269,"end":272,"id":40},{"text":"unlabeled","start":273,"end":282,"id":41},{"text":"set","start":283,"end":286,"id":42},{"text":".","start":286,"end":287,"id":43}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":158,"end":190,"token_start":27,"token_end":29,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"There has been an explosion of interest in statistical models for analyzing network data, and considerable interest in the class of exponential random graph (ERG) models, especially in connection with difficulties in computing maximum likelihood estimates.","_input_hash":1399533402,"_task_hash":1342146315,"tokens":[{"text":"There","start":0,"end":5,"id":0},{"text":"has","start":6,"end":9,"id":1},{"text":"been","start":10,"end":14,"id":2},{"text":"an","start":15,"end":17,"id":3},{"text":"explosion","start":18,"end":27,"id":4},{"text":"of","start":28,"end":30,"id":5},{"text":"interest","start":31,"end":39,"id":6},{"text":"in","start":40,"end":42,"id":7},{"text":"statistical","start":43,"end":54,"id":8},{"text":"models","start":55,"end":61,"id":9},{"text":"for","start":62,"end":65,"id":10},{"text":"analyzing","start":66,"end":75,"id":11},{"text":"network","start":76,"end":83,"id":12},{"text":"data","start":84,"end":88,"id":13},{"text":",","start":88,"end":89,"id":14},{"text":"and","start":90,"end":93,"id":15},{"text":"considerable","start":94,"end":106,"id":16},{"text":"interest","start":107,"end":115,"id":17},{"text":"in","start":116,"end":118,"id":18},{"text":"the","start":119,"end":122,"id":19},{"text":"class","start":123,"end":128,"id":20},{"text":"of","start":129,"end":131,"id":21},{"text":"exponential","start":132,"end":143,"id":22},{"text":"random","start":144,"end":150,"id":23},{"text":"graph","start":151,"end":156,"id":24},{"text":"(","start":157,"end":158,"id":25},{"text":"ERG","start":158,"end":161,"id":26},{"text":")","start":161,"end":162,"id":27},{"text":"models","start":163,"end":169,"id":28},{"text":",","start":169,"end":170,"id":29},{"text":"especially","start":171,"end":181,"id":30},{"text":"in","start":182,"end":184,"id":31},{"text":"connection","start":185,"end":195,"id":32},{"text":"with","start":196,"end":200,"id":33},{"text":"difficulties","start":201,"end":213,"id":34},{"text":"in","start":214,"end":216,"id":35},{"text":"computing","start":217,"end":226,"id":36},{"text":"maximum","start":227,"end":234,"id":37},{"text":"likelihood","start":235,"end":245,"id":38},{"text":"estimates","start":246,"end":255,"id":39},{"text":".","start":255,"end":256,"id":40}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":132,"end":156,"token_start":22,"token_end":24,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"The idea leads to a family of algorithms, called `Iterative Detection-Estimation (IDE)', which converge to the solution by successive detection and estimation of its active part.","_input_hash":415472018,"_task_hash":-1589273072,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"idea","start":4,"end":8,"id":1},{"text":"leads","start":9,"end":14,"id":2},{"text":"to","start":15,"end":17,"id":3},{"text":"a","start":18,"end":19,"id":4},{"text":"family","start":20,"end":26,"id":5},{"text":"of","start":27,"end":29,"id":6},{"text":"algorithms","start":30,"end":40,"id":7},{"text":",","start":40,"end":41,"id":8},{"text":"called","start":42,"end":48,"id":9},{"text":"`","start":49,"end":50,"id":10},{"text":"Iterative","start":50,"end":59,"id":11},{"text":"Detection","start":60,"end":69,"id":12},{"text":"-","start":69,"end":70,"id":13},{"text":"Estimation","start":70,"end":80,"id":14},{"text":"(","start":81,"end":82,"id":15},{"text":"IDE","start":82,"end":85,"id":16},{"text":")","start":85,"end":86,"id":17},{"text":"'","start":86,"end":87,"id":18},{"text":",","start":87,"end":88,"id":19},{"text":"which","start":89,"end":94,"id":20},{"text":"converge","start":95,"end":103,"id":21},{"text":"to","start":104,"end":106,"id":22},{"text":"the","start":107,"end":110,"id":23},{"text":"solution","start":111,"end":119,"id":24},{"text":"by","start":120,"end":122,"id":25},{"text":"successive","start":123,"end":133,"id":26},{"text":"detection","start":134,"end":143,"id":27},{"text":"and","start":144,"end":147,"id":28},{"text":"estimation","start":148,"end":158,"id":29},{"text":"of","start":159,"end":161,"id":30},{"text":"its","start":162,"end":165,"id":31},{"text":"active","start":166,"end":172,"id":32},{"text":"part","start":173,"end":177,"id":33},{"text":".","start":177,"end":178,"id":34}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":50,"end":80,"token_start":11,"token_end":14,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"For fast mixing graphs, we show that the cost functions introduced by Shi and Malik can be well approximated as the rate of loss of predictive information about the location of random walkers on the graph.","_input_hash":-1891153758,"_task_hash":1014870552,"tokens":[{"text":"For","start":0,"end":3,"id":0},{"text":"fast","start":4,"end":8,"id":1},{"text":"mixing","start":9,"end":15,"id":2},{"text":"graphs","start":16,"end":22,"id":3},{"text":",","start":22,"end":23,"id":4},{"text":"we","start":24,"end":26,"id":5},{"text":"show","start":27,"end":31,"id":6},{"text":"that","start":32,"end":36,"id":7},{"text":"the","start":37,"end":40,"id":8},{"text":"cost","start":41,"end":45,"id":9},{"text":"functions","start":46,"end":55,"id":10},{"text":"introduced","start":56,"end":66,"id":11},{"text":"by","start":67,"end":69,"id":12},{"text":"Shi","start":70,"end":73,"id":13},{"text":"and","start":74,"end":77,"id":14},{"text":"Malik","start":78,"end":83,"id":15},{"text":"can","start":84,"end":87,"id":16},{"text":"be","start":88,"end":90,"id":17},{"text":"well","start":91,"end":95,"id":18},{"text":"approximated","start":96,"end":108,"id":19},{"text":"as","start":109,"end":111,"id":20},{"text":"the","start":112,"end":115,"id":21},{"text":"rate","start":116,"end":120,"id":22},{"text":"of","start":121,"end":123,"id":23},{"text":"loss","start":124,"end":128,"id":24},{"text":"of","start":129,"end":131,"id":25},{"text":"predictive","start":132,"end":142,"id":26},{"text":"information","start":143,"end":154,"id":27},{"text":"about","start":155,"end":160,"id":28},{"text":"the","start":161,"end":164,"id":29},{"text":"location","start":165,"end":173,"id":30},{"text":"of","start":174,"end":176,"id":31},{"text":"random","start":177,"end":183,"id":32},{"text":"walkers","start":184,"end":191,"id":33},{"text":"on","start":192,"end":194,"id":34},{"text":"the","start":195,"end":198,"id":35},{"text":"graph","start":199,"end":204,"id":36},{"text":".","start":204,"end":205,"id":37}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"The same approach has also been used for learning features from data for other purposes, e.g., image classification, but tuning the dictionary in a supervised way for these tasks has proven to be more difficult.","_input_hash":1224610203,"_task_hash":-855153702,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"same","start":4,"end":8,"id":1},{"text":"approach","start":9,"end":17,"id":2},{"text":"has","start":18,"end":21,"id":3},{"text":"also","start":22,"end":26,"id":4},{"text":"been","start":27,"end":31,"id":5},{"text":"used","start":32,"end":36,"id":6},{"text":"for","start":37,"end":40,"id":7},{"text":"learning","start":41,"end":49,"id":8},{"text":"features","start":50,"end":58,"id":9},{"text":"from","start":59,"end":63,"id":10},{"text":"data","start":64,"end":68,"id":11},{"text":"for","start":69,"end":72,"id":12},{"text":"other","start":73,"end":78,"id":13},{"text":"purposes","start":79,"end":87,"id":14},{"text":",","start":87,"end":88,"id":15},{"text":"e.g.","start":89,"end":93,"id":16},{"text":",","start":93,"end":94,"id":17},{"text":"image","start":95,"end":100,"id":18},{"text":"classification","start":101,"end":115,"id":19},{"text":",","start":115,"end":116,"id":20},{"text":"but","start":117,"end":120,"id":21},{"text":"tuning","start":121,"end":127,"id":22},{"text":"the","start":128,"end":131,"id":23},{"text":"dictionary","start":132,"end":142,"id":24},{"text":"in","start":143,"end":145,"id":25},{"text":"a","start":146,"end":147,"id":26},{"text":"supervised","start":148,"end":158,"id":27},{"text":"way","start":159,"end":162,"id":28},{"text":"for","start":163,"end":166,"id":29},{"text":"these","start":167,"end":172,"id":30},{"text":"tasks","start":173,"end":178,"id":31},{"text":"has","start":179,"end":182,"id":32},{"text":"proven","start":183,"end":189,"id":33},{"text":"to","start":190,"end":192,"id":34},{"text":"be","start":193,"end":195,"id":35},{"text":"more","start":196,"end":200,"id":36},{"text":"difficult","start":201,"end":210,"id":37},{"text":".","start":210,"end":211,"id":38}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Due to practial limitations the model can be restricted to a class such as linear regression models, which we address in this study.","_input_hash":-1146729600,"_task_hash":779763509,"tokens":[{"text":"Due","start":0,"end":3,"id":0},{"text":"to","start":4,"end":6,"id":1},{"text":"practial","start":7,"end":15,"id":2},{"text":"limitations","start":16,"end":27,"id":3},{"text":"the","start":28,"end":31,"id":4},{"text":"model","start":32,"end":37,"id":5},{"text":"can","start":38,"end":41,"id":6},{"text":"be","start":42,"end":44,"id":7},{"text":"restricted","start":45,"end":55,"id":8},{"text":"to","start":56,"end":58,"id":9},{"text":"a","start":59,"end":60,"id":10},{"text":"class","start":61,"end":66,"id":11},{"text":"such","start":67,"end":71,"id":12},{"text":"as","start":72,"end":74,"id":13},{"text":"linear","start":75,"end":81,"id":14},{"text":"regression","start":82,"end":92,"id":15},{"text":"models","start":93,"end":99,"id":16},{"text":",","start":99,"end":100,"id":17},{"text":"which","start":101,"end":106,"id":18},{"text":"we","start":107,"end":109,"id":19},{"text":"address","start":110,"end":117,"id":20},{"text":"in","start":118,"end":120,"id":21},{"text":"this","start":121,"end":125,"id":22},{"text":"study","start":126,"end":131,"id":23},{"text":".","start":131,"end":132,"id":24}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":75,"end":92,"token_start":14,"token_end":15,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"  Simulations reveal that the Lasso performs equally well in terms of model selection performance on both Poisson-like data and homoscedastic data (with properly scaled noise variance), across a range of parameterizations.","_input_hash":-1089105131,"_task_hash":448192419,"tokens":[{"text":"  ","start":0,"end":2,"id":0},{"text":"Simulations","start":2,"end":13,"id":1},{"text":"reveal","start":14,"end":20,"id":2},{"text":"that","start":21,"end":25,"id":3},{"text":"the","start":26,"end":29,"id":4},{"text":"Lasso","start":30,"end":35,"id":5},{"text":"performs","start":36,"end":44,"id":6},{"text":"equally","start":45,"end":52,"id":7},{"text":"well","start":53,"end":57,"id":8},{"text":"in","start":58,"end":60,"id":9},{"text":"terms","start":61,"end":66,"id":10},{"text":"of","start":67,"end":69,"id":11},{"text":"model","start":70,"end":75,"id":12},{"text":"selection","start":76,"end":85,"id":13},{"text":"performance","start":86,"end":97,"id":14},{"text":"on","start":98,"end":100,"id":15},{"text":"both","start":101,"end":105,"id":16},{"text":"Poisson","start":106,"end":113,"id":17},{"text":"-","start":113,"end":114,"id":18},{"text":"like","start":114,"end":118,"id":19},{"text":"data","start":119,"end":123,"id":20},{"text":"and","start":124,"end":127,"id":21},{"text":"homoscedastic","start":128,"end":141,"id":22},{"text":"data","start":142,"end":146,"id":23},{"text":"(","start":147,"end":148,"id":24},{"text":"with","start":148,"end":152,"id":25},{"text":"properly","start":153,"end":161,"id":26},{"text":"scaled","start":162,"end":168,"id":27},{"text":"noise","start":169,"end":174,"id":28},{"text":"variance","start":175,"end":183,"id":29},{"text":")","start":183,"end":184,"id":30},{"text":",","start":184,"end":185,"id":31},{"text":"across","start":186,"end":192,"id":32},{"text":"a","start":193,"end":194,"id":33},{"text":"range","start":195,"end":200,"id":34},{"text":"of","start":201,"end":203,"id":35},{"text":"parameterizations","start":204,"end":221,"id":36},{"text":".","start":221,"end":222,"id":37}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":30,"end":35,"token_start":5,"token_end":5,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"It makes the direct definition of a multinomial distribution over permutation space impractical for all but a very small $n$. In this work we propose an embedding of all $n!$ permutations for a given $n$ in a surface of a hypersphere defined in $\\mathbbm{R}^{(n-1)^2}$. As a result of the embedding, we acquire ability to define continuous distributions over a hypersphere with all the benefits of directional statistics.","_input_hash":708902930,"_task_hash":188494324,"tokens":[{"text":"It","start":0,"end":2,"id":0},{"text":"makes","start":3,"end":8,"id":1},{"text":"the","start":9,"end":12,"id":2},{"text":"direct","start":13,"end":19,"id":3},{"text":"definition","start":20,"end":30,"id":4},{"text":"of","start":31,"end":33,"id":5},{"text":"a","start":34,"end":35,"id":6},{"text":"multinomial","start":36,"end":47,"id":7},{"text":"distribution","start":48,"end":60,"id":8},{"text":"over","start":61,"end":65,"id":9},{"text":"permutation","start":66,"end":77,"id":10},{"text":"space","start":78,"end":83,"id":11},{"text":"impractical","start":84,"end":95,"id":12},{"text":"for","start":96,"end":99,"id":13},{"text":"all","start":100,"end":103,"id":14},{"text":"but","start":104,"end":107,"id":15},{"text":"a","start":108,"end":109,"id":16},{"text":"very","start":110,"end":114,"id":17},{"text":"small","start":115,"end":120,"id":18},{"text":"$","start":121,"end":122,"id":19},{"text":"n$.","start":122,"end":125,"id":20},{"text":"In","start":126,"end":128,"id":21},{"text":"this","start":129,"end":133,"id":22},{"text":"work","start":134,"end":138,"id":23},{"text":"we","start":139,"end":141,"id":24},{"text":"propose","start":142,"end":149,"id":25},{"text":"an","start":150,"end":152,"id":26},{"text":"embedding","start":153,"end":162,"id":27},{"text":"of","start":163,"end":165,"id":28},{"text":"all","start":166,"end":169,"id":29},{"text":"$","start":170,"end":171,"id":30},{"text":"n!$","start":171,"end":174,"id":31},{"text":"permutations","start":175,"end":187,"id":32},{"text":"for","start":188,"end":191,"id":33},{"text":"a","start":192,"end":193,"id":34},{"text":"given","start":194,"end":199,"id":35},{"text":"$","start":200,"end":201,"id":36},{"text":"n$","start":201,"end":203,"id":37},{"text":"in","start":204,"end":206,"id":38},{"text":"a","start":207,"end":208,"id":39},{"text":"surface","start":209,"end":216,"id":40},{"text":"of","start":217,"end":219,"id":41},{"text":"a","start":220,"end":221,"id":42},{"text":"hypersphere","start":222,"end":233,"id":43},{"text":"defined","start":234,"end":241,"id":44},{"text":"in","start":242,"end":244,"id":45},{"text":"$","start":245,"end":246,"id":46},{"text":"\\mathbbm{R}^{(n-1)^2}$.","start":246,"end":269,"id":47},{"text":"As","start":270,"end":272,"id":48},{"text":"a","start":273,"end":274,"id":49},{"text":"result","start":275,"end":281,"id":50},{"text":"of","start":282,"end":284,"id":51},{"text":"the","start":285,"end":288,"id":52},{"text":"embedding","start":289,"end":298,"id":53},{"text":",","start":298,"end":299,"id":54},{"text":"we","start":300,"end":302,"id":55},{"text":"acquire","start":303,"end":310,"id":56},{"text":"ability","start":311,"end":318,"id":57},{"text":"to","start":319,"end":321,"id":58},{"text":"define","start":322,"end":328,"id":59},{"text":"continuous","start":329,"end":339,"id":60},{"text":"distributions","start":340,"end":353,"id":61},{"text":"over","start":354,"end":358,"id":62},{"text":"a","start":359,"end":360,"id":63},{"text":"hypersphere","start":361,"end":372,"id":64},{"text":"with","start":373,"end":377,"id":65},{"text":"all","start":378,"end":381,"id":66},{"text":"the","start":382,"end":385,"id":67},{"text":"benefits","start":386,"end":394,"id":68},{"text":"of","start":395,"end":397,"id":69},{"text":"directional","start":398,"end":409,"id":70},{"text":"statistics","start":410,"end":420,"id":71},{"text":".","start":420,"end":421,"id":72}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"This modification also removes the need for regularization when the number of neighbors is larger than the dimension of the input.","_input_hash":-46676230,"_task_hash":-509110954,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"modification","start":5,"end":17,"id":1},{"text":"also","start":18,"end":22,"id":2},{"text":"removes","start":23,"end":30,"id":3},{"text":"the","start":31,"end":34,"id":4},{"text":"need","start":35,"end":39,"id":5},{"text":"for","start":40,"end":43,"id":6},{"text":"regularization","start":44,"end":58,"id":7},{"text":"when","start":59,"end":63,"id":8},{"text":"the","start":64,"end":67,"id":9},{"text":"number","start":68,"end":74,"id":10},{"text":"of","start":75,"end":77,"id":11},{"text":"neighbors","start":78,"end":87,"id":12},{"text":"is","start":88,"end":90,"id":13},{"text":"larger","start":91,"end":97,"id":14},{"text":"than","start":98,"end":102,"id":15},{"text":"the","start":103,"end":106,"id":16},{"text":"dimension","start":107,"end":116,"id":17},{"text":"of","start":117,"end":119,"id":18},{"text":"the","start":120,"end":123,"id":19},{"text":"input","start":124,"end":129,"id":20},{"text":".","start":129,"end":130,"id":21}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"In this paper, we consider the problem of \"hyper-sparse aggregation\".","_input_hash":1984426396,"_task_hash":1798868814,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"this","start":3,"end":7,"id":1},{"text":"paper","start":8,"end":13,"id":2},{"text":",","start":13,"end":14,"id":3},{"text":"we","start":15,"end":17,"id":4},{"text":"consider","start":18,"end":26,"id":5},{"text":"the","start":27,"end":30,"id":6},{"text":"problem","start":31,"end":38,"id":7},{"text":"of","start":39,"end":41,"id":8},{"text":"\"","start":42,"end":43,"id":9},{"text":"hyper","start":43,"end":48,"id":10},{"text":"-","start":48,"end":49,"id":11},{"text":"sparse","start":49,"end":55,"id":12},{"text":"aggregation","start":56,"end":67,"id":13},{"text":"\"","start":67,"end":68,"id":14},{"text":".","start":68,"end":69,"id":15}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We propose and analyze an optimal approach for allocations, if continuous-valued allocations are allowed.","_input_hash":-30090669,"_task_hash":-2055234131,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"propose","start":3,"end":10,"id":1},{"text":"and","start":11,"end":14,"id":2},{"text":"analyze","start":15,"end":22,"id":3},{"text":"an","start":23,"end":25,"id":4},{"text":"optimal","start":26,"end":33,"id":5},{"text":"approach","start":34,"end":42,"id":6},{"text":"for","start":43,"end":46,"id":7},{"text":"allocations","start":47,"end":58,"id":8},{"text":",","start":58,"end":59,"id":9},{"text":"if","start":60,"end":62,"id":10},{"text":"continuous","start":63,"end":73,"id":11},{"text":"-","start":73,"end":74,"id":12},{"text":"valued","start":74,"end":80,"id":13},{"text":"allocations","start":81,"end":92,"id":14},{"text":"are","start":93,"end":96,"id":15},{"text":"allowed","start":97,"end":104,"id":16},{"text":".","start":104,"end":105,"id":17}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"Being among the easiest ways to find meaningful structure from discrete data, Latent Dirichlet Allocation (LDA) and related component models have been applied widely.","_input_hash":-96609974,"_task_hash":348892886,"tokens":[{"text":"Being","start":0,"end":5,"id":0},{"text":"among","start":6,"end":11,"id":1},{"text":"the","start":12,"end":15,"id":2},{"text":"easiest","start":16,"end":23,"id":3},{"text":"ways","start":24,"end":28,"id":4},{"text":"to","start":29,"end":31,"id":5},{"text":"find","start":32,"end":36,"id":6},{"text":"meaningful","start":37,"end":47,"id":7},{"text":"structure","start":48,"end":57,"id":8},{"text":"from","start":58,"end":62,"id":9},{"text":"discrete","start":63,"end":71,"id":10},{"text":"data","start":72,"end":76,"id":11},{"text":",","start":76,"end":77,"id":12},{"text":"Latent","start":78,"end":84,"id":13},{"text":"Dirichlet","start":85,"end":94,"id":14},{"text":"Allocation","start":95,"end":105,"id":15},{"text":"(","start":106,"end":107,"id":16},{"text":"LDA","start":107,"end":110,"id":17},{"text":")","start":110,"end":111,"id":18},{"text":"and","start":112,"end":115,"id":19},{"text":"related","start":116,"end":123,"id":20},{"text":"component","start":124,"end":133,"id":21},{"text":"models","start":134,"end":140,"id":22},{"text":"have","start":141,"end":145,"id":23},{"text":"been","start":146,"end":150,"id":24},{"text":"applied","start":151,"end":158,"id":25},{"text":"widely","start":159,"end":165,"id":26},{"text":".","start":165,"end":166,"id":27}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":78,"end":105,"token_start":13,"token_end":15,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Online detection of changepoints is useful in modelling and prediction of time series in application areas such as finance, biometrics, and robotics.","_input_hash":-2147316778,"_task_hash":392928165,"tokens":[{"text":"Online","start":0,"end":6,"id":0},{"text":"detection","start":7,"end":16,"id":1},{"text":"of","start":17,"end":19,"id":2},{"text":"changepoints","start":20,"end":32,"id":3},{"text":"is","start":33,"end":35,"id":4},{"text":"useful","start":36,"end":42,"id":5},{"text":"in","start":43,"end":45,"id":6},{"text":"modelling","start":46,"end":55,"id":7},{"text":"and","start":56,"end":59,"id":8},{"text":"prediction","start":60,"end":70,"id":9},{"text":"of","start":71,"end":73,"id":10},{"text":"time","start":74,"end":78,"id":11},{"text":"series","start":79,"end":85,"id":12},{"text":"in","start":86,"end":88,"id":13},{"text":"application","start":89,"end":100,"id":14},{"text":"areas","start":101,"end":106,"id":15},{"text":"such","start":107,"end":111,"id":16},{"text":"as","start":112,"end":114,"id":17},{"text":"finance","start":115,"end":122,"id":18},{"text":",","start":122,"end":123,"id":19},{"text":"biometrics","start":124,"end":134,"id":20},{"text":",","start":134,"end":135,"id":21},{"text":"and","start":136,"end":139,"id":22},{"text":"robotics","start":140,"end":148,"id":23},{"text":".","start":148,"end":149,"id":24}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We focus on threshold-based correlation screening methods for three related applications:","_input_hash":1308241370,"_task_hash":-1411180185,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"focus","start":3,"end":8,"id":1},{"text":"on","start":9,"end":11,"id":2},{"text":"threshold","start":12,"end":21,"id":3},{"text":"-","start":21,"end":22,"id":4},{"text":"based","start":22,"end":27,"id":5},{"text":"correlation","start":28,"end":39,"id":6},{"text":"screening","start":40,"end":49,"id":7},{"text":"methods","start":50,"end":57,"id":8},{"text":"for","start":58,"end":61,"id":9},{"text":"three","start":62,"end":67,"id":10},{"text":"related","start":68,"end":75,"id":11},{"text":"applications","start":76,"end":88,"id":12},{"text":":","start":88,"end":89,"id":13}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":12,"end":49,"token_start":3,"token_end":7,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Inverse inference, or \"brain reading\", is a recent paradigm for analyzing functional magnetic resonance imaging (fMRI) data, based on pattern recognition and statistical learning.","_input_hash":103699495,"_task_hash":-479051937,"tokens":[{"text":"Inverse","start":0,"end":7,"id":0},{"text":"inference","start":8,"end":17,"id":1},{"text":",","start":17,"end":18,"id":2},{"text":"or","start":19,"end":21,"id":3},{"text":"\"","start":22,"end":23,"id":4},{"text":"brain","start":23,"end":28,"id":5},{"text":"reading","start":29,"end":36,"id":6},{"text":"\"","start":36,"end":37,"id":7},{"text":",","start":37,"end":38,"id":8},{"text":"is","start":39,"end":41,"id":9},{"text":"a","start":42,"end":43,"id":10},{"text":"recent","start":44,"end":50,"id":11},{"text":"paradigm","start":51,"end":59,"id":12},{"text":"for","start":60,"end":63,"id":13},{"text":"analyzing","start":64,"end":73,"id":14},{"text":"functional","start":74,"end":84,"id":15},{"text":"magnetic","start":85,"end":93,"id":16},{"text":"resonance","start":94,"end":103,"id":17},{"text":"imaging","start":104,"end":111,"id":18},{"text":"(","start":112,"end":113,"id":19},{"text":"fMRI","start":113,"end":117,"id":20},{"text":")","start":117,"end":118,"id":21},{"text":"data","start":119,"end":123,"id":22},{"text":",","start":123,"end":124,"id":23},{"text":"based","start":125,"end":130,"id":24},{"text":"on","start":131,"end":133,"id":25},{"text":"pattern","start":134,"end":141,"id":26},{"text":"recognition","start":142,"end":153,"id":27},{"text":"and","start":154,"end":157,"id":28},{"text":"statistical","start":158,"end":169,"id":29},{"text":"learning","start":170,"end":178,"id":30},{"text":".","start":178,"end":179,"id":31}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We provide Markov chain Monte Carlo algorithms for inference in these belief networks and explore the structures learned on several image data sets.","_input_hash":-495814547,"_task_hash":-94937361,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"provide","start":3,"end":10,"id":1},{"text":"Markov","start":11,"end":17,"id":2},{"text":"chain","start":18,"end":23,"id":3},{"text":"Monte","start":24,"end":29,"id":4},{"text":"Carlo","start":30,"end":35,"id":5},{"text":"algorithms","start":36,"end":46,"id":6},{"text":"for","start":47,"end":50,"id":7},{"text":"inference","start":51,"end":60,"id":8},{"text":"in","start":61,"end":63,"id":9},{"text":"these","start":64,"end":69,"id":10},{"text":"belief","start":70,"end":76,"id":11},{"text":"networks","start":77,"end":85,"id":12},{"text":"and","start":86,"end":89,"id":13},{"text":"explore","start":90,"end":97,"id":14},{"text":"the","start":98,"end":101,"id":15},{"text":"structures","start":102,"end":112,"id":16},{"text":"learned","start":113,"end":120,"id":17},{"text":"on","start":121,"end":123,"id":18},{"text":"several","start":124,"end":131,"id":19},{"text":"image","start":132,"end":137,"id":20},{"text":"data","start":138,"end":142,"id":21},{"text":"sets","start":143,"end":147,"id":22},{"text":".","start":147,"end":148,"id":23}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":11,"end":35,"token_start":2,"token_end":5,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"While it is easy to generate visually appealing images, we demonstrate that our model also yields the best performance reported to date when evaluated with respect to the cross-entropy rate, a measure tightly linked to the average log-likelihood.","_input_hash":-823499273,"_task_hash":-159021184,"tokens":[{"text":"While","start":0,"end":5,"id":0},{"text":"it","start":6,"end":8,"id":1},{"text":"is","start":9,"end":11,"id":2},{"text":"easy","start":12,"end":16,"id":3},{"text":"to","start":17,"end":19,"id":4},{"text":"generate","start":20,"end":28,"id":5},{"text":"visually","start":29,"end":37,"id":6},{"text":"appealing","start":38,"end":47,"id":7},{"text":"images","start":48,"end":54,"id":8},{"text":",","start":54,"end":55,"id":9},{"text":"we","start":56,"end":58,"id":10},{"text":"demonstrate","start":59,"end":70,"id":11},{"text":"that","start":71,"end":75,"id":12},{"text":"our","start":76,"end":79,"id":13},{"text":"model","start":80,"end":85,"id":14},{"text":"also","start":86,"end":90,"id":15},{"text":"yields","start":91,"end":97,"id":16},{"text":"the","start":98,"end":101,"id":17},{"text":"best","start":102,"end":106,"id":18},{"text":"performance","start":107,"end":118,"id":19},{"text":"reported","start":119,"end":127,"id":20},{"text":"to","start":128,"end":130,"id":21},{"text":"date","start":131,"end":135,"id":22},{"text":"when","start":136,"end":140,"id":23},{"text":"evaluated","start":141,"end":150,"id":24},{"text":"with","start":151,"end":155,"id":25},{"text":"respect","start":156,"end":163,"id":26},{"text":"to","start":164,"end":166,"id":27},{"text":"the","start":167,"end":170,"id":28},{"text":"cross","start":171,"end":176,"id":29},{"text":"-","start":176,"end":177,"id":30},{"text":"entropy","start":177,"end":184,"id":31},{"text":"rate","start":185,"end":189,"id":32},{"text":",","start":189,"end":190,"id":33},{"text":"a","start":191,"end":192,"id":34},{"text":"measure","start":193,"end":200,"id":35},{"text":"tightly","start":201,"end":208,"id":36},{"text":"linked","start":209,"end":215,"id":37},{"text":"to","start":216,"end":218,"id":38},{"text":"the","start":219,"end":222,"id":39},{"text":"average","start":223,"end":230,"id":40},{"text":"log","start":231,"end":234,"id":41},{"text":"-","start":234,"end":235,"id":42},{"text":"likelihood","start":235,"end":245,"id":43},{"text":".","start":245,"end":246,"id":44}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We propose two algorithms for approximating its solution.","_input_hash":-1643745972,"_task_hash":-1754255784,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"propose","start":3,"end":10,"id":1},{"text":"two","start":11,"end":14,"id":2},{"text":"algorithms","start":15,"end":25,"id":3},{"text":"for","start":26,"end":29,"id":4},{"text":"approximating","start":30,"end":43,"id":5},{"text":"its","start":44,"end":47,"id":6},{"text":"solution","start":48,"end":56,"id":7},{"text":".","start":56,"end":57,"id":8}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"The Johnson-Lindenstrauss Lemma allows for the projection of $n$ points in $p-$dimensional Euclidean space onto a $k-$dimensional Euclidean space, with $k \\ge \\frac{24\\ln \\emph{n}}{3\\epsilon^2-2\\epsilon^3}$, so that the pairwise distances are preserved within a factor of $1\\pm\\epsilon$. Here, working directly with the distributions of the random distances rather than resorting to the moment generating function technique, an improvement on the lower bound for $k$ is obtained.","_input_hash":-651631983,"_task_hash":-1244763824,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"Johnson","start":4,"end":11,"id":1},{"text":"-","start":11,"end":12,"id":2},{"text":"Lindenstrauss","start":12,"end":25,"id":3},{"text":"Lemma","start":26,"end":31,"id":4},{"text":"allows","start":32,"end":38,"id":5},{"text":"for","start":39,"end":42,"id":6},{"text":"the","start":43,"end":46,"id":7},{"text":"projection","start":47,"end":57,"id":8},{"text":"of","start":58,"end":60,"id":9},{"text":"$","start":61,"end":62,"id":10},{"text":"n$","start":62,"end":64,"id":11},{"text":"points","start":65,"end":71,"id":12},{"text":"in","start":72,"end":74,"id":13},{"text":"$","start":75,"end":76,"id":14},{"text":"p-$dimensional","start":76,"end":90,"id":15},{"text":"Euclidean","start":91,"end":100,"id":16},{"text":"space","start":101,"end":106,"id":17},{"text":"onto","start":107,"end":111,"id":18},{"text":"a","start":112,"end":113,"id":19},{"text":"$","start":114,"end":115,"id":20},{"text":"k-$dimensional","start":115,"end":129,"id":21},{"text":"Euclidean","start":130,"end":139,"id":22},{"text":"space","start":140,"end":145,"id":23},{"text":",","start":145,"end":146,"id":24},{"text":"with","start":147,"end":151,"id":25},{"text":"$","start":152,"end":153,"id":26},{"text":"k","start":153,"end":154,"id":27},{"text":"\\ge","start":155,"end":158,"id":28},{"text":"\\frac{24\\ln","start":159,"end":170,"id":29},{"text":"\\emph{n}}{3\\epsilon^2","start":171,"end":192,"id":30},{"text":"-","start":192,"end":193,"id":31},{"text":"2\\epsilon^3}$","start":193,"end":206,"id":32},{"text":",","start":206,"end":207,"id":33},{"text":"so","start":208,"end":210,"id":34},{"text":"that","start":211,"end":215,"id":35},{"text":"the","start":216,"end":219,"id":36},{"text":"pairwise","start":220,"end":228,"id":37},{"text":"distances","start":229,"end":238,"id":38},{"text":"are","start":239,"end":242,"id":39},{"text":"preserved","start":243,"end":252,"id":40},{"text":"within","start":253,"end":259,"id":41},{"text":"a","start":260,"end":261,"id":42},{"text":"factor","start":262,"end":268,"id":43},{"text":"of","start":269,"end":271,"id":44},{"text":"$","start":272,"end":273,"id":45},{"text":"1\\pm\\epsilon$.","start":273,"end":287,"id":46},{"text":"Here","start":288,"end":292,"id":47},{"text":",","start":292,"end":293,"id":48},{"text":"working","start":294,"end":301,"id":49},{"text":"directly","start":302,"end":310,"id":50},{"text":"with","start":311,"end":315,"id":51},{"text":"the","start":316,"end":319,"id":52},{"text":"distributions","start":320,"end":333,"id":53},{"text":"of","start":334,"end":336,"id":54},{"text":"the","start":337,"end":340,"id":55},{"text":"random","start":341,"end":347,"id":56},{"text":"distances","start":348,"end":357,"id":57},{"text":"rather","start":358,"end":364,"id":58},{"text":"than","start":365,"end":369,"id":59},{"text":"resorting","start":370,"end":379,"id":60},{"text":"to","start":380,"end":382,"id":61},{"text":"the","start":383,"end":386,"id":62},{"text":"moment","start":387,"end":393,"id":63},{"text":"generating","start":394,"end":404,"id":64},{"text":"function","start":405,"end":413,"id":65},{"text":"technique","start":414,"end":423,"id":66},{"text":",","start":423,"end":424,"id":67},{"text":"an","start":425,"end":427,"id":68},{"text":"improvement","start":428,"end":439,"id":69},{"text":"on","start":440,"end":442,"id":70},{"text":"the","start":443,"end":446,"id":71},{"text":"lower","start":447,"end":452,"id":72},{"text":"bound","start":453,"end":458,"id":73},{"text":"for","start":459,"end":462,"id":74},{"text":"$","start":463,"end":464,"id":75},{"text":"k$","start":464,"end":466,"id":76},{"text":"is","start":467,"end":469,"id":77},{"text":"obtained","start":470,"end":478,"id":78},{"text":".","start":478,"end":479,"id":79}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"The different views are assumed to be connected by having paired samples;","_input_hash":702098655,"_task_hash":1770841791,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"different","start":4,"end":13,"id":1},{"text":"views","start":14,"end":19,"id":2},{"text":"are","start":20,"end":23,"id":3},{"text":"assumed","start":24,"end":31,"id":4},{"text":"to","start":32,"end":34,"id":5},{"text":"be","start":35,"end":37,"id":6},{"text":"connected","start":38,"end":47,"id":7},{"text":"by","start":48,"end":50,"id":8},{"text":"having","start":51,"end":57,"id":9},{"text":"paired","start":58,"end":64,"id":10},{"text":"samples","start":65,"end":72,"id":11},{"text":";","start":72,"end":73,"id":12}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Density modeling is notoriously difficult for high dimensional data.","_input_hash":-410685504,"_task_hash":-167365044,"tokens":[{"text":"Density","start":0,"end":7,"id":0},{"text":"modeling","start":8,"end":16,"id":1},{"text":"is","start":17,"end":19,"id":2},{"text":"notoriously","start":20,"end":31,"id":3},{"text":"difficult","start":32,"end":41,"id":4},{"text":"for","start":42,"end":45,"id":5},{"text":"high","start":46,"end":50,"id":6},{"text":"dimensional","start":51,"end":62,"id":7},{"text":"data","start":63,"end":67,"id":8},{"text":".","start":67,"end":68,"id":9}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":0,"end":7,"token_start":0,"token_end":0,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"The idea is to select causal hypotheses for which the conditional density of every variable, given its causes, becomes smooth.","_input_hash":1327662163,"_task_hash":2094304353,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"idea","start":4,"end":8,"id":1},{"text":"is","start":9,"end":11,"id":2},{"text":"to","start":12,"end":14,"id":3},{"text":"select","start":15,"end":21,"id":4},{"text":"causal","start":22,"end":28,"id":5},{"text":"hypotheses","start":29,"end":39,"id":6},{"text":"for","start":40,"end":43,"id":7},{"text":"which","start":44,"end":49,"id":8},{"text":"the","start":50,"end":53,"id":9},{"text":"conditional","start":54,"end":65,"id":10},{"text":"density","start":66,"end":73,"id":11},{"text":"of","start":74,"end":76,"id":12},{"text":"every","start":77,"end":82,"id":13},{"text":"variable","start":83,"end":91,"id":14},{"text":",","start":91,"end":92,"id":15},{"text":"given","start":93,"end":98,"id":16},{"text":"its","start":99,"end":102,"id":17},{"text":"causes","start":103,"end":109,"id":18},{"text":",","start":109,"end":110,"id":19},{"text":"becomes","start":111,"end":118,"id":20},{"text":"smooth","start":119,"end":125,"id":21},{"text":".","start":125,"end":126,"id":22}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"Structural equation models and Bayesian networks have been widely used to analyze causal relations between continuous variables.","_input_hash":-852652282,"_task_hash":916555105,"tokens":[{"text":"Structural","start":0,"end":10,"id":0},{"text":"equation","start":11,"end":19,"id":1},{"text":"models","start":20,"end":26,"id":2},{"text":"and","start":27,"end":30,"id":3},{"text":"Bayesian","start":31,"end":39,"id":4},{"text":"networks","start":40,"end":48,"id":5},{"text":"have","start":49,"end":53,"id":6},{"text":"been","start":54,"end":58,"id":7},{"text":"widely","start":59,"end":65,"id":8},{"text":"used","start":66,"end":70,"id":9},{"text":"to","start":71,"end":73,"id":10},{"text":"analyze","start":74,"end":81,"id":11},{"text":"causal","start":82,"end":88,"id":12},{"text":"relations","start":89,"end":98,"id":13},{"text":"between","start":99,"end":106,"id":14},{"text":"continuous","start":107,"end":117,"id":15},{"text":"variables","start":118,"end":127,"id":16},{"text":".","start":127,"end":128,"id":17}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":0,"end":19,"token_start":0,"token_end":1,"label":"ALGO","answer":"accept"},{"start":31,"end":48,"token_start":4,"token_end":5,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"In that case, the kernel between x and x' is a a function of the Gram matrices produced by \\kappa on observations and subsequences of observations enumerated in x and x'.","_input_hash":-1676449526,"_task_hash":-1706410097,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"that","start":3,"end":7,"id":1},{"text":"case","start":8,"end":12,"id":2},{"text":",","start":12,"end":13,"id":3},{"text":"the","start":14,"end":17,"id":4},{"text":"kernel","start":18,"end":24,"id":5},{"text":"between","start":25,"end":32,"id":6},{"text":"x","start":33,"end":34,"id":7},{"text":"and","start":35,"end":38,"id":8},{"text":"x","start":39,"end":40,"id":9},{"text":"'","start":40,"end":41,"id":10},{"text":"is","start":42,"end":44,"id":11},{"text":"a","start":45,"end":46,"id":12},{"text":"a","start":47,"end":48,"id":13},{"text":"function","start":49,"end":57,"id":14},{"text":"of","start":58,"end":60,"id":15},{"text":"the","start":61,"end":64,"id":16},{"text":"Gram","start":65,"end":69,"id":17},{"text":"matrices","start":70,"end":78,"id":18},{"text":"produced","start":79,"end":87,"id":19},{"text":"by","start":88,"end":90,"id":20},{"text":"\\kappa","start":91,"end":97,"id":21},{"text":"on","start":98,"end":100,"id":22},{"text":"observations","start":101,"end":113,"id":23},{"text":"and","start":114,"end":117,"id":24},{"text":"subsequences","start":118,"end":130,"id":25},{"text":"of","start":131,"end":133,"id":26},{"text":"observations","start":134,"end":146,"id":27},{"text":"enumerated","start":147,"end":157,"id":28},{"text":"in","start":158,"end":160,"id":29},{"text":"x","start":161,"end":162,"id":30},{"text":"and","start":163,"end":166,"id":31},{"text":"x","start":167,"end":168,"id":32},{"text":"'","start":168,"end":169,"id":33},{"text":".","start":169,"end":170,"id":34}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"bounded and unbounded percentage of traffic, and bounded false positive rate.","_input_hash":1838088007,"_task_hash":615499688,"tokens":[{"text":"bounded","start":0,"end":7,"id":0},{"text":"and","start":8,"end":11,"id":1},{"text":"unbounded","start":12,"end":21,"id":2},{"text":"percentage","start":22,"end":32,"id":3},{"text":"of","start":33,"end":35,"id":4},{"text":"traffic","start":36,"end":43,"id":5},{"text":",","start":43,"end":44,"id":6},{"text":"and","start":45,"end":48,"id":7},{"text":"bounded","start":49,"end":56,"id":8},{"text":"false","start":57,"end":62,"id":9},{"text":"positive","start":63,"end":71,"id":10},{"text":"rate","start":72,"end":76,"id":11},{"text":".","start":76,"end":77,"id":12}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"In this paper, we propose a method to find exogenous variables in a linear non-Gaussian causal model, which requires much smaller sample sizes than conventional methods and works even when p>>n.","_input_hash":-335248842,"_task_hash":-735785139,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"this","start":3,"end":7,"id":1},{"text":"paper","start":8,"end":13,"id":2},{"text":",","start":13,"end":14,"id":3},{"text":"we","start":15,"end":17,"id":4},{"text":"propose","start":18,"end":25,"id":5},{"text":"a","start":26,"end":27,"id":6},{"text":"method","start":28,"end":34,"id":7},{"text":"to","start":35,"end":37,"id":8},{"text":"find","start":38,"end":42,"id":9},{"text":"exogenous","start":43,"end":52,"id":10},{"text":"variables","start":53,"end":62,"id":11},{"text":"in","start":63,"end":65,"id":12},{"text":"a","start":66,"end":67,"id":13},{"text":"linear","start":68,"end":74,"id":14},{"text":"non","start":75,"end":78,"id":15},{"text":"-","start":78,"end":79,"id":16},{"text":"Gaussian","start":79,"end":87,"id":17},{"text":"causal","start":88,"end":94,"id":18},{"text":"model","start":95,"end":100,"id":19},{"text":",","start":100,"end":101,"id":20},{"text":"which","start":102,"end":107,"id":21},{"text":"requires","start":108,"end":116,"id":22},{"text":"much","start":117,"end":121,"id":23},{"text":"smaller","start":122,"end":129,"id":24},{"text":"sample","start":130,"end":136,"id":25},{"text":"sizes","start":137,"end":142,"id":26},{"text":"than","start":143,"end":147,"id":27},{"text":"conventional","start":148,"end":160,"id":28},{"text":"methods","start":161,"end":168,"id":29},{"text":"and","start":169,"end":172,"id":30},{"text":"works","start":173,"end":178,"id":31},{"text":"even","start":179,"end":183,"id":32},{"text":"when","start":184,"end":188,"id":33},{"text":"p>>n","start":189,"end":193,"id":34},{"text":".","start":193,"end":194,"id":35}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":68,"end":94,"token_start":14,"token_end":18,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Due to space limitations, our submission \"Source Separation and Clustering of Phase-Locked Subspaces\", accepted for publication on the IEEE Transactions on Neural Networks in 2011, presented some results without proof.","_input_hash":-575539518,"_task_hash":16201345,"tokens":[{"text":"Due","start":0,"end":3,"id":0},{"text":"to","start":4,"end":6,"id":1},{"text":"space","start":7,"end":12,"id":2},{"text":"limitations","start":13,"end":24,"id":3},{"text":",","start":24,"end":25,"id":4},{"text":"our","start":26,"end":29,"id":5},{"text":"submission","start":30,"end":40,"id":6},{"text":"\"","start":41,"end":42,"id":7},{"text":"Source","start":42,"end":48,"id":8},{"text":"Separation","start":49,"end":59,"id":9},{"text":"and","start":60,"end":63,"id":10},{"text":"Clustering","start":64,"end":74,"id":11},{"text":"of","start":75,"end":77,"id":12},{"text":"Phase","start":78,"end":83,"id":13},{"text":"-","start":83,"end":84,"id":14},{"text":"Locked","start":84,"end":90,"id":15},{"text":"Subspaces","start":91,"end":100,"id":16},{"text":"\"","start":100,"end":101,"id":17},{"text":",","start":101,"end":102,"id":18},{"text":"accepted","start":103,"end":111,"id":19},{"text":"for","start":112,"end":115,"id":20},{"text":"publication","start":116,"end":127,"id":21},{"text":"on","start":128,"end":130,"id":22},{"text":"the","start":131,"end":134,"id":23},{"text":"IEEE","start":135,"end":139,"id":24},{"text":"Transactions","start":140,"end":152,"id":25},{"text":"on","start":153,"end":155,"id":26},{"text":"Neural","start":156,"end":162,"id":27},{"text":"Networks","start":163,"end":171,"id":28},{"text":"in","start":172,"end":174,"id":29},{"text":"2011","start":175,"end":179,"id":30},{"text":",","start":179,"end":180,"id":31},{"text":"presented","start":181,"end":190,"id":32},{"text":"some","start":191,"end":195,"id":33},{"text":"results","start":196,"end":203,"id":34},{"text":"without","start":204,"end":211,"id":35},{"text":"proof","start":212,"end":217,"id":36},{"text":".","start":217,"end":218,"id":37}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":156,"end":171,"token_start":27,"token_end":28,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"movie ratings predicted from reviews, and the political tone of amendments in the U.S. Senate based on the amendment text.","_input_hash":-1436499935,"_task_hash":691054206,"tokens":[{"text":"movie","start":0,"end":5,"id":0},{"text":"ratings","start":6,"end":13,"id":1},{"text":"predicted","start":14,"end":23,"id":2},{"text":"from","start":24,"end":28,"id":3},{"text":"reviews","start":29,"end":36,"id":4},{"text":",","start":36,"end":37,"id":5},{"text":"and","start":38,"end":41,"id":6},{"text":"the","start":42,"end":45,"id":7},{"text":"political","start":46,"end":55,"id":8},{"text":"tone","start":56,"end":60,"id":9},{"text":"of","start":61,"end":63,"id":10},{"text":"amendments","start":64,"end":74,"id":11},{"text":"in","start":75,"end":77,"id":12},{"text":"the","start":78,"end":81,"id":13},{"text":"U.S.","start":82,"end":86,"id":14},{"text":"Senate","start":87,"end":93,"id":15},{"text":"based","start":94,"end":99,"id":16},{"text":"on","start":100,"end":102,"id":17},{"text":"the","start":103,"end":106,"id":18},{"text":"amendment","start":107,"end":116,"id":19},{"text":"text","start":117,"end":121,"id":20},{"text":".","start":121,"end":122,"id":21}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"This idea is closely related to total variation denoising.","_input_hash":-271450721,"_task_hash":235859992,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"idea","start":5,"end":9,"id":1},{"text":"is","start":10,"end":12,"id":2},{"text":"closely","start":13,"end":20,"id":3},{"text":"related","start":21,"end":28,"id":4},{"text":"to","start":29,"end":31,"id":5},{"text":"total","start":32,"end":37,"id":6},{"text":"variation","start":38,"end":47,"id":7},{"text":"denoising","start":48,"end":57,"id":8},{"text":".","start":57,"end":58,"id":9}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Current approaches involve repeatedly solving a batch learning problem, which makes them inadequate for large scale scenarios.","_input_hash":-711734251,"_task_hash":93516184,"tokens":[{"text":"Current","start":0,"end":7,"id":0},{"text":"approaches","start":8,"end":18,"id":1},{"text":"involve","start":19,"end":26,"id":2},{"text":"repeatedly","start":27,"end":37,"id":3},{"text":"solving","start":38,"end":45,"id":4},{"text":"a","start":46,"end":47,"id":5},{"text":"batch","start":48,"end":53,"id":6},{"text":"learning","start":54,"end":62,"id":7},{"text":"problem","start":63,"end":70,"id":8},{"text":",","start":70,"end":71,"id":9},{"text":"which","start":72,"end":77,"id":10},{"text":"makes","start":78,"end":83,"id":11},{"text":"them","start":84,"end":88,"id":12},{"text":"inadequate","start":89,"end":99,"id":13},{"text":"for","start":100,"end":103,"id":14},{"text":"large","start":104,"end":109,"id":15},{"text":"scale","start":110,"end":115,"id":16},{"text":"scenarios","start":116,"end":125,"id":17},{"text":".","start":125,"end":126,"id":18}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"  In contrast to the previous methods, our algorithm requires no algorithmic parameters and is guaranteed to converge to the right solution within a small fixed number of steps if the data strictly follows the model.","_input_hash":1051659357,"_task_hash":46469974,"tokens":[{"text":"  ","start":0,"end":2,"id":0},{"text":"In","start":2,"end":4,"id":1},{"text":"contrast","start":5,"end":13,"id":2},{"text":"to","start":14,"end":16,"id":3},{"text":"the","start":17,"end":20,"id":4},{"text":"previous","start":21,"end":29,"id":5},{"text":"methods","start":30,"end":37,"id":6},{"text":",","start":37,"end":38,"id":7},{"text":"our","start":39,"end":42,"id":8},{"text":"algorithm","start":43,"end":52,"id":9},{"text":"requires","start":53,"end":61,"id":10},{"text":"no","start":62,"end":64,"id":11},{"text":"algorithmic","start":65,"end":76,"id":12},{"text":"parameters","start":77,"end":87,"id":13},{"text":"and","start":88,"end":91,"id":14},{"text":"is","start":92,"end":94,"id":15},{"text":"guaranteed","start":95,"end":105,"id":16},{"text":"to","start":106,"end":108,"id":17},{"text":"converge","start":109,"end":117,"id":18},{"text":"to","start":118,"end":120,"id":19},{"text":"the","start":121,"end":124,"id":20},{"text":"right","start":125,"end":130,"id":21},{"text":"solution","start":131,"end":139,"id":22},{"text":"within","start":140,"end":146,"id":23},{"text":"a","start":147,"end":148,"id":24},{"text":"small","start":149,"end":154,"id":25},{"text":"fixed","start":155,"end":160,"id":26},{"text":"number","start":161,"end":167,"id":27},{"text":"of","start":168,"end":170,"id":28},{"text":"steps","start":171,"end":176,"id":29},{"text":"if","start":177,"end":179,"id":30},{"text":"the","start":180,"end":183,"id":31},{"text":"data","start":184,"end":188,"id":32},{"text":"strictly","start":189,"end":197,"id":33},{"text":"follows","start":198,"end":205,"id":34},{"text":"the","start":206,"end":209,"id":35},{"text":"model","start":210,"end":215,"id":36},{"text":".","start":215,"end":216,"id":37}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"In some cases, we proceed directly to clusters and do not directly determine the distances.","_input_hash":-1311218509,"_task_hash":-1134132768,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"some","start":3,"end":7,"id":1},{"text":"cases","start":8,"end":13,"id":2},{"text":",","start":13,"end":14,"id":3},{"text":"we","start":15,"end":17,"id":4},{"text":"proceed","start":18,"end":25,"id":5},{"text":"directly","start":26,"end":34,"id":6},{"text":"to","start":35,"end":37,"id":7},{"text":"clusters","start":38,"end":46,"id":8},{"text":"and","start":47,"end":50,"id":9},{"text":"do","start":51,"end":53,"id":10},{"text":"not","start":54,"end":57,"id":11},{"text":"directly","start":58,"end":66,"id":12},{"text":"determine","start":67,"end":76,"id":13},{"text":"the","start":77,"end":80,"id":14},{"text":"distances","start":81,"end":90,"id":15},{"text":".","start":90,"end":91,"id":16}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Moreover, the computational complexity of the training algorithm is reduced from O(n^3) to O(n^2) using preconditioned conjugate gradient, where n is the combined number of labeled and unlabeled examples.","_input_hash":-83149934,"_task_hash":-578274512,"tokens":[{"text":"Moreover","start":0,"end":8,"id":0},{"text":",","start":8,"end":9,"id":1},{"text":"the","start":10,"end":13,"id":2},{"text":"computational","start":14,"end":27,"id":3},{"text":"complexity","start":28,"end":38,"id":4},{"text":"of","start":39,"end":41,"id":5},{"text":"the","start":42,"end":45,"id":6},{"text":"training","start":46,"end":54,"id":7},{"text":"algorithm","start":55,"end":64,"id":8},{"text":"is","start":65,"end":67,"id":9},{"text":"reduced","start":68,"end":75,"id":10},{"text":"from","start":76,"end":80,"id":11},{"text":"O(n^3","start":81,"end":86,"id":12},{"text":")","start":86,"end":87,"id":13},{"text":"to","start":88,"end":90,"id":14},{"text":"O(n^2","start":91,"end":96,"id":15},{"text":")","start":96,"end":97,"id":16},{"text":"using","start":98,"end":103,"id":17},{"text":"preconditioned","start":104,"end":118,"id":18},{"text":"conjugate","start":119,"end":128,"id":19},{"text":"gradient","start":129,"end":137,"id":20},{"text":",","start":137,"end":138,"id":21},{"text":"where","start":139,"end":144,"id":22},{"text":"n","start":145,"end":146,"id":23},{"text":"is","start":147,"end":149,"id":24},{"text":"the","start":150,"end":153,"id":25},{"text":"combined","start":154,"end":162,"id":26},{"text":"number","start":163,"end":169,"id":27},{"text":"of","start":170,"end":172,"id":28},{"text":"labeled","start":173,"end":180,"id":29},{"text":"and","start":181,"end":184,"id":30},{"text":"unlabeled","start":185,"end":194,"id":31},{"text":"examples","start":195,"end":203,"id":32},{"text":".","start":203,"end":204,"id":33}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Our analysis shows the set of groups must be chosen with caution - an overly complex set of groups will damage the analysis.","_input_hash":-505806003,"_task_hash":1177836083,"tokens":[{"text":"Our","start":0,"end":3,"id":0},{"text":"analysis","start":4,"end":12,"id":1},{"text":"shows","start":13,"end":18,"id":2},{"text":"the","start":19,"end":22,"id":3},{"text":"set","start":23,"end":26,"id":4},{"text":"of","start":27,"end":29,"id":5},{"text":"groups","start":30,"end":36,"id":6},{"text":"must","start":37,"end":41,"id":7},{"text":"be","start":42,"end":44,"id":8},{"text":"chosen","start":45,"end":51,"id":9},{"text":"with","start":52,"end":56,"id":10},{"text":"caution","start":57,"end":64,"id":11},{"text":"-","start":65,"end":66,"id":12},{"text":"an","start":67,"end":69,"id":13},{"text":"overly","start":70,"end":76,"id":14},{"text":"complex","start":77,"end":84,"id":15},{"text":"set","start":85,"end":88,"id":16},{"text":"of","start":89,"end":91,"id":17},{"text":"groups","start":92,"end":98,"id":18},{"text":"will","start":99,"end":103,"id":19},{"text":"damage","start":104,"end":110,"id":20},{"text":"the","start":111,"end":114,"id":21},{"text":"analysis","start":115,"end":123,"id":22},{"text":".","start":123,"end":124,"id":23}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We consider the problem of binary classification where one can, for a particular cost, choose not to classify an observation.","_input_hash":-1660335345,"_task_hash":-2061475922,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"consider","start":3,"end":11,"id":1},{"text":"the","start":12,"end":15,"id":2},{"text":"problem","start":16,"end":23,"id":3},{"text":"of","start":24,"end":26,"id":4},{"text":"binary","start":27,"end":33,"id":5},{"text":"classification","start":34,"end":48,"id":6},{"text":"where","start":49,"end":54,"id":7},{"text":"one","start":55,"end":58,"id":8},{"text":"can","start":59,"end":62,"id":9},{"text":",","start":62,"end":63,"id":10},{"text":"for","start":64,"end":67,"id":11},{"text":"a","start":68,"end":69,"id":12},{"text":"particular","start":70,"end":80,"id":13},{"text":"cost","start":81,"end":85,"id":14},{"text":",","start":85,"end":86,"id":15},{"text":"choose","start":87,"end":93,"id":16},{"text":"not","start":94,"end":97,"id":17},{"text":"to","start":98,"end":100,"id":18},{"text":"classify","start":101,"end":109,"id":19},{"text":"an","start":110,"end":112,"id":20},{"text":"observation","start":113,"end":124,"id":21},{"text":".","start":124,"end":125,"id":22}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":27,"end":48,"token_start":5,"token_end":6,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"SLIM is closest in spirit to LiNGAM (Shimizu et al.,","_input_hash":-1355154678,"_task_hash":664683579,"tokens":[{"text":"SLIM","start":0,"end":4,"id":0},{"text":"is","start":5,"end":7,"id":1},{"text":"closest","start":8,"end":15,"id":2},{"text":"in","start":16,"end":18,"id":3},{"text":"spirit","start":19,"end":25,"id":4},{"text":"to","start":26,"end":28,"id":5},{"text":"LiNGAM","start":29,"end":35,"id":6},{"text":"(","start":36,"end":37,"id":7},{"text":"Shimizu","start":37,"end":44,"id":8},{"text":"et","start":45,"end":47,"id":9},{"text":"al","start":48,"end":50,"id":10},{"text":".","start":50,"end":51,"id":11},{"text":",","start":51,"end":52,"id":12}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":29,"end":35,"token_start":6,"token_end":6,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We describe a new method for visualizing topics, the distributions over terms that are automatically extracted from large text corpora using latent variable models.","_input_hash":1236677145,"_task_hash":-987905172,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"describe","start":3,"end":11,"id":1},{"text":"a","start":12,"end":13,"id":2},{"text":"new","start":14,"end":17,"id":3},{"text":"method","start":18,"end":24,"id":4},{"text":"for","start":25,"end":28,"id":5},{"text":"visualizing","start":29,"end":40,"id":6},{"text":"topics","start":41,"end":47,"id":7},{"text":",","start":47,"end":48,"id":8},{"text":"the","start":49,"end":52,"id":9},{"text":"distributions","start":53,"end":66,"id":10},{"text":"over","start":67,"end":71,"id":11},{"text":"terms","start":72,"end":77,"id":12},{"text":"that","start":78,"end":82,"id":13},{"text":"are","start":83,"end":86,"id":14},{"text":"automatically","start":87,"end":100,"id":15},{"text":"extracted","start":101,"end":110,"id":16},{"text":"from","start":111,"end":115,"id":17},{"text":"large","start":116,"end":121,"id":18},{"text":"text","start":122,"end":126,"id":19},{"text":"corpora","start":127,"end":134,"id":20},{"text":"using","start":135,"end":140,"id":21},{"text":"latent","start":141,"end":147,"id":22},{"text":"variable","start":148,"end":156,"id":23},{"text":"models","start":157,"end":163,"id":24},{"text":".","start":163,"end":164,"id":25}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":141,"end":156,"token_start":22,"token_end":23,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We study the inference problem of density ratios and apply a semi-parametric density-ratio estimator to the two-sample homogeneity test.","_input_hash":1098462718,"_task_hash":570824939,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"study","start":3,"end":8,"id":1},{"text":"the","start":9,"end":12,"id":2},{"text":"inference","start":13,"end":22,"id":3},{"text":"problem","start":23,"end":30,"id":4},{"text":"of","start":31,"end":33,"id":5},{"text":"density","start":34,"end":41,"id":6},{"text":"ratios","start":42,"end":48,"id":7},{"text":"and","start":49,"end":52,"id":8},{"text":"apply","start":53,"end":58,"id":9},{"text":"a","start":59,"end":60,"id":10},{"text":"semi","start":61,"end":65,"id":11},{"text":"-","start":65,"end":66,"id":12},{"text":"parametric","start":66,"end":76,"id":13},{"text":"density","start":77,"end":84,"id":14},{"text":"-","start":84,"end":85,"id":15},{"text":"ratio","start":85,"end":90,"id":16},{"text":"estimator","start":91,"end":100,"id":17},{"text":"to","start":101,"end":103,"id":18},{"text":"the","start":104,"end":107,"id":19},{"text":"two","start":108,"end":111,"id":20},{"text":"-","start":111,"end":112,"id":21},{"text":"sample","start":112,"end":118,"id":22},{"text":"homogeneity","start":119,"end":130,"id":23},{"text":"test","start":131,"end":135,"id":24},{"text":".","start":135,"end":136,"id":25}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"We also present two novel dimension-reduction techniques that attempt to minimize the suggested measure, and compare the results of these techniques to the results of existing algorithms.","_input_hash":-589856197,"_task_hash":789082106,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"also","start":3,"end":7,"id":1},{"text":"present","start":8,"end":15,"id":2},{"text":"two","start":16,"end":19,"id":3},{"text":"novel","start":20,"end":25,"id":4},{"text":"dimension","start":26,"end":35,"id":5},{"text":"-","start":35,"end":36,"id":6},{"text":"reduction","start":36,"end":45,"id":7},{"text":"techniques","start":46,"end":56,"id":8},{"text":"that","start":57,"end":61,"id":9},{"text":"attempt","start":62,"end":69,"id":10},{"text":"to","start":70,"end":72,"id":11},{"text":"minimize","start":73,"end":81,"id":12},{"text":"the","start":82,"end":85,"id":13},{"text":"suggested","start":86,"end":95,"id":14},{"text":"measure","start":96,"end":103,"id":15},{"text":",","start":103,"end":104,"id":16},{"text":"and","start":105,"end":108,"id":17},{"text":"compare","start":109,"end":116,"id":18},{"text":"the","start":117,"end":120,"id":19},{"text":"results","start":121,"end":128,"id":20},{"text":"of","start":129,"end":131,"id":21},{"text":"these","start":132,"end":137,"id":22},{"text":"techniques","start":138,"end":148,"id":23},{"text":"to","start":149,"end":151,"id":24},{"text":"the","start":152,"end":155,"id":25},{"text":"results","start":156,"end":163,"id":26},{"text":"of","start":164,"end":166,"id":27},{"text":"existing","start":167,"end":175,"id":28},{"text":"algorithms","start":176,"end":186,"id":29},{"text":".","start":186,"end":187,"id":30}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We compare the efficiency of our approximation to several other well known approximations of the Dirichlet process and demonstrate a substantial improvement.","_input_hash":812853047,"_task_hash":778392751,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"compare","start":3,"end":10,"id":1},{"text":"the","start":11,"end":14,"id":2},{"text":"efficiency","start":15,"end":25,"id":3},{"text":"of","start":26,"end":28,"id":4},{"text":"our","start":29,"end":32,"id":5},{"text":"approximation","start":33,"end":46,"id":6},{"text":"to","start":47,"end":49,"id":7},{"text":"several","start":50,"end":57,"id":8},{"text":"other","start":58,"end":63,"id":9},{"text":"well","start":64,"end":68,"id":10},{"text":"known","start":69,"end":74,"id":11},{"text":"approximations","start":75,"end":89,"id":12},{"text":"of","start":90,"end":92,"id":13},{"text":"the","start":93,"end":96,"id":14},{"text":"Dirichlet","start":97,"end":106,"id":15},{"text":"process","start":107,"end":114,"id":16},{"text":"and","start":115,"end":118,"id":17},{"text":"demonstrate","start":119,"end":130,"id":18},{"text":"a","start":131,"end":132,"id":19},{"text":"substantial","start":133,"end":144,"id":20},{"text":"improvement","start":145,"end":156,"id":21},{"text":".","start":156,"end":157,"id":22}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":97,"end":106,"token_start":15,"token_end":15,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"The regularization used induces the selection of spatially coherent predictive brain regions simultaneously at different scales.","_input_hash":-346081680,"_task_hash":283627904,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"regularization","start":4,"end":18,"id":1},{"text":"used","start":19,"end":23,"id":2},{"text":"induces","start":24,"end":31,"id":3},{"text":"the","start":32,"end":35,"id":4},{"text":"selection","start":36,"end":45,"id":5},{"text":"of","start":46,"end":48,"id":6},{"text":"spatially","start":49,"end":58,"id":7},{"text":"coherent","start":59,"end":67,"id":8},{"text":"predictive","start":68,"end":78,"id":9},{"text":"brain","start":79,"end":84,"id":10},{"text":"regions","start":85,"end":92,"id":11},{"text":"simultaneously","start":93,"end":107,"id":12},{"text":"at","start":108,"end":110,"id":13},{"text":"different","start":111,"end":120,"id":14},{"text":"scales","start":121,"end":127,"id":15},{"text":".","start":127,"end":128,"id":16}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"It is to be expected in such cases that learning algorithms will have to deal with manipulated data aimed at hampering decision making.","_input_hash":-363845449,"_task_hash":371133315,"tokens":[{"text":"It","start":0,"end":2,"id":0},{"text":"is","start":3,"end":5,"id":1},{"text":"to","start":6,"end":8,"id":2},{"text":"be","start":9,"end":11,"id":3},{"text":"expected","start":12,"end":20,"id":4},{"text":"in","start":21,"end":23,"id":5},{"text":"such","start":24,"end":28,"id":6},{"text":"cases","start":29,"end":34,"id":7},{"text":"that","start":35,"end":39,"id":8},{"text":"learning","start":40,"end":48,"id":9},{"text":"algorithms","start":49,"end":59,"id":10},{"text":"will","start":60,"end":64,"id":11},{"text":"have","start":65,"end":69,"id":12},{"text":"to","start":70,"end":72,"id":13},{"text":"deal","start":73,"end":77,"id":14},{"text":"with","start":78,"end":82,"id":15},{"text":"manipulated","start":83,"end":94,"id":16},{"text":"data","start":95,"end":99,"id":17},{"text":"aimed","start":100,"end":105,"id":18},{"text":"at","start":106,"end":108,"id":19},{"text":"hampering","start":109,"end":118,"id":20},{"text":"decision","start":119,"end":127,"id":21},{"text":"making","start":128,"end":134,"id":22},{"text":".","start":134,"end":135,"id":23}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"2001) in p-adic geometry.","_input_hash":-933078138,"_task_hash":-914511922,"tokens":[{"text":"2001","start":0,"end":4,"id":0},{"text":")","start":4,"end":5,"id":1},{"text":"in","start":6,"end":8,"id":2},{"text":"p","start":9,"end":10,"id":3},{"text":"-","start":10,"end":11,"id":4},{"text":"adic","start":11,"end":15,"id":5},{"text":"geometry","start":16,"end":24,"id":6},{"text":".","start":24,"end":25,"id":7}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"The discovery of non-linear causal relationship under additive non-Gaussian noise models has attracted considerable attention recently because of their high flexibility.","_input_hash":-36975775,"_task_hash":-2081912637,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"discovery","start":4,"end":13,"id":1},{"text":"of","start":14,"end":16,"id":2},{"text":"non","start":17,"end":20,"id":3},{"text":"-","start":20,"end":21,"id":4},{"text":"linear","start":21,"end":27,"id":5},{"text":"causal","start":28,"end":34,"id":6},{"text":"relationship","start":35,"end":47,"id":7},{"text":"under","start":48,"end":53,"id":8},{"text":"additive","start":54,"end":62,"id":9},{"text":"non","start":63,"end":66,"id":10},{"text":"-","start":66,"end":67,"id":11},{"text":"Gaussian","start":67,"end":75,"id":12},{"text":"noise","start":76,"end":81,"id":13},{"text":"models","start":82,"end":88,"id":14},{"text":"has","start":89,"end":92,"id":15},{"text":"attracted","start":93,"end":102,"id":16},{"text":"considerable","start":103,"end":115,"id":17},{"text":"attention","start":116,"end":125,"id":18},{"text":"recently","start":126,"end":134,"id":19},{"text":"because","start":135,"end":142,"id":20},{"text":"of","start":143,"end":145,"id":21},{"text":"their","start":146,"end":151,"id":22},{"text":"high","start":152,"end":156,"id":23},{"text":"flexibility","start":157,"end":168,"id":24},{"text":".","start":168,"end":169,"id":25}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":54,"end":81,"token_start":9,"token_end":13,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Genetic algorithm is implemented for network optimization and estimating the missing data.","_input_hash":-1636818188,"_task_hash":-47578204,"tokens":[{"text":"Genetic","start":0,"end":7,"id":0},{"text":"algorithm","start":8,"end":17,"id":1},{"text":"is","start":18,"end":20,"id":2},{"text":"implemented","start":21,"end":32,"id":3},{"text":"for","start":33,"end":36,"id":4},{"text":"network","start":37,"end":44,"id":5},{"text":"optimization","start":45,"end":57,"id":6},{"text":"and","start":58,"end":61,"id":7},{"text":"estimating","start":62,"end":72,"id":8},{"text":"the","start":73,"end":76,"id":9},{"text":"missing","start":77,"end":84,"id":10},{"text":"data","start":85,"end":89,"id":11},{"text":".","start":89,"end":90,"id":12}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":0,"end":7,"token_start":0,"token_end":0,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Moreover, the algebraic viewpoint on probability distributions allows us to invoke the theory of Algebraic Geometry, which we demonstrate in a compact proof for an identifiability criterion.","_input_hash":-1147683666,"_task_hash":2097332248,"tokens":[{"text":"Moreover","start":0,"end":8,"id":0},{"text":",","start":8,"end":9,"id":1},{"text":"the","start":10,"end":13,"id":2},{"text":"algebraic","start":14,"end":23,"id":3},{"text":"viewpoint","start":24,"end":33,"id":4},{"text":"on","start":34,"end":36,"id":5},{"text":"probability","start":37,"end":48,"id":6},{"text":"distributions","start":49,"end":62,"id":7},{"text":"allows","start":63,"end":69,"id":8},{"text":"us","start":70,"end":72,"id":9},{"text":"to","start":73,"end":75,"id":10},{"text":"invoke","start":76,"end":82,"id":11},{"text":"the","start":83,"end":86,"id":12},{"text":"theory","start":87,"end":93,"id":13},{"text":"of","start":94,"end":96,"id":14},{"text":"Algebraic","start":97,"end":106,"id":15},{"text":"Geometry","start":107,"end":115,"id":16},{"text":",","start":115,"end":116,"id":17},{"text":"which","start":117,"end":122,"id":18},{"text":"we","start":123,"end":125,"id":19},{"text":"demonstrate","start":126,"end":137,"id":20},{"text":"in","start":138,"end":140,"id":21},{"text":"a","start":141,"end":142,"id":22},{"text":"compact","start":143,"end":150,"id":23},{"text":"proof","start":151,"end":156,"id":24},{"text":"for","start":157,"end":160,"id":25},{"text":"an","start":161,"end":163,"id":26},{"text":"identifiability","start":164,"end":179,"id":27},{"text":"criterion","start":180,"end":189,"id":28},{"text":".","start":189,"end":190,"id":29}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":97,"end":115,"token_start":15,"token_end":16,"label":"ALGO","answer":"reject"}],"answer":"reject"}
{"text":"Our proposal significantly enhances the state-of-the-art for such moderate sized problems and gracefully scales to larger problems where other algorithms become practically infeasible.","_input_hash":966270846,"_task_hash":61880936,"tokens":[{"text":"Our","start":0,"end":3,"id":0},{"text":"proposal","start":4,"end":12,"id":1},{"text":"significantly","start":13,"end":26,"id":2},{"text":"enhances","start":27,"end":35,"id":3},{"text":"the","start":36,"end":39,"id":4},{"text":"state","start":40,"end":45,"id":5},{"text":"-","start":45,"end":46,"id":6},{"text":"of","start":46,"end":48,"id":7},{"text":"-","start":48,"end":49,"id":8},{"text":"the","start":49,"end":52,"id":9},{"text":"-","start":52,"end":53,"id":10},{"text":"art","start":53,"end":56,"id":11},{"text":"for","start":57,"end":60,"id":12},{"text":"such","start":61,"end":65,"id":13},{"text":"moderate","start":66,"end":74,"id":14},{"text":"sized","start":75,"end":80,"id":15},{"text":"problems","start":81,"end":89,"id":16},{"text":"and","start":90,"end":93,"id":17},{"text":"gracefully","start":94,"end":104,"id":18},{"text":"scales","start":105,"end":111,"id":19},{"text":"to","start":112,"end":114,"id":20},{"text":"larger","start":115,"end":121,"id":21},{"text":"problems","start":122,"end":130,"id":22},{"text":"where","start":131,"end":136,"id":23},{"text":"other","start":137,"end":142,"id":24},{"text":"algorithms","start":143,"end":153,"id":25},{"text":"become","start":154,"end":160,"id":26},{"text":"practically","start":161,"end":172,"id":27},{"text":"infeasible","start":173,"end":183,"id":28},{"text":".","start":183,"end":184,"id":29}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"When applied to text documents, our method learns hierarchies of topics, thus providing a competitive alternative to probabilistic topic models.","_input_hash":774547266,"_task_hash":-1376305421,"tokens":[{"text":"When","start":0,"end":4,"id":0},{"text":"applied","start":5,"end":12,"id":1},{"text":"to","start":13,"end":15,"id":2},{"text":"text","start":16,"end":20,"id":3},{"text":"documents","start":21,"end":30,"id":4},{"text":",","start":30,"end":31,"id":5},{"text":"our","start":32,"end":35,"id":6},{"text":"method","start":36,"end":42,"id":7},{"text":"learns","start":43,"end":49,"id":8},{"text":"hierarchies","start":50,"end":61,"id":9},{"text":"of","start":62,"end":64,"id":10},{"text":"topics","start":65,"end":71,"id":11},{"text":",","start":71,"end":72,"id":12},{"text":"thus","start":73,"end":77,"id":13},{"text":"providing","start":78,"end":87,"id":14},{"text":"a","start":88,"end":89,"id":15},{"text":"competitive","start":90,"end":101,"id":16},{"text":"alternative","start":102,"end":113,"id":17},{"text":"to","start":114,"end":116,"id":18},{"text":"probabilistic","start":117,"end":130,"id":19},{"text":"topic","start":131,"end":136,"id":20},{"text":"models","start":137,"end":143,"id":21},{"text":".","start":143,"end":144,"id":22}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":117,"end":130,"token_start":19,"token_end":19,"label":"ALGO","answer":"reject"}],"answer":"reject"}
{"text":"  There are two situations where such high dimensional features arise.","_input_hash":237257470,"_task_hash":-1881310355,"tokens":[{"text":"  ","start":0,"end":2,"id":0},{"text":"There","start":2,"end":7,"id":1},{"text":"are","start":8,"end":11,"id":2},{"text":"two","start":12,"end":15,"id":3},{"text":"situations","start":16,"end":26,"id":4},{"text":"where","start":27,"end":32,"id":5},{"text":"such","start":33,"end":37,"id":6},{"text":"high","start":38,"end":42,"id":7},{"text":"dimensional","start":43,"end":54,"id":8},{"text":"features","start":55,"end":63,"id":9},{"text":"arise","start":64,"end":69,"id":10},{"text":".","start":69,"end":70,"id":11}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"An interesting consequence is that the size of the test sample is not required to grow to infinity for the consistency of the cross-validation procedure.","_input_hash":-1544597140,"_task_hash":-844183427,"tokens":[{"text":"An","start":0,"end":2,"id":0},{"text":"interesting","start":3,"end":14,"id":1},{"text":"consequence","start":15,"end":26,"id":2},{"text":"is","start":27,"end":29,"id":3},{"text":"that","start":30,"end":34,"id":4},{"text":"the","start":35,"end":38,"id":5},{"text":"size","start":39,"end":43,"id":6},{"text":"of","start":44,"end":46,"id":7},{"text":"the","start":47,"end":50,"id":8},{"text":"test","start":51,"end":55,"id":9},{"text":"sample","start":56,"end":62,"id":10},{"text":"is","start":63,"end":65,"id":11},{"text":"not","start":66,"end":69,"id":12},{"text":"required","start":70,"end":78,"id":13},{"text":"to","start":79,"end":81,"id":14},{"text":"grow","start":82,"end":86,"id":15},{"text":"to","start":87,"end":89,"id":16},{"text":"infinity","start":90,"end":98,"id":17},{"text":"for","start":99,"end":102,"id":18},{"text":"the","start":103,"end":106,"id":19},{"text":"consistency","start":107,"end":118,"id":20},{"text":"of","start":119,"end":121,"id":21},{"text":"the","start":122,"end":125,"id":22},{"text":"cross","start":126,"end":131,"id":23},{"text":"-","start":131,"end":132,"id":24},{"text":"validation","start":132,"end":142,"id":25},{"text":"procedure","start":143,"end":152,"id":26},{"text":".","start":152,"end":153,"id":27}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"This ranking scheme serves as a surrogate for density;","_input_hash":917690931,"_task_hash":1682430728,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"ranking","start":5,"end":12,"id":1},{"text":"scheme","start":13,"end":19,"id":2},{"text":"serves","start":20,"end":26,"id":3},{"text":"as","start":27,"end":29,"id":4},{"text":"a","start":30,"end":31,"id":5},{"text":"surrogate","start":32,"end":41,"id":6},{"text":"for","start":42,"end":45,"id":7},{"text":"density","start":46,"end":53,"id":8},{"text":";","start":53,"end":54,"id":9}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We present a procedure for effective estimation of entropy and mutual information from small-sample data, and apply it to the problem of inferring high-dimensional gene association networks.","_input_hash":1305692642,"_task_hash":408214070,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"present","start":3,"end":10,"id":1},{"text":"a","start":11,"end":12,"id":2},{"text":"procedure","start":13,"end":22,"id":3},{"text":"for","start":23,"end":26,"id":4},{"text":"effective","start":27,"end":36,"id":5},{"text":"estimation","start":37,"end":47,"id":6},{"text":"of","start":48,"end":50,"id":7},{"text":"entropy","start":51,"end":58,"id":8},{"text":"and","start":59,"end":62,"id":9},{"text":"mutual","start":63,"end":69,"id":10},{"text":"information","start":70,"end":81,"id":11},{"text":"from","start":82,"end":86,"id":12},{"text":"small","start":87,"end":92,"id":13},{"text":"-","start":92,"end":93,"id":14},{"text":"sample","start":93,"end":99,"id":15},{"text":"data","start":100,"end":104,"id":16},{"text":",","start":104,"end":105,"id":17},{"text":"and","start":106,"end":109,"id":18},{"text":"apply","start":110,"end":115,"id":19},{"text":"it","start":116,"end":118,"id":20},{"text":"to","start":119,"end":121,"id":21},{"text":"the","start":122,"end":125,"id":22},{"text":"problem","start":126,"end":133,"id":23},{"text":"of","start":134,"end":136,"id":24},{"text":"inferring","start":137,"end":146,"id":25},{"text":"high","start":147,"end":151,"id":26},{"text":"-","start":151,"end":152,"id":27},{"text":"dimensional","start":152,"end":163,"id":28},{"text":"gene","start":164,"end":168,"id":29},{"text":"association","start":169,"end":180,"id":30},{"text":"networks","start":181,"end":189,"id":31},{"text":".","start":189,"end":190,"id":32}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"This is carried out through a small simulation study and the analysis of several real functional data sets.","_input_hash":1263647153,"_task_hash":-1692974147,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"is","start":5,"end":7,"id":1},{"text":"carried","start":8,"end":15,"id":2},{"text":"out","start":16,"end":19,"id":3},{"text":"through","start":20,"end":27,"id":4},{"text":"a","start":28,"end":29,"id":5},{"text":"small","start":30,"end":35,"id":6},{"text":"simulation","start":36,"end":46,"id":7},{"text":"study","start":47,"end":52,"id":8},{"text":"and","start":53,"end":56,"id":9},{"text":"the","start":57,"end":60,"id":10},{"text":"analysis","start":61,"end":69,"id":11},{"text":"of","start":70,"end":72,"id":12},{"text":"several","start":73,"end":80,"id":13},{"text":"real","start":81,"end":85,"id":14},{"text":"functional","start":86,"end":96,"id":15},{"text":"data","start":97,"end":101,"id":16},{"text":"sets","start":102,"end":106,"id":17},{"text":".","start":106,"end":107,"id":18}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"However, in many applications, additional conditions on the structure of the regression vector and its sparsity pattern are available.","_input_hash":1534893567,"_task_hash":357376953,"tokens":[{"text":"However","start":0,"end":7,"id":0},{"text":",","start":7,"end":8,"id":1},{"text":"in","start":9,"end":11,"id":2},{"text":"many","start":12,"end":16,"id":3},{"text":"applications","start":17,"end":29,"id":4},{"text":",","start":29,"end":30,"id":5},{"text":"additional","start":31,"end":41,"id":6},{"text":"conditions","start":42,"end":52,"id":7},{"text":"on","start":53,"end":55,"id":8},{"text":"the","start":56,"end":59,"id":9},{"text":"structure","start":60,"end":69,"id":10},{"text":"of","start":70,"end":72,"id":11},{"text":"the","start":73,"end":76,"id":12},{"text":"regression","start":77,"end":87,"id":13},{"text":"vector","start":88,"end":94,"id":14},{"text":"and","start":95,"end":98,"id":15},{"text":"its","start":99,"end":102,"id":16},{"text":"sparsity","start":103,"end":111,"id":17},{"text":"pattern","start":112,"end":119,"id":18},{"text":"are","start":120,"end":123,"id":19},{"text":"available","start":124,"end":133,"id":20},{"text":".","start":133,"end":134,"id":21}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We theoretically investigate the convergence rate and support consistency (i.e., correctly identifying the subset of non-zero coefficients in the large sample limit) of multiple kernel learning (MKL).","_input_hash":1348829627,"_task_hash":1875384758,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"theoretically","start":3,"end":16,"id":1},{"text":"investigate","start":17,"end":28,"id":2},{"text":"the","start":29,"end":32,"id":3},{"text":"convergence","start":33,"end":44,"id":4},{"text":"rate","start":45,"end":49,"id":5},{"text":"and","start":50,"end":53,"id":6},{"text":"support","start":54,"end":61,"id":7},{"text":"consistency","start":62,"end":73,"id":8},{"text":"(","start":74,"end":75,"id":9},{"text":"i.e.","start":75,"end":79,"id":10},{"text":",","start":79,"end":80,"id":11},{"text":"correctly","start":81,"end":90,"id":12},{"text":"identifying","start":91,"end":102,"id":13},{"text":"the","start":103,"end":106,"id":14},{"text":"subset","start":107,"end":113,"id":15},{"text":"of","start":114,"end":116,"id":16},{"text":"non","start":117,"end":120,"id":17},{"text":"-","start":120,"end":121,"id":18},{"text":"zero","start":121,"end":125,"id":19},{"text":"coefficients","start":126,"end":138,"id":20},{"text":"in","start":139,"end":141,"id":21},{"text":"the","start":142,"end":145,"id":22},{"text":"large","start":146,"end":151,"id":23},{"text":"sample","start":152,"end":158,"id":24},{"text":"limit","start":159,"end":164,"id":25},{"text":")","start":164,"end":165,"id":26},{"text":"of","start":166,"end":168,"id":27},{"text":"multiple","start":169,"end":177,"id":28},{"text":"kernel","start":178,"end":184,"id":29},{"text":"learning","start":185,"end":193,"id":30},{"text":"(","start":194,"end":195,"id":31},{"text":"MKL","start":195,"end":198,"id":32},{"text":")","start":198,"end":199,"id":33},{"text":".","start":199,"end":200,"id":34}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":169,"end":193,"token_start":28,"token_end":30,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We consider a Gaussian process formulation of the multiple kernel learning problem.","_input_hash":-1388654521,"_task_hash":-469471995,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"consider","start":3,"end":11,"id":1},{"text":"a","start":12,"end":13,"id":2},{"text":"Gaussian","start":14,"end":22,"id":3},{"text":"process","start":23,"end":30,"id":4},{"text":"formulation","start":31,"end":42,"id":5},{"text":"of","start":43,"end":45,"id":6},{"text":"the","start":46,"end":49,"id":7},{"text":"multiple","start":50,"end":58,"id":8},{"text":"kernel","start":59,"end":65,"id":9},{"text":"learning","start":66,"end":74,"id":10},{"text":"problem","start":75,"end":82,"id":11},{"text":".","start":82,"end":83,"id":12}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"In recent years, a series of papers has analyzed the behavior of this method from a theoretical point of view.","_input_hash":-1941245184,"_task_hash":-1509237239,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"recent","start":3,"end":9,"id":1},{"text":"years","start":10,"end":15,"id":2},{"text":",","start":15,"end":16,"id":3},{"text":"a","start":17,"end":18,"id":4},{"text":"series","start":19,"end":25,"id":5},{"text":"of","start":26,"end":28,"id":6},{"text":"papers","start":29,"end":35,"id":7},{"text":"has","start":36,"end":39,"id":8},{"text":"analyzed","start":40,"end":48,"id":9},{"text":"the","start":49,"end":52,"id":10},{"text":"behavior","start":53,"end":61,"id":11},{"text":"of","start":62,"end":64,"id":12},{"text":"this","start":65,"end":69,"id":13},{"text":"method","start":70,"end":76,"id":14},{"text":"from","start":77,"end":81,"id":15},{"text":"a","start":82,"end":83,"id":16},{"text":"theoretical","start":84,"end":95,"id":17},{"text":"point","start":96,"end":101,"id":18},{"text":"of","start":102,"end":104,"id":19},{"text":"view","start":105,"end":109,"id":20},{"text":".","start":109,"end":110,"id":21}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"Based on the RCD sampler, we developed a series of sophisticated sampling algorithms for iHMMs, including blocked Gibbs sampling, beam sampling, and split-merge sampling, that outperformed conventional iHMM samplers in experiments","_input_hash":-498727691,"_task_hash":-1403434600,"tokens":[{"text":"Based","start":0,"end":5,"id":0},{"text":"on","start":6,"end":8,"id":1},{"text":"the","start":9,"end":12,"id":2},{"text":"RCD","start":13,"end":16,"id":3},{"text":"sampler","start":17,"end":24,"id":4},{"text":",","start":24,"end":25,"id":5},{"text":"we","start":26,"end":28,"id":6},{"text":"developed","start":29,"end":38,"id":7},{"text":"a","start":39,"end":40,"id":8},{"text":"series","start":41,"end":47,"id":9},{"text":"of","start":48,"end":50,"id":10},{"text":"sophisticated","start":51,"end":64,"id":11},{"text":"sampling","start":65,"end":73,"id":12},{"text":"algorithms","start":74,"end":84,"id":13},{"text":"for","start":85,"end":88,"id":14},{"text":"iHMMs","start":89,"end":94,"id":15},{"text":",","start":94,"end":95,"id":16},{"text":"including","start":96,"end":105,"id":17},{"text":"blocked","start":106,"end":113,"id":18},{"text":"Gibbs","start":114,"end":119,"id":19},{"text":"sampling","start":120,"end":128,"id":20},{"text":",","start":128,"end":129,"id":21},{"text":"beam","start":130,"end":134,"id":22},{"text":"sampling","start":135,"end":143,"id":23},{"text":",","start":143,"end":144,"id":24},{"text":"and","start":145,"end":148,"id":25},{"text":"split","start":149,"end":154,"id":26},{"text":"-","start":154,"end":155,"id":27},{"text":"merge","start":155,"end":160,"id":28},{"text":"sampling","start":161,"end":169,"id":29},{"text":",","start":169,"end":170,"id":30},{"text":"that","start":171,"end":175,"id":31},{"text":"outperformed","start":176,"end":188,"id":32},{"text":"conventional","start":189,"end":201,"id":33},{"text":"iHMM","start":202,"end":206,"id":34},{"text":"samplers","start":207,"end":215,"id":35},{"text":"in","start":216,"end":218,"id":36},{"text":"experiments","start":219,"end":230,"id":37}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Their Bayesian KNN (BKNN) approach uses a pseudo-likelihood function, and standard Markov chain Monte Carlo (MCMC) techniques to draw posterior samples.","_input_hash":-1316639529,"_task_hash":584578800,"tokens":[{"text":"Their","start":0,"end":5,"id":0},{"text":"Bayesian","start":6,"end":14,"id":1},{"text":"KNN","start":15,"end":18,"id":2},{"text":"(","start":19,"end":20,"id":3},{"text":"BKNN","start":20,"end":24,"id":4},{"text":")","start":24,"end":25,"id":5},{"text":"approach","start":26,"end":34,"id":6},{"text":"uses","start":35,"end":39,"id":7},{"text":"a","start":40,"end":41,"id":8},{"text":"pseudo","start":42,"end":48,"id":9},{"text":"-","start":48,"end":49,"id":10},{"text":"likelihood","start":49,"end":59,"id":11},{"text":"function","start":60,"end":68,"id":12},{"text":",","start":68,"end":69,"id":13},{"text":"and","start":70,"end":73,"id":14},{"text":"standard","start":74,"end":82,"id":15},{"text":"Markov","start":83,"end":89,"id":16},{"text":"chain","start":90,"end":95,"id":17},{"text":"Monte","start":96,"end":101,"id":18},{"text":"Carlo","start":102,"end":107,"id":19},{"text":"(","start":108,"end":109,"id":20},{"text":"MCMC","start":109,"end":113,"id":21},{"text":")","start":113,"end":114,"id":22},{"text":"techniques","start":115,"end":125,"id":23},{"text":"to","start":126,"end":128,"id":24},{"text":"draw","start":129,"end":133,"id":25},{"text":"posterior","start":134,"end":143,"id":26},{"text":"samples","start":144,"end":151,"id":27},{"text":".","start":151,"end":152,"id":28}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":6,"end":18,"token_start":1,"token_end":2,"label":"ALGO","answer":"accept"},{"start":83,"end":107,"token_start":16,"token_end":19,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Inferring the causal structure of a set of random variables from a finite sample of the joint distribution is an important problem in science.","_input_hash":-226428674,"_task_hash":1127368461,"tokens":[{"text":"Inferring","start":0,"end":9,"id":0},{"text":"the","start":10,"end":13,"id":1},{"text":"causal","start":14,"end":20,"id":2},{"text":"structure","start":21,"end":30,"id":3},{"text":"of","start":31,"end":33,"id":4},{"text":"a","start":34,"end":35,"id":5},{"text":"set","start":36,"end":39,"id":6},{"text":"of","start":40,"end":42,"id":7},{"text":"random","start":43,"end":49,"id":8},{"text":"variables","start":50,"end":59,"id":9},{"text":"from","start":60,"end":64,"id":10},{"text":"a","start":65,"end":66,"id":11},{"text":"finite","start":67,"end":73,"id":12},{"text":"sample","start":74,"end":80,"id":13},{"text":"of","start":81,"end":83,"id":14},{"text":"the","start":84,"end":87,"id":15},{"text":"joint","start":88,"end":93,"id":16},{"text":"distribution","start":94,"end":106,"id":17},{"text":"is","start":107,"end":109,"id":18},{"text":"an","start":110,"end":112,"id":19},{"text":"important","start":113,"end":122,"id":20},{"text":"problem","start":123,"end":130,"id":21},{"text":"in","start":131,"end":133,"id":22},{"text":"science","start":134,"end":141,"id":23},{"text":".","start":141,"end":142,"id":24}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We present and prove conditions on the manifold that are necessary for the success of the algorithms.","_input_hash":-1272831787,"_task_hash":-387941999,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"present","start":3,"end":10,"id":1},{"text":"and","start":11,"end":14,"id":2},{"text":"prove","start":15,"end":20,"id":3},{"text":"conditions","start":21,"end":31,"id":4},{"text":"on","start":32,"end":34,"id":5},{"text":"the","start":35,"end":38,"id":6},{"text":"manifold","start":39,"end":47,"id":7},{"text":"that","start":48,"end":52,"id":8},{"text":"are","start":53,"end":56,"id":9},{"text":"necessary","start":57,"end":66,"id":10},{"text":"for","start":67,"end":70,"id":11},{"text":"the","start":71,"end":74,"id":12},{"text":"success","start":75,"end":82,"id":13},{"text":"of","start":83,"end":85,"id":14},{"text":"the","start":86,"end":89,"id":15},{"text":"algorithms","start":90,"end":100,"id":16},{"text":".","start":100,"end":101,"id":17}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"However, apart from a few special cases, it is hard or impossible to calculate analytically.","_input_hash":461360444,"_task_hash":-2085271174,"tokens":[{"text":"However","start":0,"end":7,"id":0},{"text":",","start":7,"end":8,"id":1},{"text":"apart","start":9,"end":14,"id":2},{"text":"from","start":15,"end":19,"id":3},{"text":"a","start":20,"end":21,"id":4},{"text":"few","start":22,"end":25,"id":5},{"text":"special","start":26,"end":33,"id":6},{"text":"cases","start":34,"end":39,"id":7},{"text":",","start":39,"end":40,"id":8},{"text":"it","start":41,"end":43,"id":9},{"text":"is","start":44,"end":46,"id":10},{"text":"hard","start":47,"end":51,"id":11},{"text":"or","start":52,"end":54,"id":12},{"text":"impossible","start":55,"end":65,"id":13},{"text":"to","start":66,"end":68,"id":14},{"text":"calculate","start":69,"end":78,"id":15},{"text":"analytically","start":79,"end":91,"id":16},{"text":".","start":91,"end":92,"id":17}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"For this purpose, we reformulate the problem in the sparse inverse covariance (concentration) domain and solve the global eigenvalue problem using a sequence of local eigenvalue problems in each of the cliques of the decomposable graph.","_input_hash":-886167127,"_task_hash":-2112496241,"tokens":[{"text":"For","start":0,"end":3,"id":0},{"text":"this","start":4,"end":8,"id":1},{"text":"purpose","start":9,"end":16,"id":2},{"text":",","start":16,"end":17,"id":3},{"text":"we","start":18,"end":20,"id":4},{"text":"reformulate","start":21,"end":32,"id":5},{"text":"the","start":33,"end":36,"id":6},{"text":"problem","start":37,"end":44,"id":7},{"text":"in","start":45,"end":47,"id":8},{"text":"the","start":48,"end":51,"id":9},{"text":"sparse","start":52,"end":58,"id":10},{"text":"inverse","start":59,"end":66,"id":11},{"text":"covariance","start":67,"end":77,"id":12},{"text":"(","start":78,"end":79,"id":13},{"text":"concentration","start":79,"end":92,"id":14},{"text":")","start":92,"end":93,"id":15},{"text":"domain","start":94,"end":100,"id":16},{"text":"and","start":101,"end":104,"id":17},{"text":"solve","start":105,"end":110,"id":18},{"text":"the","start":111,"end":114,"id":19},{"text":"global","start":115,"end":121,"id":20},{"text":"eigenvalue","start":122,"end":132,"id":21},{"text":"problem","start":133,"end":140,"id":22},{"text":"using","start":141,"end":146,"id":23},{"text":"a","start":147,"end":148,"id":24},{"text":"sequence","start":149,"end":157,"id":25},{"text":"of","start":158,"end":160,"id":26},{"text":"local","start":161,"end":166,"id":27},{"text":"eigenvalue","start":167,"end":177,"id":28},{"text":"problems","start":178,"end":186,"id":29},{"text":"in","start":187,"end":189,"id":30},{"text":"each","start":190,"end":194,"id":31},{"text":"of","start":195,"end":197,"id":32},{"text":"the","start":198,"end":201,"id":33},{"text":"cliques","start":202,"end":209,"id":34},{"text":"of","start":210,"end":212,"id":35},{"text":"the","start":213,"end":216,"id":36},{"text":"decomposable","start":217,"end":229,"id":37},{"text":"graph","start":230,"end":235,"id":38},{"text":".","start":235,"end":236,"id":39}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Second, it is short and simple and it follows from a direct application of Blackwell's approachability theorem to carefully chosen vector-valued payoff function and convex target set.","_input_hash":-1499825962,"_task_hash":792909598,"tokens":[{"text":"Second","start":0,"end":6,"id":0},{"text":",","start":6,"end":7,"id":1},{"text":"it","start":8,"end":10,"id":2},{"text":"is","start":11,"end":13,"id":3},{"text":"short","start":14,"end":19,"id":4},{"text":"and","start":20,"end":23,"id":5},{"text":"simple","start":24,"end":30,"id":6},{"text":"and","start":31,"end":34,"id":7},{"text":"it","start":35,"end":37,"id":8},{"text":"follows","start":38,"end":45,"id":9},{"text":"from","start":46,"end":50,"id":10},{"text":"a","start":51,"end":52,"id":11},{"text":"direct","start":53,"end":59,"id":12},{"text":"application","start":60,"end":71,"id":13},{"text":"of","start":72,"end":74,"id":14},{"text":"Blackwell","start":75,"end":84,"id":15},{"text":"'s","start":84,"end":86,"id":16},{"text":"approachability","start":87,"end":102,"id":17},{"text":"theorem","start":103,"end":110,"id":18},{"text":"to","start":111,"end":113,"id":19},{"text":"carefully","start":114,"end":123,"id":20},{"text":"chosen","start":124,"end":130,"id":21},{"text":"vector","start":131,"end":137,"id":22},{"text":"-","start":137,"end":138,"id":23},{"text":"valued","start":138,"end":144,"id":24},{"text":"payoff","start":145,"end":151,"id":25},{"text":"function","start":152,"end":160,"id":26},{"text":"and","start":161,"end":164,"id":27},{"text":"convex","start":165,"end":171,"id":28},{"text":"target","start":172,"end":178,"id":29},{"text":"set","start":179,"end":182,"id":30},{"text":".","start":182,"end":183,"id":31}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"We also study its dependence on margin parameters of the classification problem.","_input_hash":-1815872367,"_task_hash":2143623407,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"also","start":3,"end":7,"id":1},{"text":"study","start":8,"end":13,"id":2},{"text":"its","start":14,"end":17,"id":3},{"text":"dependence","start":18,"end":28,"id":4},{"text":"on","start":29,"end":31,"id":5},{"text":"margin","start":32,"end":38,"id":6},{"text":"parameters","start":39,"end":49,"id":7},{"text":"of","start":50,"end":52,"id":8},{"text":"the","start":53,"end":56,"id":9},{"text":"classification","start":57,"end":71,"id":10},{"text":"problem","start":72,"end":79,"id":11},{"text":".","start":79,"end":80,"id":12}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Namely, given a dictionary $F = \\{f_1, ..., f_M \\}$ of functions, we look for an optimal aggregation algorithm that writes $\\tilde f = \\sum_{j=1}^M \\theta_j f_j$ with as many zero coefficients $\\theta_j$ as possible.","_input_hash":-990215100,"_task_hash":883469789,"tokens":[{"text":"Namely","start":0,"end":6,"id":0},{"text":",","start":6,"end":7,"id":1},{"text":"given","start":8,"end":13,"id":2},{"text":"a","start":14,"end":15,"id":3},{"text":"dictionary","start":16,"end":26,"id":4},{"text":"$","start":27,"end":28,"id":5},{"text":"F","start":28,"end":29,"id":6},{"text":"=","start":30,"end":31,"id":7},{"text":"\\{f_1","start":32,"end":37,"id":8},{"text":",","start":37,"end":38,"id":9},{"text":"...","start":39,"end":42,"id":10},{"text":",","start":42,"end":43,"id":11},{"text":"f_M","start":44,"end":47,"id":12},{"text":"\\}$","start":48,"end":51,"id":13},{"text":"of","start":52,"end":54,"id":14},{"text":"functions","start":55,"end":64,"id":15},{"text":",","start":64,"end":65,"id":16},{"text":"we","start":66,"end":68,"id":17},{"text":"look","start":69,"end":73,"id":18},{"text":"for","start":74,"end":77,"id":19},{"text":"an","start":78,"end":80,"id":20},{"text":"optimal","start":81,"end":88,"id":21},{"text":"aggregation","start":89,"end":100,"id":22},{"text":"algorithm","start":101,"end":110,"id":23},{"text":"that","start":111,"end":115,"id":24},{"text":"writes","start":116,"end":122,"id":25},{"text":"$","start":123,"end":124,"id":26},{"text":"\\tilde","start":124,"end":130,"id":27},{"text":"f","start":131,"end":132,"id":28},{"text":"=","start":133,"end":134,"id":29},{"text":"\\sum_{j=1}^M","start":135,"end":147,"id":30},{"text":"\\theta_j","start":148,"end":156,"id":31},{"text":"f_j$","start":157,"end":161,"id":32},{"text":"with","start":162,"end":166,"id":33},{"text":"as","start":167,"end":169,"id":34},{"text":"many","start":170,"end":174,"id":35},{"text":"zero","start":175,"end":179,"id":36},{"text":"coefficients","start":180,"end":192,"id":37},{"text":"$","start":193,"end":194,"id":38},{"text":"\\theta_j$","start":194,"end":203,"id":39},{"text":"as","start":204,"end":206,"id":40},{"text":"possible","start":207,"end":215,"id":41},{"text":".","start":215,"end":216,"id":42}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":81,"end":100,"token_start":21,"token_end":22,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Screening of variables, as introduced in \\cite{fan08sis}, is an efficient and highly scalable way to remove many irrelevant variables from the set of all variables, while retaining all the relevant variables.","_input_hash":-110168013,"_task_hash":-318382854,"tokens":[{"text":"Screening","start":0,"end":9,"id":0},{"text":"of","start":10,"end":12,"id":1},{"text":"variables","start":13,"end":22,"id":2},{"text":",","start":22,"end":23,"id":3},{"text":"as","start":24,"end":26,"id":4},{"text":"introduced","start":27,"end":37,"id":5},{"text":"in","start":38,"end":40,"id":6},{"text":"\\cite{fan08sis","start":41,"end":55,"id":7},{"text":"}","start":55,"end":56,"id":8},{"text":",","start":56,"end":57,"id":9},{"text":"is","start":58,"end":60,"id":10},{"text":"an","start":61,"end":63,"id":11},{"text":"efficient","start":64,"end":73,"id":12},{"text":"and","start":74,"end":77,"id":13},{"text":"highly","start":78,"end":84,"id":14},{"text":"scalable","start":85,"end":93,"id":15},{"text":"way","start":94,"end":97,"id":16},{"text":"to","start":98,"end":100,"id":17},{"text":"remove","start":101,"end":107,"id":18},{"text":"many","start":108,"end":112,"id":19},{"text":"irrelevant","start":113,"end":123,"id":20},{"text":"variables","start":124,"end":133,"id":21},{"text":"from","start":134,"end":138,"id":22},{"text":"the","start":139,"end":142,"id":23},{"text":"set","start":143,"end":146,"id":24},{"text":"of","start":147,"end":149,"id":25},{"text":"all","start":150,"end":153,"id":26},{"text":"variables","start":154,"end":163,"id":27},{"text":",","start":163,"end":164,"id":28},{"text":"while","start":165,"end":170,"id":29},{"text":"retaining","start":171,"end":180,"id":30},{"text":"all","start":181,"end":184,"id":31},{"text":"the","start":185,"end":188,"id":32},{"text":"relevant","start":189,"end":197,"id":33},{"text":"variables","start":198,"end":207,"id":34},{"text":".","start":207,"end":208,"id":35}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"In particular, this approach is able to incorporate invariance information (lighting, elevation, etc.)","_input_hash":341525364,"_task_hash":-1602501504,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"particular","start":3,"end":13,"id":1},{"text":",","start":13,"end":14,"id":2},{"text":"this","start":15,"end":19,"id":3},{"text":"approach","start":20,"end":28,"id":4},{"text":"is","start":29,"end":31,"id":5},{"text":"able","start":32,"end":36,"id":6},{"text":"to","start":37,"end":39,"id":7},{"text":"incorporate","start":40,"end":51,"id":8},{"text":"invariance","start":52,"end":62,"id":9},{"text":"information","start":63,"end":74,"id":10},{"text":"(","start":75,"end":76,"id":11},{"text":"lighting","start":76,"end":84,"id":12},{"text":",","start":84,"end":85,"id":13},{"text":"elevation","start":86,"end":95,"id":14},{"text":",","start":95,"end":96,"id":15},{"text":"etc","start":97,"end":100,"id":16},{"text":".","start":100,"end":101,"id":17},{"text":")","start":101,"end":102,"id":18}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"As opposed to other papers, we consider the actual k-means algorithm and do not ignore its property of getting stuck in local optima.","_input_hash":1791070084,"_task_hash":-1943510812,"tokens":[{"text":"As","start":0,"end":2,"id":0},{"text":"opposed","start":3,"end":10,"id":1},{"text":"to","start":11,"end":13,"id":2},{"text":"other","start":14,"end":19,"id":3},{"text":"papers","start":20,"end":26,"id":4},{"text":",","start":26,"end":27,"id":5},{"text":"we","start":28,"end":30,"id":6},{"text":"consider","start":31,"end":39,"id":7},{"text":"the","start":40,"end":43,"id":8},{"text":"actual","start":44,"end":50,"id":9},{"text":"k","start":51,"end":52,"id":10},{"text":"-","start":52,"end":53,"id":11},{"text":"means","start":53,"end":58,"id":12},{"text":"algorithm","start":59,"end":68,"id":13},{"text":"and","start":69,"end":72,"id":14},{"text":"do","start":73,"end":75,"id":15},{"text":"not","start":76,"end":79,"id":16},{"text":"ignore","start":80,"end":86,"id":17},{"text":"its","start":87,"end":90,"id":18},{"text":"property","start":91,"end":99,"id":19},{"text":"of","start":100,"end":102,"id":20},{"text":"getting","start":103,"end":110,"id":21},{"text":"stuck","start":111,"end":116,"id":22},{"text":"in","start":117,"end":119,"id":23},{"text":"local","start":120,"end":125,"id":24},{"text":"optima","start":126,"end":132,"id":25},{"text":".","start":132,"end":133,"id":26}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":51,"end":58,"token_start":10,"token_end":12,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Under a latent function interpretation of the convolution transform we establish dependencies between output variables.","_input_hash":26082937,"_task_hash":1240780086,"tokens":[{"text":"Under","start":0,"end":5,"id":0},{"text":"a","start":6,"end":7,"id":1},{"text":"latent","start":8,"end":14,"id":2},{"text":"function","start":15,"end":23,"id":3},{"text":"interpretation","start":24,"end":38,"id":4},{"text":"of","start":39,"end":41,"id":5},{"text":"the","start":42,"end":45,"id":6},{"text":"convolution","start":46,"end":57,"id":7},{"text":"transform","start":58,"end":67,"id":8},{"text":"we","start":68,"end":70,"id":9},{"text":"establish","start":71,"end":80,"id":10},{"text":"dependencies","start":81,"end":93,"id":11},{"text":"between","start":94,"end":101,"id":12},{"text":"output","start":102,"end":108,"id":13},{"text":"variables","start":109,"end":118,"id":14},{"text":".","start":118,"end":119,"id":15}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"However, they are difficult to analyze in the high dimensional setting.","_input_hash":1158839226,"_task_hash":181744740,"tokens":[{"text":"However","start":0,"end":7,"id":0},{"text":",","start":7,"end":8,"id":1},{"text":"they","start":9,"end":13,"id":2},{"text":"are","start":14,"end":17,"id":3},{"text":"difficult","start":18,"end":27,"id":4},{"text":"to","start":28,"end":30,"id":5},{"text":"analyze","start":31,"end":38,"id":6},{"text":"in","start":39,"end":41,"id":7},{"text":"the","start":42,"end":45,"id":8},{"text":"high","start":46,"end":50,"id":9},{"text":"dimensional","start":51,"end":62,"id":10},{"text":"setting","start":63,"end":70,"id":11},{"text":".","start":70,"end":71,"id":12}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"In this paper we describe several such approaches in the common framework of graphical models.","_input_hash":-1238520531,"_task_hash":1198219167,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"this","start":3,"end":7,"id":1},{"text":"paper","start":8,"end":13,"id":2},{"text":"we","start":14,"end":16,"id":3},{"text":"describe","start":17,"end":25,"id":4},{"text":"several","start":26,"end":33,"id":5},{"text":"such","start":34,"end":38,"id":6},{"text":"approaches","start":39,"end":49,"id":7},{"text":"in","start":50,"end":52,"id":8},{"text":"the","start":53,"end":56,"id":9},{"text":"common","start":57,"end":63,"id":10},{"text":"framework","start":64,"end":73,"id":11},{"text":"of","start":74,"end":76,"id":12},{"text":"graphical","start":77,"end":86,"id":13},{"text":"models","start":87,"end":93,"id":14},{"text":".","start":93,"end":94,"id":15}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"This allows the design of norms adapted to specific prior knowledge expressed in terms of nonzero patterns.","_input_hash":-348166248,"_task_hash":-1653476760,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"allows","start":5,"end":11,"id":1},{"text":"the","start":12,"end":15,"id":2},{"text":"design","start":16,"end":22,"id":3},{"text":"of","start":23,"end":25,"id":4},{"text":"norms","start":26,"end":31,"id":5},{"text":"adapted","start":32,"end":39,"id":6},{"text":"to","start":40,"end":42,"id":7},{"text":"specific","start":43,"end":51,"id":8},{"text":"prior","start":52,"end":57,"id":9},{"text":"knowledge","start":58,"end":67,"id":10},{"text":"expressed","start":68,"end":77,"id":11},{"text":"in","start":78,"end":80,"id":12},{"text":"terms","start":81,"end":86,"id":13},{"text":"of","start":87,"end":89,"id":14},{"text":"nonzero","start":90,"end":97,"id":15},{"text":"patterns","start":98,"end":106,"id":16},{"text":".","start":106,"end":107,"id":17}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"In particular our rates are local to a query x and depend only on the way masses of balls centered at x vary with radius.","_input_hash":943853431,"_task_hash":-284048054,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"particular","start":3,"end":13,"id":1},{"text":"our","start":14,"end":17,"id":2},{"text":"rates","start":18,"end":23,"id":3},{"text":"are","start":24,"end":27,"id":4},{"text":"local","start":28,"end":33,"id":5},{"text":"to","start":34,"end":36,"id":6},{"text":"a","start":37,"end":38,"id":7},{"text":"query","start":39,"end":44,"id":8},{"text":"x","start":45,"end":46,"id":9},{"text":"and","start":47,"end":50,"id":10},{"text":"depend","start":51,"end":57,"id":11},{"text":"only","start":58,"end":62,"id":12},{"text":"on","start":63,"end":65,"id":13},{"text":"the","start":66,"end":69,"id":14},{"text":"way","start":70,"end":73,"id":15},{"text":"masses","start":74,"end":80,"id":16},{"text":"of","start":81,"end":83,"id":17},{"text":"balls","start":84,"end":89,"id":18},{"text":"centered","start":90,"end":98,"id":19},{"text":"at","start":99,"end":101,"id":20},{"text":"x","start":102,"end":103,"id":21},{"text":"vary","start":104,"end":108,"id":22},{"text":"with","start":109,"end":113,"id":23},{"text":"radius","start":114,"end":120,"id":24},{"text":".","start":120,"end":121,"id":25}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We give experimental evidence that the new algorithm is significantly more robust against label noise than existing boosting algorithm.","_input_hash":-1045206845,"_task_hash":-2094188250,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"give","start":3,"end":7,"id":1},{"text":"experimental","start":8,"end":20,"id":2},{"text":"evidence","start":21,"end":29,"id":3},{"text":"that","start":30,"end":34,"id":4},{"text":"the","start":35,"end":38,"id":5},{"text":"new","start":39,"end":42,"id":6},{"text":"algorithm","start":43,"end":52,"id":7},{"text":"is","start":53,"end":55,"id":8},{"text":"significantly","start":56,"end":69,"id":9},{"text":"more","start":70,"end":74,"id":10},{"text":"robust","start":75,"end":81,"id":11},{"text":"against","start":82,"end":89,"id":12},{"text":"label","start":90,"end":95,"id":13},{"text":"noise","start":96,"end":101,"id":14},{"text":"than","start":102,"end":106,"id":15},{"text":"existing","start":107,"end":115,"id":16},{"text":"boosting","start":116,"end":124,"id":17},{"text":"algorithm","start":125,"end":134,"id":18},{"text":".","start":134,"end":135,"id":19}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":116,"end":124,"token_start":17,"token_end":17,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Finally, we evaluate the short order statistics of the models, using the Kullback-Leibler divergence between test sequences and model samples, and show that our proposed methods match the statistics of the music genre significantly better than the VMM.","_input_hash":1658512288,"_task_hash":2004098626,"tokens":[{"text":"Finally","start":0,"end":7,"id":0},{"text":",","start":7,"end":8,"id":1},{"text":"we","start":9,"end":11,"id":2},{"text":"evaluate","start":12,"end":20,"id":3},{"text":"the","start":21,"end":24,"id":4},{"text":"short","start":25,"end":30,"id":5},{"text":"order","start":31,"end":36,"id":6},{"text":"statistics","start":37,"end":47,"id":7},{"text":"of","start":48,"end":50,"id":8},{"text":"the","start":51,"end":54,"id":9},{"text":"models","start":55,"end":61,"id":10},{"text":",","start":61,"end":62,"id":11},{"text":"using","start":63,"end":68,"id":12},{"text":"the","start":69,"end":72,"id":13},{"text":"Kullback","start":73,"end":81,"id":14},{"text":"-","start":81,"end":82,"id":15},{"text":"Leibler","start":82,"end":89,"id":16},{"text":"divergence","start":90,"end":100,"id":17},{"text":"between","start":101,"end":108,"id":18},{"text":"test","start":109,"end":113,"id":19},{"text":"sequences","start":114,"end":123,"id":20},{"text":"and","start":124,"end":127,"id":21},{"text":"model","start":128,"end":133,"id":22},{"text":"samples","start":134,"end":141,"id":23},{"text":",","start":141,"end":142,"id":24},{"text":"and","start":143,"end":146,"id":25},{"text":"show","start":147,"end":151,"id":26},{"text":"that","start":152,"end":156,"id":27},{"text":"our","start":157,"end":160,"id":28},{"text":"proposed","start":161,"end":169,"id":29},{"text":"methods","start":170,"end":177,"id":30},{"text":"match","start":178,"end":183,"id":31},{"text":"the","start":184,"end":187,"id":32},{"text":"statistics","start":188,"end":198,"id":33},{"text":"of","start":199,"end":201,"id":34},{"text":"the","start":202,"end":205,"id":35},{"text":"music","start":206,"end":211,"id":36},{"text":"genre","start":212,"end":217,"id":37},{"text":"significantly","start":218,"end":231,"id":38},{"text":"better","start":232,"end":238,"id":39},{"text":"than","start":239,"end":243,"id":40},{"text":"the","start":244,"end":247,"id":41},{"text":"VMM","start":248,"end":251,"id":42},{"text":".","start":251,"end":252,"id":43}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We show how to use a semiparametric Gaussian copula--or \"nonparanormal\"--for high dimensional inference.","_input_hash":-1958123589,"_task_hash":-1405093619,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"show","start":3,"end":7,"id":1},{"text":"how","start":8,"end":11,"id":2},{"text":"to","start":12,"end":14,"id":3},{"text":"use","start":15,"end":18,"id":4},{"text":"a","start":19,"end":20,"id":5},{"text":"semiparametric","start":21,"end":35,"id":6},{"text":"Gaussian","start":36,"end":44,"id":7},{"text":"copula","start":45,"end":51,"id":8},{"text":"--","start":51,"end":53,"id":9},{"text":"or","start":53,"end":55,"id":10},{"text":"\"","start":56,"end":57,"id":11},{"text":"nonparanormal\"--for","start":57,"end":76,"id":12},{"text":"high","start":77,"end":81,"id":13},{"text":"dimensional","start":82,"end":93,"id":14},{"text":"inference","start":94,"end":103,"id":15},{"text":".","start":103,"end":104,"id":16}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":21,"end":35,"token_start":6,"token_end":6,"label":"ALGO","answer":"reject"},{"start":36,"end":44,"token_start":7,"token_end":7,"label":"ALGO","answer":"reject"}],"answer":"reject"}
{"text":"Previous methods estimate a causal ordering of variables and their connection strengths based on a single dataset.","_input_hash":-743561154,"_task_hash":132665524,"tokens":[{"text":"Previous","start":0,"end":8,"id":0},{"text":"methods","start":9,"end":16,"id":1},{"text":"estimate","start":17,"end":25,"id":2},{"text":"a","start":26,"end":27,"id":3},{"text":"causal","start":28,"end":34,"id":4},{"text":"ordering","start":35,"end":43,"id":5},{"text":"of","start":44,"end":46,"id":6},{"text":"variables","start":47,"end":56,"id":7},{"text":"and","start":57,"end":60,"id":8},{"text":"their","start":61,"end":66,"id":9},{"text":"connection","start":67,"end":77,"id":10},{"text":"strengths","start":78,"end":87,"id":11},{"text":"based","start":88,"end":93,"id":12},{"text":"on","start":94,"end":96,"id":13},{"text":"a","start":97,"end":98,"id":14},{"text":"single","start":99,"end":105,"id":15},{"text":"dataset","start":106,"end":113,"id":16},{"text":".","start":113,"end":114,"id":17}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"However, the GPLVM is not trained as a density model and therefore yields bad density estimates.","_input_hash":1037458206,"_task_hash":-2085319297,"tokens":[{"text":"However","start":0,"end":7,"id":0},{"text":",","start":7,"end":8,"id":1},{"text":"the","start":9,"end":12,"id":2},{"text":"GPLVM","start":13,"end":18,"id":3},{"text":"is","start":19,"end":21,"id":4},{"text":"not","start":22,"end":25,"id":5},{"text":"trained","start":26,"end":33,"id":6},{"text":"as","start":34,"end":36,"id":7},{"text":"a","start":37,"end":38,"id":8},{"text":"density","start":39,"end":46,"id":9},{"text":"model","start":47,"end":52,"id":10},{"text":"and","start":53,"end":56,"id":11},{"text":"therefore","start":57,"end":66,"id":12},{"text":"yields","start":67,"end":73,"id":13},{"text":"bad","start":74,"end":77,"id":14},{"text":"density","start":78,"end":85,"id":15},{"text":"estimates","start":86,"end":95,"id":16},{"text":".","start":95,"end":96,"id":17}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Previous work focuses on the regression problem where conditions on the design matrix complicate the analysis.","_input_hash":135940515,"_task_hash":1543857818,"tokens":[{"text":"Previous","start":0,"end":8,"id":0},{"text":"work","start":9,"end":13,"id":1},{"text":"focuses","start":14,"end":21,"id":2},{"text":"on","start":22,"end":24,"id":3},{"text":"the","start":25,"end":28,"id":4},{"text":"regression","start":29,"end":39,"id":5},{"text":"problem","start":40,"end":47,"id":6},{"text":"where","start":48,"end":53,"id":7},{"text":"conditions","start":54,"end":64,"id":8},{"text":"on","start":65,"end":67,"id":9},{"text":"the","start":68,"end":71,"id":10},{"text":"design","start":72,"end":78,"id":11},{"text":"matrix","start":79,"end":85,"id":12},{"text":"complicate","start":86,"end":96,"id":13},{"text":"the","start":97,"end":100,"id":14},{"text":"analysis","start":101,"end":109,"id":15},{"text":".","start":109,"end":110,"id":16}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We slightly generalize the formalism introduced by \\cite{DUD03} to cover a large variety of cross-validation procedures including leave-one-out cross-validation, $k$-fold cross-validation, hold-out cross-validation (or split sample), and the leave-$\\upsilon$-out cross-validation.","_input_hash":217596268,"_task_hash":-1385420886,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"slightly","start":3,"end":11,"id":1},{"text":"generalize","start":12,"end":22,"id":2},{"text":"the","start":23,"end":26,"id":3},{"text":"formalism","start":27,"end":36,"id":4},{"text":"introduced","start":37,"end":47,"id":5},{"text":"by","start":48,"end":50,"id":6},{"text":"\\cite{DUD03","start":51,"end":62,"id":7},{"text":"}","start":62,"end":63,"id":8},{"text":"to","start":64,"end":66,"id":9},{"text":"cover","start":67,"end":72,"id":10},{"text":"a","start":73,"end":74,"id":11},{"text":"large","start":75,"end":80,"id":12},{"text":"variety","start":81,"end":88,"id":13},{"text":"of","start":89,"end":91,"id":14},{"text":"cross","start":92,"end":97,"id":15},{"text":"-","start":97,"end":98,"id":16},{"text":"validation","start":98,"end":108,"id":17},{"text":"procedures","start":109,"end":119,"id":18},{"text":"including","start":120,"end":129,"id":19},{"text":"leave","start":130,"end":135,"id":20},{"text":"-","start":135,"end":136,"id":21},{"text":"one","start":136,"end":139,"id":22},{"text":"-","start":139,"end":140,"id":23},{"text":"out","start":140,"end":143,"id":24},{"text":"cross","start":144,"end":149,"id":25},{"text":"-","start":149,"end":150,"id":26},{"text":"validation","start":150,"end":160,"id":27},{"text":",","start":160,"end":161,"id":28},{"text":"$","start":162,"end":163,"id":29},{"text":"k$-fold","start":163,"end":170,"id":30},{"text":"cross","start":171,"end":176,"id":31},{"text":"-","start":176,"end":177,"id":32},{"text":"validation","start":177,"end":187,"id":33},{"text":",","start":187,"end":188,"id":34},{"text":"hold","start":189,"end":193,"id":35},{"text":"-","start":193,"end":194,"id":36},{"text":"out","start":194,"end":197,"id":37},{"text":"cross","start":198,"end":203,"id":38},{"text":"-","start":203,"end":204,"id":39},{"text":"validation","start":204,"end":214,"id":40},{"text":"(","start":215,"end":216,"id":41},{"text":"or","start":216,"end":218,"id":42},{"text":"split","start":219,"end":224,"id":43},{"text":"sample","start":225,"end":231,"id":44},{"text":")","start":231,"end":232,"id":45},{"text":",","start":232,"end":233,"id":46},{"text":"and","start":234,"end":237,"id":47},{"text":"the","start":238,"end":241,"id":48},{"text":"leave-$\\upsilon$-out","start":242,"end":262,"id":49},{"text":"cross","start":263,"end":268,"id":50},{"text":"-","start":268,"end":269,"id":51},{"text":"validation","start":269,"end":279,"id":52},{"text":".","start":279,"end":280,"id":53}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":130,"end":160,"token_start":20,"token_end":27,"label":"ALGO","answer":"accept"},{"start":162,"end":187,"token_start":29,"token_end":33,"label":"ALGO","answer":"accept"},{"start":189,"end":214,"token_start":35,"token_end":40,"label":"ALGO","answer":"accept"},{"start":242,"end":279,"token_start":49,"token_end":52,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Existing results on its performance apply to discrete combinatorial optimization where the optimization variables can assume only a finite set of possible values.","_input_hash":-1427032138,"_task_hash":36898530,"tokens":[{"text":"Existing","start":0,"end":8,"id":0},{"text":"results","start":9,"end":16,"id":1},{"text":"on","start":17,"end":19,"id":2},{"text":"its","start":20,"end":23,"id":3},{"text":"performance","start":24,"end":35,"id":4},{"text":"apply","start":36,"end":41,"id":5},{"text":"to","start":42,"end":44,"id":6},{"text":"discrete","start":45,"end":53,"id":7},{"text":"combinatorial","start":54,"end":67,"id":8},{"text":"optimization","start":68,"end":80,"id":9},{"text":"where","start":81,"end":86,"id":10},{"text":"the","start":87,"end":90,"id":11},{"text":"optimization","start":91,"end":103,"id":12},{"text":"variables","start":104,"end":113,"id":13},{"text":"can","start":114,"end":117,"id":14},{"text":"assume","start":118,"end":124,"id":15},{"text":"only","start":125,"end":129,"id":16},{"text":"a","start":130,"end":131,"id":17},{"text":"finite","start":132,"end":138,"id":18},{"text":"set","start":139,"end":142,"id":19},{"text":"of","start":143,"end":145,"id":20},{"text":"possible","start":146,"end":154,"id":21},{"text":"values","start":155,"end":161,"id":22},{"text":".","start":161,"end":162,"id":23}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Particular attention is paid to penalties based on linear differential operators.","_input_hash":729542804,"_task_hash":-618845201,"tokens":[{"text":"Particular","start":0,"end":10,"id":0},{"text":"attention","start":11,"end":20,"id":1},{"text":"is","start":21,"end":23,"id":2},{"text":"paid","start":24,"end":28,"id":3},{"text":"to","start":29,"end":31,"id":4},{"text":"penalties","start":32,"end":41,"id":5},{"text":"based","start":42,"end":47,"id":6},{"text":"on","start":48,"end":50,"id":7},{"text":"linear","start":51,"end":57,"id":8},{"text":"differential","start":58,"end":70,"id":9},{"text":"operators","start":71,"end":80,"id":10},{"text":".","start":80,"end":81,"id":11}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We offer insights also on practical implications of precision of data measurement.","_input_hash":1024813203,"_task_hash":210994003,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"offer","start":3,"end":8,"id":1},{"text":"insights","start":9,"end":17,"id":2},{"text":"also","start":18,"end":22,"id":3},{"text":"on","start":23,"end":25,"id":4},{"text":"practical","start":26,"end":35,"id":5},{"text":"implications","start":36,"end":48,"id":6},{"text":"of","start":49,"end":51,"id":7},{"text":"precision","start":52,"end":61,"id":8},{"text":"of","start":62,"end":64,"id":9},{"text":"data","start":65,"end":69,"id":10},{"text":"measurement","start":70,"end":81,"id":11},{"text":".","start":81,"end":82,"id":12}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"In the proposed test procedure, the f-divergence between two probability densities is estimated using a density-ratio estimator.","_input_hash":-934151411,"_task_hash":-410209735,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"the","start":3,"end":6,"id":1},{"text":"proposed","start":7,"end":15,"id":2},{"text":"test","start":16,"end":20,"id":3},{"text":"procedure","start":21,"end":30,"id":4},{"text":",","start":30,"end":31,"id":5},{"text":"the","start":32,"end":35,"id":6},{"text":"f","start":36,"end":37,"id":7},{"text":"-","start":37,"end":38,"id":8},{"text":"divergence","start":38,"end":48,"id":9},{"text":"between","start":49,"end":56,"id":10},{"text":"two","start":57,"end":60,"id":11},{"text":"probability","start":61,"end":72,"id":12},{"text":"densities","start":73,"end":82,"id":13},{"text":"is","start":83,"end":85,"id":14},{"text":"estimated","start":86,"end":95,"id":15},{"text":"using","start":96,"end":101,"id":16},{"text":"a","start":102,"end":103,"id":17},{"text":"density","start":104,"end":111,"id":18},{"text":"-","start":111,"end":112,"id":19},{"text":"ratio","start":112,"end":117,"id":20},{"text":"estimator","start":118,"end":127,"id":21},{"text":".","start":127,"end":128,"id":22}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"Gaussian process priors are imposed over the latent functions and generalised inverse Gaussians on their associated weights.","_input_hash":1555538673,"_task_hash":-1520294206,"tokens":[{"text":"Gaussian","start":0,"end":8,"id":0},{"text":"process","start":9,"end":16,"id":1},{"text":"priors","start":17,"end":23,"id":2},{"text":"are","start":24,"end":27,"id":3},{"text":"imposed","start":28,"end":35,"id":4},{"text":"over","start":36,"end":40,"id":5},{"text":"the","start":41,"end":44,"id":6},{"text":"latent","start":45,"end":51,"id":7},{"text":"functions","start":52,"end":61,"id":8},{"text":"and","start":62,"end":65,"id":9},{"text":"generalised","start":66,"end":77,"id":10},{"text":"inverse","start":78,"end":85,"id":11},{"text":"Gaussians","start":86,"end":95,"id":12},{"text":"on","start":96,"end":98,"id":13},{"text":"their","start":99,"end":104,"id":14},{"text":"associated","start":105,"end":115,"id":15},{"text":"weights","start":116,"end":123,"id":16},{"text":".","start":123,"end":124,"id":17}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"These kernels are compared to other known kernels using a set of benchmark classification tasks carried out with support vector machines.","_input_hash":-1125535534,"_task_hash":-1230859159,"tokens":[{"text":"These","start":0,"end":5,"id":0},{"text":"kernels","start":6,"end":13,"id":1},{"text":"are","start":14,"end":17,"id":2},{"text":"compared","start":18,"end":26,"id":3},{"text":"to","start":27,"end":29,"id":4},{"text":"other","start":30,"end":35,"id":5},{"text":"known","start":36,"end":41,"id":6},{"text":"kernels","start":42,"end":49,"id":7},{"text":"using","start":50,"end":55,"id":8},{"text":"a","start":56,"end":57,"id":9},{"text":"set","start":58,"end":61,"id":10},{"text":"of","start":62,"end":64,"id":11},{"text":"benchmark","start":65,"end":74,"id":12},{"text":"classification","start":75,"end":89,"id":13},{"text":"tasks","start":90,"end":95,"id":14},{"text":"carried","start":96,"end":103,"id":15},{"text":"out","start":104,"end":107,"id":16},{"text":"with","start":108,"end":112,"id":17},{"text":"support","start":113,"end":120,"id":18},{"text":"vector","start":121,"end":127,"id":19},{"text":"machines","start":128,"end":136,"id":20},{"text":".","start":136,"end":137,"id":21}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Our experimental evaluation carried out on real HTTP and exploit traces confirms the tightness of our theoretical bounds and practicality of our protection mechanisms.","_input_hash":1836513628,"_task_hash":188608810,"tokens":[{"text":"Our","start":0,"end":3,"id":0},{"text":"experimental","start":4,"end":16,"id":1},{"text":"evaluation","start":17,"end":27,"id":2},{"text":"carried","start":28,"end":35,"id":3},{"text":"out","start":36,"end":39,"id":4},{"text":"on","start":40,"end":42,"id":5},{"text":"real","start":43,"end":47,"id":6},{"text":"HTTP","start":48,"end":52,"id":7},{"text":"and","start":53,"end":56,"id":8},{"text":"exploit","start":57,"end":64,"id":9},{"text":"traces","start":65,"end":71,"id":10},{"text":"confirms","start":72,"end":80,"id":11},{"text":"the","start":81,"end":84,"id":12},{"text":"tightness","start":85,"end":94,"id":13},{"text":"of","start":95,"end":97,"id":14},{"text":"our","start":98,"end":101,"id":15},{"text":"theoretical","start":102,"end":113,"id":16},{"text":"bounds","start":114,"end":120,"id":17},{"text":"and","start":121,"end":124,"id":18},{"text":"practicality","start":125,"end":137,"id":19},{"text":"of","start":138,"end":140,"id":20},{"text":"our","start":141,"end":144,"id":21},{"text":"protection","start":145,"end":155,"id":22},{"text":"mechanisms","start":156,"end":166,"id":23},{"text":".","start":166,"end":167,"id":24}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Moreover, we present a convergent optimization algorithm for solving regularized least squares with these penalty functions.","_input_hash":1839975856,"_task_hash":-542083507,"tokens":[{"text":"Moreover","start":0,"end":8,"id":0},{"text":",","start":8,"end":9,"id":1},{"text":"we","start":10,"end":12,"id":2},{"text":"present","start":13,"end":20,"id":3},{"text":"a","start":21,"end":22,"id":4},{"text":"convergent","start":23,"end":33,"id":5},{"text":"optimization","start":34,"end":46,"id":6},{"text":"algorithm","start":47,"end":56,"id":7},{"text":"for","start":57,"end":60,"id":8},{"text":"solving","start":61,"end":68,"id":9},{"text":"regularized","start":69,"end":80,"id":10},{"text":"least","start":81,"end":86,"id":11},{"text":"squares","start":87,"end":94,"id":12},{"text":"with","start":95,"end":99,"id":13},{"text":"these","start":100,"end":105,"id":14},{"text":"penalty","start":106,"end":113,"id":15},{"text":"functions","start":114,"end":123,"id":16},{"text":".","start":123,"end":124,"id":17}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":23,"end":46,"token_start":5,"token_end":6,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Finally, a new fast and robust ATR algorithm (developed in the present work) has been presented.","_input_hash":-596023086,"_task_hash":-2057639486,"tokens":[{"text":"Finally","start":0,"end":7,"id":0},{"text":",","start":7,"end":8,"id":1},{"text":"a","start":9,"end":10,"id":2},{"text":"new","start":11,"end":14,"id":3},{"text":"fast","start":15,"end":19,"id":4},{"text":"and","start":20,"end":23,"id":5},{"text":"robust","start":24,"end":30,"id":6},{"text":"ATR","start":31,"end":34,"id":7},{"text":"algorithm","start":35,"end":44,"id":8},{"text":"(","start":45,"end":46,"id":9},{"text":"developed","start":46,"end":55,"id":10},{"text":"in","start":56,"end":58,"id":11},{"text":"the","start":59,"end":62,"id":12},{"text":"present","start":63,"end":70,"id":13},{"text":"work","start":71,"end":75,"id":14},{"text":")","start":75,"end":76,"id":15},{"text":"has","start":77,"end":80,"id":16},{"text":"been","start":81,"end":85,"id":17},{"text":"presented","start":86,"end":95,"id":18},{"text":".","start":95,"end":96,"id":19}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":31,"end":34,"token_start":7,"token_end":7,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Specifically, we present an application to information retrieval in which documents are modeled as paths down a random tree, and the preferential attachment dynamics of the nCRP leads to clustering of documents according to sharing of topics at multiple levels of abstraction.","_input_hash":1951118404,"_task_hash":1289881453,"tokens":[{"text":"Specifically","start":0,"end":12,"id":0},{"text":",","start":12,"end":13,"id":1},{"text":"we","start":14,"end":16,"id":2},{"text":"present","start":17,"end":24,"id":3},{"text":"an","start":25,"end":27,"id":4},{"text":"application","start":28,"end":39,"id":5},{"text":"to","start":40,"end":42,"id":6},{"text":"information","start":43,"end":54,"id":7},{"text":"retrieval","start":55,"end":64,"id":8},{"text":"in","start":65,"end":67,"id":9},{"text":"which","start":68,"end":73,"id":10},{"text":"documents","start":74,"end":83,"id":11},{"text":"are","start":84,"end":87,"id":12},{"text":"modeled","start":88,"end":95,"id":13},{"text":"as","start":96,"end":98,"id":14},{"text":"paths","start":99,"end":104,"id":15},{"text":"down","start":105,"end":109,"id":16},{"text":"a","start":110,"end":111,"id":17},{"text":"random","start":112,"end":118,"id":18},{"text":"tree","start":119,"end":123,"id":19},{"text":",","start":123,"end":124,"id":20},{"text":"and","start":125,"end":128,"id":21},{"text":"the","start":129,"end":132,"id":22},{"text":"preferential","start":133,"end":145,"id":23},{"text":"attachment","start":146,"end":156,"id":24},{"text":"dynamics","start":157,"end":165,"id":25},{"text":"of","start":166,"end":168,"id":26},{"text":"the","start":169,"end":172,"id":27},{"text":"nCRP","start":173,"end":177,"id":28},{"text":"leads","start":178,"end":183,"id":29},{"text":"to","start":184,"end":186,"id":30},{"text":"clustering","start":187,"end":197,"id":31},{"text":"of","start":198,"end":200,"id":32},{"text":"documents","start":201,"end":210,"id":33},{"text":"according","start":211,"end":220,"id":34},{"text":"to","start":221,"end":223,"id":35},{"text":"sharing","start":224,"end":231,"id":36},{"text":"of","start":232,"end":234,"id":37},{"text":"topics","start":235,"end":241,"id":38},{"text":"at","start":242,"end":244,"id":39},{"text":"multiple","start":245,"end":253,"id":40},{"text":"levels","start":254,"end":260,"id":41},{"text":"of","start":261,"end":263,"id":42},{"text":"abstraction","start":264,"end":275,"id":43},{"text":".","start":275,"end":276,"id":44}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"The final classification of incoming input data is therefore defined as the stationary state of the meta-learning system using simple majority rule, yet the minority clusters that share opposite classification outcome can be observed in the system.","_input_hash":360348176,"_task_hash":53910799,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"final","start":4,"end":9,"id":1},{"text":"classification","start":10,"end":24,"id":2},{"text":"of","start":25,"end":27,"id":3},{"text":"incoming","start":28,"end":36,"id":4},{"text":"input","start":37,"end":42,"id":5},{"text":"data","start":43,"end":47,"id":6},{"text":"is","start":48,"end":50,"id":7},{"text":"therefore","start":51,"end":60,"id":8},{"text":"defined","start":61,"end":68,"id":9},{"text":"as","start":69,"end":71,"id":10},{"text":"the","start":72,"end":75,"id":11},{"text":"stationary","start":76,"end":86,"id":12},{"text":"state","start":87,"end":92,"id":13},{"text":"of","start":93,"end":95,"id":14},{"text":"the","start":96,"end":99,"id":15},{"text":"meta","start":100,"end":104,"id":16},{"text":"-","start":104,"end":105,"id":17},{"text":"learning","start":105,"end":113,"id":18},{"text":"system","start":114,"end":120,"id":19},{"text":"using","start":121,"end":126,"id":20},{"text":"simple","start":127,"end":133,"id":21},{"text":"majority","start":134,"end":142,"id":22},{"text":"rule","start":143,"end":147,"id":23},{"text":",","start":147,"end":148,"id":24},{"text":"yet","start":149,"end":152,"id":25},{"text":"the","start":153,"end":156,"id":26},{"text":"minority","start":157,"end":165,"id":27},{"text":"clusters","start":166,"end":174,"id":28},{"text":"that","start":175,"end":179,"id":29},{"text":"share","start":180,"end":185,"id":30},{"text":"opposite","start":186,"end":194,"id":31},{"text":"classification","start":195,"end":209,"id":32},{"text":"outcome","start":210,"end":217,"id":33},{"text":"can","start":218,"end":221,"id":34},{"text":"be","start":222,"end":224,"id":35},{"text":"observed","start":225,"end":233,"id":36},{"text":"in","start":234,"end":236,"id":37},{"text":"the","start":237,"end":240,"id":38},{"text":"system","start":241,"end":247,"id":39},{"text":".","start":247,"end":248,"id":40}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Finally, we present numerical results that demonstrate our claims.","_input_hash":896022460,"_task_hash":-851466995,"tokens":[{"text":"Finally","start":0,"end":7,"id":0},{"text":",","start":7,"end":8,"id":1},{"text":"we","start":9,"end":11,"id":2},{"text":"present","start":12,"end":19,"id":3},{"text":"numerical","start":20,"end":29,"id":4},{"text":"results","start":30,"end":37,"id":5},{"text":"that","start":38,"end":42,"id":6},{"text":"demonstrate","start":43,"end":54,"id":7},{"text":"our","start":55,"end":58,"id":8},{"text":"claims","start":59,"end":65,"id":9},{"text":".","start":65,"end":66,"id":10}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"The difficulty of dealing with these distributions is caused by the size of their domain, which is factorial in the number of considered entities ($n!$).","_input_hash":-102247442,"_task_hash":422470984,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"difficulty","start":4,"end":14,"id":1},{"text":"of","start":15,"end":17,"id":2},{"text":"dealing","start":18,"end":25,"id":3},{"text":"with","start":26,"end":30,"id":4},{"text":"these","start":31,"end":36,"id":5},{"text":"distributions","start":37,"end":50,"id":6},{"text":"is","start":51,"end":53,"id":7},{"text":"caused","start":54,"end":60,"id":8},{"text":"by","start":61,"end":63,"id":9},{"text":"the","start":64,"end":67,"id":10},{"text":"size","start":68,"end":72,"id":11},{"text":"of","start":73,"end":75,"id":12},{"text":"their","start":76,"end":81,"id":13},{"text":"domain","start":82,"end":88,"id":14},{"text":",","start":88,"end":89,"id":15},{"text":"which","start":90,"end":95,"id":16},{"text":"is","start":96,"end":98,"id":17},{"text":"factorial","start":99,"end":108,"id":18},{"text":"in","start":109,"end":111,"id":19},{"text":"the","start":112,"end":115,"id":20},{"text":"number","start":116,"end":122,"id":21},{"text":"of","start":123,"end":125,"id":22},{"text":"considered","start":126,"end":136,"id":23},{"text":"entities","start":137,"end":145,"id":24},{"text":"(","start":146,"end":147,"id":25},{"text":"$","start":147,"end":148,"id":26},{"text":"n!$","start":148,"end":151,"id":27},{"text":")","start":151,"end":152,"id":28},{"text":".","start":152,"end":153,"id":29}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"one chooses the number of clusters such that the corresponding clustering results are \"most stable\".","_input_hash":1276947594,"_task_hash":-855397707,"tokens":[{"text":"one","start":0,"end":3,"id":0},{"text":"chooses","start":4,"end":11,"id":1},{"text":"the","start":12,"end":15,"id":2},{"text":"number","start":16,"end":22,"id":3},{"text":"of","start":23,"end":25,"id":4},{"text":"clusters","start":26,"end":34,"id":5},{"text":"such","start":35,"end":39,"id":6},{"text":"that","start":40,"end":44,"id":7},{"text":"the","start":45,"end":48,"id":8},{"text":"corresponding","start":49,"end":62,"id":9},{"text":"clustering","start":63,"end":73,"id":10},{"text":"results","start":74,"end":81,"id":11},{"text":"are","start":82,"end":85,"id":12},{"text":"\"","start":86,"end":87,"id":13},{"text":"most","start":87,"end":91,"id":14},{"text":"stable","start":92,"end":98,"id":15},{"text":"\"","start":98,"end":99,"id":16},{"text":".","start":99,"end":100,"id":17}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":49,"end":73,"token_start":9,"token_end":10,"label":"ALGO","answer":"reject"}],"answer":"reject"}
{"text":"Just as additive models extend linear models by replacing linear functions with a set of one-dimensional smooth functions, the nonparanormal extends the normal by transforming the variables by smooth functions.","_input_hash":652053292,"_task_hash":1606718128,"tokens":[{"text":"Just","start":0,"end":4,"id":0},{"text":"as","start":5,"end":7,"id":1},{"text":"additive","start":8,"end":16,"id":2},{"text":"models","start":17,"end":23,"id":3},{"text":"extend","start":24,"end":30,"id":4},{"text":"linear","start":31,"end":37,"id":5},{"text":"models","start":38,"end":44,"id":6},{"text":"by","start":45,"end":47,"id":7},{"text":"replacing","start":48,"end":57,"id":8},{"text":"linear","start":58,"end":64,"id":9},{"text":"functions","start":65,"end":74,"id":10},{"text":"with","start":75,"end":79,"id":11},{"text":"a","start":80,"end":81,"id":12},{"text":"set","start":82,"end":85,"id":13},{"text":"of","start":86,"end":88,"id":14},{"text":"one","start":89,"end":92,"id":15},{"text":"-","start":92,"end":93,"id":16},{"text":"dimensional","start":93,"end":104,"id":17},{"text":"smooth","start":105,"end":111,"id":18},{"text":"functions","start":112,"end":121,"id":19},{"text":",","start":121,"end":122,"id":20},{"text":"the","start":123,"end":126,"id":21},{"text":"nonparanormal","start":127,"end":140,"id":22},{"text":"extends","start":141,"end":148,"id":23},{"text":"the","start":149,"end":152,"id":24},{"text":"normal","start":153,"end":159,"id":25},{"text":"by","start":160,"end":162,"id":26},{"text":"transforming","start":163,"end":175,"id":27},{"text":"the","start":176,"end":179,"id":28},{"text":"variables","start":180,"end":189,"id":29},{"text":"by","start":190,"end":192,"id":30},{"text":"smooth","start":193,"end":199,"id":31},{"text":"functions","start":200,"end":209,"id":32},{"text":".","start":209,"end":210,"id":33}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":24,"end":30,"token_start":4,"token_end":4,"label":"ALGO","answer":"reject"},{"start":163,"end":175,"token_start":27,"token_end":27,"label":"ALGO","answer":"reject"}],"answer":"reject"}
{"text":"Using this, we argue that there are situations in which the excess risk of our method is of order 1/n, while the excess risk of empirical risk minimization is of order 1/sqrt/{n}.","_input_hash":-1371932534,"_task_hash":1519199756,"tokens":[{"text":"Using","start":0,"end":5,"id":0},{"text":"this","start":6,"end":10,"id":1},{"text":",","start":10,"end":11,"id":2},{"text":"we","start":12,"end":14,"id":3},{"text":"argue","start":15,"end":20,"id":4},{"text":"that","start":21,"end":25,"id":5},{"text":"there","start":26,"end":31,"id":6},{"text":"are","start":32,"end":35,"id":7},{"text":"situations","start":36,"end":46,"id":8},{"text":"in","start":47,"end":49,"id":9},{"text":"which","start":50,"end":55,"id":10},{"text":"the","start":56,"end":59,"id":11},{"text":"excess","start":60,"end":66,"id":12},{"text":"risk","start":67,"end":71,"id":13},{"text":"of","start":72,"end":74,"id":14},{"text":"our","start":75,"end":78,"id":15},{"text":"method","start":79,"end":85,"id":16},{"text":"is","start":86,"end":88,"id":17},{"text":"of","start":89,"end":91,"id":18},{"text":"order","start":92,"end":97,"id":19},{"text":"1","start":98,"end":99,"id":20},{"text":"/","start":99,"end":100,"id":21},{"text":"n","start":100,"end":101,"id":22},{"text":",","start":101,"end":102,"id":23},{"text":"while","start":103,"end":108,"id":24},{"text":"the","start":109,"end":112,"id":25},{"text":"excess","start":113,"end":119,"id":26},{"text":"risk","start":120,"end":124,"id":27},{"text":"of","start":125,"end":127,"id":28},{"text":"empirical","start":128,"end":137,"id":29},{"text":"risk","start":138,"end":142,"id":30},{"text":"minimization","start":143,"end":155,"id":31},{"text":"is","start":156,"end":158,"id":32},{"text":"of","start":159,"end":161,"id":33},{"text":"order","start":162,"end":167,"id":34},{"text":"1","start":168,"end":169,"id":35},{"text":"/","start":169,"end":170,"id":36},{"text":"sqrt/{n","start":170,"end":177,"id":37},{"text":"}","start":177,"end":178,"id":38},{"text":".","start":178,"end":179,"id":39}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"We illustrate the approach by analyzing E. coli gene expression data and computing an entropy-based gene-association network from gene expression data.","_input_hash":-1542246212,"_task_hash":1128590524,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"illustrate","start":3,"end":13,"id":1},{"text":"the","start":14,"end":17,"id":2},{"text":"approach","start":18,"end":26,"id":3},{"text":"by","start":27,"end":29,"id":4},{"text":"analyzing","start":30,"end":39,"id":5},{"text":"E.","start":40,"end":42,"id":6},{"text":"coli","start":43,"end":47,"id":7},{"text":"gene","start":48,"end":52,"id":8},{"text":"expression","start":53,"end":63,"id":9},{"text":"data","start":64,"end":68,"id":10},{"text":"and","start":69,"end":72,"id":11},{"text":"computing","start":73,"end":82,"id":12},{"text":"an","start":83,"end":85,"id":13},{"text":"entropy","start":86,"end":93,"id":14},{"text":"-","start":93,"end":94,"id":15},{"text":"based","start":94,"end":99,"id":16},{"text":"gene","start":100,"end":104,"id":17},{"text":"-","start":104,"end":105,"id":18},{"text":"association","start":105,"end":116,"id":19},{"text":"network","start":117,"end":124,"id":20},{"text":"from","start":125,"end":129,"id":21},{"text":"gene","start":130,"end":134,"id":22},{"text":"expression","start":135,"end":145,"id":23},{"text":"data","start":146,"end":150,"id":24},{"text":".","start":150,"end":151,"id":25}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"In a complex systems perspective, the decisional states are thus the \"emerging\" patterns corresponding to the utility function.","_input_hash":1018471510,"_task_hash":1819015503,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"a","start":3,"end":4,"id":1},{"text":"complex","start":5,"end":12,"id":2},{"text":"systems","start":13,"end":20,"id":3},{"text":"perspective","start":21,"end":32,"id":4},{"text":",","start":32,"end":33,"id":5},{"text":"the","start":34,"end":37,"id":6},{"text":"decisional","start":38,"end":48,"id":7},{"text":"states","start":49,"end":55,"id":8},{"text":"are","start":56,"end":59,"id":9},{"text":"thus","start":60,"end":64,"id":10},{"text":"the","start":65,"end":68,"id":11},{"text":"\"","start":69,"end":70,"id":12},{"text":"emerging","start":70,"end":78,"id":13},{"text":"\"","start":78,"end":79,"id":14},{"text":"patterns","start":80,"end":88,"id":15},{"text":"corresponding","start":89,"end":102,"id":16},{"text":"to","start":103,"end":105,"id":17},{"text":"the","start":106,"end":109,"id":18},{"text":"utility","start":110,"end":117,"id":19},{"text":"function","start":118,"end":126,"id":20},{"text":".","start":126,"end":127,"id":21}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"The local linear embedding algorithm (LLE) is a non-linear dimension-reducing technique, widely used due to its computational simplicity and intuitive approach.","_input_hash":-1398033812,"_task_hash":-2121668117,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"local","start":4,"end":9,"id":1},{"text":"linear","start":10,"end":16,"id":2},{"text":"embedding","start":17,"end":26,"id":3},{"text":"algorithm","start":27,"end":36,"id":4},{"text":"(","start":37,"end":38,"id":5},{"text":"LLE","start":38,"end":41,"id":6},{"text":")","start":41,"end":42,"id":7},{"text":"is","start":43,"end":45,"id":8},{"text":"a","start":46,"end":47,"id":9},{"text":"non","start":48,"end":51,"id":10},{"text":"-","start":51,"end":52,"id":11},{"text":"linear","start":52,"end":58,"id":12},{"text":"dimension","start":59,"end":68,"id":13},{"text":"-","start":68,"end":69,"id":14},{"text":"reducing","start":69,"end":77,"id":15},{"text":"technique","start":78,"end":87,"id":16},{"text":",","start":87,"end":88,"id":17},{"text":"widely","start":89,"end":95,"id":18},{"text":"used","start":96,"end":100,"id":19},{"text":"due","start":101,"end":104,"id":20},{"text":"to","start":105,"end":107,"id":21},{"text":"its","start":108,"end":111,"id":22},{"text":"computational","start":112,"end":125,"id":23},{"text":"simplicity","start":126,"end":136,"id":24},{"text":"and","start":137,"end":140,"id":25},{"text":"intuitive","start":141,"end":150,"id":26},{"text":"approach","start":151,"end":159,"id":27},{"text":".","start":159,"end":160,"id":28}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":4,"end":26,"token_start":1,"token_end":3,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"DILN is a generalization of the hierarchical Dirichlet process (HDP) that models correlation structure between the weights of the atoms at the group level.","_input_hash":1071219786,"_task_hash":-24188743,"tokens":[{"text":"DILN","start":0,"end":4,"id":0},{"text":"is","start":5,"end":7,"id":1},{"text":"a","start":8,"end":9,"id":2},{"text":"generalization","start":10,"end":24,"id":3},{"text":"of","start":25,"end":27,"id":4},{"text":"the","start":28,"end":31,"id":5},{"text":"hierarchical","start":32,"end":44,"id":6},{"text":"Dirichlet","start":45,"end":54,"id":7},{"text":"process","start":55,"end":62,"id":8},{"text":"(","start":63,"end":64,"id":9},{"text":"HDP","start":64,"end":67,"id":10},{"text":")","start":67,"end":68,"id":11},{"text":"that","start":69,"end":73,"id":12},{"text":"models","start":74,"end":80,"id":13},{"text":"correlation","start":81,"end":92,"id":14},{"text":"structure","start":93,"end":102,"id":15},{"text":"between","start":103,"end":110,"id":16},{"text":"the","start":111,"end":114,"id":17},{"text":"weights","start":115,"end":122,"id":18},{"text":"of","start":123,"end":125,"id":19},{"text":"the","start":126,"end":129,"id":20},{"text":"atoms","start":130,"end":135,"id":21},{"text":"at","start":136,"end":138,"id":22},{"text":"the","start":139,"end":142,"id":23},{"text":"group","start":143,"end":148,"id":24},{"text":"level","start":149,"end":154,"id":25},{"text":".","start":154,"end":155,"id":26}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":32,"end":54,"token_start":6,"token_end":7,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Recently, methods using additive noise models have been suggested to approach the case of continuous variables.","_input_hash":-1397565243,"_task_hash":-126148105,"tokens":[{"text":"Recently","start":0,"end":8,"id":0},{"text":",","start":8,"end":9,"id":1},{"text":"methods","start":10,"end":17,"id":2},{"text":"using","start":18,"end":23,"id":3},{"text":"additive","start":24,"end":32,"id":4},{"text":"noise","start":33,"end":38,"id":5},{"text":"models","start":39,"end":45,"id":6},{"text":"have","start":46,"end":50,"id":7},{"text":"been","start":51,"end":55,"id":8},{"text":"suggested","start":56,"end":65,"id":9},{"text":"to","start":66,"end":68,"id":10},{"text":"approach","start":69,"end":77,"id":11},{"text":"the","start":78,"end":81,"id":12},{"text":"case","start":82,"end":86,"id":13},{"text":"of","start":87,"end":89,"id":14},{"text":"continuous","start":90,"end":100,"id":15},{"text":"variables","start":101,"end":110,"id":16},{"text":".","start":110,"end":111,"id":17}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":24,"end":38,"token_start":4,"token_end":5,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We operate on the primal problem and use a subtle variation of block-coordinate-methods which drastically reduces the computational complexity by orders of magnitude.","_input_hash":1360799698,"_task_hash":811780817,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"operate","start":3,"end":10,"id":1},{"text":"on","start":11,"end":13,"id":2},{"text":"the","start":14,"end":17,"id":3},{"text":"primal","start":18,"end":24,"id":4},{"text":"problem","start":25,"end":32,"id":5},{"text":"and","start":33,"end":36,"id":6},{"text":"use","start":37,"end":40,"id":7},{"text":"a","start":41,"end":42,"id":8},{"text":"subtle","start":43,"end":49,"id":9},{"text":"variation","start":50,"end":59,"id":10},{"text":"of","start":60,"end":62,"id":11},{"text":"block","start":63,"end":68,"id":12},{"text":"-","start":68,"end":69,"id":13},{"text":"coordinate","start":69,"end":79,"id":14},{"text":"-","start":79,"end":80,"id":15},{"text":"methods","start":80,"end":87,"id":16},{"text":"which","start":88,"end":93,"id":17},{"text":"drastically","start":94,"end":105,"id":18},{"text":"reduces","start":106,"end":113,"id":19},{"text":"the","start":114,"end":117,"id":20},{"text":"computational","start":118,"end":131,"id":21},{"text":"complexity","start":132,"end":142,"id":22},{"text":"by","start":143,"end":145,"id":23},{"text":"orders","start":146,"end":152,"id":24},{"text":"of","start":153,"end":155,"id":25},{"text":"magnitude","start":156,"end":165,"id":26},{"text":".","start":165,"end":166,"id":27}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"This article introduces both a new algorithm for reconstructing epsilon-machines from data, as well as the decisional states.","_input_hash":-318982839,"_task_hash":-104927174,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"article","start":5,"end":12,"id":1},{"text":"introduces","start":13,"end":23,"id":2},{"text":"both","start":24,"end":28,"id":3},{"text":"a","start":29,"end":30,"id":4},{"text":"new","start":31,"end":34,"id":5},{"text":"algorithm","start":35,"end":44,"id":6},{"text":"for","start":45,"end":48,"id":7},{"text":"reconstructing","start":49,"end":63,"id":8},{"text":"epsilon","start":64,"end":71,"id":9},{"text":"-","start":71,"end":72,"id":10},{"text":"machines","start":72,"end":80,"id":11},{"text":"from","start":81,"end":85,"id":12},{"text":"data","start":86,"end":90,"id":13},{"text":",","start":90,"end":91,"id":14},{"text":"as","start":92,"end":94,"id":15},{"text":"well","start":95,"end":99,"id":16},{"text":"as","start":100,"end":102,"id":17},{"text":"the","start":103,"end":106,"id":18},{"text":"decisional","start":107,"end":117,"id":19},{"text":"states","start":118,"end":124,"id":20},{"text":".","start":124,"end":125,"id":21}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":64,"end":80,"token_start":9,"token_end":11,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We rectify this omission, providing high probability concentration results for the proposed estimator and deriving corresponding generalization bounds.","_input_hash":164707853,"_task_hash":1915050500,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"rectify","start":3,"end":10,"id":1},{"text":"this","start":11,"end":15,"id":2},{"text":"omission","start":16,"end":24,"id":3},{"text":",","start":24,"end":25,"id":4},{"text":"providing","start":26,"end":35,"id":5},{"text":"high","start":36,"end":40,"id":6},{"text":"probability","start":41,"end":52,"id":7},{"text":"concentration","start":53,"end":66,"id":8},{"text":"results","start":67,"end":74,"id":9},{"text":"for","start":75,"end":78,"id":10},{"text":"the","start":79,"end":82,"id":11},{"text":"proposed","start":83,"end":91,"id":12},{"text":"estimator","start":92,"end":101,"id":13},{"text":"and","start":102,"end":105,"id":14},{"text":"deriving","start":106,"end":114,"id":15},{"text":"corresponding","start":115,"end":128,"id":16},{"text":"generalization","start":129,"end":143,"id":17},{"text":"bounds","start":144,"end":150,"id":18},{"text":".","start":150,"end":151,"id":19}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"It is known that there exist $p$-adic representation of dendrograms.","_input_hash":-2145759776,"_task_hash":-591247322,"tokens":[{"text":"It","start":0,"end":2,"id":0},{"text":"is","start":3,"end":5,"id":1},{"text":"known","start":6,"end":11,"id":2},{"text":"that","start":12,"end":16,"id":3},{"text":"there","start":17,"end":22,"id":4},{"text":"exist","start":23,"end":28,"id":5},{"text":"$","start":29,"end":30,"id":6},{"text":"p$-adic","start":30,"end":37,"id":7},{"text":"representation","start":38,"end":52,"id":8},{"text":"of","start":53,"end":55,"id":9},{"text":"dendrograms","start":56,"end":67,"id":10},{"text":".","start":67,"end":68,"id":11}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"As in other formulations such as the LASSO and forward step-wise regression we are interested in sparsifying the feature set while preserving generalization ability.","_input_hash":872316592,"_task_hash":367886105,"tokens":[{"text":"As","start":0,"end":2,"id":0},{"text":"in","start":3,"end":5,"id":1},{"text":"other","start":6,"end":11,"id":2},{"text":"formulations","start":12,"end":24,"id":3},{"text":"such","start":25,"end":29,"id":4},{"text":"as","start":30,"end":32,"id":5},{"text":"the","start":33,"end":36,"id":6},{"text":"LASSO","start":37,"end":42,"id":7},{"text":"and","start":43,"end":46,"id":8},{"text":"forward","start":47,"end":54,"id":9},{"text":"step","start":55,"end":59,"id":10},{"text":"-","start":59,"end":60,"id":11},{"text":"wise","start":60,"end":64,"id":12},{"text":"regression","start":65,"end":75,"id":13},{"text":"we","start":76,"end":78,"id":14},{"text":"are","start":79,"end":82,"id":15},{"text":"interested","start":83,"end":93,"id":16},{"text":"in","start":94,"end":96,"id":17},{"text":"sparsifying","start":97,"end":108,"id":18},{"text":"the","start":109,"end":112,"id":19},{"text":"feature","start":113,"end":120,"id":20},{"text":"set","start":121,"end":124,"id":21},{"text":"while","start":125,"end":130,"id":22},{"text":"preserving","start":131,"end":141,"id":23},{"text":"generalization","start":142,"end":156,"id":24},{"text":"ability","start":157,"end":164,"id":25},{"text":".","start":164,"end":165,"id":26}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":37,"end":42,"token_start":7,"token_end":7,"label":"ALGO","answer":"accept"},{"start":47,"end":75,"token_start":9,"token_end":13,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We focus on MKL with block-l1 regularization (inducing sparse kernel combination), block-l2 regularization (inducing uniform kernel combination), and elastic-net regularization (including both block-l1 and block-l2 regularization).","_input_hash":1878284019,"_task_hash":-258321964,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"focus","start":3,"end":8,"id":1},{"text":"on","start":9,"end":11,"id":2},{"text":"MKL","start":12,"end":15,"id":3},{"text":"with","start":16,"end":20,"id":4},{"text":"block","start":21,"end":26,"id":5},{"text":"-","start":26,"end":27,"id":6},{"text":"l1","start":27,"end":29,"id":7},{"text":"regularization","start":30,"end":44,"id":8},{"text":"(","start":45,"end":46,"id":9},{"text":"inducing","start":46,"end":54,"id":10},{"text":"sparse","start":55,"end":61,"id":11},{"text":"kernel","start":62,"end":68,"id":12},{"text":"combination","start":69,"end":80,"id":13},{"text":")","start":80,"end":81,"id":14},{"text":",","start":81,"end":82,"id":15},{"text":"block","start":83,"end":88,"id":16},{"text":"-","start":88,"end":89,"id":17},{"text":"l2","start":89,"end":91,"id":18},{"text":"regularization","start":92,"end":106,"id":19},{"text":"(","start":107,"end":108,"id":20},{"text":"inducing","start":108,"end":116,"id":21},{"text":"uniform","start":117,"end":124,"id":22},{"text":"kernel","start":125,"end":131,"id":23},{"text":"combination","start":132,"end":143,"id":24},{"text":")","start":143,"end":144,"id":25},{"text":",","start":144,"end":145,"id":26},{"text":"and","start":146,"end":149,"id":27},{"text":"elastic","start":150,"end":157,"id":28},{"text":"-","start":157,"end":158,"id":29},{"text":"net","start":158,"end":161,"id":30},{"text":"regularization","start":162,"end":176,"id":31},{"text":"(","start":177,"end":178,"id":32},{"text":"including","start":178,"end":187,"id":33},{"text":"both","start":188,"end":192,"id":34},{"text":"block","start":193,"end":198,"id":35},{"text":"-","start":198,"end":199,"id":36},{"text":"l1","start":199,"end":201,"id":37},{"text":"and","start":202,"end":205,"id":38},{"text":"block","start":206,"end":211,"id":39},{"text":"-","start":211,"end":212,"id":40},{"text":"l2","start":212,"end":214,"id":41},{"text":"regularization","start":215,"end":229,"id":42},{"text":")","start":229,"end":230,"id":43},{"text":".","start":230,"end":231,"id":44}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Recently, it was shown that use of non-Gaussianity identifies the full structure of a linear acyclic model, i.e., a causal ordering of variables and their connection strengths, without using any prior knowledge on the network structure, which is not the case with conventional methods.","_input_hash":-392422549,"_task_hash":-403750533,"tokens":[{"text":"Recently","start":0,"end":8,"id":0},{"text":",","start":8,"end":9,"id":1},{"text":"it","start":10,"end":12,"id":2},{"text":"was","start":13,"end":16,"id":3},{"text":"shown","start":17,"end":22,"id":4},{"text":"that","start":23,"end":27,"id":5},{"text":"use","start":28,"end":31,"id":6},{"text":"of","start":32,"end":34,"id":7},{"text":"non","start":35,"end":38,"id":8},{"text":"-","start":38,"end":39,"id":9},{"text":"Gaussianity","start":39,"end":50,"id":10},{"text":"identifies","start":51,"end":61,"id":11},{"text":"the","start":62,"end":65,"id":12},{"text":"full","start":66,"end":70,"id":13},{"text":"structure","start":71,"end":80,"id":14},{"text":"of","start":81,"end":83,"id":15},{"text":"a","start":84,"end":85,"id":16},{"text":"linear","start":86,"end":92,"id":17},{"text":"acyclic","start":93,"end":100,"id":18},{"text":"model","start":101,"end":106,"id":19},{"text":",","start":106,"end":107,"id":20},{"text":"i.e.","start":108,"end":112,"id":21},{"text":",","start":112,"end":113,"id":22},{"text":"a","start":114,"end":115,"id":23},{"text":"causal","start":116,"end":122,"id":24},{"text":"ordering","start":123,"end":131,"id":25},{"text":"of","start":132,"end":134,"id":26},{"text":"variables","start":135,"end":144,"id":27},{"text":"and","start":145,"end":148,"id":28},{"text":"their","start":149,"end":154,"id":29},{"text":"connection","start":155,"end":165,"id":30},{"text":"strengths","start":166,"end":175,"id":31},{"text":",","start":175,"end":176,"id":32},{"text":"without","start":177,"end":184,"id":33},{"text":"using","start":185,"end":190,"id":34},{"text":"any","start":191,"end":194,"id":35},{"text":"prior","start":195,"end":200,"id":36},{"text":"knowledge","start":201,"end":210,"id":37},{"text":"on","start":211,"end":213,"id":38},{"text":"the","start":214,"end":217,"id":39},{"text":"network","start":218,"end":225,"id":40},{"text":"structure","start":226,"end":235,"id":41},{"text":",","start":235,"end":236,"id":42},{"text":"which","start":237,"end":242,"id":43},{"text":"is","start":243,"end":245,"id":44},{"text":"not","start":246,"end":249,"id":45},{"text":"the","start":250,"end":253,"id":46},{"text":"case","start":254,"end":258,"id":47},{"text":"with","start":259,"end":263,"id":48},{"text":"conventional","start":264,"end":276,"id":49},{"text":"methods","start":277,"end":284,"id":50},{"text":".","start":284,"end":285,"id":51}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":86,"end":100,"token_start":17,"token_end":18,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Experimentally, SLIM performs equally well or better than LiNGAM with comparable computational complexity.","_input_hash":-1884239515,"_task_hash":1587891882,"tokens":[{"text":"Experimentally","start":0,"end":14,"id":0},{"text":",","start":14,"end":15,"id":1},{"text":"SLIM","start":16,"end":20,"id":2},{"text":"performs","start":21,"end":29,"id":3},{"text":"equally","start":30,"end":37,"id":4},{"text":"well","start":38,"end":42,"id":5},{"text":"or","start":43,"end":45,"id":6},{"text":"better","start":46,"end":52,"id":7},{"text":"than","start":53,"end":57,"id":8},{"text":"LiNGAM","start":58,"end":64,"id":9},{"text":"with","start":65,"end":69,"id":10},{"text":"comparable","start":70,"end":80,"id":11},{"text":"computational","start":81,"end":94,"id":12},{"text":"complexity","start":95,"end":105,"id":13},{"text":".","start":105,"end":106,"id":14}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":58,"end":64,"token_start":9,"token_end":9,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"This series expansion is shown to be useful for numerical calculations of the JSD, when the probability distributions are nearly equal, and for which, consequently, small numerical errors dominate evaluation.","_input_hash":-508395004,"_task_hash":296377828,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"series","start":5,"end":11,"id":1},{"text":"expansion","start":12,"end":21,"id":2},{"text":"is","start":22,"end":24,"id":3},{"text":"shown","start":25,"end":30,"id":4},{"text":"to","start":31,"end":33,"id":5},{"text":"be","start":34,"end":36,"id":6},{"text":"useful","start":37,"end":43,"id":7},{"text":"for","start":44,"end":47,"id":8},{"text":"numerical","start":48,"end":57,"id":9},{"text":"calculations","start":58,"end":70,"id":10},{"text":"of","start":71,"end":73,"id":11},{"text":"the","start":74,"end":77,"id":12},{"text":"JSD","start":78,"end":81,"id":13},{"text":",","start":81,"end":82,"id":14},{"text":"when","start":83,"end":87,"id":15},{"text":"the","start":88,"end":91,"id":16},{"text":"probability","start":92,"end":103,"id":17},{"text":"distributions","start":104,"end":117,"id":18},{"text":"are","start":118,"end":121,"id":19},{"text":"nearly","start":122,"end":128,"id":20},{"text":"equal","start":129,"end":134,"id":21},{"text":",","start":134,"end":135,"id":22},{"text":"and","start":136,"end":139,"id":23},{"text":"for","start":140,"end":143,"id":24},{"text":"which","start":144,"end":149,"id":25},{"text":",","start":149,"end":150,"id":26},{"text":"consequently","start":151,"end":163,"id":27},{"text":",","start":163,"end":164,"id":28},{"text":"small","start":165,"end":170,"id":29},{"text":"numerical","start":171,"end":180,"id":30},{"text":"errors","start":181,"end":187,"id":31},{"text":"dominate","start":188,"end":196,"id":32},{"text":"evaluation","start":197,"end":207,"id":33},{"text":".","start":207,"end":208,"id":34}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"CDOM is usually formulated as finding a mapping from objects in one domain (photos) to objects in the other domain (frame) so that the pairwise dependency is maximized.","_input_hash":-403488014,"_task_hash":1433263164,"tokens":[{"text":"CDOM","start":0,"end":4,"id":0},{"text":"is","start":5,"end":7,"id":1},{"text":"usually","start":8,"end":15,"id":2},{"text":"formulated","start":16,"end":26,"id":3},{"text":"as","start":27,"end":29,"id":4},{"text":"finding","start":30,"end":37,"id":5},{"text":"a","start":38,"end":39,"id":6},{"text":"mapping","start":40,"end":47,"id":7},{"text":"from","start":48,"end":52,"id":8},{"text":"objects","start":53,"end":60,"id":9},{"text":"in","start":61,"end":63,"id":10},{"text":"one","start":64,"end":67,"id":11},{"text":"domain","start":68,"end":74,"id":12},{"text":"(","start":75,"end":76,"id":13},{"text":"photos","start":76,"end":82,"id":14},{"text":")","start":82,"end":83,"id":15},{"text":"to","start":84,"end":86,"id":16},{"text":"objects","start":87,"end":94,"id":17},{"text":"in","start":95,"end":97,"id":18},{"text":"the","start":98,"end":101,"id":19},{"text":"other","start":102,"end":107,"id":20},{"text":"domain","start":108,"end":114,"id":21},{"text":"(","start":115,"end":116,"id":22},{"text":"frame","start":116,"end":121,"id":23},{"text":")","start":121,"end":122,"id":24},{"text":"so","start":123,"end":125,"id":25},{"text":"that","start":126,"end":130,"id":26},{"text":"the","start":131,"end":134,"id":27},{"text":"pairwise","start":135,"end":143,"id":28},{"text":"dependency","start":144,"end":154,"id":29},{"text":"is","start":155,"end":157,"id":30},{"text":"maximized","start":158,"end":167,"id":31},{"text":".","start":167,"end":168,"id":32}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"The distance metric plays an important role in nearest neighbor (NN) classification.","_input_hash":533487841,"_task_hash":-1008111325,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"distance","start":4,"end":12,"id":1},{"text":"metric","start":13,"end":19,"id":2},{"text":"plays","start":20,"end":25,"id":3},{"text":"an","start":26,"end":28,"id":4},{"text":"important","start":29,"end":38,"id":5},{"text":"role","start":39,"end":43,"id":6},{"text":"in","start":44,"end":46,"id":7},{"text":"nearest","start":47,"end":54,"id":8},{"text":"neighbor","start":55,"end":63,"id":9},{"text":"(","start":64,"end":65,"id":10},{"text":"NN","start":65,"end":67,"id":11},{"text":")","start":67,"end":68,"id":12},{"text":"classification","start":69,"end":83,"id":13},{"text":".","start":83,"end":84,"id":14}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":47,"end":63,"token_start":8,"token_end":9,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We characterise when margin losses can be proper composite losses, explicitly show how to determine a symmetric loss in full from half of one of its partial losses, introduce an intrinsic parametrisation of composite binary losses and give a complete characterisation of the relationship between proper losses and ``classification calibrated'' losses.","_input_hash":-1876321540,"_task_hash":-1041933555,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"characterise","start":3,"end":15,"id":1},{"text":"when","start":16,"end":20,"id":2},{"text":"margin","start":21,"end":27,"id":3},{"text":"losses","start":28,"end":34,"id":4},{"text":"can","start":35,"end":38,"id":5},{"text":"be","start":39,"end":41,"id":6},{"text":"proper","start":42,"end":48,"id":7},{"text":"composite","start":49,"end":58,"id":8},{"text":"losses","start":59,"end":65,"id":9},{"text":",","start":65,"end":66,"id":10},{"text":"explicitly","start":67,"end":77,"id":11},{"text":"show","start":78,"end":82,"id":12},{"text":"how","start":83,"end":86,"id":13},{"text":"to","start":87,"end":89,"id":14},{"text":"determine","start":90,"end":99,"id":15},{"text":"a","start":100,"end":101,"id":16},{"text":"symmetric","start":102,"end":111,"id":17},{"text":"loss","start":112,"end":116,"id":18},{"text":"in","start":117,"end":119,"id":19},{"text":"full","start":120,"end":124,"id":20},{"text":"from","start":125,"end":129,"id":21},{"text":"half","start":130,"end":134,"id":22},{"text":"of","start":135,"end":137,"id":23},{"text":"one","start":138,"end":141,"id":24},{"text":"of","start":142,"end":144,"id":25},{"text":"its","start":145,"end":148,"id":26},{"text":"partial","start":149,"end":156,"id":27},{"text":"losses","start":157,"end":163,"id":28},{"text":",","start":163,"end":164,"id":29},{"text":"introduce","start":165,"end":174,"id":30},{"text":"an","start":175,"end":177,"id":31},{"text":"intrinsic","start":178,"end":187,"id":32},{"text":"parametrisation","start":188,"end":203,"id":33},{"text":"of","start":204,"end":206,"id":34},{"text":"composite","start":207,"end":216,"id":35},{"text":"binary","start":217,"end":223,"id":36},{"text":"losses","start":224,"end":230,"id":37},{"text":"and","start":231,"end":234,"id":38},{"text":"give","start":235,"end":239,"id":39},{"text":"a","start":240,"end":241,"id":40},{"text":"complete","start":242,"end":250,"id":41},{"text":"characterisation","start":251,"end":267,"id":42},{"text":"of","start":268,"end":270,"id":43},{"text":"the","start":271,"end":274,"id":44},{"text":"relationship","start":275,"end":287,"id":45},{"text":"between","start":288,"end":295,"id":46},{"text":"proper","start":296,"end":302,"id":47},{"text":"losses","start":303,"end":309,"id":48},{"text":"and","start":310,"end":313,"id":49},{"text":"`","start":314,"end":315,"id":50},{"text":"`","start":315,"end":316,"id":51},{"text":"classification","start":316,"end":330,"id":52},{"text":"calibrated","start":331,"end":341,"id":53},{"text":"'","start":341,"end":342,"id":54},{"text":"'","start":342,"end":343,"id":55},{"text":"losses","start":344,"end":350,"id":56},{"text":".","start":350,"end":351,"id":57}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"screening for variables with large correlations within a single treatment (autocorrelation screening);","_input_hash":233053007,"_task_hash":1161262760,"tokens":[{"text":"screening","start":0,"end":9,"id":0},{"text":"for","start":10,"end":13,"id":1},{"text":"variables","start":14,"end":23,"id":2},{"text":"with","start":24,"end":28,"id":3},{"text":"large","start":29,"end":34,"id":4},{"text":"correlations","start":35,"end":47,"id":5},{"text":"within","start":48,"end":54,"id":6},{"text":"a","start":55,"end":56,"id":7},{"text":"single","start":57,"end":63,"id":8},{"text":"treatment","start":64,"end":73,"id":9},{"text":"(","start":74,"end":75,"id":10},{"text":"autocorrelation","start":75,"end":90,"id":11},{"text":"screening","start":91,"end":100,"id":12},{"text":")","start":100,"end":101,"id":13},{"text":";","start":101,"end":102,"id":14}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We study the problem of estimating from data, a sparse approximation to the inverse covariance matrix.","_input_hash":1998688333,"_task_hash":-1210427739,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"study","start":3,"end":8,"id":1},{"text":"the","start":9,"end":12,"id":2},{"text":"problem","start":13,"end":20,"id":3},{"text":"of","start":21,"end":23,"id":4},{"text":"estimating","start":24,"end":34,"id":5},{"text":"from","start":35,"end":39,"id":6},{"text":"data","start":40,"end":44,"id":7},{"text":",","start":44,"end":45,"id":8},{"text":"a","start":46,"end":47,"id":9},{"text":"sparse","start":48,"end":54,"id":10},{"text":"approximation","start":55,"end":68,"id":11},{"text":"to","start":69,"end":71,"id":12},{"text":"the","start":72,"end":75,"id":13},{"text":"inverse","start":76,"end":83,"id":14},{"text":"covariance","start":84,"end":94,"id":15},{"text":"matrix","start":95,"end":101,"id":16},{"text":".","start":101,"end":102,"id":17}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Nonparametric methods are widely applicable to statistical inference problems, since they rely on a few modeling assumptions.","_input_hash":-727319785,"_task_hash":-966605982,"tokens":[{"text":"Nonparametric","start":0,"end":13,"id":0},{"text":"methods","start":14,"end":21,"id":1},{"text":"are","start":22,"end":25,"id":2},{"text":"widely","start":26,"end":32,"id":3},{"text":"applicable","start":33,"end":43,"id":4},{"text":"to","start":44,"end":46,"id":5},{"text":"statistical","start":47,"end":58,"id":6},{"text":"inference","start":59,"end":68,"id":7},{"text":"problems","start":69,"end":77,"id":8},{"text":",","start":77,"end":78,"id":9},{"text":"since","start":79,"end":84,"id":10},{"text":"they","start":85,"end":89,"id":11},{"text":"rely","start":90,"end":94,"id":12},{"text":"on","start":95,"end":97,"id":13},{"text":"a","start":98,"end":99,"id":14},{"text":"few","start":100,"end":103,"id":15},{"text":"modeling","start":104,"end":112,"id":16},{"text":"assumptions","start":113,"end":124,"id":17},{"text":".","start":124,"end":125,"id":18}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":0,"end":13,"token_start":0,"token_end":0,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"The step size leading to the global maximum of the contrast along the search direction is found among the roots of a fourth-degree polynomial.","_input_hash":-602567725,"_task_hash":644587622,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"step","start":4,"end":8,"id":1},{"text":"size","start":9,"end":13,"id":2},{"text":"leading","start":14,"end":21,"id":3},{"text":"to","start":22,"end":24,"id":4},{"text":"the","start":25,"end":28,"id":5},{"text":"global","start":29,"end":35,"id":6},{"text":"maximum","start":36,"end":43,"id":7},{"text":"of","start":44,"end":46,"id":8},{"text":"the","start":47,"end":50,"id":9},{"text":"contrast","start":51,"end":59,"id":10},{"text":"along","start":60,"end":65,"id":11},{"text":"the","start":66,"end":69,"id":12},{"text":"search","start":70,"end":76,"id":13},{"text":"direction","start":77,"end":86,"id":14},{"text":"is","start":87,"end":89,"id":15},{"text":"found","start":90,"end":95,"id":16},{"text":"among","start":96,"end":101,"id":17},{"text":"the","start":102,"end":105,"id":18},{"text":"roots","start":106,"end":111,"id":19},{"text":"of","start":112,"end":114,"id":20},{"text":"a","start":115,"end":116,"id":21},{"text":"fourth","start":117,"end":123,"id":22},{"text":"-","start":123,"end":124,"id":23},{"text":"degree","start":124,"end":130,"id":24},{"text":"polynomial","start":131,"end":141,"id":25},{"text":".","start":141,"end":142,"id":26}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"In this paper, we present a family of convex penalty functions, which encode prior knowledge on the structure of the vector formed by the absolute values of the regression coefficients.","_input_hash":2074119255,"_task_hash":-394132537,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"this","start":3,"end":7,"id":1},{"text":"paper","start":8,"end":13,"id":2},{"text":",","start":13,"end":14,"id":3},{"text":"we","start":15,"end":17,"id":4},{"text":"present","start":18,"end":25,"id":5},{"text":"a","start":26,"end":27,"id":6},{"text":"family","start":28,"end":34,"id":7},{"text":"of","start":35,"end":37,"id":8},{"text":"convex","start":38,"end":44,"id":9},{"text":"penalty","start":45,"end":52,"id":10},{"text":"functions","start":53,"end":62,"id":11},{"text":",","start":62,"end":63,"id":12},{"text":"which","start":64,"end":69,"id":13},{"text":"encode","start":70,"end":76,"id":14},{"text":"prior","start":77,"end":82,"id":15},{"text":"knowledge","start":83,"end":92,"id":16},{"text":"on","start":93,"end":95,"id":17},{"text":"the","start":96,"end":99,"id":18},{"text":"structure","start":100,"end":109,"id":19},{"text":"of","start":110,"end":112,"id":20},{"text":"the","start":113,"end":116,"id":21},{"text":"vector","start":117,"end":123,"id":22},{"text":"formed","start":124,"end":130,"id":23},{"text":"by","start":131,"end":133,"id":24},{"text":"the","start":134,"end":137,"id":25},{"text":"absolute","start":138,"end":146,"id":26},{"text":"values","start":147,"end":153,"id":27},{"text":"of","start":154,"end":156,"id":28},{"text":"the","start":157,"end":160,"id":29},{"text":"regression","start":161,"end":171,"id":30},{"text":"coefficients","start":172,"end":184,"id":31},{"text":".","start":184,"end":185,"id":32}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"This means that we can use it to include points with potentially high impact to the classifier decision process while removing those that are less relevant.","_input_hash":671299892,"_task_hash":811796498,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"means","start":5,"end":10,"id":1},{"text":"that","start":11,"end":15,"id":2},{"text":"we","start":16,"end":18,"id":3},{"text":"can","start":19,"end":22,"id":4},{"text":"use","start":23,"end":26,"id":5},{"text":"it","start":27,"end":29,"id":6},{"text":"to","start":30,"end":32,"id":7},{"text":"include","start":33,"end":40,"id":8},{"text":"points","start":41,"end":47,"id":9},{"text":"with","start":48,"end":52,"id":10},{"text":"potentially","start":53,"end":64,"id":11},{"text":"high","start":65,"end":69,"id":12},{"text":"impact","start":70,"end":76,"id":13},{"text":"to","start":77,"end":79,"id":14},{"text":"the","start":80,"end":83,"id":15},{"text":"classifier","start":84,"end":94,"id":16},{"text":"decision","start":95,"end":103,"id":17},{"text":"process","start":104,"end":111,"id":18},{"text":"while","start":112,"end":117,"id":19},{"text":"removing","start":118,"end":126,"id":20},{"text":"those","start":127,"end":132,"id":21},{"text":"that","start":133,"end":137,"id":22},{"text":"are","start":138,"end":141,"id":23},{"text":"less","start":142,"end":146,"id":24},{"text":"relevant","start":147,"end":155,"id":25},{"text":".","start":155,"end":156,"id":26}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"It is shown that it depends on circumstances like the embedding dimension, the time series predictability, or the base frequency, whether the driving force itself or a slower subcomponent is detected.","_input_hash":-1705359368,"_task_hash":760229032,"tokens":[{"text":"It","start":0,"end":2,"id":0},{"text":"is","start":3,"end":5,"id":1},{"text":"shown","start":6,"end":11,"id":2},{"text":"that","start":12,"end":16,"id":3},{"text":"it","start":17,"end":19,"id":4},{"text":"depends","start":20,"end":27,"id":5},{"text":"on","start":28,"end":30,"id":6},{"text":"circumstances","start":31,"end":44,"id":7},{"text":"like","start":45,"end":49,"id":8},{"text":"the","start":50,"end":53,"id":9},{"text":"embedding","start":54,"end":63,"id":10},{"text":"dimension","start":64,"end":73,"id":11},{"text":",","start":73,"end":74,"id":12},{"text":"the","start":75,"end":78,"id":13},{"text":"time","start":79,"end":83,"id":14},{"text":"series","start":84,"end":90,"id":15},{"text":"predictability","start":91,"end":105,"id":16},{"text":",","start":105,"end":106,"id":17},{"text":"or","start":107,"end":109,"id":18},{"text":"the","start":110,"end":113,"id":19},{"text":"base","start":114,"end":118,"id":20},{"text":"frequency","start":119,"end":128,"id":21},{"text":",","start":128,"end":129,"id":22},{"text":"whether","start":130,"end":137,"id":23},{"text":"the","start":138,"end":141,"id":24},{"text":"driving","start":142,"end":149,"id":25},{"text":"force","start":150,"end":155,"id":26},{"text":"itself","start":156,"end":162,"id":27},{"text":"or","start":163,"end":165,"id":28},{"text":"a","start":166,"end":167,"id":29},{"text":"slower","start":168,"end":174,"id":30},{"text":"subcomponent","start":175,"end":187,"id":31},{"text":"is","start":188,"end":190,"id":32},{"text":"detected","start":191,"end":199,"id":33},{"text":".","start":199,"end":200,"id":34}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We use the results of this paper for partially justifying recent effective algorithms for modeling data by mixtures of multiple subspaces as well as for discussing the effect of using variants of lp minimizations in RANSAC-type strategies for single subspace recovery.","_input_hash":-1979514270,"_task_hash":-6099370,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"use","start":3,"end":6,"id":1},{"text":"the","start":7,"end":10,"id":2},{"text":"results","start":11,"end":18,"id":3},{"text":"of","start":19,"end":21,"id":4},{"text":"this","start":22,"end":26,"id":5},{"text":"paper","start":27,"end":32,"id":6},{"text":"for","start":33,"end":36,"id":7},{"text":"partially","start":37,"end":46,"id":8},{"text":"justifying","start":47,"end":57,"id":9},{"text":"recent","start":58,"end":64,"id":10},{"text":"effective","start":65,"end":74,"id":11},{"text":"algorithms","start":75,"end":85,"id":12},{"text":"for","start":86,"end":89,"id":13},{"text":"modeling","start":90,"end":98,"id":14},{"text":"data","start":99,"end":103,"id":15},{"text":"by","start":104,"end":106,"id":16},{"text":"mixtures","start":107,"end":115,"id":17},{"text":"of","start":116,"end":118,"id":18},{"text":"multiple","start":119,"end":127,"id":19},{"text":"subspaces","start":128,"end":137,"id":20},{"text":"as","start":138,"end":140,"id":21},{"text":"well","start":141,"end":145,"id":22},{"text":"as","start":146,"end":148,"id":23},{"text":"for","start":149,"end":152,"id":24},{"text":"discussing","start":153,"end":163,"id":25},{"text":"the","start":164,"end":167,"id":26},{"text":"effect","start":168,"end":174,"id":27},{"text":"of","start":175,"end":177,"id":28},{"text":"using","start":178,"end":183,"id":29},{"text":"variants","start":184,"end":192,"id":30},{"text":"of","start":193,"end":195,"id":31},{"text":"lp","start":196,"end":198,"id":32},{"text":"minimizations","start":199,"end":212,"id":33},{"text":"in","start":213,"end":215,"id":34},{"text":"RANSAC","start":216,"end":222,"id":35},{"text":"-","start":222,"end":223,"id":36},{"text":"type","start":223,"end":227,"id":37},{"text":"strategies","start":228,"end":238,"id":38},{"text":"for","start":239,"end":242,"id":39},{"text":"single","start":243,"end":249,"id":40},{"text":"subspace","start":250,"end":258,"id":41},{"text":"recovery","start":259,"end":267,"id":42},{"text":".","start":267,"end":268,"id":43}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":65,"end":74,"token_start":11,"token_end":11,"label":"ALGO","answer":"reject"}],"answer":"reject"}
{"text":"Through experiments with real-world datasets, we show that LSIR compares favorably with a state-of-the-art causal inference method.","_input_hash":-840092464,"_task_hash":925714154,"tokens":[{"text":"Through","start":0,"end":7,"id":0},{"text":"experiments","start":8,"end":19,"id":1},{"text":"with","start":20,"end":24,"id":2},{"text":"real","start":25,"end":29,"id":3},{"text":"-","start":29,"end":30,"id":4},{"text":"world","start":30,"end":35,"id":5},{"text":"datasets","start":36,"end":44,"id":6},{"text":",","start":44,"end":45,"id":7},{"text":"we","start":46,"end":48,"id":8},{"text":"show","start":49,"end":53,"id":9},{"text":"that","start":54,"end":58,"id":10},{"text":"LSIR","start":59,"end":63,"id":11},{"text":"compares","start":64,"end":72,"id":12},{"text":"favorably","start":73,"end":82,"id":13},{"text":"with","start":83,"end":87,"id":14},{"text":"a","start":88,"end":89,"id":15},{"text":"state","start":90,"end":95,"id":16},{"text":"-","start":95,"end":96,"id":17},{"text":"of","start":96,"end":98,"id":18},{"text":"-","start":98,"end":99,"id":19},{"text":"the","start":99,"end":102,"id":20},{"text":"-","start":102,"end":103,"id":21},{"text":"art","start":103,"end":106,"id":22},{"text":"causal","start":107,"end":113,"id":23},{"text":"inference","start":114,"end":123,"id":24},{"text":"method","start":124,"end":130,"id":25},{"text":".","start":130,"end":131,"id":26}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"This work complements previous empirical findings by providing sound theoretical guarantees for the proposed estimation procedure.","_input_hash":1593596978,"_task_hash":1378292479,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"work","start":5,"end":9,"id":1},{"text":"complements","start":10,"end":21,"id":2},{"text":"previous","start":22,"end":30,"id":3},{"text":"empirical","start":31,"end":40,"id":4},{"text":"findings","start":41,"end":49,"id":5},{"text":"by","start":50,"end":52,"id":6},{"text":"providing","start":53,"end":62,"id":7},{"text":"sound","start":63,"end":68,"id":8},{"text":"theoretical","start":69,"end":80,"id":9},{"text":"guarantees","start":81,"end":91,"id":10},{"text":"for","start":92,"end":95,"id":11},{"text":"the","start":96,"end":99,"id":12},{"text":"proposed","start":100,"end":108,"id":13},{"text":"estimation","start":109,"end":119,"id":14},{"text":"procedure","start":120,"end":129,"id":15},{"text":".","start":129,"end":130,"id":16}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We study losses for binary classification and class probability estimation and extend the understanding of them from margin losses to general composite losses which are the composition of a proper loss with a link function.","_input_hash":710153448,"_task_hash":-593455855,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"study","start":3,"end":8,"id":1},{"text":"losses","start":9,"end":15,"id":2},{"text":"for","start":16,"end":19,"id":3},{"text":"binary","start":20,"end":26,"id":4},{"text":"classification","start":27,"end":41,"id":5},{"text":"and","start":42,"end":45,"id":6},{"text":"class","start":46,"end":51,"id":7},{"text":"probability","start":52,"end":63,"id":8},{"text":"estimation","start":64,"end":74,"id":9},{"text":"and","start":75,"end":78,"id":10},{"text":"extend","start":79,"end":85,"id":11},{"text":"the","start":86,"end":89,"id":12},{"text":"understanding","start":90,"end":103,"id":13},{"text":"of","start":104,"end":106,"id":14},{"text":"them","start":107,"end":111,"id":15},{"text":"from","start":112,"end":116,"id":16},{"text":"margin","start":117,"end":123,"id":17},{"text":"losses","start":124,"end":130,"id":18},{"text":"to","start":131,"end":133,"id":19},{"text":"general","start":134,"end":141,"id":20},{"text":"composite","start":142,"end":151,"id":21},{"text":"losses","start":152,"end":158,"id":22},{"text":"which","start":159,"end":164,"id":23},{"text":"are","start":165,"end":168,"id":24},{"text":"the","start":169,"end":172,"id":25},{"text":"composition","start":173,"end":184,"id":26},{"text":"of","start":185,"end":187,"id":27},{"text":"a","start":188,"end":189,"id":28},{"text":"proper","start":190,"end":196,"id":29},{"text":"loss","start":197,"end":201,"id":30},{"text":"with","start":202,"end":206,"id":31},{"text":"a","start":207,"end":208,"id":32},{"text":"link","start":209,"end":213,"id":33},{"text":"function","start":214,"end":222,"id":34},{"text":".","start":222,"end":223,"id":35}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":20,"end":41,"token_start":4,"token_end":5,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Despite the recent progress towards efficient multiple kernel learning (MKL), the structured output case remains an open research front.","_input_hash":-340461261,"_task_hash":-1333114707,"tokens":[{"text":"Despite","start":0,"end":7,"id":0},{"text":"the","start":8,"end":11,"id":1},{"text":"recent","start":12,"end":18,"id":2},{"text":"progress","start":19,"end":27,"id":3},{"text":"towards","start":28,"end":35,"id":4},{"text":"efficient","start":36,"end":45,"id":5},{"text":"multiple","start":46,"end":54,"id":6},{"text":"kernel","start":55,"end":61,"id":7},{"text":"learning","start":62,"end":70,"id":8},{"text":"(","start":71,"end":72,"id":9},{"text":"MKL","start":72,"end":75,"id":10},{"text":")","start":75,"end":76,"id":11},{"text":",","start":76,"end":77,"id":12},{"text":"the","start":78,"end":81,"id":13},{"text":"structured","start":82,"end":92,"id":14},{"text":"output","start":93,"end":99,"id":15},{"text":"case","start":100,"end":104,"id":16},{"text":"remains","start":105,"end":112,"id":17},{"text":"an","start":113,"end":115,"id":18},{"text":"open","start":116,"end":120,"id":19},{"text":"research","start":121,"end":129,"id":20},{"text":"front","start":130,"end":135,"id":21},{"text":".","start":135,"end":136,"id":22}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":46,"end":70,"token_start":6,"token_end":8,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"In this paper we consider sparse and identifiable linear latent variable (factor) and linear Bayesian network models for parsimonious analysis of multivariate data.","_input_hash":835553860,"_task_hash":-229359257,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"this","start":3,"end":7,"id":1},{"text":"paper","start":8,"end":13,"id":2},{"text":"we","start":14,"end":16,"id":3},{"text":"consider","start":17,"end":25,"id":4},{"text":"sparse","start":26,"end":32,"id":5},{"text":"and","start":33,"end":36,"id":6},{"text":"identifiable","start":37,"end":49,"id":7},{"text":"linear","start":50,"end":56,"id":8},{"text":"latent","start":57,"end":63,"id":9},{"text":"variable","start":64,"end":72,"id":10},{"text":"(","start":73,"end":74,"id":11},{"text":"factor","start":74,"end":80,"id":12},{"text":")","start":80,"end":81,"id":13},{"text":"and","start":82,"end":85,"id":14},{"text":"linear","start":86,"end":92,"id":15},{"text":"Bayesian","start":93,"end":101,"id":16},{"text":"network","start":102,"end":109,"id":17},{"text":"models","start":110,"end":116,"id":18},{"text":"for","start":117,"end":120,"id":19},{"text":"parsimonious","start":121,"end":133,"id":20},{"text":"analysis","start":134,"end":142,"id":21},{"text":"of","start":143,"end":145,"id":22},{"text":"multivariate","start":146,"end":158,"id":23},{"text":"data","start":159,"end":163,"id":24},{"text":".","start":163,"end":164,"id":25}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":86,"end":109,"token_start":15,"token_end":17,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Original results on the arc lengths, angles and curvature of the transformations are proposed, and visualized on artificial data sets by classical multidimensional scaling.","_input_hash":455801541,"_task_hash":-63490062,"tokens":[{"text":"Original","start":0,"end":8,"id":0},{"text":"results","start":9,"end":16,"id":1},{"text":"on","start":17,"end":19,"id":2},{"text":"the","start":20,"end":23,"id":3},{"text":"arc","start":24,"end":27,"id":4},{"text":"lengths","start":28,"end":35,"id":5},{"text":",","start":35,"end":36,"id":6},{"text":"angles","start":37,"end":43,"id":7},{"text":"and","start":44,"end":47,"id":8},{"text":"curvature","start":48,"end":57,"id":9},{"text":"of","start":58,"end":60,"id":10},{"text":"the","start":61,"end":64,"id":11},{"text":"transformations","start":65,"end":80,"id":12},{"text":"are","start":81,"end":84,"id":13},{"text":"proposed","start":85,"end":93,"id":14},{"text":",","start":93,"end":94,"id":15},{"text":"and","start":95,"end":98,"id":16},{"text":"visualized","start":99,"end":109,"id":17},{"text":"on","start":110,"end":112,"id":18},{"text":"artificial","start":113,"end":123,"id":19},{"text":"data","start":124,"end":128,"id":20},{"text":"sets","start":129,"end":133,"id":21},{"text":"by","start":134,"end":136,"id":22},{"text":"classical","start":137,"end":146,"id":23},{"text":"multidimensional","start":147,"end":163,"id":24},{"text":"scaling","start":164,"end":171,"id":25},{"text":".","start":171,"end":172,"id":26}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Then we show that our derived learning rate of lp-MKL achieves the minimax optimal rate on the lp-mixed-norm ball.","_input_hash":-1424516395,"_task_hash":-48196591,"tokens":[{"text":"Then","start":0,"end":4,"id":0},{"text":"we","start":5,"end":7,"id":1},{"text":"show","start":8,"end":12,"id":2},{"text":"that","start":13,"end":17,"id":3},{"text":"our","start":18,"end":21,"id":4},{"text":"derived","start":22,"end":29,"id":5},{"text":"learning","start":30,"end":38,"id":6},{"text":"rate","start":39,"end":43,"id":7},{"text":"of","start":44,"end":46,"id":8},{"text":"lp","start":47,"end":49,"id":9},{"text":"-","start":49,"end":50,"id":10},{"text":"MKL","start":50,"end":53,"id":11},{"text":"achieves","start":54,"end":62,"id":12},{"text":"the","start":63,"end":66,"id":13},{"text":"minimax","start":67,"end":74,"id":14},{"text":"optimal","start":75,"end":82,"id":15},{"text":"rate","start":83,"end":87,"id":16},{"text":"on","start":88,"end":90,"id":17},{"text":"the","start":91,"end":94,"id":18},{"text":"lp","start":95,"end":97,"id":19},{"text":"-","start":97,"end":98,"id":20},{"text":"mixed","start":98,"end":103,"id":21},{"text":"-","start":103,"end":104,"id":22},{"text":"norm","start":104,"end":108,"id":23},{"text":"ball","start":109,"end":113,"id":24},{"text":".","start":113,"end":114,"id":25}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"To find solutions to these problems, we will study an l_1 regularized maximum likelihood method, related to the fused lasso method and l_1 trend filtering, where the parameters to be estimated are free to vary at each sample.","_input_hash":1544488984,"_task_hash":-406340464,"tokens":[{"text":"To","start":0,"end":2,"id":0},{"text":"find","start":3,"end":7,"id":1},{"text":"solutions","start":8,"end":17,"id":2},{"text":"to","start":18,"end":20,"id":3},{"text":"these","start":21,"end":26,"id":4},{"text":"problems","start":27,"end":35,"id":5},{"text":",","start":35,"end":36,"id":6},{"text":"we","start":37,"end":39,"id":7},{"text":"will","start":40,"end":44,"id":8},{"text":"study","start":45,"end":50,"id":9},{"text":"an","start":51,"end":53,"id":10},{"text":"l_1","start":54,"end":57,"id":11},{"text":"regularized","start":58,"end":69,"id":12},{"text":"maximum","start":70,"end":77,"id":13},{"text":"likelihood","start":78,"end":88,"id":14},{"text":"method","start":89,"end":95,"id":15},{"text":",","start":95,"end":96,"id":16},{"text":"related","start":97,"end":104,"id":17},{"text":"to","start":105,"end":107,"id":18},{"text":"the","start":108,"end":111,"id":19},{"text":"fused","start":112,"end":117,"id":20},{"text":"lasso","start":118,"end":123,"id":21},{"text":"method","start":124,"end":130,"id":22},{"text":"and","start":131,"end":134,"id":23},{"text":"l_1","start":135,"end":138,"id":24},{"text":"trend","start":139,"end":144,"id":25},{"text":"filtering","start":145,"end":154,"id":26},{"text":",","start":154,"end":155,"id":27},{"text":"where","start":156,"end":161,"id":28},{"text":"the","start":162,"end":165,"id":29},{"text":"parameters","start":166,"end":176,"id":30},{"text":"to","start":177,"end":179,"id":31},{"text":"be","start":180,"end":182,"id":32},{"text":"estimated","start":183,"end":192,"id":33},{"text":"are","start":193,"end":196,"id":34},{"text":"free","start":197,"end":201,"id":35},{"text":"to","start":202,"end":204,"id":36},{"text":"vary","start":205,"end":209,"id":37},{"text":"at","start":210,"end":212,"id":38},{"text":"each","start":213,"end":217,"id":39},{"text":"sample","start":218,"end":224,"id":40},{"text":".","start":224,"end":225,"id":41}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":54,"end":88,"token_start":11,"token_end":14,"label":"ALGO","answer":"accept"},{"start":118,"end":123,"token_start":21,"token_end":21,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"If some of the variables take only values in proper subsets of R^n, these conditionals can induce different families of joint distributions even for Markov-equivalent graphs.","_input_hash":827848449,"_task_hash":1511977818,"tokens":[{"text":"If","start":0,"end":2,"id":0},{"text":"some","start":3,"end":7,"id":1},{"text":"of","start":8,"end":10,"id":2},{"text":"the","start":11,"end":14,"id":3},{"text":"variables","start":15,"end":24,"id":4},{"text":"take","start":25,"end":29,"id":5},{"text":"only","start":30,"end":34,"id":6},{"text":"values","start":35,"end":41,"id":7},{"text":"in","start":42,"end":44,"id":8},{"text":"proper","start":45,"end":51,"id":9},{"text":"subsets","start":52,"end":59,"id":10},{"text":"of","start":60,"end":62,"id":11},{"text":"R^n","start":63,"end":66,"id":12},{"text":",","start":66,"end":67,"id":13},{"text":"these","start":68,"end":73,"id":14},{"text":"conditionals","start":74,"end":86,"id":15},{"text":"can","start":87,"end":90,"id":16},{"text":"induce","start":91,"end":97,"id":17},{"text":"different","start":98,"end":107,"id":18},{"text":"families","start":108,"end":116,"id":19},{"text":"of","start":117,"end":119,"id":20},{"text":"joint","start":120,"end":125,"id":21},{"text":"distributions","start":126,"end":139,"id":22},{"text":"even","start":140,"end":144,"id":23},{"text":"for","start":145,"end":148,"id":24},{"text":"Markov","start":149,"end":155,"id":25},{"text":"-","start":155,"end":156,"id":26},{"text":"equivalent","start":156,"end":166,"id":27},{"text":"graphs","start":167,"end":173,"id":28},{"text":".","start":173,"end":174,"id":29}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"We introduce a new Bayesian model for hierarchical clustering based on a prior over trees called Kingman's coalescent.","_input_hash":2068280802,"_task_hash":-1791110143,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"introduce","start":3,"end":12,"id":1},{"text":"a","start":13,"end":14,"id":2},{"text":"new","start":15,"end":18,"id":3},{"text":"Bayesian","start":19,"end":27,"id":4},{"text":"model","start":28,"end":33,"id":5},{"text":"for","start":34,"end":37,"id":6},{"text":"hierarchical","start":38,"end":50,"id":7},{"text":"clustering","start":51,"end":61,"id":8},{"text":"based","start":62,"end":67,"id":9},{"text":"on","start":68,"end":70,"id":10},{"text":"a","start":71,"end":72,"id":11},{"text":"prior","start":73,"end":78,"id":12},{"text":"over","start":79,"end":83,"id":13},{"text":"trees","start":84,"end":89,"id":14},{"text":"called","start":90,"end":96,"id":15},{"text":"Kingman","start":97,"end":104,"id":16},{"text":"'s","start":104,"end":106,"id":17},{"text":"coalescent","start":107,"end":117,"id":18},{"text":".","start":117,"end":118,"id":19}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"However, it may be shown that sufficiently sparse solutions may be identified uniquely.","_input_hash":-1061976155,"_task_hash":1368849593,"tokens":[{"text":"However","start":0,"end":7,"id":0},{"text":",","start":7,"end":8,"id":1},{"text":"it","start":9,"end":11,"id":2},{"text":"may","start":12,"end":15,"id":3},{"text":"be","start":16,"end":18,"id":4},{"text":"shown","start":19,"end":24,"id":5},{"text":"that","start":25,"end":29,"id":6},{"text":"sufficiently","start":30,"end":42,"id":7},{"text":"sparse","start":43,"end":49,"id":8},{"text":"solutions","start":50,"end":59,"id":9},{"text":"may","start":60,"end":63,"id":10},{"text":"be","start":64,"end":66,"id":11},{"text":"identified","start":67,"end":77,"id":12},{"text":"uniquely","start":78,"end":86,"id":13},{"text":".","start":86,"end":87,"id":14}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"For signals such as natural images that admit such sparse representations, it is now well established that these models are well suited to restoration tasks.","_input_hash":1848857112,"_task_hash":-748868030,"tokens":[{"text":"For","start":0,"end":3,"id":0},{"text":"signals","start":4,"end":11,"id":1},{"text":"such","start":12,"end":16,"id":2},{"text":"as","start":17,"end":19,"id":3},{"text":"natural","start":20,"end":27,"id":4},{"text":"images","start":28,"end":34,"id":5},{"text":"that","start":35,"end":39,"id":6},{"text":"admit","start":40,"end":45,"id":7},{"text":"such","start":46,"end":50,"id":8},{"text":"sparse","start":51,"end":57,"id":9},{"text":"representations","start":58,"end":73,"id":10},{"text":",","start":73,"end":74,"id":11},{"text":"it","start":75,"end":77,"id":12},{"text":"is","start":78,"end":80,"id":13},{"text":"now","start":81,"end":84,"id":14},{"text":"well","start":85,"end":89,"id":15},{"text":"established","start":90,"end":101,"id":16},{"text":"that","start":102,"end":106,"id":17},{"text":"these","start":107,"end":112,"id":18},{"text":"models","start":113,"end":119,"id":19},{"text":"are","start":120,"end":123,"id":20},{"text":"well","start":124,"end":128,"id":21},{"text":"suited","start":129,"end":135,"id":22},{"text":"to","start":136,"end":138,"id":23},{"text":"restoration","start":139,"end":150,"id":24},{"text":"tasks","start":151,"end":156,"id":25},{"text":".","start":156,"end":157,"id":26}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"The novel robust spline-based smoother is adopted to cleanse load curve data, a key task aiding operational decisions in the envisioned smart grid system.","_input_hash":-2062123243,"_task_hash":1748543987,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"novel","start":4,"end":9,"id":1},{"text":"robust","start":10,"end":16,"id":2},{"text":"spline","start":17,"end":23,"id":3},{"text":"-","start":23,"end":24,"id":4},{"text":"based","start":24,"end":29,"id":5},{"text":"smoother","start":30,"end":38,"id":6},{"text":"is","start":39,"end":41,"id":7},{"text":"adopted","start":42,"end":49,"id":8},{"text":"to","start":50,"end":52,"id":9},{"text":"cleanse","start":53,"end":60,"id":10},{"text":"load","start":61,"end":65,"id":11},{"text":"curve","start":66,"end":71,"id":12},{"text":"data","start":72,"end":76,"id":13},{"text":",","start":76,"end":77,"id":14},{"text":"a","start":78,"end":79,"id":15},{"text":"key","start":80,"end":83,"id":16},{"text":"task","start":84,"end":88,"id":17},{"text":"aiding","start":89,"end":95,"id":18},{"text":"operational","start":96,"end":107,"id":19},{"text":"decisions","start":108,"end":117,"id":20},{"text":"in","start":118,"end":120,"id":21},{"text":"the","start":121,"end":124,"id":22},{"text":"envisioned","start":125,"end":135,"id":23},{"text":"smart","start":136,"end":141,"id":24},{"text":"grid","start":142,"end":146,"id":25},{"text":"system","start":147,"end":153,"id":26},{"text":".","start":153,"end":154,"id":27}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We derive a family of iterative expectationmaximization (EM) algorithms to estimate the parameters of each model and propose suitable procedures to initialize the parameters and to align the order of the estimated sources across all frequency bins based on their estimated directions of arrival (DOA).","_input_hash":-473871123,"_task_hash":980455228,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"derive","start":3,"end":9,"id":1},{"text":"a","start":10,"end":11,"id":2},{"text":"family","start":12,"end":18,"id":3},{"text":"of","start":19,"end":21,"id":4},{"text":"iterative","start":22,"end":31,"id":5},{"text":"expectationmaximization","start":32,"end":55,"id":6},{"text":"(","start":56,"end":57,"id":7},{"text":"EM","start":57,"end":59,"id":8},{"text":")","start":59,"end":60,"id":9},{"text":"algorithms","start":61,"end":71,"id":10},{"text":"to","start":72,"end":74,"id":11},{"text":"estimate","start":75,"end":83,"id":12},{"text":"the","start":84,"end":87,"id":13},{"text":"parameters","start":88,"end":98,"id":14},{"text":"of","start":99,"end":101,"id":15},{"text":"each","start":102,"end":106,"id":16},{"text":"model","start":107,"end":112,"id":17},{"text":"and","start":113,"end":116,"id":18},{"text":"propose","start":117,"end":124,"id":19},{"text":"suitable","start":125,"end":133,"id":20},{"text":"procedures","start":134,"end":144,"id":21},{"text":"to","start":145,"end":147,"id":22},{"text":"initialize","start":148,"end":158,"id":23},{"text":"the","start":159,"end":162,"id":24},{"text":"parameters","start":163,"end":173,"id":25},{"text":"and","start":174,"end":177,"id":26},{"text":"to","start":178,"end":180,"id":27},{"text":"align","start":181,"end":186,"id":28},{"text":"the","start":187,"end":190,"id":29},{"text":"order","start":191,"end":196,"id":30},{"text":"of","start":197,"end":199,"id":31},{"text":"the","start":200,"end":203,"id":32},{"text":"estimated","start":204,"end":213,"id":33},{"text":"sources","start":214,"end":221,"id":34},{"text":"across","start":222,"end":228,"id":35},{"text":"all","start":229,"end":232,"id":36},{"text":"frequency","start":233,"end":242,"id":37},{"text":"bins","start":243,"end":247,"id":38},{"text":"based","start":248,"end":253,"id":39},{"text":"on","start":254,"end":256,"id":40},{"text":"their","start":257,"end":262,"id":41},{"text":"estimated","start":263,"end":272,"id":42},{"text":"directions","start":273,"end":283,"id":43},{"text":"of","start":284,"end":286,"id":44},{"text":"arrival","start":287,"end":294,"id":45},{"text":"(","start":295,"end":296,"id":46},{"text":"DOA","start":296,"end":299,"id":47},{"text":")","start":299,"end":300,"id":48},{"text":".","start":300,"end":301,"id":49}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":32,"end":55,"token_start":6,"token_end":6,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Previous local approaches aimed at analyzed the case $p=1$ only while our analysis covers all cases $1\\leq p\\leq\\infty$, assuming the different feature mappings corresponding to the different kernels to be uncorrelated.","_input_hash":1713107974,"_task_hash":-1938077020,"tokens":[{"text":"Previous","start":0,"end":8,"id":0},{"text":"local","start":9,"end":14,"id":1},{"text":"approaches","start":15,"end":25,"id":2},{"text":"aimed","start":26,"end":31,"id":3},{"text":"at","start":32,"end":34,"id":4},{"text":"analyzed","start":35,"end":43,"id":5},{"text":"the","start":44,"end":47,"id":6},{"text":"case","start":48,"end":52,"id":7},{"text":"$","start":53,"end":54,"id":8},{"text":"p=1","start":54,"end":57,"id":9},{"text":"$","start":57,"end":58,"id":10},{"text":"only","start":59,"end":63,"id":11},{"text":"while","start":64,"end":69,"id":12},{"text":"our","start":70,"end":73,"id":13},{"text":"analysis","start":74,"end":82,"id":14},{"text":"covers","start":83,"end":89,"id":15},{"text":"all","start":90,"end":93,"id":16},{"text":"cases","start":94,"end":99,"id":17},{"text":"$","start":100,"end":101,"id":18},{"text":"1\\leq","start":101,"end":106,"id":19},{"text":"p\\leq\\infty$","start":107,"end":119,"id":20},{"text":",","start":119,"end":120,"id":21},{"text":"assuming","start":121,"end":129,"id":22},{"text":"the","start":130,"end":133,"id":23},{"text":"different","start":134,"end":143,"id":24},{"text":"feature","start":144,"end":151,"id":25},{"text":"mappings","start":152,"end":160,"id":26},{"text":"corresponding","start":161,"end":174,"id":27},{"text":"to","start":175,"end":177,"id":28},{"text":"the","start":178,"end":181,"id":29},{"text":"different","start":182,"end":191,"id":30},{"text":"kernels","start":192,"end":199,"id":31},{"text":"to","start":200,"end":202,"id":32},{"text":"be","start":203,"end":205,"id":33},{"text":"uncorrelated","start":206,"end":218,"id":34},{"text":".","start":218,"end":219,"id":35}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We also provide both theoretical and empirical support for our active set selection strategy being a good approximation of a full Gaussian process classifier.","_input_hash":-1061266474,"_task_hash":-233842900,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"also","start":3,"end":7,"id":1},{"text":"provide","start":8,"end":15,"id":2},{"text":"both","start":16,"end":20,"id":3},{"text":"theoretical","start":21,"end":32,"id":4},{"text":"and","start":33,"end":36,"id":5},{"text":"empirical","start":37,"end":46,"id":6},{"text":"support","start":47,"end":54,"id":7},{"text":"for","start":55,"end":58,"id":8},{"text":"our","start":59,"end":62,"id":9},{"text":"active","start":63,"end":69,"id":10},{"text":"set","start":70,"end":73,"id":11},{"text":"selection","start":74,"end":83,"id":12},{"text":"strategy","start":84,"end":92,"id":13},{"text":"being","start":93,"end":98,"id":14},{"text":"a","start":99,"end":100,"id":15},{"text":"good","start":101,"end":105,"id":16},{"text":"approximation","start":106,"end":119,"id":17},{"text":"of","start":120,"end":122,"id":18},{"text":"a","start":123,"end":124,"id":19},{"text":"full","start":125,"end":129,"id":20},{"text":"Gaussian","start":130,"end":138,"id":21},{"text":"process","start":139,"end":146,"id":22},{"text":"classifier","start":147,"end":157,"id":23},{"text":".","start":157,"end":158,"id":24}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":130,"end":157,"token_start":21,"token_end":23,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We test sLDA on two real-world problems:","_input_hash":-333578088,"_task_hash":-520742590,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"test","start":3,"end":7,"id":1},{"text":"sLDA","start":8,"end":12,"id":2},{"text":"on","start":13,"end":15,"id":3},{"text":"two","start":16,"end":19,"id":4},{"text":"real","start":20,"end":24,"id":5},{"text":"-","start":24,"end":25,"id":6},{"text":"world","start":25,"end":30,"id":7},{"text":"problems","start":31,"end":39,"id":8},{"text":":","start":39,"end":40,"id":9}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"This family subsumes the $\\ell_1$ norm and is flexible enough to include different models of sparsity patterns, which are of practical and theoretical importance.","_input_hash":1032534764,"_task_hash":2000981939,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"family","start":5,"end":11,"id":1},{"text":"subsumes","start":12,"end":20,"id":2},{"text":"the","start":21,"end":24,"id":3},{"text":"$","start":25,"end":26,"id":4},{"text":"\\ell_1","start":26,"end":32,"id":5},{"text":"$","start":32,"end":33,"id":6},{"text":"norm","start":34,"end":38,"id":7},{"text":"and","start":39,"end":42,"id":8},{"text":"is","start":43,"end":45,"id":9},{"text":"flexible","start":46,"end":54,"id":10},{"text":"enough","start":55,"end":61,"id":11},{"text":"to","start":62,"end":64,"id":12},{"text":"include","start":65,"end":72,"id":13},{"text":"different","start":73,"end":82,"id":14},{"text":"models","start":83,"end":89,"id":15},{"text":"of","start":90,"end":92,"id":16},{"text":"sparsity","start":93,"end":101,"id":17},{"text":"patterns","start":102,"end":110,"id":18},{"text":",","start":110,"end":111,"id":19},{"text":"which","start":112,"end":117,"id":20},{"text":"are","start":118,"end":121,"id":21},{"text":"of","start":122,"end":124,"id":22},{"text":"practical","start":125,"end":134,"id":23},{"text":"and","start":135,"end":138,"id":24},{"text":"theoretical","start":139,"end":150,"id":25},{"text":"importance","start":151,"end":161,"id":26},{"text":".","start":161,"end":162,"id":27}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"persistence and functoriality.","_input_hash":690850410,"_task_hash":2015507077,"tokens":[{"text":"persistence","start":0,"end":11,"id":0},{"text":"and","start":12,"end":15,"id":1},{"text":"functoriality","start":16,"end":29,"id":2},{"text":".","start":29,"end":30,"id":3}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Compared to existing techniques, the DP-GLM provides a single model (and corresponding inference algorithms) that performs well in many regression settings.","_input_hash":131905234,"_task_hash":-813022950,"tokens":[{"text":"Compared","start":0,"end":8,"id":0},{"text":"to","start":9,"end":11,"id":1},{"text":"existing","start":12,"end":20,"id":2},{"text":"techniques","start":21,"end":31,"id":3},{"text":",","start":31,"end":32,"id":4},{"text":"the","start":33,"end":36,"id":5},{"text":"DP","start":37,"end":39,"id":6},{"text":"-","start":39,"end":40,"id":7},{"text":"GLM","start":40,"end":43,"id":8},{"text":"provides","start":44,"end":52,"id":9},{"text":"a","start":53,"end":54,"id":10},{"text":"single","start":55,"end":61,"id":11},{"text":"model","start":62,"end":67,"id":12},{"text":"(","start":68,"end":69,"id":13},{"text":"and","start":69,"end":72,"id":14},{"text":"corresponding","start":73,"end":86,"id":15},{"text":"inference","start":87,"end":96,"id":16},{"text":"algorithms","start":97,"end":107,"id":17},{"text":")","start":107,"end":108,"id":18},{"text":"that","start":109,"end":113,"id":19},{"text":"performs","start":114,"end":122,"id":20},{"text":"well","start":123,"end":127,"id":21},{"text":"in","start":128,"end":130,"id":22},{"text":"many","start":131,"end":135,"id":23},{"text":"regression","start":136,"end":146,"id":24},{"text":"settings","start":147,"end":155,"id":25},{"text":".","start":155,"end":156,"id":26}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We evaluate DP-GLM on several data sets, comparing it to modern methods of nonparametric regression like CART, Bayesian trees and Gaussian processes.","_input_hash":-2094321264,"_task_hash":-1766372915,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"evaluate","start":3,"end":11,"id":1},{"text":"DP","start":12,"end":14,"id":2},{"text":"-","start":14,"end":15,"id":3},{"text":"GLM","start":15,"end":18,"id":4},{"text":"on","start":19,"end":21,"id":5},{"text":"several","start":22,"end":29,"id":6},{"text":"data","start":30,"end":34,"id":7},{"text":"sets","start":35,"end":39,"id":8},{"text":",","start":39,"end":40,"id":9},{"text":"comparing","start":41,"end":50,"id":10},{"text":"it","start":51,"end":53,"id":11},{"text":"to","start":54,"end":56,"id":12},{"text":"modern","start":57,"end":63,"id":13},{"text":"methods","start":64,"end":71,"id":14},{"text":"of","start":72,"end":74,"id":15},{"text":"nonparametric","start":75,"end":88,"id":16},{"text":"regression","start":89,"end":99,"id":17},{"text":"like","start":100,"end":104,"id":18},{"text":"CART","start":105,"end":109,"id":19},{"text":",","start":109,"end":110,"id":20},{"text":"Bayesian","start":111,"end":119,"id":21},{"text":"trees","start":120,"end":125,"id":22},{"text":"and","start":126,"end":129,"id":23},{"text":"Gaussian","start":130,"end":138,"id":24},{"text":"processes","start":139,"end":148,"id":25},{"text":".","start":148,"end":149,"id":26}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":105,"end":109,"token_start":19,"token_end":19,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Our bound is sharper than the convergence rates ever shown, and has a property that the smoother the truth is, the faster the convergence rate is.","_input_hash":-1640363030,"_task_hash":-729187607,"tokens":[{"text":"Our","start":0,"end":3,"id":0},{"text":"bound","start":4,"end":9,"id":1},{"text":"is","start":10,"end":12,"id":2},{"text":"sharper","start":13,"end":20,"id":3},{"text":"than","start":21,"end":25,"id":4},{"text":"the","start":26,"end":29,"id":5},{"text":"convergence","start":30,"end":41,"id":6},{"text":"rates","start":42,"end":47,"id":7},{"text":"ever","start":48,"end":52,"id":8},{"text":"shown","start":53,"end":58,"id":9},{"text":",","start":58,"end":59,"id":10},{"text":"and","start":60,"end":63,"id":11},{"text":"has","start":64,"end":67,"id":12},{"text":"a","start":68,"end":69,"id":13},{"text":"property","start":70,"end":78,"id":14},{"text":"that","start":79,"end":83,"id":15},{"text":"the","start":84,"end":87,"id":16},{"text":"smoother","start":88,"end":96,"id":17},{"text":"the","start":97,"end":100,"id":18},{"text":"truth","start":101,"end":106,"id":19},{"text":"is","start":107,"end":109,"id":20},{"text":",","start":109,"end":110,"id":21},{"text":"the","start":111,"end":114,"id":22},{"text":"faster","start":115,"end":121,"id":23},{"text":"the","start":122,"end":125,"id":24},{"text":"convergence","start":126,"end":137,"id":25},{"text":"rate","start":138,"end":142,"id":26},{"text":"is","start":143,"end":145,"id":27},{"text":".","start":145,"end":146,"id":28}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Although some previous work addressed the handling of malicious data in the context of supervised learning, very little is known about the behavior of anomaly detection methods in such scenarios.","_input_hash":-29242478,"_task_hash":-1043603022,"tokens":[{"text":"Although","start":0,"end":8,"id":0},{"text":"some","start":9,"end":13,"id":1},{"text":"previous","start":14,"end":22,"id":2},{"text":"work","start":23,"end":27,"id":3},{"text":"addressed","start":28,"end":37,"id":4},{"text":"the","start":38,"end":41,"id":5},{"text":"handling","start":42,"end":50,"id":6},{"text":"of","start":51,"end":53,"id":7},{"text":"malicious","start":54,"end":63,"id":8},{"text":"data","start":64,"end":68,"id":9},{"text":"in","start":69,"end":71,"id":10},{"text":"the","start":72,"end":75,"id":11},{"text":"context","start":76,"end":83,"id":12},{"text":"of","start":84,"end":86,"id":13},{"text":"supervised","start":87,"end":97,"id":14},{"text":"learning","start":98,"end":106,"id":15},{"text":",","start":106,"end":107,"id":16},{"text":"very","start":108,"end":112,"id":17},{"text":"little","start":113,"end":119,"id":18},{"text":"is","start":120,"end":122,"id":19},{"text":"known","start":123,"end":128,"id":20},{"text":"about","start":129,"end":134,"id":21},{"text":"the","start":135,"end":138,"id":22},{"text":"behavior","start":139,"end":147,"id":23},{"text":"of","start":148,"end":150,"id":24},{"text":"anomaly","start":151,"end":158,"id":25},{"text":"detection","start":159,"end":168,"id":26},{"text":"methods","start":169,"end":176,"id":27},{"text":"in","start":177,"end":179,"id":28},{"text":"such","start":180,"end":184,"id":29},{"text":"scenarios","start":185,"end":194,"id":30},{"text":".","start":194,"end":195,"id":31}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Computer simulations and tests on real load curve data corroborate the effectiveness of the novel sparsity-controlling robust estimators.","_input_hash":-190626541,"_task_hash":-1160475533,"tokens":[{"text":"Computer","start":0,"end":8,"id":0},{"text":"simulations","start":9,"end":20,"id":1},{"text":"and","start":21,"end":24,"id":2},{"text":"tests","start":25,"end":30,"id":3},{"text":"on","start":31,"end":33,"id":4},{"text":"real","start":34,"end":38,"id":5},{"text":"load","start":39,"end":43,"id":6},{"text":"curve","start":44,"end":49,"id":7},{"text":"data","start":50,"end":54,"id":8},{"text":"corroborate","start":55,"end":66,"id":9},{"text":"the","start":67,"end":70,"id":10},{"text":"effectiveness","start":71,"end":84,"id":11},{"text":"of","start":85,"end":87,"id":12},{"text":"the","start":88,"end":91,"id":13},{"text":"novel","start":92,"end":97,"id":14},{"text":"sparsity","start":98,"end":106,"id":15},{"text":"-","start":106,"end":107,"id":16},{"text":"controlling","start":107,"end":118,"id":17},{"text":"robust","start":119,"end":125,"id":18},{"text":"estimators","start":126,"end":136,"id":19},{"text":".","start":136,"end":137,"id":20}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Section 1.3 was incorrect, and 2.1 will be removed from further submissions.","_input_hash":-1085226706,"_task_hash":599624711,"tokens":[{"text":"Section","start":0,"end":7,"id":0},{"text":"1.3","start":8,"end":11,"id":1},{"text":"was","start":12,"end":15,"id":2},{"text":"incorrect","start":16,"end":25,"id":3},{"text":",","start":25,"end":26,"id":4},{"text":"and","start":27,"end":30,"id":5},{"text":"2.1","start":31,"end":34,"id":6},{"text":"will","start":35,"end":39,"id":7},{"text":"be","start":40,"end":42,"id":8},{"text":"removed","start":43,"end":50,"id":9},{"text":"from","start":51,"end":55,"id":10},{"text":"further","start":56,"end":63,"id":11},{"text":"submissions","start":64,"end":75,"id":12},{"text":".","start":75,"end":76,"id":13}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"it extends this negative result to any anytime policy.","_input_hash":-2127674872,"_task_hash":-72742135,"tokens":[{"text":"it","start":0,"end":2,"id":0},{"text":"extends","start":3,"end":10,"id":1},{"text":"this","start":11,"end":15,"id":2},{"text":"negative","start":16,"end":24,"id":3},{"text":"result","start":25,"end":31,"id":4},{"text":"to","start":32,"end":34,"id":5},{"text":"any","start":35,"end":38,"id":6},{"text":"anytime","start":39,"end":46,"id":7},{"text":"policy","start":47,"end":53,"id":8},{"text":".","start":53,"end":54,"id":9}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"The equations show that optimization is computationally expensive.","_input_hash":1928351634,"_task_hash":1322417302,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"equations","start":4,"end":13,"id":1},{"text":"show","start":14,"end":18,"id":2},{"text":"that","start":19,"end":23,"id":3},{"text":"optimization","start":24,"end":36,"id":4},{"text":"is","start":37,"end":39,"id":5},{"text":"computationally","start":40,"end":55,"id":6},{"text":"expensive","start":56,"end":65,"id":7},{"text":".","start":65,"end":66,"id":8}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"$Keywords$ Adaptive Lasso;","_input_hash":-1254579297,"_task_hash":1245077399,"tokens":[{"text":"$","start":0,"end":1,"id":0},{"text":"Keywords$","start":1,"end":10,"id":1},{"text":"Adaptive","start":11,"end":19,"id":2},{"text":"Lasso","start":20,"end":25,"id":3},{"text":";","start":25,"end":26,"id":4}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":11,"end":25,"token_start":2,"token_end":3,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We provide a complete explicit characterisation of the convexity of composite binary losses in terms of the link function and the weight function associated with the proper loss which make up the composite loss.","_input_hash":1564873780,"_task_hash":-981444873,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"provide","start":3,"end":10,"id":1},{"text":"a","start":11,"end":12,"id":2},{"text":"complete","start":13,"end":21,"id":3},{"text":"explicit","start":22,"end":30,"id":4},{"text":"characterisation","start":31,"end":47,"id":5},{"text":"of","start":48,"end":50,"id":6},{"text":"the","start":51,"end":54,"id":7},{"text":"convexity","start":55,"end":64,"id":8},{"text":"of","start":65,"end":67,"id":9},{"text":"composite","start":68,"end":77,"id":10},{"text":"binary","start":78,"end":84,"id":11},{"text":"losses","start":85,"end":91,"id":12},{"text":"in","start":92,"end":94,"id":13},{"text":"terms","start":95,"end":100,"id":14},{"text":"of","start":101,"end":103,"id":15},{"text":"the","start":104,"end":107,"id":16},{"text":"link","start":108,"end":112,"id":17},{"text":"function","start":113,"end":121,"id":18},{"text":"and","start":122,"end":125,"id":19},{"text":"the","start":126,"end":129,"id":20},{"text":"weight","start":130,"end":136,"id":21},{"text":"function","start":137,"end":145,"id":22},{"text":"associated","start":146,"end":156,"id":23},{"text":"with","start":157,"end":161,"id":24},{"text":"the","start":162,"end":165,"id":25},{"text":"proper","start":166,"end":172,"id":26},{"text":"loss","start":173,"end":177,"id":27},{"text":"which","start":178,"end":183,"id":28},{"text":"make","start":184,"end":188,"id":29},{"text":"up","start":189,"end":191,"id":30},{"text":"the","start":192,"end":195,"id":31},{"text":"composite","start":196,"end":205,"id":32},{"text":"loss","start":206,"end":210,"id":33},{"text":".","start":210,"end":211,"id":34}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"A brief discussion about learning and symmetry breaking based on our results is also presented.","_input_hash":-1615577558,"_task_hash":102416772,"tokens":[{"text":"A","start":0,"end":1,"id":0},{"text":"brief","start":2,"end":7,"id":1},{"text":"discussion","start":8,"end":18,"id":2},{"text":"about","start":19,"end":24,"id":3},{"text":"learning","start":25,"end":33,"id":4},{"text":"and","start":34,"end":37,"id":5},{"text":"symmetry","start":38,"end":46,"id":6},{"text":"breaking","start":47,"end":55,"id":7},{"text":"based","start":56,"end":61,"id":8},{"text":"on","start":62,"end":64,"id":9},{"text":"our","start":65,"end":68,"id":10},{"text":"results","start":69,"end":76,"id":11},{"text":"is","start":77,"end":79,"id":12},{"text":"also","start":80,"end":84,"id":13},{"text":"presented","start":85,"end":94,"id":14},{"text":".","start":94,"end":95,"id":15}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Under this unified framework, we present an overview of existing results and discuss their connections.","_input_hash":923015972,"_task_hash":869063557,"tokens":[{"text":"Under","start":0,"end":5,"id":0},{"text":"this","start":6,"end":10,"id":1},{"text":"unified","start":11,"end":18,"id":2},{"text":"framework","start":19,"end":28,"id":3},{"text":",","start":28,"end":29,"id":4},{"text":"we","start":30,"end":32,"id":5},{"text":"present","start":33,"end":40,"id":6},{"text":"an","start":41,"end":43,"id":7},{"text":"overview","start":44,"end":52,"id":8},{"text":"of","start":53,"end":55,"id":9},{"text":"existing","start":56,"end":64,"id":10},{"text":"results","start":65,"end":72,"id":11},{"text":"and","start":73,"end":76,"id":12},{"text":"discuss","start":77,"end":84,"id":13},{"text":"their","start":85,"end":90,"id":14},{"text":"connections","start":91,"end":102,"id":15},{"text":".","start":102,"end":103,"id":16}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Some experiments show advantages and shortcomings of the resulting regression method in comparison to the standard Nadaraya-Watson regression technique, which approximates the optimum by the expectation value.","_input_hash":730716699,"_task_hash":1295401661,"tokens":[{"text":"Some","start":0,"end":4,"id":0},{"text":"experiments","start":5,"end":16,"id":1},{"text":"show","start":17,"end":21,"id":2},{"text":"advantages","start":22,"end":32,"id":3},{"text":"and","start":33,"end":36,"id":4},{"text":"shortcomings","start":37,"end":49,"id":5},{"text":"of","start":50,"end":52,"id":6},{"text":"the","start":53,"end":56,"id":7},{"text":"resulting","start":57,"end":66,"id":8},{"text":"regression","start":67,"end":77,"id":9},{"text":"method","start":78,"end":84,"id":10},{"text":"in","start":85,"end":87,"id":11},{"text":"comparison","start":88,"end":98,"id":12},{"text":"to","start":99,"end":101,"id":13},{"text":"the","start":102,"end":105,"id":14},{"text":"standard","start":106,"end":114,"id":15},{"text":"Nadaraya","start":115,"end":123,"id":16},{"text":"-","start":123,"end":124,"id":17},{"text":"Watson","start":124,"end":130,"id":18},{"text":"regression","start":131,"end":141,"id":19},{"text":"technique","start":142,"end":151,"id":20},{"text":",","start":151,"end":152,"id":21},{"text":"which","start":153,"end":158,"id":22},{"text":"approximates","start":159,"end":171,"id":23},{"text":"the","start":172,"end":175,"id":24},{"text":"optimum","start":176,"end":183,"id":25},{"text":"by","start":184,"end":186,"id":26},{"text":"the","start":187,"end":190,"id":27},{"text":"expectation","start":191,"end":202,"id":28},{"text":"value","start":203,"end":208,"id":29},{"text":".","start":208,"end":209,"id":30}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":115,"end":141,"token_start":16,"token_end":19,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"it also implies new results that improve existing ones even for standard formulations such as L1 regularization.","_input_hash":-1851492984,"_task_hash":260557002,"tokens":[{"text":"it","start":0,"end":2,"id":0},{"text":"also","start":3,"end":7,"id":1},{"text":"implies","start":8,"end":15,"id":2},{"text":"new","start":16,"end":19,"id":3},{"text":"results","start":20,"end":27,"id":4},{"text":"that","start":28,"end":32,"id":5},{"text":"improve","start":33,"end":40,"id":6},{"text":"existing","start":41,"end":49,"id":7},{"text":"ones","start":50,"end":54,"id":8},{"text":"even","start":55,"end":59,"id":9},{"text":"for","start":60,"end":63,"id":10},{"text":"standard","start":64,"end":72,"id":11},{"text":"formulations","start":73,"end":85,"id":12},{"text":"such","start":86,"end":90,"id":13},{"text":"as","start":91,"end":93,"id":14},{"text":"L1","start":94,"end":96,"id":15},{"text":"regularization","start":97,"end":111,"id":16},{"text":".","start":111,"end":112,"id":17}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We show how it can be interpreted as a density model in the observed space.","_input_hash":-1130648324,"_task_hash":-449197602,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"show","start":3,"end":7,"id":1},{"text":"how","start":8,"end":11,"id":2},{"text":"it","start":12,"end":14,"id":3},{"text":"can","start":15,"end":18,"id":4},{"text":"be","start":19,"end":21,"id":5},{"text":"interpreted","start":22,"end":33,"id":6},{"text":"as","start":34,"end":36,"id":7},{"text":"a","start":37,"end":38,"id":8},{"text":"density","start":39,"end":46,"id":9},{"text":"model","start":47,"end":52,"id":10},{"text":"in","start":53,"end":55,"id":11},{"text":"the","start":56,"end":59,"id":12},{"text":"observed","start":60,"end":68,"id":13},{"text":"space","start":69,"end":74,"id":14},{"text":".","start":74,"end":75,"id":15}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We study the empirical performance of the DILN topic model on four corpora, comparing performance with the HDP and the correlated topic model (CTM).","_input_hash":1285059631,"_task_hash":1948425527,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"study","start":3,"end":8,"id":1},{"text":"the","start":9,"end":12,"id":2},{"text":"empirical","start":13,"end":22,"id":3},{"text":"performance","start":23,"end":34,"id":4},{"text":"of","start":35,"end":37,"id":5},{"text":"the","start":38,"end":41,"id":6},{"text":"DILN","start":42,"end":46,"id":7},{"text":"topic","start":47,"end":52,"id":8},{"text":"model","start":53,"end":58,"id":9},{"text":"on","start":59,"end":61,"id":10},{"text":"four","start":62,"end":66,"id":11},{"text":"corpora","start":67,"end":74,"id":12},{"text":",","start":74,"end":75,"id":13},{"text":"comparing","start":76,"end":85,"id":14},{"text":"performance","start":86,"end":97,"id":15},{"text":"with","start":98,"end":102,"id":16},{"text":"the","start":103,"end":106,"id":17},{"text":"HDP","start":107,"end":110,"id":18},{"text":"and","start":111,"end":114,"id":19},{"text":"the","start":115,"end":118,"id":20},{"text":"correlated","start":119,"end":129,"id":21},{"text":"topic","start":130,"end":135,"id":22},{"text":"model","start":136,"end":141,"id":23},{"text":"(","start":142,"end":143,"id":24},{"text":"CTM","start":143,"end":146,"id":25},{"text":")","start":146,"end":147,"id":26},{"text":".","start":147,"end":148,"id":27}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":42,"end":52,"token_start":7,"token_end":8,"label":"ALGO","answer":"accept"},{"start":119,"end":135,"token_start":21,"token_end":22,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We demonstrate the application of our methodology in the context of decentralized anomaly detection in the Abilene backbone network.","_input_hash":178100919,"_task_hash":-249685443,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"demonstrate","start":3,"end":14,"id":1},{"text":"the","start":15,"end":18,"id":2},{"text":"application","start":19,"end":30,"id":3},{"text":"of","start":31,"end":33,"id":4},{"text":"our","start":34,"end":37,"id":5},{"text":"methodology","start":38,"end":49,"id":6},{"text":"in","start":50,"end":52,"id":7},{"text":"the","start":53,"end":56,"id":8},{"text":"context","start":57,"end":64,"id":9},{"text":"of","start":65,"end":67,"id":10},{"text":"decentralized","start":68,"end":81,"id":11},{"text":"anomaly","start":82,"end":89,"id":12},{"text":"detection","start":90,"end":99,"id":13},{"text":"in","start":100,"end":102,"id":14},{"text":"the","start":103,"end":106,"id":15},{"text":"Abilene","start":107,"end":114,"id":16},{"text":"backbone","start":115,"end":123,"id":17},{"text":"network","start":124,"end":131,"id":18},{"text":".","start":131,"end":132,"id":19}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We introduce a new nearest-prototype classifier, the prototype vector machine (PVM).","_input_hash":-69021237,"_task_hash":689094382,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"introduce","start":3,"end":12,"id":1},{"text":"a","start":13,"end":14,"id":2},{"text":"new","start":15,"end":18,"id":3},{"text":"nearest","start":19,"end":26,"id":4},{"text":"-","start":26,"end":27,"id":5},{"text":"prototype","start":27,"end":36,"id":6},{"text":"classifier","start":37,"end":47,"id":7},{"text":",","start":47,"end":48,"id":8},{"text":"the","start":49,"end":52,"id":9},{"text":"prototype","start":53,"end":62,"id":10},{"text":"vector","start":63,"end":69,"id":11},{"text":"machine","start":70,"end":77,"id":12},{"text":"(","start":78,"end":79,"id":13},{"text":"PVM","start":79,"end":82,"id":14},{"text":")","start":82,"end":83,"id":15},{"text":".","start":83,"end":84,"id":16}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":19,"end":47,"token_start":4,"token_end":7,"label":"ALGO","answer":"accept"},{"start":53,"end":77,"token_start":10,"token_end":12,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We exploit the conditional independencies present naturally in the model.","_input_hash":1482967409,"_task_hash":-1911340019,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"exploit","start":3,"end":10,"id":1},{"text":"the","start":11,"end":14,"id":2},{"text":"conditional","start":15,"end":26,"id":3},{"text":"independencies","start":27,"end":41,"id":4},{"text":"present","start":42,"end":49,"id":5},{"text":"naturally","start":50,"end":59,"id":6},{"text":"in","start":60,"end":62,"id":7},{"text":"the","start":63,"end":66,"id":8},{"text":"model","start":67,"end":72,"id":9},{"text":".","start":72,"end":73,"id":10}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"However, in several applications, the standard model does not describe the important features of the data.","_input_hash":-337129729,"_task_hash":779213477,"tokens":[{"text":"However","start":0,"end":7,"id":0},{"text":",","start":7,"end":8,"id":1},{"text":"in","start":9,"end":11,"id":2},{"text":"several","start":12,"end":19,"id":3},{"text":"applications","start":20,"end":32,"id":4},{"text":",","start":32,"end":33,"id":5},{"text":"the","start":34,"end":37,"id":6},{"text":"standard","start":38,"end":46,"id":7},{"text":"model","start":47,"end":52,"id":8},{"text":"does","start":53,"end":57,"id":9},{"text":"not","start":58,"end":61,"id":10},{"text":"describe","start":62,"end":70,"id":11},{"text":"the","start":71,"end":74,"id":12},{"text":"important","start":75,"end":84,"id":13},{"text":"features","start":85,"end":93,"id":14},{"text":"of","start":94,"end":96,"id":15},{"text":"the","start":97,"end":100,"id":16},{"text":"data","start":101,"end":105,"id":17},{"text":".","start":105,"end":106,"id":18}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"This enables us to prove that it is reasonable to select the number of clusters based on stability scores.","_input_hash":775854868,"_task_hash":-1528433280,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"enables","start":5,"end":12,"id":1},{"text":"us","start":13,"end":15,"id":2},{"text":"to","start":16,"end":18,"id":3},{"text":"prove","start":19,"end":24,"id":4},{"text":"that","start":25,"end":29,"id":5},{"text":"it","start":30,"end":32,"id":6},{"text":"is","start":33,"end":35,"id":7},{"text":"reasonable","start":36,"end":46,"id":8},{"text":"to","start":47,"end":49,"id":9},{"text":"select","start":50,"end":56,"id":10},{"text":"the","start":57,"end":60,"id":11},{"text":"number","start":61,"end":67,"id":12},{"text":"of","start":68,"end":70,"id":13},{"text":"clusters","start":71,"end":79,"id":14},{"text":"based","start":80,"end":85,"id":15},{"text":"on","start":86,"end":88,"id":16},{"text":"stability","start":89,"end":98,"id":17},{"text":"scores","start":99,"end":105,"id":18},{"text":".","start":105,"end":106,"id":19}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"The new REMAPF algorithm estimates both the epsilon-machine and the decisional states from data.","_input_hash":-1763963857,"_task_hash":1528574478,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"new","start":4,"end":7,"id":1},{"text":"REMAPF","start":8,"end":14,"id":2},{"text":"algorithm","start":15,"end":24,"id":3},{"text":"estimates","start":25,"end":34,"id":4},{"text":"both","start":35,"end":39,"id":5},{"text":"the","start":40,"end":43,"id":6},{"text":"epsilon","start":44,"end":51,"id":7},{"text":"-","start":51,"end":52,"id":8},{"text":"machine","start":52,"end":59,"id":9},{"text":"and","start":60,"end":63,"id":10},{"text":"the","start":64,"end":67,"id":11},{"text":"decisional","start":68,"end":78,"id":12},{"text":"states","start":79,"end":85,"id":13},{"text":"from","start":86,"end":90,"id":14},{"text":"data","start":91,"end":95,"id":15},{"text":".","start":95,"end":96,"id":16}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":8,"end":14,"token_start":2,"token_end":2,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Some applications of this method in the multi/hyper-spectral imagery domain to a satellite view of Paris and to an image of the Mars planet are also presented.","_input_hash":2095742317,"_task_hash":530776238,"tokens":[{"text":"Some","start":0,"end":4,"id":0},{"text":"applications","start":5,"end":17,"id":1},{"text":"of","start":18,"end":20,"id":2},{"text":"this","start":21,"end":25,"id":3},{"text":"method","start":26,"end":32,"id":4},{"text":"in","start":33,"end":35,"id":5},{"text":"the","start":36,"end":39,"id":6},{"text":"multi","start":40,"end":45,"id":7},{"text":"/","start":45,"end":46,"id":8},{"text":"hyper","start":46,"end":51,"id":9},{"text":"-","start":51,"end":52,"id":10},{"text":"spectral","start":52,"end":60,"id":11},{"text":"imagery","start":61,"end":68,"id":12},{"text":"domain","start":69,"end":75,"id":13},{"text":"to","start":76,"end":78,"id":14},{"text":"a","start":79,"end":80,"id":15},{"text":"satellite","start":81,"end":90,"id":16},{"text":"view","start":91,"end":95,"id":17},{"text":"of","start":96,"end":98,"id":18},{"text":"Paris","start":99,"end":104,"id":19},{"text":"and","start":105,"end":108,"id":20},{"text":"to","start":109,"end":111,"id":21},{"text":"an","start":112,"end":114,"id":22},{"text":"image","start":115,"end":120,"id":23},{"text":"of","start":121,"end":123,"id":24},{"text":"the","start":124,"end":127,"id":25},{"text":"Mars","start":128,"end":132,"id":26},{"text":"planet","start":133,"end":139,"id":27},{"text":"are","start":140,"end":143,"id":28},{"text":"also","start":144,"end":148,"id":29},{"text":"presented","start":149,"end":158,"id":30},{"text":".","start":158,"end":159,"id":31}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"In an extensive experimental study we show that this algorithm works both on synthetic and real data sets.","_input_hash":-2100705351,"_task_hash":-1297722206,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"an","start":3,"end":5,"id":1},{"text":"extensive","start":6,"end":15,"id":2},{"text":"experimental","start":16,"end":28,"id":3},{"text":"study","start":29,"end":34,"id":4},{"text":"we","start":35,"end":37,"id":5},{"text":"show","start":38,"end":42,"id":6},{"text":"that","start":43,"end":47,"id":7},{"text":"this","start":48,"end":52,"id":8},{"text":"algorithm","start":53,"end":62,"id":9},{"text":"works","start":63,"end":68,"id":10},{"text":"both","start":69,"end":73,"id":11},{"text":"on","start":74,"end":76,"id":12},{"text":"synthetic","start":77,"end":86,"id":13},{"text":"and","start":87,"end":90,"id":14},{"text":"real","start":91,"end":95,"id":15},{"text":"data","start":96,"end":100,"id":16},{"text":"sets","start":101,"end":105,"id":17},{"text":".","start":105,"end":106,"id":18}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"rather, we form kernel density estimates of the bivariate and univariate marginals, and apply Kruskal's algorithm to estimate the optimal forest on held out data.","_input_hash":-1117850548,"_task_hash":-193204563,"tokens":[{"text":"rather","start":0,"end":6,"id":0},{"text":",","start":6,"end":7,"id":1},{"text":"we","start":8,"end":10,"id":2},{"text":"form","start":11,"end":15,"id":3},{"text":"kernel","start":16,"end":22,"id":4},{"text":"density","start":23,"end":30,"id":5},{"text":"estimates","start":31,"end":40,"id":6},{"text":"of","start":41,"end":43,"id":7},{"text":"the","start":44,"end":47,"id":8},{"text":"bivariate","start":48,"end":57,"id":9},{"text":"and","start":58,"end":61,"id":10},{"text":"univariate","start":62,"end":72,"id":11},{"text":"marginals","start":73,"end":82,"id":12},{"text":",","start":82,"end":83,"id":13},{"text":"and","start":84,"end":87,"id":14},{"text":"apply","start":88,"end":93,"id":15},{"text":"Kruskal","start":94,"end":101,"id":16},{"text":"'s","start":101,"end":103,"id":17},{"text":"algorithm","start":104,"end":113,"id":18},{"text":"to","start":114,"end":116,"id":19},{"text":"estimate","start":117,"end":125,"id":20},{"text":"the","start":126,"end":129,"id":21},{"text":"optimal","start":130,"end":137,"id":22},{"text":"forest","start":138,"end":144,"id":23},{"text":"on","start":145,"end":147,"id":24},{"text":"held","start":148,"end":152,"id":25},{"text":"out","start":153,"end":156,"id":26},{"text":"data","start":157,"end":161,"id":27},{"text":".","start":161,"end":162,"id":28}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":94,"end":103,"token_start":16,"token_end":17,"label":"ALGO","answer":"accept"},{"start":138,"end":144,"token_start":23,"token_end":23,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"For completeness, we include numerical simulations in the paper.","_input_hash":-1381506018,"_task_hash":370247468,"tokens":[{"text":"For","start":0,"end":3,"id":0},{"text":"completeness","start":4,"end":16,"id":1},{"text":",","start":16,"end":17,"id":2},{"text":"we","start":18,"end":20,"id":3},{"text":"include","start":21,"end":28,"id":4},{"text":"numerical","start":29,"end":38,"id":5},{"text":"simulations","start":39,"end":50,"id":6},{"text":"in","start":51,"end":53,"id":7},{"text":"the","start":54,"end":57,"id":8},{"text":"paper","start":58,"end":63,"id":9},{"text":".","start":63,"end":64,"id":10}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Also, we propose clustering regularization restricting creation of additional clusters which are not significant or are not essentially different comparing with existing clusters.","_input_hash":-1797782799,"_task_hash":-1702199178,"tokens":[{"text":"Also","start":0,"end":4,"id":0},{"text":",","start":4,"end":5,"id":1},{"text":"we","start":6,"end":8,"id":2},{"text":"propose","start":9,"end":16,"id":3},{"text":"clustering","start":17,"end":27,"id":4},{"text":"regularization","start":28,"end":42,"id":5},{"text":"restricting","start":43,"end":54,"id":6},{"text":"creation","start":55,"end":63,"id":7},{"text":"of","start":64,"end":66,"id":8},{"text":"additional","start":67,"end":77,"id":9},{"text":"clusters","start":78,"end":86,"id":10},{"text":"which","start":87,"end":92,"id":11},{"text":"are","start":93,"end":96,"id":12},{"text":"not","start":97,"end":100,"id":13},{"text":"significant","start":101,"end":112,"id":14},{"text":"or","start":113,"end":115,"id":15},{"text":"are","start":116,"end":119,"id":16},{"text":"not","start":120,"end":123,"id":17},{"text":"essentially","start":124,"end":135,"id":18},{"text":"different","start":136,"end":145,"id":19},{"text":"comparing","start":146,"end":155,"id":20},{"text":"with","start":156,"end":160,"id":21},{"text":"existing","start":161,"end":169,"id":22},{"text":"clusters","start":170,"end":178,"id":23},{"text":".","start":178,"end":179,"id":24}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Recent methods for estimating sparse undirected graphs for real-valued data in high dimensional problems rely heavily on the assumption of normality.","_input_hash":1125546306,"_task_hash":1720958623,"tokens":[{"text":"Recent","start":0,"end":6,"id":0},{"text":"methods","start":7,"end":14,"id":1},{"text":"for","start":15,"end":18,"id":2},{"text":"estimating","start":19,"end":29,"id":3},{"text":"sparse","start":30,"end":36,"id":4},{"text":"undirected","start":37,"end":47,"id":5},{"text":"graphs","start":48,"end":54,"id":6},{"text":"for","start":55,"end":58,"id":7},{"text":"real","start":59,"end":63,"id":8},{"text":"-","start":63,"end":64,"id":9},{"text":"valued","start":64,"end":70,"id":10},{"text":"data","start":71,"end":75,"id":11},{"text":"in","start":76,"end":78,"id":12},{"text":"high","start":79,"end":83,"id":13},{"text":"dimensional","start":84,"end":95,"id":14},{"text":"problems","start":96,"end":104,"id":15},{"text":"rely","start":105,"end":109,"id":16},{"text":"heavily","start":110,"end":117,"id":17},{"text":"on","start":118,"end":120,"id":18},{"text":"the","start":121,"end":124,"id":19},{"text":"assumption","start":125,"end":135,"id":20},{"text":"of","start":136,"end":138,"id":21},{"text":"normality","start":139,"end":148,"id":22},{"text":".","start":148,"end":149,"id":23}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":0,"end":6,"token_start":0,"token_end":0,"label":"ALGO","answer":"reject"},{"start":7,"end":14,"token_start":1,"token_end":1,"label":"ALGO","answer":"reject"},{"start":19,"end":29,"token_start":3,"token_end":3,"label":"ALGO","answer":"reject"}],"answer":"reject"}
{"text":"Since selectors are suboptimal aggregation procedures, this proves that 2 is the minimal number of elements of $F$ required for the construction of an optimal aggregation procedures in every situations.","_input_hash":293426480,"_task_hash":-1564923605,"tokens":[{"text":"Since","start":0,"end":5,"id":0},{"text":"selectors","start":6,"end":15,"id":1},{"text":"are","start":16,"end":19,"id":2},{"text":"suboptimal","start":20,"end":30,"id":3},{"text":"aggregation","start":31,"end":42,"id":4},{"text":"procedures","start":43,"end":53,"id":5},{"text":",","start":53,"end":54,"id":6},{"text":"this","start":55,"end":59,"id":7},{"text":"proves","start":60,"end":66,"id":8},{"text":"that","start":67,"end":71,"id":9},{"text":"2","start":72,"end":73,"id":10},{"text":"is","start":74,"end":76,"id":11},{"text":"the","start":77,"end":80,"id":12},{"text":"minimal","start":81,"end":88,"id":13},{"text":"number","start":89,"end":95,"id":14},{"text":"of","start":96,"end":98,"id":15},{"text":"elements","start":99,"end":107,"id":16},{"text":"of","start":108,"end":110,"id":17},{"text":"$","start":111,"end":112,"id":18},{"text":"F$","start":112,"end":114,"id":19},{"text":"required","start":115,"end":123,"id":20},{"text":"for","start":124,"end":127,"id":21},{"text":"the","start":128,"end":131,"id":22},{"text":"construction","start":132,"end":144,"id":23},{"text":"of","start":145,"end":147,"id":24},{"text":"an","start":148,"end":150,"id":25},{"text":"optimal","start":151,"end":158,"id":26},{"text":"aggregation","start":159,"end":170,"id":27},{"text":"procedures","start":171,"end":181,"id":28},{"text":"in","start":182,"end":184,"id":29},{"text":"every","start":185,"end":190,"id":30},{"text":"situations","start":191,"end":201,"id":31},{"text":".","start":201,"end":202,"id":32}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We further show that elastic-net MKL requires a milder condition for being consistent than block-l1 MKL.","_input_hash":473646691,"_task_hash":-2015993868,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"further","start":3,"end":10,"id":1},{"text":"show","start":11,"end":15,"id":2},{"text":"that","start":16,"end":20,"id":3},{"text":"elastic","start":21,"end":28,"id":4},{"text":"-","start":28,"end":29,"id":5},{"text":"net","start":29,"end":32,"id":6},{"text":"MKL","start":33,"end":36,"id":7},{"text":"requires","start":37,"end":45,"id":8},{"text":"a","start":46,"end":47,"id":9},{"text":"milder","start":48,"end":54,"id":10},{"text":"condition","start":55,"end":64,"id":11},{"text":"for","start":65,"end":68,"id":12},{"text":"being","start":69,"end":74,"id":13},{"text":"consistent","start":75,"end":85,"id":14},{"text":"than","start":86,"end":90,"id":15},{"text":"block","start":91,"end":96,"id":16},{"text":"-","start":96,"end":97,"id":17},{"text":"l1","start":97,"end":99,"id":18},{"text":"MKL","start":100,"end":103,"id":19},{"text":".","start":103,"end":104,"id":20}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We establish the basic properties of these penalty functions and discuss some examples where they can be computed explicitly.","_input_hash":-2019039687,"_task_hash":-2138102470,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"establish","start":3,"end":12,"id":1},{"text":"the","start":13,"end":16,"id":2},{"text":"basic","start":17,"end":22,"id":3},{"text":"properties","start":23,"end":33,"id":4},{"text":"of","start":34,"end":36,"id":5},{"text":"these","start":37,"end":42,"id":6},{"text":"penalty","start":43,"end":50,"id":7},{"text":"functions","start":51,"end":60,"id":8},{"text":"and","start":61,"end":64,"id":9},{"text":"discuss","start":65,"end":72,"id":10},{"text":"some","start":73,"end":77,"id":11},{"text":"examples","start":78,"end":86,"id":12},{"text":"where","start":87,"end":92,"id":13},{"text":"they","start":93,"end":97,"id":14},{"text":"can","start":98,"end":101,"id":15},{"text":"be","start":102,"end":104,"id":16},{"text":"computed","start":105,"end":113,"id":17},{"text":"explicitly","start":114,"end":124,"id":18},{"text":".","start":124,"end":125,"id":19}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We extend the result in the setting where the number of blocks grows slowly with the number of nodes.","_input_hash":1165733174,"_task_hash":-1551453896,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"extend","start":3,"end":9,"id":1},{"text":"the","start":10,"end":13,"id":2},{"text":"result","start":14,"end":20,"id":3},{"text":"in","start":21,"end":23,"id":4},{"text":"the","start":24,"end":27,"id":5},{"text":"setting","start":28,"end":35,"id":6},{"text":"where","start":36,"end":41,"id":7},{"text":"the","start":42,"end":45,"id":8},{"text":"number","start":46,"end":52,"id":9},{"text":"of","start":53,"end":55,"id":10},{"text":"blocks","start":56,"end":62,"id":11},{"text":"grows","start":63,"end":68,"id":12},{"text":"slowly","start":69,"end":75,"id":13},{"text":"with","start":76,"end":80,"id":14},{"text":"the","start":81,"end":84,"id":15},{"text":"number","start":85,"end":91,"id":16},{"text":"of","start":92,"end":94,"id":17},{"text":"nodes","start":95,"end":100,"id":18},{"text":".","start":100,"end":101,"id":19}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We also introduce a kernel-free framework to analyze graph constructions with shrinking neighborhoods in general and apply it to analyze locally linear embedding (LLE).","_input_hash":-452178091,"_task_hash":-1600898839,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"also","start":3,"end":7,"id":1},{"text":"introduce","start":8,"end":17,"id":2},{"text":"a","start":18,"end":19,"id":3},{"text":"kernel","start":20,"end":26,"id":4},{"text":"-","start":26,"end":27,"id":5},{"text":"free","start":27,"end":31,"id":6},{"text":"framework","start":32,"end":41,"id":7},{"text":"to","start":42,"end":44,"id":8},{"text":"analyze","start":45,"end":52,"id":9},{"text":"graph","start":53,"end":58,"id":10},{"text":"constructions","start":59,"end":72,"id":11},{"text":"with","start":73,"end":77,"id":12},{"text":"shrinking","start":78,"end":87,"id":13},{"text":"neighborhoods","start":88,"end":101,"id":14},{"text":"in","start":102,"end":104,"id":15},{"text":"general","start":105,"end":112,"id":16},{"text":"and","start":113,"end":116,"id":17},{"text":"apply","start":117,"end":122,"id":18},{"text":"it","start":123,"end":125,"id":19},{"text":"to","start":126,"end":128,"id":20},{"text":"analyze","start":129,"end":136,"id":21},{"text":"locally","start":137,"end":144,"id":22},{"text":"linear","start":145,"end":151,"id":23},{"text":"embedding","start":152,"end":161,"id":24},{"text":"(","start":162,"end":163,"id":25},{"text":"LLE","start":163,"end":166,"id":26},{"text":")","start":166,"end":167,"id":27},{"text":".","start":167,"end":168,"id":28}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":137,"end":161,"token_start":22,"token_end":24,"label":"ALGO","answer":"accept"},{"start":163,"end":166,"token_start":26,"token_end":26,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"2005) may involve a set of features of a graph that is exponential in the number of vertices.","_input_hash":181811664,"_task_hash":1250550150,"tokens":[{"text":"2005","start":0,"end":4,"id":0},{"text":")","start":4,"end":5,"id":1},{"text":"may","start":6,"end":9,"id":2},{"text":"involve","start":10,"end":17,"id":3},{"text":"a","start":18,"end":19,"id":4},{"text":"set","start":20,"end":23,"id":5},{"text":"of","start":24,"end":26,"id":6},{"text":"features","start":27,"end":35,"id":7},{"text":"of","start":36,"end":38,"id":8},{"text":"a","start":39,"end":40,"id":9},{"text":"graph","start":41,"end":46,"id":10},{"text":"that","start":47,"end":51,"id":11},{"text":"is","start":52,"end":54,"id":12},{"text":"exponential","start":55,"end":66,"id":13},{"text":"in","start":67,"end":69,"id":14},{"text":"the","start":70,"end":73,"id":15},{"text":"number","start":74,"end":80,"id":16},{"text":"of","start":81,"end":83,"id":17},{"text":"vertices","start":84,"end":92,"id":18},{"text":".","start":92,"end":93,"id":19}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We demonstrate this algorithm on collections of scientific abstracts from several journals.","_input_hash":1728424860,"_task_hash":-1387184893,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"demonstrate","start":3,"end":14,"id":1},{"text":"this","start":15,"end":19,"id":2},{"text":"algorithm","start":20,"end":29,"id":3},{"text":"on","start":30,"end":32,"id":4},{"text":"collections","start":33,"end":44,"id":5},{"text":"of","start":45,"end":47,"id":6},{"text":"scientific","start":48,"end":58,"id":7},{"text":"abstracts","start":59,"end":68,"id":8},{"text":"from","start":69,"end":73,"id":9},{"text":"several","start":74,"end":81,"id":10},{"text":"journals","start":82,"end":90,"id":11},{"text":".","start":90,"end":91,"id":12}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"In many situations, however, the variables of interest are discrete or even have only finitely many states.","_input_hash":-1501278765,"_task_hash":860137680,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"many","start":3,"end":7,"id":1},{"text":"situations","start":8,"end":18,"id":2},{"text":",","start":18,"end":19,"id":3},{"text":"however","start":20,"end":27,"id":4},{"text":",","start":27,"end":28,"id":5},{"text":"the","start":29,"end":32,"id":6},{"text":"variables","start":33,"end":42,"id":7},{"text":"of","start":43,"end":45,"id":8},{"text":"interest","start":46,"end":54,"id":9},{"text":"are","start":55,"end":58,"id":10},{"text":"discrete","start":59,"end":67,"id":11},{"text":"or","start":68,"end":70,"id":12},{"text":"even","start":71,"end":75,"id":13},{"text":"have","start":76,"end":80,"id":14},{"text":"only","start":81,"end":85,"id":15},{"text":"finitely","start":86,"end":94,"id":16},{"text":"many","start":95,"end":99,"id":17},{"text":"states","start":100,"end":106,"id":18},{"text":".","start":106,"end":107,"id":19}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"A rewritten version will be posted in the future.","_input_hash":-946478256,"_task_hash":1798048684,"tokens":[{"text":"A","start":0,"end":1,"id":0},{"text":"rewritten","start":2,"end":11,"id":1},{"text":"version","start":12,"end":19,"id":2},{"text":"will","start":20,"end":24,"id":3},{"text":"be","start":25,"end":27,"id":4},{"text":"posted","start":28,"end":34,"id":5},{"text":"in","start":35,"end":37,"id":6},{"text":"the","start":38,"end":41,"id":7},{"text":"future","start":42,"end":48,"id":8},{"text":".","start":48,"end":49,"id":9}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"and the conditions for sparsistent estimation and the convergence rate of both the partition boundaries and the network structure are established for the first time for such estimators.","_input_hash":-2022113594,"_task_hash":-930676783,"tokens":[{"text":"and","start":0,"end":3,"id":0},{"text":"the","start":4,"end":7,"id":1},{"text":"conditions","start":8,"end":18,"id":2},{"text":"for","start":19,"end":22,"id":3},{"text":"sparsistent","start":23,"end":34,"id":4},{"text":"estimation","start":35,"end":45,"id":5},{"text":"and","start":46,"end":49,"id":6},{"text":"the","start":50,"end":53,"id":7},{"text":"convergence","start":54,"end":65,"id":8},{"text":"rate","start":66,"end":70,"id":9},{"text":"of","start":71,"end":73,"id":10},{"text":"both","start":74,"end":78,"id":11},{"text":"the","start":79,"end":82,"id":12},{"text":"partition","start":83,"end":92,"id":13},{"text":"boundaries","start":93,"end":103,"id":14},{"text":"and","start":104,"end":107,"id":15},{"text":"the","start":108,"end":111,"id":16},{"text":"network","start":112,"end":119,"id":17},{"text":"structure","start":120,"end":129,"id":18},{"text":"are","start":130,"end":133,"id":19},{"text":"established","start":134,"end":145,"id":20},{"text":"for","start":146,"end":149,"id":21},{"text":"the","start":150,"end":153,"id":22},{"text":"first","start":154,"end":159,"id":23},{"text":"time","start":160,"end":164,"id":24},{"text":"for","start":165,"end":168,"id":25},{"text":"such","start":169,"end":173,"id":26},{"text":"estimators","start":174,"end":184,"id":27},{"text":".","start":184,"end":185,"id":28}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Conceptually, it ensures that there exists a function that can predict the label information, without explicitly instantiating that function.","_input_hash":998705639,"_task_hash":-1850091070,"tokens":[{"text":"Conceptually","start":0,"end":12,"id":0},{"text":",","start":12,"end":13,"id":1},{"text":"it","start":14,"end":16,"id":2},{"text":"ensures","start":17,"end":24,"id":3},{"text":"that","start":25,"end":29,"id":4},{"text":"there","start":30,"end":35,"id":5},{"text":"exists","start":36,"end":42,"id":6},{"text":"a","start":43,"end":44,"id":7},{"text":"function","start":45,"end":53,"id":8},{"text":"that","start":54,"end":58,"id":9},{"text":"can","start":59,"end":62,"id":10},{"text":"predict","start":63,"end":70,"id":11},{"text":"the","start":71,"end":74,"id":12},{"text":"label","start":75,"end":80,"id":13},{"text":"information","start":81,"end":92,"id":14},{"text":",","start":92,"end":93,"id":15},{"text":"without","start":94,"end":101,"id":16},{"text":"explicitly","start":102,"end":112,"id":17},{"text":"instantiating","start":113,"end":126,"id":18},{"text":"that","start":127,"end":131,"id":19},{"text":"function","start":132,"end":140,"id":20},{"text":".","start":140,"end":141,"id":21}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"The theory naturally extends from single trees to ensembles of trees and applies to methods like random forests.","_input_hash":316397493,"_task_hash":687200591,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"theory","start":4,"end":10,"id":1},{"text":"naturally","start":11,"end":20,"id":2},{"text":"extends","start":21,"end":28,"id":3},{"text":"from","start":29,"end":33,"id":4},{"text":"single","start":34,"end":40,"id":5},{"text":"trees","start":41,"end":46,"id":6},{"text":"to","start":47,"end":49,"id":7},{"text":"ensembles","start":50,"end":59,"id":8},{"text":"of","start":60,"end":62,"id":9},{"text":"trees","start":63,"end":68,"id":10},{"text":"and","start":69,"end":72,"id":11},{"text":"applies","start":73,"end":80,"id":12},{"text":"to","start":81,"end":83,"id":13},{"text":"methods","start":84,"end":91,"id":14},{"text":"like","start":92,"end":96,"id":15},{"text":"random","start":97,"end":103,"id":16},{"text":"forests","start":104,"end":111,"id":17},{"text":".","start":111,"end":112,"id":18}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":97,"end":111,"token_start":16,"token_end":17,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"The number of compressed parameters may have converged before considering the highest possible order.","_input_hash":291336523,"_task_hash":-1464749194,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"number","start":4,"end":10,"id":1},{"text":"of","start":11,"end":13,"id":2},{"text":"compressed","start":14,"end":24,"id":3},{"text":"parameters","start":25,"end":35,"id":4},{"text":"may","start":36,"end":39,"id":5},{"text":"have","start":40,"end":44,"id":6},{"text":"converged","start":45,"end":54,"id":7},{"text":"before","start":55,"end":61,"id":8},{"text":"considering","start":62,"end":73,"id":9},{"text":"the","start":74,"end":77,"id":10},{"text":"highest","start":78,"end":85,"id":11},{"text":"possible","start":86,"end":94,"id":12},{"text":"order","start":95,"end":100,"id":13},{"text":".","start":100,"end":101,"id":14}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We explore further properties of this unique scheme, stability and convergence are established.","_input_hash":-1864270311,"_task_hash":647865512,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"explore","start":3,"end":10,"id":1},{"text":"further","start":11,"end":18,"id":2},{"text":"properties","start":19,"end":29,"id":3},{"text":"of","start":30,"end":32,"id":4},{"text":"this","start":33,"end":37,"id":5},{"text":"unique","start":38,"end":44,"id":6},{"text":"scheme","start":45,"end":51,"id":7},{"text":",","start":51,"end":52,"id":8},{"text":"stability","start":53,"end":62,"id":9},{"text":"and","start":63,"end":66,"id":10},{"text":"convergence","start":67,"end":78,"id":11},{"text":"are","start":79,"end":82,"id":12},{"text":"established","start":83,"end":94,"id":13},{"text":".","start":94,"end":95,"id":14}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"In order to demonstrate the usefulness of divergences in our problem, the method with informational divergence as similarity measure is compared with the same method using classical metrics.","_input_hash":-1786147655,"_task_hash":-639912228,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"order","start":3,"end":8,"id":1},{"text":"to","start":9,"end":11,"id":2},{"text":"demonstrate","start":12,"end":23,"id":3},{"text":"the","start":24,"end":27,"id":4},{"text":"usefulness","start":28,"end":38,"id":5},{"text":"of","start":39,"end":41,"id":6},{"text":"divergences","start":42,"end":53,"id":7},{"text":"in","start":54,"end":56,"id":8},{"text":"our","start":57,"end":60,"id":9},{"text":"problem","start":61,"end":68,"id":10},{"text":",","start":68,"end":69,"id":11},{"text":"the","start":70,"end":73,"id":12},{"text":"method","start":74,"end":80,"id":13},{"text":"with","start":81,"end":85,"id":14},{"text":"informational","start":86,"end":99,"id":15},{"text":"divergence","start":100,"end":110,"id":16},{"text":"as","start":111,"end":113,"id":17},{"text":"similarity","start":114,"end":124,"id":18},{"text":"measure","start":125,"end":132,"id":19},{"text":"is","start":133,"end":135,"id":20},{"text":"compared","start":136,"end":144,"id":21},{"text":"with","start":145,"end":149,"id":22},{"text":"the","start":150,"end":153,"id":23},{"text":"same","start":154,"end":158,"id":24},{"text":"method","start":159,"end":165,"id":25},{"text":"using","start":166,"end":171,"id":26},{"text":"classical","start":172,"end":181,"id":27},{"text":"metrics","start":182,"end":189,"id":28},{"text":".","start":189,"end":190,"id":29}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"General loss functions and class of predictors with both finite and infinite VC-dimension are considered.","_input_hash":-2123165080,"_task_hash":530920151,"tokens":[{"text":"General","start":0,"end":7,"id":0},{"text":"loss","start":8,"end":12,"id":1},{"text":"functions","start":13,"end":22,"id":2},{"text":"and","start":23,"end":26,"id":3},{"text":"class","start":27,"end":32,"id":4},{"text":"of","start":33,"end":35,"id":5},{"text":"predictors","start":36,"end":46,"id":6},{"text":"with","start":47,"end":51,"id":7},{"text":"both","start":52,"end":56,"id":8},{"text":"finite","start":57,"end":63,"id":9},{"text":"and","start":64,"end":67,"id":10},{"text":"infinite","start":68,"end":76,"id":11},{"text":"VC","start":77,"end":79,"id":12},{"text":"-","start":79,"end":80,"id":13},{"text":"dimension","start":80,"end":89,"id":14},{"text":"are","start":90,"end":93,"id":15},{"text":"considered","start":94,"end":104,"id":16},{"text":".","start":104,"end":105,"id":17}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We present a probabilistic model for natural images which is based on Gaussian scale mixtures and a simple multiscale representation.","_input_hash":1759111278,"_task_hash":-1858320337,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"present","start":3,"end":10,"id":1},{"text":"a","start":11,"end":12,"id":2},{"text":"probabilistic","start":13,"end":26,"id":3},{"text":"model","start":27,"end":32,"id":4},{"text":"for","start":33,"end":36,"id":5},{"text":"natural","start":37,"end":44,"id":6},{"text":"images","start":45,"end":51,"id":7},{"text":"which","start":52,"end":57,"id":8},{"text":"is","start":58,"end":60,"id":9},{"text":"based","start":61,"end":66,"id":10},{"text":"on","start":67,"end":69,"id":11},{"text":"Gaussian","start":70,"end":78,"id":12},{"text":"scale","start":79,"end":84,"id":13},{"text":"mixtures","start":85,"end":93,"id":14},{"text":"and","start":94,"end":97,"id":15},{"text":"a","start":98,"end":99,"id":16},{"text":"simple","start":100,"end":106,"id":17},{"text":"multiscale","start":107,"end":117,"id":18},{"text":"representation","start":118,"end":132,"id":19},{"text":".","start":132,"end":133,"id":20}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":13,"end":26,"token_start":3,"token_end":3,"label":"ALGO","answer":"accept"},{"start":70,"end":93,"token_start":12,"token_end":14,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"On the other hand, if K>1 and p>1, then we show that both all underlying subspaces and the best l0 subspace cannot be recovered and even nearly recovered.","_input_hash":-1290926402,"_task_hash":196427689,"tokens":[{"text":"On","start":0,"end":2,"id":0},{"text":"the","start":3,"end":6,"id":1},{"text":"other","start":7,"end":12,"id":2},{"text":"hand","start":13,"end":17,"id":3},{"text":",","start":17,"end":18,"id":4},{"text":"if","start":19,"end":21,"id":5},{"text":"K>1","start":22,"end":25,"id":6},{"text":"and","start":26,"end":29,"id":7},{"text":"p>1","start":30,"end":33,"id":8},{"text":",","start":33,"end":34,"id":9},{"text":"then","start":35,"end":39,"id":10},{"text":"we","start":40,"end":42,"id":11},{"text":"show","start":43,"end":47,"id":12},{"text":"that","start":48,"end":52,"id":13},{"text":"both","start":53,"end":57,"id":14},{"text":"all","start":58,"end":61,"id":15},{"text":"underlying","start":62,"end":72,"id":16},{"text":"subspaces","start":73,"end":82,"id":17},{"text":"and","start":83,"end":86,"id":18},{"text":"the","start":87,"end":90,"id":19},{"text":"best","start":91,"end":95,"id":20},{"text":"l0","start":96,"end":98,"id":21},{"text":"subspace","start":99,"end":107,"id":22},{"text":"can","start":108,"end":111,"id":23},{"text":"not","start":111,"end":114,"id":24},{"text":"be","start":115,"end":117,"id":25},{"text":"recovered","start":118,"end":127,"id":26},{"text":"and","start":128,"end":131,"id":27},{"text":"even","start":132,"end":136,"id":28},{"text":"nearly","start":137,"end":143,"id":29},{"text":"recovered","start":144,"end":153,"id":30},{"text":".","start":153,"end":154,"id":31}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"Experiments demonstrate the superior scalability of the proposed approach, when compared to the fastest existing RankSVM implementations.","_input_hash":357139733,"_task_hash":-1710842585,"tokens":[{"text":"Experiments","start":0,"end":11,"id":0},{"text":"demonstrate","start":12,"end":23,"id":1},{"text":"the","start":24,"end":27,"id":2},{"text":"superior","start":28,"end":36,"id":3},{"text":"scalability","start":37,"end":48,"id":4},{"text":"of","start":49,"end":51,"id":5},{"text":"the","start":52,"end":55,"id":6},{"text":"proposed","start":56,"end":64,"id":7},{"text":"approach","start":65,"end":73,"id":8},{"text":",","start":73,"end":74,"id":9},{"text":"when","start":75,"end":79,"id":10},{"text":"compared","start":80,"end":88,"id":11},{"text":"to","start":89,"end":91,"id":12},{"text":"the","start":92,"end":95,"id":13},{"text":"fastest","start":96,"end":103,"id":14},{"text":"existing","start":104,"end":112,"id":15},{"text":"RankSVM","start":113,"end":120,"id":16},{"text":"implementations","start":121,"end":136,"id":17},{"text":".","start":136,"end":137,"id":18}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":113,"end":120,"token_start":16,"token_end":16,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Based on the topology of the network, we propose an approximate statistical graphical model and distribute the computation of PCA.","_input_hash":1813334575,"_task_hash":1493916161,"tokens":[{"text":"Based","start":0,"end":5,"id":0},{"text":"on","start":6,"end":8,"id":1},{"text":"the","start":9,"end":12,"id":2},{"text":"topology","start":13,"end":21,"id":3},{"text":"of","start":22,"end":24,"id":4},{"text":"the","start":25,"end":28,"id":5},{"text":"network","start":29,"end":36,"id":6},{"text":",","start":36,"end":37,"id":7},{"text":"we","start":38,"end":40,"id":8},{"text":"propose","start":41,"end":48,"id":9},{"text":"an","start":49,"end":51,"id":10},{"text":"approximate","start":52,"end":63,"id":11},{"text":"statistical","start":64,"end":75,"id":12},{"text":"graphical","start":76,"end":85,"id":13},{"text":"model","start":86,"end":91,"id":14},{"text":"and","start":92,"end":95,"id":15},{"text":"distribute","start":96,"end":106,"id":16},{"text":"the","start":107,"end":110,"id":17},{"text":"computation","start":111,"end":122,"id":18},{"text":"of","start":123,"end":125,"id":19},{"text":"PCA","start":126,"end":129,"id":20},{"text":".","start":129,"end":130,"id":21}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":126,"end":129,"token_start":20,"token_end":20,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"An estimation curve with transition phases depending on the cross-validation procedure and not only on the percentage of observations in the test sample gives a simple rule on how to choose the cross-validation.","_input_hash":1106405742,"_task_hash":-1583822556,"tokens":[{"text":"An","start":0,"end":2,"id":0},{"text":"estimation","start":3,"end":13,"id":1},{"text":"curve","start":14,"end":19,"id":2},{"text":"with","start":20,"end":24,"id":3},{"text":"transition","start":25,"end":35,"id":4},{"text":"phases","start":36,"end":42,"id":5},{"text":"depending","start":43,"end":52,"id":6},{"text":"on","start":53,"end":55,"id":7},{"text":"the","start":56,"end":59,"id":8},{"text":"cross","start":60,"end":65,"id":9},{"text":"-","start":65,"end":66,"id":10},{"text":"validation","start":66,"end":76,"id":11},{"text":"procedure","start":77,"end":86,"id":12},{"text":"and","start":87,"end":90,"id":13},{"text":"not","start":91,"end":94,"id":14},{"text":"only","start":95,"end":99,"id":15},{"text":"on","start":100,"end":102,"id":16},{"text":"the","start":103,"end":106,"id":17},{"text":"percentage","start":107,"end":117,"id":18},{"text":"of","start":118,"end":120,"id":19},{"text":"observations","start":121,"end":133,"id":20},{"text":"in","start":134,"end":136,"id":21},{"text":"the","start":137,"end":140,"id":22},{"text":"test","start":141,"end":145,"id":23},{"text":"sample","start":146,"end":152,"id":24},{"text":"gives","start":153,"end":158,"id":25},{"text":"a","start":159,"end":160,"id":26},{"text":"simple","start":161,"end":167,"id":27},{"text":"rule","start":168,"end":172,"id":28},{"text":"on","start":173,"end":175,"id":29},{"text":"how","start":176,"end":179,"id":30},{"text":"to","start":180,"end":182,"id":31},{"text":"choose","start":183,"end":189,"id":32},{"text":"the","start":190,"end":193,"id":33},{"text":"cross","start":194,"end":199,"id":34},{"text":"-","start":199,"end":200,"id":35},{"text":"validation","start":200,"end":210,"id":36},{"text":".","start":210,"end":211,"id":37}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"  We consider the case of one binary and one real-valued variable where the method can distinguish between cause and effect.","_input_hash":470315968,"_task_hash":-1088335074,"tokens":[{"text":"  ","start":0,"end":2,"id":0},{"text":"We","start":2,"end":4,"id":1},{"text":"consider","start":5,"end":13,"id":2},{"text":"the","start":14,"end":17,"id":3},{"text":"case","start":18,"end":22,"id":4},{"text":"of","start":23,"end":25,"id":5},{"text":"one","start":26,"end":29,"id":6},{"text":"binary","start":30,"end":36,"id":7},{"text":"and","start":37,"end":40,"id":8},{"text":"one","start":41,"end":44,"id":9},{"text":"real","start":45,"end":49,"id":10},{"text":"-","start":49,"end":50,"id":11},{"text":"valued","start":50,"end":56,"id":12},{"text":"variable","start":57,"end":65,"id":13},{"text":"where","start":66,"end":71,"id":14},{"text":"the","start":72,"end":75,"id":15},{"text":"method","start":76,"end":82,"id":16},{"text":"can","start":83,"end":86,"id":17},{"text":"distinguish","start":87,"end":98,"id":18},{"text":"between","start":99,"end":106,"id":19},{"text":"cause","start":107,"end":112,"id":20},{"text":"and","start":113,"end":116,"id":21},{"text":"effect","start":117,"end":123,"id":22},{"text":".","start":123,"end":124,"id":23}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"However, the necessity of obtaining sensitivity measures as degrees of freedom for model selection or confidence intervals for more detailed analysis requires cubic runtime, and thus constitutes a computational bottleneck in real-world data analysis.","_input_hash":-487395949,"_task_hash":807936817,"tokens":[{"text":"However","start":0,"end":7,"id":0},{"text":",","start":7,"end":8,"id":1},{"text":"the","start":9,"end":12,"id":2},{"text":"necessity","start":13,"end":22,"id":3},{"text":"of","start":23,"end":25,"id":4},{"text":"obtaining","start":26,"end":35,"id":5},{"text":"sensitivity","start":36,"end":47,"id":6},{"text":"measures","start":48,"end":56,"id":7},{"text":"as","start":57,"end":59,"id":8},{"text":"degrees","start":60,"end":67,"id":9},{"text":"of","start":68,"end":70,"id":10},{"text":"freedom","start":71,"end":78,"id":11},{"text":"for","start":79,"end":82,"id":12},{"text":"model","start":83,"end":88,"id":13},{"text":"selection","start":89,"end":98,"id":14},{"text":"or","start":99,"end":101,"id":15},{"text":"confidence","start":102,"end":112,"id":16},{"text":"intervals","start":113,"end":122,"id":17},{"text":"for","start":123,"end":126,"id":18},{"text":"more","start":127,"end":131,"id":19},{"text":"detailed","start":132,"end":140,"id":20},{"text":"analysis","start":141,"end":149,"id":21},{"text":"requires","start":150,"end":158,"id":22},{"text":"cubic","start":159,"end":164,"id":23},{"text":"runtime","start":165,"end":172,"id":24},{"text":",","start":172,"end":173,"id":25},{"text":"and","start":174,"end":177,"id":26},{"text":"thus","start":178,"end":182,"id":27},{"text":"constitutes","start":183,"end":194,"id":28},{"text":"a","start":195,"end":196,"id":29},{"text":"computational","start":197,"end":210,"id":30},{"text":"bottleneck","start":211,"end":221,"id":31},{"text":"in","start":222,"end":224,"id":32},{"text":"real","start":225,"end":229,"id":33},{"text":"-","start":229,"end":230,"id":34},{"text":"world","start":230,"end":235,"id":35},{"text":"data","start":236,"end":240,"id":36},{"text":"analysis","start":241,"end":249,"id":37},{"text":".","start":249,"end":250,"id":38}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"We present the Procrustes measure, a novel measure based on Procrustes rotation that enables quantitative comparison of the output of manifold-based embedding algorithms (such as LLE (Roweis and Saul, 2000) and Isomap (Tenenbaum et al, 2000)).","_input_hash":1911758319,"_task_hash":-1685724997,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"present","start":3,"end":10,"id":1},{"text":"the","start":11,"end":14,"id":2},{"text":"Procrustes","start":15,"end":25,"id":3},{"text":"measure","start":26,"end":33,"id":4},{"text":",","start":33,"end":34,"id":5},{"text":"a","start":35,"end":36,"id":6},{"text":"novel","start":37,"end":42,"id":7},{"text":"measure","start":43,"end":50,"id":8},{"text":"based","start":51,"end":56,"id":9},{"text":"on","start":57,"end":59,"id":10},{"text":"Procrustes","start":60,"end":70,"id":11},{"text":"rotation","start":71,"end":79,"id":12},{"text":"that","start":80,"end":84,"id":13},{"text":"enables","start":85,"end":92,"id":14},{"text":"quantitative","start":93,"end":105,"id":15},{"text":"comparison","start":106,"end":116,"id":16},{"text":"of","start":117,"end":119,"id":17},{"text":"the","start":120,"end":123,"id":18},{"text":"output","start":124,"end":130,"id":19},{"text":"of","start":131,"end":133,"id":20},{"text":"manifold","start":134,"end":142,"id":21},{"text":"-","start":142,"end":143,"id":22},{"text":"based","start":143,"end":148,"id":23},{"text":"embedding","start":149,"end":158,"id":24},{"text":"algorithms","start":159,"end":169,"id":25},{"text":"(","start":170,"end":171,"id":26},{"text":"such","start":171,"end":175,"id":27},{"text":"as","start":176,"end":178,"id":28},{"text":"LLE","start":179,"end":182,"id":29},{"text":"(","start":183,"end":184,"id":30},{"text":"Roweis","start":184,"end":190,"id":31},{"text":"and","start":191,"end":194,"id":32},{"text":"Saul","start":195,"end":199,"id":33},{"text":",","start":199,"end":200,"id":34},{"text":"2000","start":201,"end":205,"id":35},{"text":")","start":205,"end":206,"id":36},{"text":"and","start":207,"end":210,"id":37},{"text":"Isomap","start":211,"end":217,"id":38},{"text":"(","start":218,"end":219,"id":39},{"text":"Tenenbaum","start":219,"end":228,"id":40},{"text":"et","start":229,"end":231,"id":41},{"text":"al","start":232,"end":234,"id":42},{"text":",","start":234,"end":235,"id":43},{"text":"2000","start":236,"end":240,"id":44},{"text":")","start":240,"end":241,"id":45},{"text":")","start":241,"end":242,"id":46},{"text":".","start":242,"end":243,"id":47}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":134,"end":158,"token_start":21,"token_end":24,"label":"ALGO","answer":"accept"},{"start":179,"end":182,"token_start":29,"token_end":29,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"bnlearn is an R package which includes several algorithms for learning the structure of Bayesian networks with either discrete or continuous variables.","_input_hash":-1603445648,"_task_hash":1197650306,"tokens":[{"text":"bnlearn","start":0,"end":7,"id":0},{"text":"is","start":8,"end":10,"id":1},{"text":"an","start":11,"end":13,"id":2},{"text":"R","start":14,"end":15,"id":3},{"text":"package","start":16,"end":23,"id":4},{"text":"which","start":24,"end":29,"id":5},{"text":"includes","start":30,"end":38,"id":6},{"text":"several","start":39,"end":46,"id":7},{"text":"algorithms","start":47,"end":57,"id":8},{"text":"for","start":58,"end":61,"id":9},{"text":"learning","start":62,"end":70,"id":10},{"text":"the","start":71,"end":74,"id":11},{"text":"structure","start":75,"end":84,"id":12},{"text":"of","start":85,"end":87,"id":13},{"text":"Bayesian","start":88,"end":96,"id":14},{"text":"networks","start":97,"end":105,"id":15},{"text":"with","start":106,"end":110,"id":16},{"text":"either","start":111,"end":117,"id":17},{"text":"discrete","start":118,"end":126,"id":18},{"text":"or","start":127,"end":129,"id":19},{"text":"continuous","start":130,"end":140,"id":20},{"text":"variables","start":141,"end":150,"id":21},{"text":".","start":150,"end":151,"id":22}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"Our method is efficient and scales gracefully to millions of variables, which we illustrate in two types of applications:","_input_hash":-416997197,"_task_hash":-2096850310,"tokens":[{"text":"Our","start":0,"end":3,"id":0},{"text":"method","start":4,"end":10,"id":1},{"text":"is","start":11,"end":13,"id":2},{"text":"efficient","start":14,"end":23,"id":3},{"text":"and","start":24,"end":27,"id":4},{"text":"scales","start":28,"end":34,"id":5},{"text":"gracefully","start":35,"end":45,"id":6},{"text":"to","start":46,"end":48,"id":7},{"text":"millions","start":49,"end":57,"id":8},{"text":"of","start":58,"end":60,"id":9},{"text":"variables","start":61,"end":70,"id":10},{"text":",","start":70,"end":71,"id":11},{"text":"which","start":72,"end":77,"id":12},{"text":"we","start":78,"end":80,"id":13},{"text":"illustrate","start":81,"end":91,"id":14},{"text":"in","start":92,"end":94,"id":15},{"text":"two","start":95,"end":98,"id":16},{"text":"types","start":99,"end":104,"id":17},{"text":"of","start":105,"end":107,"id":18},{"text":"applications","start":108,"end":120,"id":19},{"text":":","start":120,"end":121,"id":20}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"We show on audio simulations that the online approach is faster for short audio signals and allows to analyze audio signals of several hours.","_input_hash":1228676212,"_task_hash":535445866,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"show","start":3,"end":7,"id":1},{"text":"on","start":8,"end":10,"id":2},{"text":"audio","start":11,"end":16,"id":3},{"text":"simulations","start":17,"end":28,"id":4},{"text":"that","start":29,"end":33,"id":5},{"text":"the","start":34,"end":37,"id":6},{"text":"online","start":38,"end":44,"id":7},{"text":"approach","start":45,"end":53,"id":8},{"text":"is","start":54,"end":56,"id":9},{"text":"faster","start":57,"end":63,"id":10},{"text":"for","start":64,"end":67,"id":11},{"text":"short","start":68,"end":73,"id":12},{"text":"audio","start":74,"end":79,"id":13},{"text":"signals","start":80,"end":87,"id":14},{"text":"and","start":88,"end":91,"id":15},{"text":"allows","start":92,"end":98,"id":16},{"text":"to","start":99,"end":101,"id":17},{"text":"analyze","start":102,"end":109,"id":18},{"text":"audio","start":110,"end":115,"id":19},{"text":"signals","start":116,"end":123,"id":20},{"text":"of","start":124,"end":126,"id":21},{"text":"several","start":127,"end":134,"id":22},{"text":"hours","start":135,"end":140,"id":23},{"text":".","start":140,"end":141,"id":24}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"SVMs can informally be described as a kind of regularized M-estimators for functions and have demonstrated their usefulness in many complicated real-life problems.","_input_hash":-1291340098,"_task_hash":-1764603687,"tokens":[{"text":"SVMs","start":0,"end":4,"id":0},{"text":"can","start":5,"end":8,"id":1},{"text":"informally","start":9,"end":19,"id":2},{"text":"be","start":20,"end":22,"id":3},{"text":"described","start":23,"end":32,"id":4},{"text":"as","start":33,"end":35,"id":5},{"text":"a","start":36,"end":37,"id":6},{"text":"kind","start":38,"end":42,"id":7},{"text":"of","start":43,"end":45,"id":8},{"text":"regularized","start":46,"end":57,"id":9},{"text":"M","start":58,"end":59,"id":10},{"text":"-","start":59,"end":60,"id":11},{"text":"estimators","start":60,"end":70,"id":12},{"text":"for","start":71,"end":74,"id":13},{"text":"functions","start":75,"end":84,"id":14},{"text":"and","start":85,"end":88,"id":15},{"text":"have","start":89,"end":93,"id":16},{"text":"demonstrated","start":94,"end":106,"id":17},{"text":"their","start":107,"end":112,"id":18},{"text":"usefulness","start":113,"end":123,"id":19},{"text":"in","start":124,"end":126,"id":20},{"text":"many","start":127,"end":131,"id":21},{"text":"complicated","start":132,"end":143,"id":22},{"text":"real","start":144,"end":148,"id":23},{"text":"-","start":148,"end":149,"id":24},{"text":"life","start":149,"end":153,"id":25},{"text":"problems","start":154,"end":162,"id":26},{"text":".","start":162,"end":163,"id":27}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":0,"end":4,"token_start":0,"token_end":0,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"The algorithm exploits a close connection between Kernel PLS and the Lanczos algorithm for approximating the eigenvalues of symmetric matrices, and uses this approximation to compute the trace of powers of the kernel matrix in quadratic runtime.","_input_hash":1727047642,"_task_hash":205637703,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"algorithm","start":4,"end":13,"id":1},{"text":"exploits","start":14,"end":22,"id":2},{"text":"a","start":23,"end":24,"id":3},{"text":"close","start":25,"end":30,"id":4},{"text":"connection","start":31,"end":41,"id":5},{"text":"between","start":42,"end":49,"id":6},{"text":"Kernel","start":50,"end":56,"id":7},{"text":"PLS","start":57,"end":60,"id":8},{"text":"and","start":61,"end":64,"id":9},{"text":"the","start":65,"end":68,"id":10},{"text":"Lanczos","start":69,"end":76,"id":11},{"text":"algorithm","start":77,"end":86,"id":12},{"text":"for","start":87,"end":90,"id":13},{"text":"approximating","start":91,"end":104,"id":14},{"text":"the","start":105,"end":108,"id":15},{"text":"eigenvalues","start":109,"end":120,"id":16},{"text":"of","start":121,"end":123,"id":17},{"text":"symmetric","start":124,"end":133,"id":18},{"text":"matrices","start":134,"end":142,"id":19},{"text":",","start":142,"end":143,"id":20},{"text":"and","start":144,"end":147,"id":21},{"text":"uses","start":148,"end":152,"id":22},{"text":"this","start":153,"end":157,"id":23},{"text":"approximation","start":158,"end":171,"id":24},{"text":"to","start":172,"end":174,"id":25},{"text":"compute","start":175,"end":182,"id":26},{"text":"the","start":183,"end":186,"id":27},{"text":"trace","start":187,"end":192,"id":28},{"text":"of","start":193,"end":195,"id":29},{"text":"powers","start":196,"end":202,"id":30},{"text":"of","start":203,"end":205,"id":31},{"text":"the","start":206,"end":209,"id":32},{"text":"kernel","start":210,"end":216,"id":33},{"text":"matrix","start":217,"end":223,"id":34},{"text":"in","start":224,"end":226,"id":35},{"text":"quadratic","start":227,"end":236,"id":36},{"text":"runtime","start":237,"end":244,"id":37},{"text":".","start":244,"end":245,"id":38}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"Vapnik et al. [","_input_hash":-1601220697,"_task_hash":1497483687,"tokens":[{"text":"Vapnik","start":0,"end":6,"id":0},{"text":"et","start":7,"end":9,"id":1},{"text":"al","start":10,"end":12,"id":2},{"text":".","start":12,"end":13,"id":3},{"text":"[","start":14,"end":15,"id":4}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"The update rule is derived by minimizing AdaBoost's loss when viewed in an incremental form.","_input_hash":1538992120,"_task_hash":362230522,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"update","start":4,"end":10,"id":1},{"text":"rule","start":11,"end":15,"id":2},{"text":"is","start":16,"end":18,"id":3},{"text":"derived","start":19,"end":26,"id":4},{"text":"by","start":27,"end":29,"id":5},{"text":"minimizing","start":30,"end":40,"id":6},{"text":"AdaBoost","start":41,"end":49,"id":7},{"text":"'s","start":49,"end":51,"id":8},{"text":"loss","start":52,"end":56,"id":9},{"text":"when","start":57,"end":61,"id":10},{"text":"viewed","start":62,"end":68,"id":11},{"text":"in","start":69,"end":71,"id":12},{"text":"an","start":72,"end":74,"id":13},{"text":"incremental","start":75,"end":86,"id":14},{"text":"form","start":87,"end":91,"id":15},{"text":".","start":91,"end":92,"id":16}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":41,"end":49,"token_start":7,"token_end":7,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"A notable advantage of LSIR over existing approaches is that tuning parameters such as the kernel width and the regularization parameter can be naturally optimized by cross-validation, allowing us to avoid overfitting in a data-dependent fashion.","_input_hash":-22009868,"_task_hash":43265780,"tokens":[{"text":"A","start":0,"end":1,"id":0},{"text":"notable","start":2,"end":9,"id":1},{"text":"advantage","start":10,"end":19,"id":2},{"text":"of","start":20,"end":22,"id":3},{"text":"LSIR","start":23,"end":27,"id":4},{"text":"over","start":28,"end":32,"id":5},{"text":"existing","start":33,"end":41,"id":6},{"text":"approaches","start":42,"end":52,"id":7},{"text":"is","start":53,"end":55,"id":8},{"text":"that","start":56,"end":60,"id":9},{"text":"tuning","start":61,"end":67,"id":10},{"text":"parameters","start":68,"end":78,"id":11},{"text":"such","start":79,"end":83,"id":12},{"text":"as","start":84,"end":86,"id":13},{"text":"the","start":87,"end":90,"id":14},{"text":"kernel","start":91,"end":97,"id":15},{"text":"width","start":98,"end":103,"id":16},{"text":"and","start":104,"end":107,"id":17},{"text":"the","start":108,"end":111,"id":18},{"text":"regularization","start":112,"end":126,"id":19},{"text":"parameter","start":127,"end":136,"id":20},{"text":"can","start":137,"end":140,"id":21},{"text":"be","start":141,"end":143,"id":22},{"text":"naturally","start":144,"end":153,"id":23},{"text":"optimized","start":154,"end":163,"id":24},{"text":"by","start":164,"end":166,"id":25},{"text":"cross","start":167,"end":172,"id":26},{"text":"-","start":172,"end":173,"id":27},{"text":"validation","start":173,"end":183,"id":28},{"text":",","start":183,"end":184,"id":29},{"text":"allowing","start":185,"end":193,"id":30},{"text":"us","start":194,"end":196,"id":31},{"text":"to","start":197,"end":199,"id":32},{"text":"avoid","start":200,"end":205,"id":33},{"text":"overfitting","start":206,"end":217,"id":34},{"text":"in","start":218,"end":220,"id":35},{"text":"a","start":221,"end":222,"id":36},{"text":"data","start":223,"end":227,"id":37},{"text":"-","start":227,"end":228,"id":38},{"text":"dependent","start":228,"end":237,"id":39},{"text":"fashion","start":238,"end":245,"id":40},{"text":".","start":245,"end":246,"id":41}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We introduce a factor analysis model that summarizes the dependencies between observed variable groups, instead of dependencies between individual variables as standard factor analysis does.","_input_hash":-846104991,"_task_hash":775770368,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"introduce","start":3,"end":12,"id":1},{"text":"a","start":13,"end":14,"id":2},{"text":"factor","start":15,"end":21,"id":3},{"text":"analysis","start":22,"end":30,"id":4},{"text":"model","start":31,"end":36,"id":5},{"text":"that","start":37,"end":41,"id":6},{"text":"summarizes","start":42,"end":52,"id":7},{"text":"the","start":53,"end":56,"id":8},{"text":"dependencies","start":57,"end":69,"id":9},{"text":"between","start":70,"end":77,"id":10},{"text":"observed","start":78,"end":86,"id":11},{"text":"variable","start":87,"end":95,"id":12},{"text":"groups","start":96,"end":102,"id":13},{"text":",","start":102,"end":103,"id":14},{"text":"instead","start":104,"end":111,"id":15},{"text":"of","start":112,"end":114,"id":16},{"text":"dependencies","start":115,"end":127,"id":17},{"text":"between","start":128,"end":135,"id":18},{"text":"individual","start":136,"end":146,"id":19},{"text":"variables","start":147,"end":156,"id":20},{"text":"as","start":157,"end":159,"id":21},{"text":"standard","start":160,"end":168,"id":22},{"text":"factor","start":169,"end":175,"id":23},{"text":"analysis","start":176,"end":184,"id":24},{"text":"does","start":185,"end":189,"id":25},{"text":".","start":189,"end":190,"id":26}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":15,"end":30,"token_start":3,"token_end":4,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"A clearer and simpler picture emerges by studying the Normal means model.","_input_hash":1905450728,"_task_hash":1554243957,"tokens":[{"text":"A","start":0,"end":1,"id":0},{"text":"clearer","start":2,"end":9,"id":1},{"text":"and","start":10,"end":13,"id":2},{"text":"simpler","start":14,"end":21,"id":3},{"text":"picture","start":22,"end":29,"id":4},{"text":"emerges","start":30,"end":37,"id":5},{"text":"by","start":38,"end":40,"id":6},{"text":"studying","start":41,"end":49,"id":7},{"text":"the","start":50,"end":53,"id":8},{"text":"Normal","start":54,"end":60,"id":9},{"text":"means","start":61,"end":66,"id":10},{"text":"model","start":67,"end":72,"id":11},{"text":".","start":72,"end":73,"id":12}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":54,"end":66,"token_start":9,"token_end":10,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"In contrast to these traditional boosting algorithms that treat a tree learner as a black box, the method we propose directly learns decision forests via fully-corrective regularized greedy search using the underlying forest structure.","_input_hash":76383042,"_task_hash":1895822876,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"contrast","start":3,"end":11,"id":1},{"text":"to","start":12,"end":14,"id":2},{"text":"these","start":15,"end":20,"id":3},{"text":"traditional","start":21,"end":32,"id":4},{"text":"boosting","start":33,"end":41,"id":5},{"text":"algorithms","start":42,"end":52,"id":6},{"text":"that","start":53,"end":57,"id":7},{"text":"treat","start":58,"end":63,"id":8},{"text":"a","start":64,"end":65,"id":9},{"text":"tree","start":66,"end":70,"id":10},{"text":"learner","start":71,"end":78,"id":11},{"text":"as","start":79,"end":81,"id":12},{"text":"a","start":82,"end":83,"id":13},{"text":"black","start":84,"end":89,"id":14},{"text":"box","start":90,"end":93,"id":15},{"text":",","start":93,"end":94,"id":16},{"text":"the","start":95,"end":98,"id":17},{"text":"method","start":99,"end":105,"id":18},{"text":"we","start":106,"end":108,"id":19},{"text":"propose","start":109,"end":116,"id":20},{"text":"directly","start":117,"end":125,"id":21},{"text":"learns","start":126,"end":132,"id":22},{"text":"decision","start":133,"end":141,"id":23},{"text":"forests","start":142,"end":149,"id":24},{"text":"via","start":150,"end":153,"id":25},{"text":"fully","start":154,"end":159,"id":26},{"text":"-","start":159,"end":160,"id":27},{"text":"corrective","start":160,"end":170,"id":28},{"text":"regularized","start":171,"end":182,"id":29},{"text":"greedy","start":183,"end":189,"id":30},{"text":"search","start":190,"end":196,"id":31},{"text":"using","start":197,"end":202,"id":32},{"text":"the","start":203,"end":206,"id":33},{"text":"underlying","start":207,"end":217,"id":34},{"text":"forest","start":218,"end":224,"id":35},{"text":"structure","start":225,"end":234,"id":36},{"text":".","start":234,"end":235,"id":37}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":33,"end":41,"token_start":5,"token_end":5,"label":"ALGO","answer":"accept"},{"start":133,"end":149,"token_start":23,"token_end":24,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We obtain asymptotic expressions for the mean number of discoveries and the phase transition thresholds as a function of the number of samples, the number of variables, and the joint sample distribution.","_input_hash":-556433128,"_task_hash":-1443861114,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"obtain","start":3,"end":9,"id":1},{"text":"asymptotic","start":10,"end":20,"id":2},{"text":"expressions","start":21,"end":32,"id":3},{"text":"for","start":33,"end":36,"id":4},{"text":"the","start":37,"end":40,"id":5},{"text":"mean","start":41,"end":45,"id":6},{"text":"number","start":46,"end":52,"id":7},{"text":"of","start":53,"end":55,"id":8},{"text":"discoveries","start":56,"end":67,"id":9},{"text":"and","start":68,"end":71,"id":10},{"text":"the","start":72,"end":75,"id":11},{"text":"phase","start":76,"end":81,"id":12},{"text":"transition","start":82,"end":92,"id":13},{"text":"thresholds","start":93,"end":103,"id":14},{"text":"as","start":104,"end":106,"id":15},{"text":"a","start":107,"end":108,"id":16},{"text":"function","start":109,"end":117,"id":17},{"text":"of","start":118,"end":120,"id":18},{"text":"the","start":121,"end":124,"id":19},{"text":"number","start":125,"end":131,"id":20},{"text":"of","start":132,"end":134,"id":21},{"text":"samples","start":135,"end":142,"id":22},{"text":",","start":142,"end":143,"id":23},{"text":"the","start":144,"end":147,"id":24},{"text":"number","start":148,"end":154,"id":25},{"text":"of","start":155,"end":157,"id":26},{"text":"variables","start":158,"end":167,"id":27},{"text":",","start":167,"end":168,"id":28},{"text":"and","start":169,"end":172,"id":29},{"text":"the","start":173,"end":176,"id":30},{"text":"joint","start":177,"end":182,"id":31},{"text":"sample","start":183,"end":189,"id":32},{"text":"distribution","start":190,"end":202,"id":33},{"text":".","start":202,"end":203,"id":34}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We propose weighting the features within a kernel with a sparse set of weights that are estimated in conjunction with the original classification or regression problem.","_input_hash":200208298,"_task_hash":374697797,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"propose","start":3,"end":10,"id":1},{"text":"weighting","start":11,"end":20,"id":2},{"text":"the","start":21,"end":24,"id":3},{"text":"features","start":25,"end":33,"id":4},{"text":"within","start":34,"end":40,"id":5},{"text":"a","start":41,"end":42,"id":6},{"text":"kernel","start":43,"end":49,"id":7},{"text":"with","start":50,"end":54,"id":8},{"text":"a","start":55,"end":56,"id":9},{"text":"sparse","start":57,"end":63,"id":10},{"text":"set","start":64,"end":67,"id":11},{"text":"of","start":68,"end":70,"id":12},{"text":"weights","start":71,"end":78,"id":13},{"text":"that","start":79,"end":83,"id":14},{"text":"are","start":84,"end":87,"id":15},{"text":"estimated","start":88,"end":97,"id":16},{"text":"in","start":98,"end":100,"id":17},{"text":"conjunction","start":101,"end":112,"id":18},{"text":"with","start":113,"end":117,"id":19},{"text":"the","start":118,"end":121,"id":20},{"text":"original","start":122,"end":130,"id":21},{"text":"classification","start":131,"end":145,"id":22},{"text":"or","start":146,"end":148,"id":23},{"text":"regression","start":149,"end":159,"id":24},{"text":"problem","start":160,"end":167,"id":25},{"text":".","start":167,"end":168,"id":26}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"In addition, a slight modification of KNIFE yields an efficient algorithm for finding feature regularization paths, or the paths of each feature's weight.","_input_hash":1018985911,"_task_hash":2120120390,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"addition","start":3,"end":11,"id":1},{"text":",","start":11,"end":12,"id":2},{"text":"a","start":13,"end":14,"id":3},{"text":"slight","start":15,"end":21,"id":4},{"text":"modification","start":22,"end":34,"id":5},{"text":"of","start":35,"end":37,"id":6},{"text":"KNIFE","start":38,"end":43,"id":7},{"text":"yields","start":44,"end":50,"id":8},{"text":"an","start":51,"end":53,"id":9},{"text":"efficient","start":54,"end":63,"id":10},{"text":"algorithm","start":64,"end":73,"id":11},{"text":"for","start":74,"end":77,"id":12},{"text":"finding","start":78,"end":85,"id":13},{"text":"feature","start":86,"end":93,"id":14},{"text":"regularization","start":94,"end":108,"id":15},{"text":"paths","start":109,"end":114,"id":16},{"text":",","start":114,"end":115,"id":17},{"text":"or","start":116,"end":118,"id":18},{"text":"the","start":119,"end":122,"id":19},{"text":"paths","start":123,"end":128,"id":20},{"text":"of","start":129,"end":131,"id":21},{"text":"each","start":132,"end":136,"id":22},{"text":"feature","start":137,"end":144,"id":23},{"text":"'s","start":144,"end":146,"id":24},{"text":"weight","start":147,"end":153,"id":25},{"text":".","start":153,"end":154,"id":26}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":38,"end":43,"token_start":7,"token_end":7,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"The superiority of this guidance mechanism is confirmed on two datasets.","_input_hash":273462590,"_task_hash":1167769936,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"superiority","start":4,"end":15,"id":1},{"text":"of","start":16,"end":18,"id":2},{"text":"this","start":19,"end":23,"id":3},{"text":"guidance","start":24,"end":32,"id":4},{"text":"mechanism","start":33,"end":42,"id":5},{"text":"is","start":43,"end":45,"id":6},{"text":"confirmed","start":46,"end":55,"id":7},{"text":"on","start":56,"end":58,"id":8},{"text":"two","start":59,"end":62,"id":9},{"text":"datasets","start":63,"end":71,"id":10},{"text":".","start":71,"end":72,"id":11}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"This work first answers an open question:","_input_hash":-908006605,"_task_hash":-1648768172,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"work","start":5,"end":9,"id":1},{"text":"first","start":10,"end":15,"id":2},{"text":"answers","start":16,"end":23,"id":3},{"text":"an","start":24,"end":26,"id":4},{"text":"open","start":27,"end":31,"id":5},{"text":"question","start":32,"end":40,"id":6},{"text":":","start":40,"end":41,"id":7}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"linear framework:","_input_hash":1078796544,"_task_hash":1378543152,"tokens":[{"text":"linear","start":0,"end":6,"id":0},{"text":"framework","start":7,"end":16,"id":1},{"text":":","start":16,"end":17,"id":2}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"When the total number of plays n is known beforehand by the agent, Audibert et al. (","_input_hash":-2127712939,"_task_hash":966879619,"tokens":[{"text":"When","start":0,"end":4,"id":0},{"text":"the","start":5,"end":8,"id":1},{"text":"total","start":9,"end":14,"id":2},{"text":"number","start":15,"end":21,"id":3},{"text":"of","start":22,"end":24,"id":4},{"text":"plays","start":25,"end":30,"id":5},{"text":"n","start":31,"end":32,"id":6},{"text":"is","start":33,"end":35,"id":7},{"text":"known","start":36,"end":41,"id":8},{"text":"beforehand","start":42,"end":52,"id":9},{"text":"by","start":53,"end":55,"id":10},{"text":"the","start":56,"end":59,"id":11},{"text":"agent","start":60,"end":65,"id":12},{"text":",","start":65,"end":66,"id":13},{"text":"Audibert","start":67,"end":75,"id":14},{"text":"et","start":76,"end":78,"id":15},{"text":"al","start":79,"end":81,"id":16},{"text":".","start":81,"end":82,"id":17},{"text":"(","start":83,"end":84,"id":18}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"We extend it for more complex Kriging models, e.g. models using derivatives.","_input_hash":-910314375,"_task_hash":2083309826,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"extend","start":3,"end":9,"id":1},{"text":"it","start":10,"end":12,"id":2},{"text":"for","start":13,"end":16,"id":3},{"text":"more","start":17,"end":21,"id":4},{"text":"complex","start":22,"end":29,"id":5},{"text":"Kriging","start":30,"end":37,"id":6},{"text":"models","start":38,"end":44,"id":7},{"text":",","start":44,"end":45,"id":8},{"text":"e.g.","start":46,"end":50,"id":9},{"text":"models","start":51,"end":57,"id":10},{"text":"using","start":58,"end":63,"id":11},{"text":"derivatives","start":64,"end":75,"id":12},{"text":".","start":75,"end":76,"id":13}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":30,"end":37,"token_start":6,"token_end":6,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"The theorem appears to be new but is closely related to results achieved by other investigators.","_input_hash":-1493657758,"_task_hash":1946779967,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"theorem","start":4,"end":11,"id":1},{"text":"appears","start":12,"end":19,"id":2},{"text":"to","start":20,"end":22,"id":3},{"text":"be","start":23,"end":25,"id":4},{"text":"new","start":26,"end":29,"id":5},{"text":"but","start":30,"end":33,"id":6},{"text":"is","start":34,"end":36,"id":7},{"text":"closely","start":37,"end":44,"id":8},{"text":"related","start":45,"end":52,"id":9},{"text":"to","start":53,"end":55,"id":10},{"text":"results","start":56,"end":63,"id":11},{"text":"achieved","start":64,"end":72,"id":12},{"text":"by","start":73,"end":75,"id":13},{"text":"other","start":76,"end":81,"id":14},{"text":"investigators","start":82,"end":95,"id":15},{"text":".","start":95,"end":96,"id":16}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"We observe a phase transition from one regime to the other and it is the purpose of this work to quantify the influence of various parameters on this phase transition.","_input_hash":-726240648,"_task_hash":-1905187295,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"observe","start":3,"end":10,"id":1},{"text":"a","start":11,"end":12,"id":2},{"text":"phase","start":13,"end":18,"id":3},{"text":"transition","start":19,"end":29,"id":4},{"text":"from","start":30,"end":34,"id":5},{"text":"one","start":35,"end":38,"id":6},{"text":"regime","start":39,"end":45,"id":7},{"text":"to","start":46,"end":48,"id":8},{"text":"the","start":49,"end":52,"id":9},{"text":"other","start":53,"end":58,"id":10},{"text":"and","start":59,"end":62,"id":11},{"text":"it","start":63,"end":65,"id":12},{"text":"is","start":66,"end":68,"id":13},{"text":"the","start":69,"end":72,"id":14},{"text":"purpose","start":73,"end":80,"id":15},{"text":"of","start":81,"end":83,"id":16},{"text":"this","start":84,"end":88,"id":17},{"text":"work","start":89,"end":93,"id":18},{"text":"to","start":94,"end":96,"id":19},{"text":"quantify","start":97,"end":105,"id":20},{"text":"the","start":106,"end":109,"id":21},{"text":"influence","start":110,"end":119,"id":22},{"text":"of","start":120,"end":122,"id":23},{"text":"various","start":123,"end":130,"id":24},{"text":"parameters","start":131,"end":141,"id":25},{"text":"on","start":142,"end":144,"id":26},{"text":"this","start":145,"end":149,"id":27},{"text":"phase","start":150,"end":155,"id":28},{"text":"transition","start":156,"end":166,"id":29},{"text":".","start":166,"end":167,"id":30}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"The most direct way to express arbitrary dependencies in datasets is to estimate the joint distribution and to apply afterwards the argmax-function to obtain the mode of the corresponding conditional distribution.","_input_hash":-1027595124,"_task_hash":179847809,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"most","start":4,"end":8,"id":1},{"text":"direct","start":9,"end":15,"id":2},{"text":"way","start":16,"end":19,"id":3},{"text":"to","start":20,"end":22,"id":4},{"text":"express","start":23,"end":30,"id":5},{"text":"arbitrary","start":31,"end":40,"id":6},{"text":"dependencies","start":41,"end":53,"id":7},{"text":"in","start":54,"end":56,"id":8},{"text":"datasets","start":57,"end":65,"id":9},{"text":"is","start":66,"end":68,"id":10},{"text":"to","start":69,"end":71,"id":11},{"text":"estimate","start":72,"end":80,"id":12},{"text":"the","start":81,"end":84,"id":13},{"text":"joint","start":85,"end":90,"id":14},{"text":"distribution","start":91,"end":103,"id":15},{"text":"and","start":104,"end":107,"id":16},{"text":"to","start":108,"end":110,"id":17},{"text":"apply","start":111,"end":116,"id":18},{"text":"afterwards","start":117,"end":127,"id":19},{"text":"the","start":128,"end":131,"id":20},{"text":"argmax","start":132,"end":138,"id":21},{"text":"-","start":138,"end":139,"id":22},{"text":"function","start":139,"end":147,"id":23},{"text":"to","start":148,"end":150,"id":24},{"text":"obtain","start":151,"end":157,"id":25},{"text":"the","start":158,"end":161,"id":26},{"text":"mode","start":162,"end":166,"id":27},{"text":"of","start":167,"end":169,"id":28},{"text":"the","start":170,"end":173,"id":29},{"text":"corresponding","start":174,"end":187,"id":30},{"text":"conditional","start":188,"end":199,"id":31},{"text":"distribution","start":200,"end":212,"id":32},{"text":".","start":212,"end":213,"id":33}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"This speed is possible due to matrix calculus based short-cuts for leave-one-out and feature addition.","_input_hash":-1995384002,"_task_hash":-2135375111,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"speed","start":5,"end":10,"id":1},{"text":"is","start":11,"end":13,"id":2},{"text":"possible","start":14,"end":22,"id":3},{"text":"due","start":23,"end":26,"id":4},{"text":"to","start":27,"end":29,"id":5},{"text":"matrix","start":30,"end":36,"id":6},{"text":"calculus","start":37,"end":45,"id":7},{"text":"based","start":46,"end":51,"id":8},{"text":"short","start":52,"end":57,"id":9},{"text":"-","start":57,"end":58,"id":10},{"text":"cuts","start":58,"end":62,"id":11},{"text":"for","start":63,"end":66,"id":12},{"text":"leave","start":67,"end":72,"id":13},{"text":"-","start":72,"end":73,"id":14},{"text":"one","start":73,"end":76,"id":15},{"text":"-","start":76,"end":77,"id":16},{"text":"out","start":77,"end":80,"id":17},{"text":"and","start":81,"end":84,"id":18},{"text":"feature","start":85,"end":92,"id":19},{"text":"addition","start":93,"end":101,"id":20},{"text":".","start":101,"end":102,"id":21}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Additionally, these results demonstrate the differences and similarities between the the grouped lasso procedure with and without overlapping groups.","_input_hash":1284595703,"_task_hash":-93676917,"tokens":[{"text":"Additionally","start":0,"end":12,"id":0},{"text":",","start":12,"end":13,"id":1},{"text":"these","start":14,"end":19,"id":2},{"text":"results","start":20,"end":27,"id":3},{"text":"demonstrate","start":28,"end":39,"id":4},{"text":"the","start":40,"end":43,"id":5},{"text":"differences","start":44,"end":55,"id":6},{"text":"and","start":56,"end":59,"id":7},{"text":"similarities","start":60,"end":72,"id":8},{"text":"between","start":73,"end":80,"id":9},{"text":"the","start":81,"end":84,"id":10},{"text":"the","start":85,"end":88,"id":11},{"text":"grouped","start":89,"end":96,"id":12},{"text":"lasso","start":97,"end":102,"id":13},{"text":"procedure","start":103,"end":112,"id":14},{"text":"with","start":113,"end":117,"id":15},{"text":"and","start":118,"end":121,"id":16},{"text":"without","start":122,"end":129,"id":17},{"text":"overlapping","start":130,"end":141,"id":18},{"text":"groups","start":142,"end":148,"id":19},{"text":".","start":148,"end":149,"id":20}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We introduce two active set rules based on different criteria, the first one prefers a model with interpretable active set parameters whereas the second puts computational complexity first, thus a model with active set parameters that directly control its complexity.","_input_hash":2097708780,"_task_hash":824997395,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"introduce","start":3,"end":12,"id":1},{"text":"two","start":13,"end":16,"id":2},{"text":"active","start":17,"end":23,"id":3},{"text":"set","start":24,"end":27,"id":4},{"text":"rules","start":28,"end":33,"id":5},{"text":"based","start":34,"end":39,"id":6},{"text":"on","start":40,"end":42,"id":7},{"text":"different","start":43,"end":52,"id":8},{"text":"criteria","start":53,"end":61,"id":9},{"text":",","start":61,"end":62,"id":10},{"text":"the","start":63,"end":66,"id":11},{"text":"first","start":67,"end":72,"id":12},{"text":"one","start":73,"end":76,"id":13},{"text":"prefers","start":77,"end":84,"id":14},{"text":"a","start":85,"end":86,"id":15},{"text":"model","start":87,"end":92,"id":16},{"text":"with","start":93,"end":97,"id":17},{"text":"interpretable","start":98,"end":111,"id":18},{"text":"active","start":112,"end":118,"id":19},{"text":"set","start":119,"end":122,"id":20},{"text":"parameters","start":123,"end":133,"id":21},{"text":"whereas","start":134,"end":141,"id":22},{"text":"the","start":142,"end":145,"id":23},{"text":"second","start":146,"end":152,"id":24},{"text":"puts","start":153,"end":157,"id":25},{"text":"computational","start":158,"end":171,"id":26},{"text":"complexity","start":172,"end":182,"id":27},{"text":"first","start":183,"end":188,"id":28},{"text":",","start":188,"end":189,"id":29},{"text":"thus","start":190,"end":194,"id":30},{"text":"a","start":195,"end":196,"id":31},{"text":"model","start":197,"end":202,"id":32},{"text":"with","start":203,"end":207,"id":33},{"text":"active","start":208,"end":214,"id":34},{"text":"set","start":215,"end":218,"id":35},{"text":"parameters","start":219,"end":229,"id":36},{"text":"that","start":230,"end":234,"id":37},{"text":"directly","start":235,"end":243,"id":38},{"text":"control","start":244,"end":251,"id":39},{"text":"its","start":252,"end":255,"id":40},{"text":"complexity","start":256,"end":266,"id":41},{"text":".","start":266,"end":267,"id":42}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We sharply characterize the performance of different penalization schemes for the problem of selecting the relevant variables in the multi-task setting.","_input_hash":-1051990232,"_task_hash":888874259,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"sharply","start":3,"end":10,"id":1},{"text":"characterize","start":11,"end":23,"id":2},{"text":"the","start":24,"end":27,"id":3},{"text":"performance","start":28,"end":39,"id":4},{"text":"of","start":40,"end":42,"id":5},{"text":"different","start":43,"end":52,"id":6},{"text":"penalization","start":53,"end":65,"id":7},{"text":"schemes","start":66,"end":73,"id":8},{"text":"for","start":74,"end":77,"id":9},{"text":"the","start":78,"end":81,"id":10},{"text":"problem","start":82,"end":89,"id":11},{"text":"of","start":90,"end":92,"id":12},{"text":"selecting","start":93,"end":102,"id":13},{"text":"the","start":103,"end":106,"id":14},{"text":"relevant","start":107,"end":115,"id":15},{"text":"variables","start":116,"end":125,"id":16},{"text":"in","start":126,"end":128,"id":17},{"text":"the","start":129,"end":132,"id":18},{"text":"multi","start":133,"end":138,"id":19},{"text":"-","start":138,"end":139,"id":20},{"text":"task","start":139,"end":143,"id":21},{"text":"setting","start":144,"end":151,"id":22},{"text":".","start":151,"end":152,"id":23}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We define a family of smooth densities and conditional densities by second order exponential models, i.e., by maximizing conditional entropy subject to first and second statistical moments.","_input_hash":-1906282048,"_task_hash":-557100930,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"define","start":3,"end":9,"id":1},{"text":"a","start":10,"end":11,"id":2},{"text":"family","start":12,"end":18,"id":3},{"text":"of","start":19,"end":21,"id":4},{"text":"smooth","start":22,"end":28,"id":5},{"text":"densities","start":29,"end":38,"id":6},{"text":"and","start":39,"end":42,"id":7},{"text":"conditional","start":43,"end":54,"id":8},{"text":"densities","start":55,"end":64,"id":9},{"text":"by","start":65,"end":67,"id":10},{"text":"second","start":68,"end":74,"id":11},{"text":"order","start":75,"end":80,"id":12},{"text":"exponential","start":81,"end":92,"id":13},{"text":"models","start":93,"end":99,"id":14},{"text":",","start":99,"end":100,"id":15},{"text":"i.e.","start":101,"end":105,"id":16},{"text":",","start":105,"end":106,"id":17},{"text":"by","start":107,"end":109,"id":18},{"text":"maximizing","start":110,"end":120,"id":19},{"text":"conditional","start":121,"end":132,"id":20},{"text":"entropy","start":133,"end":140,"id":21},{"text":"subject","start":141,"end":148,"id":22},{"text":"to","start":149,"end":151,"id":23},{"text":"first","start":152,"end":157,"id":24},{"text":"and","start":158,"end":161,"id":25},{"text":"second","start":162,"end":168,"id":26},{"text":"statistical","start":169,"end":180,"id":27},{"text":"moments","start":181,"end":188,"id":28},{"text":".","start":188,"end":189,"id":29}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"formalization of learning and attack processes, derivation of an optimal attack, analysis of its efficiency and constraints.","_input_hash":-1898018722,"_task_hash":1991588775,"tokens":[{"text":"formalization","start":0,"end":13,"id":0},{"text":"of","start":14,"end":16,"id":1},{"text":"learning","start":17,"end":25,"id":2},{"text":"and","start":26,"end":29,"id":3},{"text":"attack","start":30,"end":36,"id":4},{"text":"processes","start":37,"end":46,"id":5},{"text":",","start":46,"end":47,"id":6},{"text":"derivation","start":48,"end":58,"id":7},{"text":"of","start":59,"end":61,"id":8},{"text":"an","start":62,"end":64,"id":9},{"text":"optimal","start":65,"end":72,"id":10},{"text":"attack","start":73,"end":79,"id":11},{"text":",","start":79,"end":80,"id":12},{"text":"analysis","start":81,"end":89,"id":13},{"text":"of","start":90,"end":92,"id":14},{"text":"its","start":93,"end":96,"id":15},{"text":"efficiency","start":97,"end":107,"id":16},{"text":"and","start":108,"end":111,"id":17},{"text":"constraints","start":112,"end":123,"id":18},{"text":".","start":123,"end":124,"id":19}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"The method combines cutting plane optimization with red-black tree based approach to subgradient calculations, and has O(m*s+m*log(m)) time complexity, where m is the number of training examples, and s the average number of non-zero features per example.","_input_hash":1704006268,"_task_hash":2071263571,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"method","start":4,"end":10,"id":1},{"text":"combines","start":11,"end":19,"id":2},{"text":"cutting","start":20,"end":27,"id":3},{"text":"plane","start":28,"end":33,"id":4},{"text":"optimization","start":34,"end":46,"id":5},{"text":"with","start":47,"end":51,"id":6},{"text":"red","start":52,"end":55,"id":7},{"text":"-","start":55,"end":56,"id":8},{"text":"black","start":56,"end":61,"id":9},{"text":"tree","start":62,"end":66,"id":10},{"text":"based","start":67,"end":72,"id":11},{"text":"approach","start":73,"end":81,"id":12},{"text":"to","start":82,"end":84,"id":13},{"text":"subgradient","start":85,"end":96,"id":14},{"text":"calculations","start":97,"end":109,"id":15},{"text":",","start":109,"end":110,"id":16},{"text":"and","start":111,"end":114,"id":17},{"text":"has","start":115,"end":118,"id":18},{"text":"O(m*s+m*log(m","start":119,"end":132,"id":19},{"text":")","start":132,"end":133,"id":20},{"text":")","start":133,"end":134,"id":21},{"text":"time","start":135,"end":139,"id":22},{"text":"complexity","start":140,"end":150,"id":23},{"text":",","start":150,"end":151,"id":24},{"text":"where","start":152,"end":157,"id":25},{"text":"m","start":158,"end":159,"id":26},{"text":"is","start":160,"end":162,"id":27},{"text":"the","start":163,"end":166,"id":28},{"text":"number","start":167,"end":173,"id":29},{"text":"of","start":174,"end":176,"id":30},{"text":"training","start":177,"end":185,"id":31},{"text":"examples","start":186,"end":194,"id":32},{"text":",","start":194,"end":195,"id":33},{"text":"and","start":196,"end":199,"id":34},{"text":"s","start":200,"end":201,"id":35},{"text":"the","start":202,"end":205,"id":36},{"text":"average","start":206,"end":213,"id":37},{"text":"number","start":214,"end":220,"id":38},{"text":"of","start":221,"end":223,"id":39},{"text":"non","start":224,"end":227,"id":40},{"text":"-","start":227,"end":228,"id":41},{"text":"zero","start":228,"end":232,"id":42},{"text":"features","start":233,"end":241,"id":43},{"text":"per","start":242,"end":245,"id":44},{"text":"example","start":246,"end":253,"id":45},{"text":".","start":253,"end":254,"id":46}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"These features are demonstrated by a comparative numerical analysis on synthetic data.","_input_hash":451875220,"_task_hash":-1027452501,"tokens":[{"text":"These","start":0,"end":5,"id":0},{"text":"features","start":6,"end":14,"id":1},{"text":"are","start":15,"end":18,"id":2},{"text":"demonstrated","start":19,"end":31,"id":3},{"text":"by","start":32,"end":34,"id":4},{"text":"a","start":35,"end":36,"id":5},{"text":"comparative","start":37,"end":48,"id":6},{"text":"numerical","start":49,"end":58,"id":7},{"text":"analysis","start":59,"end":67,"id":8},{"text":"on","start":68,"end":70,"id":9},{"text":"synthetic","start":71,"end":80,"id":10},{"text":"data","start":81,"end":85,"id":11},{"text":".","start":85,"end":86,"id":12}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"The performance of any binary classifier based on the training data is characterized by the excess risk.","_input_hash":-1138005955,"_task_hash":1867628457,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"performance","start":4,"end":15,"id":1},{"text":"of","start":16,"end":18,"id":2},{"text":"any","start":19,"end":22,"id":3},{"text":"binary","start":23,"end":29,"id":4},{"text":"classifier","start":30,"end":40,"id":5},{"text":"based","start":41,"end":46,"id":6},{"text":"on","start":47,"end":49,"id":7},{"text":"the","start":50,"end":53,"id":8},{"text":"training","start":54,"end":62,"id":9},{"text":"data","start":63,"end":67,"id":10},{"text":"is","start":68,"end":70,"id":11},{"text":"characterized","start":71,"end":84,"id":12},{"text":"by","start":85,"end":87,"id":13},{"text":"the","start":88,"end":91,"id":14},{"text":"excess","start":92,"end":98,"id":15},{"text":"risk","start":99,"end":103,"id":16},{"text":".","start":103,"end":104,"id":17}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":23,"end":40,"token_start":4,"token_end":5,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We propose a computationally efficient method for joint parameter and model inference, and model comparison.","_input_hash":50858937,"_task_hash":1257563108,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"propose","start":3,"end":10,"id":1},{"text":"a","start":11,"end":12,"id":2},{"text":"computationally","start":13,"end":28,"id":3},{"text":"efficient","start":29,"end":38,"id":4},{"text":"method","start":39,"end":45,"id":5},{"text":"for","start":46,"end":49,"id":6},{"text":"joint","start":50,"end":55,"id":7},{"text":"parameter","start":56,"end":65,"id":8},{"text":"and","start":66,"end":69,"id":9},{"text":"model","start":70,"end":75,"id":10},{"text":"inference","start":76,"end":85,"id":11},{"text":",","start":85,"end":86,"id":12},{"text":"and","start":87,"end":90,"id":13},{"text":"model","start":91,"end":96,"id":14},{"text":"comparison","start":97,"end":107,"id":15},{"text":".","start":107,"end":108,"id":16}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Sparsity in the kernel weights is obtained by adopting a hierarchical Bayesian approach:","_input_hash":1678044855,"_task_hash":1689121966,"tokens":[{"text":"Sparsity","start":0,"end":8,"id":0},{"text":"in","start":9,"end":11,"id":1},{"text":"the","start":12,"end":15,"id":2},{"text":"kernel","start":16,"end":22,"id":3},{"text":"weights","start":23,"end":30,"id":4},{"text":"is","start":31,"end":33,"id":5},{"text":"obtained","start":34,"end":42,"id":6},{"text":"by","start":43,"end":45,"id":7},{"text":"adopting","start":46,"end":54,"id":8},{"text":"a","start":55,"end":56,"id":9},{"text":"hierarchical","start":57,"end":69,"id":10},{"text":"Bayesian","start":70,"end":78,"id":11},{"text":"approach","start":79,"end":87,"id":12},{"text":":","start":87,"end":88,"id":13}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"This requires a few key new ideas.","_input_hash":144786417,"_task_hash":-1057917835,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"requires","start":5,"end":13,"id":1},{"text":"a","start":14,"end":15,"id":2},{"text":"few","start":16,"end":19,"id":3},{"text":"key","start":20,"end":23,"id":4},{"text":"new","start":24,"end":27,"id":5},{"text":"ideas","start":28,"end":33,"id":6},{"text":".","start":33,"end":34,"id":7}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Thus we can index the complexity of our random sequence by the number of states of the automata.","_input_hash":-530968138,"_task_hash":-70775467,"tokens":[{"text":"Thus","start":0,"end":4,"id":0},{"text":"we","start":5,"end":7,"id":1},{"text":"can","start":8,"end":11,"id":2},{"text":"index","start":12,"end":17,"id":3},{"text":"the","start":18,"end":21,"id":4},{"text":"complexity","start":22,"end":32,"id":5},{"text":"of","start":33,"end":35,"id":6},{"text":"our","start":36,"end":39,"id":7},{"text":"random","start":40,"end":46,"id":8},{"text":"sequence","start":47,"end":55,"id":9},{"text":"by","start":56,"end":58,"id":10},{"text":"the","start":59,"end":62,"id":11},{"text":"number","start":63,"end":69,"id":12},{"text":"of","start":70,"end":72,"id":13},{"text":"states","start":73,"end":79,"id":14},{"text":"of","start":80,"end":82,"id":15},{"text":"the","start":83,"end":86,"id":16},{"text":"automata","start":87,"end":95,"id":17},{"text":".","start":95,"end":96,"id":18}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"In this article, we derive concentration inequalities for the cross-validation estimate of the generalization error for empirical risk minimizers.","_input_hash":1902169871,"_task_hash":-1167396275,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"this","start":3,"end":7,"id":1},{"text":"article","start":8,"end":15,"id":2},{"text":",","start":15,"end":16,"id":3},{"text":"we","start":17,"end":19,"id":4},{"text":"derive","start":20,"end":26,"id":5},{"text":"concentration","start":27,"end":40,"id":6},{"text":"inequalities","start":41,"end":53,"id":7},{"text":"for","start":54,"end":57,"id":8},{"text":"the","start":58,"end":61,"id":9},{"text":"cross","start":62,"end":67,"id":10},{"text":"-","start":67,"end":68,"id":11},{"text":"validation","start":68,"end":78,"id":12},{"text":"estimate","start":79,"end":87,"id":13},{"text":"of","start":88,"end":90,"id":14},{"text":"the","start":91,"end":94,"id":15},{"text":"generalization","start":95,"end":109,"id":16},{"text":"error","start":110,"end":115,"id":17},{"text":"for","start":116,"end":119,"id":18},{"text":"empirical","start":120,"end":129,"id":19},{"text":"risk","start":130,"end":134,"id":20},{"text":"minimizers","start":135,"end":145,"id":21},{"text":".","start":145,"end":146,"id":22}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"The results from our simulations show that the post processing methods we have considered here are promising.","_input_hash":-688610903,"_task_hash":1945823277,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"results","start":4,"end":11,"id":1},{"text":"from","start":12,"end":16,"id":2},{"text":"our","start":17,"end":20,"id":3},{"text":"simulations","start":21,"end":32,"id":4},{"text":"show","start":33,"end":37,"id":5},{"text":"that","start":38,"end":42,"id":6},{"text":"the","start":43,"end":46,"id":7},{"text":"post","start":47,"end":51,"id":8},{"text":"processing","start":52,"end":62,"id":9},{"text":"methods","start":63,"end":70,"id":10},{"text":"we","start":71,"end":73,"id":11},{"text":"have","start":74,"end":78,"id":12},{"text":"considered","start":79,"end":89,"id":13},{"text":"here","start":90,"end":94,"id":14},{"text":"are","start":95,"end":98,"id":15},{"text":"promising","start":99,"end":108,"id":16},{"text":".","start":108,"end":109,"id":17}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"The intrinsic underlying structure of the system is modeled by an epsilon-machine and its causal states.","_input_hash":-1157640748,"_task_hash":-1655682722,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"intrinsic","start":4,"end":13,"id":1},{"text":"underlying","start":14,"end":24,"id":2},{"text":"structure","start":25,"end":34,"id":3},{"text":"of","start":35,"end":37,"id":4},{"text":"the","start":38,"end":41,"id":5},{"text":"system","start":42,"end":48,"id":6},{"text":"is","start":49,"end":51,"id":7},{"text":"modeled","start":52,"end":59,"id":8},{"text":"by","start":60,"end":62,"id":9},{"text":"an","start":63,"end":65,"id":10},{"text":"epsilon","start":66,"end":73,"id":11},{"text":"-","start":73,"end":74,"id":12},{"text":"machine","start":74,"end":81,"id":13},{"text":"and","start":82,"end":85,"id":14},{"text":"its","start":86,"end":89,"id":15},{"text":"causal","start":90,"end":96,"id":16},{"text":"states","start":97,"end":103,"id":17},{"text":".","start":103,"end":104,"id":18}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Selecting important features in non-linear or kernel spaces is a difficult challenge in both classification and regression problems.","_input_hash":-365126464,"_task_hash":-231898137,"tokens":[{"text":"Selecting","start":0,"end":9,"id":0},{"text":"important","start":10,"end":19,"id":1},{"text":"features","start":20,"end":28,"id":2},{"text":"in","start":29,"end":31,"id":3},{"text":"non","start":32,"end":35,"id":4},{"text":"-","start":35,"end":36,"id":5},{"text":"linear","start":36,"end":42,"id":6},{"text":"or","start":43,"end":45,"id":7},{"text":"kernel","start":46,"end":52,"id":8},{"text":"spaces","start":53,"end":59,"id":9},{"text":"is","start":60,"end":62,"id":10},{"text":"a","start":63,"end":64,"id":11},{"text":"difficult","start":65,"end":74,"id":12},{"text":"challenge","start":75,"end":84,"id":13},{"text":"in","start":85,"end":87,"id":14},{"text":"both","start":88,"end":92,"id":15},{"text":"classification","start":93,"end":107,"id":16},{"text":"and","start":108,"end":111,"id":17},{"text":"regression","start":112,"end":122,"id":18},{"text":"problems","start":123,"end":131,"id":19},{"text":".","start":131,"end":132,"id":20}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"A simple distance-based discriminant algorithm illustrates the theory, intimately connected to the Gaussian kernels of Machine Learning.","_input_hash":-1075725848,"_task_hash":368881766,"tokens":[{"text":"A","start":0,"end":1,"id":0},{"text":"simple","start":2,"end":8,"id":1},{"text":"distance","start":9,"end":17,"id":2},{"text":"-","start":17,"end":18,"id":3},{"text":"based","start":18,"end":23,"id":4},{"text":"discriminant","start":24,"end":36,"id":5},{"text":"algorithm","start":37,"end":46,"id":6},{"text":"illustrates","start":47,"end":58,"id":7},{"text":"the","start":59,"end":62,"id":8},{"text":"theory","start":63,"end":69,"id":9},{"text":",","start":69,"end":70,"id":10},{"text":"intimately","start":71,"end":81,"id":11},{"text":"connected","start":82,"end":91,"id":12},{"text":"to","start":92,"end":94,"id":13},{"text":"the","start":95,"end":98,"id":14},{"text":"Gaussian","start":99,"end":107,"id":15},{"text":"kernels","start":108,"end":115,"id":16},{"text":"of","start":116,"end":118,"id":17},{"text":"Machine","start":119,"end":126,"id":18},{"text":"Learning","start":127,"end":135,"id":19},{"text":".","start":135,"end":136,"id":20}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":9,"end":36,"token_start":2,"token_end":5,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We study both the simultaneous recovery of all K underlying subspaces and the recovery of the best l0 subspace (i.e., with largest number of points) by minimizing the lp-averaged distances of data points from d-dimensional subspaces of the D-dimensional space.","_input_hash":-709730746,"_task_hash":1942975112,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"study","start":3,"end":8,"id":1},{"text":"both","start":9,"end":13,"id":2},{"text":"the","start":14,"end":17,"id":3},{"text":"simultaneous","start":18,"end":30,"id":4},{"text":"recovery","start":31,"end":39,"id":5},{"text":"of","start":40,"end":42,"id":6},{"text":"all","start":43,"end":46,"id":7},{"text":"K","start":47,"end":48,"id":8},{"text":"underlying","start":49,"end":59,"id":9},{"text":"subspaces","start":60,"end":69,"id":10},{"text":"and","start":70,"end":73,"id":11},{"text":"the","start":74,"end":77,"id":12},{"text":"recovery","start":78,"end":86,"id":13},{"text":"of","start":87,"end":89,"id":14},{"text":"the","start":90,"end":93,"id":15},{"text":"best","start":94,"end":98,"id":16},{"text":"l0","start":99,"end":101,"id":17},{"text":"subspace","start":102,"end":110,"id":18},{"text":"(","start":111,"end":112,"id":19},{"text":"i.e.","start":112,"end":116,"id":20},{"text":",","start":116,"end":117,"id":21},{"text":"with","start":118,"end":122,"id":22},{"text":"largest","start":123,"end":130,"id":23},{"text":"number","start":131,"end":137,"id":24},{"text":"of","start":138,"end":140,"id":25},{"text":"points","start":141,"end":147,"id":26},{"text":")","start":147,"end":148,"id":27},{"text":"by","start":149,"end":151,"id":28},{"text":"minimizing","start":152,"end":162,"id":29},{"text":"the","start":163,"end":166,"id":30},{"text":"lp","start":167,"end":169,"id":31},{"text":"-","start":169,"end":170,"id":32},{"text":"averaged","start":170,"end":178,"id":33},{"text":"distances","start":179,"end":188,"id":34},{"text":"of","start":189,"end":191,"id":35},{"text":"data","start":192,"end":196,"id":36},{"text":"points","start":197,"end":203,"id":37},{"text":"from","start":204,"end":208,"id":38},{"text":"d","start":209,"end":210,"id":39},{"text":"-","start":210,"end":211,"id":40},{"text":"dimensional","start":211,"end":222,"id":41},{"text":"subspaces","start":223,"end":232,"id":42},{"text":"of","start":233,"end":235,"id":43},{"text":"the","start":236,"end":239,"id":44},{"text":"D","start":240,"end":241,"id":45},{"text":"-","start":241,"end":242,"id":46},{"text":"dimensional","start":242,"end":253,"id":47},{"text":"space","start":254,"end":259,"id":48},{"text":".","start":259,"end":260,"id":49}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"We propose an active set selection framework for Gaussian process classification for cases when the dataset is large enough to render its inference prohibitive.","_input_hash":-1555338573,"_task_hash":-1444133568,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"propose","start":3,"end":10,"id":1},{"text":"an","start":11,"end":13,"id":2},{"text":"active","start":14,"end":20,"id":3},{"text":"set","start":21,"end":24,"id":4},{"text":"selection","start":25,"end":34,"id":5},{"text":"framework","start":35,"end":44,"id":6},{"text":"for","start":45,"end":48,"id":7},{"text":"Gaussian","start":49,"end":57,"id":8},{"text":"process","start":58,"end":65,"id":9},{"text":"classification","start":66,"end":80,"id":10},{"text":"for","start":81,"end":84,"id":11},{"text":"cases","start":85,"end":90,"id":12},{"text":"when","start":91,"end":95,"id":13},{"text":"the","start":96,"end":99,"id":14},{"text":"dataset","start":100,"end":107,"id":15},{"text":"is","start":108,"end":110,"id":16},{"text":"large","start":111,"end":116,"id":17},{"text":"enough","start":117,"end":123,"id":18},{"text":"to","start":124,"end":126,"id":19},{"text":"render","start":127,"end":133,"id":20},{"text":"its","start":134,"end":137,"id":21},{"text":"inference","start":138,"end":147,"id":22},{"text":"prohibitive","start":148,"end":159,"id":23},{"text":".","start":159,"end":160,"id":24}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":49,"end":80,"token_start":8,"token_end":10,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We develop novel greedy and sequential Monte Carlo inferences which operate in a bottom-up agglomerative fashion.","_input_hash":1376566390,"_task_hash":-1945168562,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"develop","start":3,"end":10,"id":1},{"text":"novel","start":11,"end":16,"id":2},{"text":"greedy","start":17,"end":23,"id":3},{"text":"and","start":24,"end":27,"id":4},{"text":"sequential","start":28,"end":38,"id":5},{"text":"Monte","start":39,"end":44,"id":6},{"text":"Carlo","start":45,"end":50,"id":7},{"text":"inferences","start":51,"end":61,"id":8},{"text":"which","start":62,"end":67,"id":9},{"text":"operate","start":68,"end":75,"id":10},{"text":"in","start":76,"end":78,"id":11},{"text":"a","start":79,"end":80,"id":12},{"text":"bottom","start":81,"end":87,"id":13},{"text":"-","start":87,"end":88,"id":14},{"text":"up","start":88,"end":90,"id":15},{"text":"agglomerative","start":91,"end":104,"id":16},{"text":"fashion","start":105,"end":112,"id":17},{"text":".","start":112,"end":113,"id":18}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"A highly scalable proximal gradient method is proposed to solve the resultant convex optimization problem;","_input_hash":-426269453,"_task_hash":-1530097809,"tokens":[{"text":"A","start":0,"end":1,"id":0},{"text":"highly","start":2,"end":8,"id":1},{"text":"scalable","start":9,"end":17,"id":2},{"text":"proximal","start":18,"end":26,"id":3},{"text":"gradient","start":27,"end":35,"id":4},{"text":"method","start":36,"end":42,"id":5},{"text":"is","start":43,"end":45,"id":6},{"text":"proposed","start":46,"end":54,"id":7},{"text":"to","start":55,"end":57,"id":8},{"text":"solve","start":58,"end":63,"id":9},{"text":"the","start":64,"end":67,"id":10},{"text":"resultant","start":68,"end":77,"id":11},{"text":"convex","start":78,"end":84,"id":12},{"text":"optimization","start":85,"end":97,"id":13},{"text":"problem","start":98,"end":105,"id":14},{"text":";","start":105,"end":106,"id":15}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":18,"end":35,"token_start":3,"token_end":4,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"In this paper, we propose a new direct method to estimate a causal ordering and connection strengths based on non-Gaussianity.","_input_hash":661152110,"_task_hash":367418313,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"this","start":3,"end":7,"id":1},{"text":"paper","start":8,"end":13,"id":2},{"text":",","start":13,"end":14,"id":3},{"text":"we","start":15,"end":17,"id":4},{"text":"propose","start":18,"end":25,"id":5},{"text":"a","start":26,"end":27,"id":6},{"text":"new","start":28,"end":31,"id":7},{"text":"direct","start":32,"end":38,"id":8},{"text":"method","start":39,"end":45,"id":9},{"text":"to","start":46,"end":48,"id":10},{"text":"estimate","start":49,"end":57,"id":11},{"text":"a","start":58,"end":59,"id":12},{"text":"causal","start":60,"end":66,"id":13},{"text":"ordering","start":67,"end":75,"id":14},{"text":"and","start":76,"end":79,"id":15},{"text":"connection","start":80,"end":90,"id":16},{"text":"strengths","start":91,"end":100,"id":17},{"text":"based","start":101,"end":106,"id":18},{"text":"on","start":107,"end":109,"id":19},{"text":"non","start":110,"end":113,"id":20},{"text":"-","start":113,"end":114,"id":21},{"text":"Gaussianity","start":114,"end":125,"id":22},{"text":".","start":125,"end":126,"id":23}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"For the case where the true kernel combination is sparse, we show a sharper convergence rate of the block-l1 and elastic-net MKL methods than the existing rate for block-l1 MKL.","_input_hash":-1791595859,"_task_hash":1279857266,"tokens":[{"text":"For","start":0,"end":3,"id":0},{"text":"the","start":4,"end":7,"id":1},{"text":"case","start":8,"end":12,"id":2},{"text":"where","start":13,"end":18,"id":3},{"text":"the","start":19,"end":22,"id":4},{"text":"true","start":23,"end":27,"id":5},{"text":"kernel","start":28,"end":34,"id":6},{"text":"combination","start":35,"end":46,"id":7},{"text":"is","start":47,"end":49,"id":8},{"text":"sparse","start":50,"end":56,"id":9},{"text":",","start":56,"end":57,"id":10},{"text":"we","start":58,"end":60,"id":11},{"text":"show","start":61,"end":65,"id":12},{"text":"a","start":66,"end":67,"id":13},{"text":"sharper","start":68,"end":75,"id":14},{"text":"convergence","start":76,"end":87,"id":15},{"text":"rate","start":88,"end":92,"id":16},{"text":"of","start":93,"end":95,"id":17},{"text":"the","start":96,"end":99,"id":18},{"text":"block","start":100,"end":105,"id":19},{"text":"-","start":105,"end":106,"id":20},{"text":"l1","start":106,"end":108,"id":21},{"text":"and","start":109,"end":112,"id":22},{"text":"elastic","start":113,"end":120,"id":23},{"text":"-","start":120,"end":121,"id":24},{"text":"net","start":121,"end":124,"id":25},{"text":"MKL","start":125,"end":128,"id":26},{"text":"methods","start":129,"end":136,"id":27},{"text":"than","start":137,"end":141,"id":28},{"text":"the","start":142,"end":145,"id":29},{"text":"existing","start":146,"end":154,"id":30},{"text":"rate","start":155,"end":159,"id":31},{"text":"for","start":160,"end":163,"id":32},{"text":"block","start":164,"end":169,"id":33},{"text":"-","start":169,"end":170,"id":34},{"text":"l1","start":170,"end":172,"id":35},{"text":"MKL","start":173,"end":176,"id":36},{"text":".","start":176,"end":177,"id":37}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":113,"end":128,"token_start":23,"token_end":26,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Simultaneous orthogonal matching pursuit;","_input_hash":1725498043,"_task_hash":1699631042,"tokens":[{"text":"Simultaneous","start":0,"end":12,"id":0},{"text":"orthogonal","start":13,"end":23,"id":1},{"text":"matching","start":24,"end":32,"id":2},{"text":"pursuit","start":33,"end":40,"id":3},{"text":";","start":40,"end":41,"id":4}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":0,"end":40,"token_start":0,"token_end":3,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"One approach to account for non-trivial correlations between outputs employs convolution processes.","_input_hash":1942995204,"_task_hash":-997009107,"tokens":[{"text":"One","start":0,"end":3,"id":0},{"text":"approach","start":4,"end":12,"id":1},{"text":"to","start":13,"end":15,"id":2},{"text":"account","start":16,"end":23,"id":3},{"text":"for","start":24,"end":27,"id":4},{"text":"non","start":28,"end":31,"id":5},{"text":"-","start":31,"end":32,"id":6},{"text":"trivial","start":32,"end":39,"id":7},{"text":"correlations","start":40,"end":52,"id":8},{"text":"between","start":53,"end":60,"id":9},{"text":"outputs","start":61,"end":68,"id":10},{"text":"employs","start":69,"end":76,"id":11},{"text":"convolution","start":77,"end":88,"id":12},{"text":"processes","start":89,"end":98,"id":13},{"text":".","start":98,"end":99,"id":14}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"We experimentally demonstrate the scalability of our algorithm and its ability to find good quality feature sets.","_input_hash":960831934,"_task_hash":389523829,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"experimentally","start":3,"end":17,"id":1},{"text":"demonstrate","start":18,"end":29,"id":2},{"text":"the","start":30,"end":33,"id":3},{"text":"scalability","start":34,"end":45,"id":4},{"text":"of","start":46,"end":48,"id":5},{"text":"our","start":49,"end":52,"id":6},{"text":"algorithm","start":53,"end":62,"id":7},{"text":"and","start":63,"end":66,"id":8},{"text":"its","start":67,"end":70,"id":9},{"text":"ability","start":71,"end":78,"id":10},{"text":"to","start":79,"end":81,"id":11},{"text":"find","start":82,"end":86,"id":12},{"text":"good","start":87,"end":91,"id":13},{"text":"quality","start":92,"end":99,"id":14},{"text":"feature","start":100,"end":107,"id":15},{"text":"sets","start":108,"end":112,"id":16},{"text":".","start":112,"end":113,"id":17}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Incorporating this information into the learning method may lead to a significant decrease of the estimation error.","_input_hash":-1272222384,"_task_hash":813786115,"tokens":[{"text":"Incorporating","start":0,"end":13,"id":0},{"text":"this","start":14,"end":18,"id":1},{"text":"information","start":19,"end":30,"id":2},{"text":"into","start":31,"end":35,"id":3},{"text":"the","start":36,"end":39,"id":4},{"text":"learning","start":40,"end":48,"id":5},{"text":"method","start":49,"end":55,"id":6},{"text":"may","start":56,"end":59,"id":7},{"text":"lead","start":60,"end":64,"id":8},{"text":"to","start":65,"end":67,"id":9},{"text":"a","start":68,"end":69,"id":10},{"text":"significant","start":70,"end":81,"id":11},{"text":"decrease","start":82,"end":90,"id":12},{"text":"of","start":91,"end":93,"id":13},{"text":"the","start":94,"end":97,"id":14},{"text":"estimation","start":98,"end":108,"id":15},{"text":"error","start":109,"end":114,"id":16},{"text":".","start":114,"end":115,"id":17}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"The popular cubic smoothing spline estimate of a regression function arises as the minimizer of the penalized sum of squares $\\sum_j(Y_j - {\\mu}(t_j))^2 + {\\lambda}\\int_a^b [{\\mu}\"(t)]^2 dt$, where the data are $t_j,Y_j$, $j=1,..., n$. The minimization is taken over an infinite-dimensional function space, the space of all functions with square integrable second derivatives.","_input_hash":479241913,"_task_hash":-589123129,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"popular","start":4,"end":11,"id":1},{"text":"cubic","start":12,"end":17,"id":2},{"text":"smoothing","start":18,"end":27,"id":3},{"text":"spline","start":28,"end":34,"id":4},{"text":"estimate","start":35,"end":43,"id":5},{"text":"of","start":44,"end":46,"id":6},{"text":"a","start":47,"end":48,"id":7},{"text":"regression","start":49,"end":59,"id":8},{"text":"function","start":60,"end":68,"id":9},{"text":"arises","start":69,"end":75,"id":10},{"text":"as","start":76,"end":78,"id":11},{"text":"the","start":79,"end":82,"id":12},{"text":"minimizer","start":83,"end":92,"id":13},{"text":"of","start":93,"end":95,"id":14},{"text":"the","start":96,"end":99,"id":15},{"text":"penalized","start":100,"end":109,"id":16},{"text":"sum","start":110,"end":113,"id":17},{"text":"of","start":114,"end":116,"id":18},{"text":"squares","start":117,"end":124,"id":19},{"text":"$","start":125,"end":126,"id":20},{"text":"\\sum_j(Y_j","start":126,"end":136,"id":21},{"text":"-","start":137,"end":138,"id":22},{"text":"{","start":139,"end":140,"id":23},{"text":"\\mu}(t_j))^2","start":140,"end":152,"id":24},{"text":"+","start":153,"end":154,"id":25},{"text":"{","start":155,"end":156,"id":26},{"text":"\\lambda}\\int_a^b","start":156,"end":172,"id":27},{"text":"[","start":173,"end":174,"id":28},{"text":"{","start":174,"end":175,"id":29},{"text":"\\mu}\"(t)]^2","start":175,"end":186,"id":30},{"text":"dt$","start":187,"end":190,"id":31},{"text":",","start":190,"end":191,"id":32},{"text":"where","start":192,"end":197,"id":33},{"text":"the","start":198,"end":201,"id":34},{"text":"data","start":202,"end":206,"id":35},{"text":"are","start":207,"end":210,"id":36},{"text":"$","start":211,"end":212,"id":37},{"text":"t_j","start":212,"end":215,"id":38},{"text":",","start":215,"end":216,"id":39},{"text":"Y_j$","start":216,"end":220,"id":40},{"text":",","start":220,"end":221,"id":41},{"text":"$","start":222,"end":223,"id":42},{"text":"j=1","start":223,"end":226,"id":43},{"text":",","start":226,"end":227,"id":44},{"text":"...","start":227,"end":230,"id":45},{"text":",","start":230,"end":231,"id":46},{"text":"n$.","start":232,"end":235,"id":47},{"text":"The","start":236,"end":239,"id":48},{"text":"minimization","start":240,"end":252,"id":49},{"text":"is","start":253,"end":255,"id":50},{"text":"taken","start":256,"end":261,"id":51},{"text":"over","start":262,"end":266,"id":52},{"text":"an","start":267,"end":269,"id":53},{"text":"infinite","start":270,"end":278,"id":54},{"text":"-","start":278,"end":279,"id":55},{"text":"dimensional","start":279,"end":290,"id":56},{"text":"function","start":291,"end":299,"id":57},{"text":"space","start":300,"end":305,"id":58},{"text":",","start":305,"end":306,"id":59},{"text":"the","start":307,"end":310,"id":60},{"text":"space","start":311,"end":316,"id":61},{"text":"of","start":317,"end":319,"id":62},{"text":"all","start":320,"end":323,"id":63},{"text":"functions","start":324,"end":333,"id":64},{"text":"with","start":334,"end":338,"id":65},{"text":"square","start":339,"end":345,"id":66},{"text":"integrable","start":346,"end":356,"id":67},{"text":"second","start":357,"end":363,"id":68},{"text":"derivatives","start":364,"end":375,"id":69},{"text":".","start":375,"end":376,"id":70}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We further provide empirical evidence on the benefit of variable selection using multiple regression outputs jointly, as opposed to performing variable selection for each output separately.","_input_hash":1643213405,"_task_hash":-1446475043,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"further","start":3,"end":10,"id":1},{"text":"provide","start":11,"end":18,"id":2},{"text":"empirical","start":19,"end":28,"id":3},{"text":"evidence","start":29,"end":37,"id":4},{"text":"on","start":38,"end":40,"id":5},{"text":"the","start":41,"end":44,"id":6},{"text":"benefit","start":45,"end":52,"id":7},{"text":"of","start":53,"end":55,"id":8},{"text":"variable","start":56,"end":64,"id":9},{"text":"selection","start":65,"end":74,"id":10},{"text":"using","start":75,"end":80,"id":11},{"text":"multiple","start":81,"end":89,"id":12},{"text":"regression","start":90,"end":100,"id":13},{"text":"outputs","start":101,"end":108,"id":14},{"text":"jointly","start":109,"end":116,"id":15},{"text":",","start":116,"end":117,"id":16},{"text":"as","start":118,"end":120,"id":17},{"text":"opposed","start":121,"end":128,"id":18},{"text":"to","start":129,"end":131,"id":19},{"text":"performing","start":132,"end":142,"id":20},{"text":"variable","start":143,"end":151,"id":21},{"text":"selection","start":152,"end":161,"id":22},{"text":"for","start":162,"end":165,"id":23},{"text":"each","start":166,"end":170,"id":24},{"text":"output","start":171,"end":177,"id":25},{"text":"separately","start":178,"end":188,"id":26},{"text":".","start":188,"end":189,"id":27}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"a Dirichlet Variable Length Markov Model (Dirichlet-VMM) and a Time Convolutional Restricted Boltzmann Machine (TC-RBM).","_input_hash":-1123981299,"_task_hash":430536846,"tokens":[{"text":"a","start":0,"end":1,"id":0},{"text":"Dirichlet","start":2,"end":11,"id":1},{"text":"Variable","start":12,"end":20,"id":2},{"text":"Length","start":21,"end":27,"id":3},{"text":"Markov","start":28,"end":34,"id":4},{"text":"Model","start":35,"end":40,"id":5},{"text":"(","start":41,"end":42,"id":6},{"text":"Dirichlet","start":42,"end":51,"id":7},{"text":"-","start":51,"end":52,"id":8},{"text":"VMM","start":52,"end":55,"id":9},{"text":")","start":55,"end":56,"id":10},{"text":"and","start":57,"end":60,"id":11},{"text":"a","start":61,"end":62,"id":12},{"text":"Time","start":63,"end":67,"id":13},{"text":"Convolutional","start":68,"end":81,"id":14},{"text":"Restricted","start":82,"end":92,"id":15},{"text":"Boltzmann","start":93,"end":102,"id":16},{"text":"Machine","start":103,"end":110,"id":17},{"text":"(","start":111,"end":112,"id":18},{"text":"TC","start":112,"end":114,"id":19},{"text":"-","start":114,"end":115,"id":20},{"text":"RBM","start":115,"end":118,"id":21},{"text":")","start":118,"end":119,"id":22},{"text":".","start":119,"end":120,"id":23}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":2,"end":34,"token_start":1,"token_end":4,"label":"ALGO","answer":"accept"},{"start":63,"end":110,"token_start":13,"token_end":17,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We carefully study the effects of different number of neighbors and weight schemes and report the results.","_input_hash":-307230380,"_task_hash":-1976115500,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"carefully","start":3,"end":12,"id":1},{"text":"study","start":13,"end":18,"id":2},{"text":"the","start":19,"end":22,"id":3},{"text":"effects","start":23,"end":30,"id":4},{"text":"of","start":31,"end":33,"id":5},{"text":"different","start":34,"end":43,"id":6},{"text":"number","start":44,"end":50,"id":7},{"text":"of","start":51,"end":53,"id":8},{"text":"neighbors","start":54,"end":63,"id":9},{"text":"and","start":64,"end":67,"id":10},{"text":"weight","start":68,"end":74,"id":11},{"text":"schemes","start":75,"end":82,"id":12},{"text":"and","start":83,"end":86,"id":13},{"text":"report","start":87,"end":93,"id":14},{"text":"the","start":94,"end":97,"id":15},{"text":"results","start":98,"end":105,"id":16},{"text":".","start":105,"end":106,"id":17}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"In this paper we formulate in general terms an approach to prove strong consistency of the Empirical Risk Minimisation inductive principle applied to the prototype or distance based clustering.","_input_hash":-214180296,"_task_hash":-1107402532,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"this","start":3,"end":7,"id":1},{"text":"paper","start":8,"end":13,"id":2},{"text":"we","start":14,"end":16,"id":3},{"text":"formulate","start":17,"end":26,"id":4},{"text":"in","start":27,"end":29,"id":5},{"text":"general","start":30,"end":37,"id":6},{"text":"terms","start":38,"end":43,"id":7},{"text":"an","start":44,"end":46,"id":8},{"text":"approach","start":47,"end":55,"id":9},{"text":"to","start":56,"end":58,"id":10},{"text":"prove","start":59,"end":64,"id":11},{"text":"strong","start":65,"end":71,"id":12},{"text":"consistency","start":72,"end":83,"id":13},{"text":"of","start":84,"end":86,"id":14},{"text":"the","start":87,"end":90,"id":15},{"text":"Empirical","start":91,"end":100,"id":16},{"text":"Risk","start":101,"end":105,"id":17},{"text":"Minimisation","start":106,"end":118,"id":18},{"text":"inductive","start":119,"end":128,"id":19},{"text":"principle","start":129,"end":138,"id":20},{"text":"applied","start":139,"end":146,"id":21},{"text":"to","start":147,"end":149,"id":22},{"text":"the","start":150,"end":153,"id":23},{"text":"prototype","start":154,"end":163,"id":24},{"text":"or","start":164,"end":166,"id":25},{"text":"distance","start":167,"end":175,"id":26},{"text":"based","start":176,"end":181,"id":27},{"text":"clustering","start":182,"end":192,"id":28},{"text":".","start":192,"end":193,"id":29}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We consider the problem of learning a binary classifier from a training set of positive and unlabeled examples, both in the inductive and in the transductive setting.","_input_hash":-131897280,"_task_hash":-1993608256,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"consider","start":3,"end":11,"id":1},{"text":"the","start":12,"end":15,"id":2},{"text":"problem","start":16,"end":23,"id":3},{"text":"of","start":24,"end":26,"id":4},{"text":"learning","start":27,"end":35,"id":5},{"text":"a","start":36,"end":37,"id":6},{"text":"binary","start":38,"end":44,"id":7},{"text":"classifier","start":45,"end":55,"id":8},{"text":"from","start":56,"end":60,"id":9},{"text":"a","start":61,"end":62,"id":10},{"text":"training","start":63,"end":71,"id":11},{"text":"set","start":72,"end":75,"id":12},{"text":"of","start":76,"end":78,"id":13},{"text":"positive","start":79,"end":87,"id":14},{"text":"and","start":88,"end":91,"id":15},{"text":"unlabeled","start":92,"end":101,"id":16},{"text":"examples","start":102,"end":110,"id":17},{"text":",","start":110,"end":111,"id":18},{"text":"both","start":112,"end":116,"id":19},{"text":"in","start":117,"end":119,"id":20},{"text":"the","start":120,"end":123,"id":21},{"text":"inductive","start":124,"end":133,"id":22},{"text":"and","start":134,"end":137,"id":23},{"text":"in","start":138,"end":140,"id":24},{"text":"the","start":141,"end":144,"id":25},{"text":"transductive","start":145,"end":157,"id":26},{"text":"setting","start":158,"end":165,"id":27},{"text":".","start":165,"end":166,"id":28}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":38,"end":55,"token_start":7,"token_end":8,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We use an embedding procedure motivated by the random dot product graph model, a particular example of the latent position model.","_input_hash":992397200,"_task_hash":-131686925,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"use","start":3,"end":6,"id":1},{"text":"an","start":7,"end":9,"id":2},{"text":"embedding","start":10,"end":19,"id":3},{"text":"procedure","start":20,"end":29,"id":4},{"text":"motivated","start":30,"end":39,"id":5},{"text":"by","start":40,"end":42,"id":6},{"text":"the","start":43,"end":46,"id":7},{"text":"random","start":47,"end":53,"id":8},{"text":"dot","start":54,"end":57,"id":9},{"text":"product","start":58,"end":65,"id":10},{"text":"graph","start":66,"end":71,"id":11},{"text":"model","start":72,"end":77,"id":12},{"text":",","start":77,"end":78,"id":13},{"text":"a","start":79,"end":80,"id":14},{"text":"particular","start":81,"end":91,"id":15},{"text":"example","start":92,"end":99,"id":16},{"text":"of","start":100,"end":102,"id":17},{"text":"the","start":103,"end":106,"id":18},{"text":"latent","start":107,"end":113,"id":19},{"text":"position","start":114,"end":122,"id":20},{"text":"model","start":123,"end":128,"id":21},{"text":".","start":128,"end":129,"id":22}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":47,"end":71,"token_start":8,"token_end":11,"label":"ALGO","answer":"accept"},{"start":107,"end":122,"token_start":19,"token_end":20,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"To learn structural information, low-dimensional structure of the data is captured by solving a non-linear, low-rank representation problem.","_input_hash":2063615933,"_task_hash":302833230,"tokens":[{"text":"To","start":0,"end":2,"id":0},{"text":"learn","start":3,"end":8,"id":1},{"text":"structural","start":9,"end":19,"id":2},{"text":"information","start":20,"end":31,"id":3},{"text":",","start":31,"end":32,"id":4},{"text":"low","start":33,"end":36,"id":5},{"text":"-","start":36,"end":37,"id":6},{"text":"dimensional","start":37,"end":48,"id":7},{"text":"structure","start":49,"end":58,"id":8},{"text":"of","start":59,"end":61,"id":9},{"text":"the","start":62,"end":65,"id":10},{"text":"data","start":66,"end":70,"id":11},{"text":"is","start":71,"end":73,"id":12},{"text":"captured","start":74,"end":82,"id":13},{"text":"by","start":83,"end":85,"id":14},{"text":"solving","start":86,"end":93,"id":15},{"text":"a","start":94,"end":95,"id":16},{"text":"non","start":96,"end":99,"id":17},{"text":"-","start":99,"end":100,"id":18},{"text":"linear","start":100,"end":106,"id":19},{"text":",","start":106,"end":107,"id":20},{"text":"low","start":108,"end":111,"id":21},{"text":"-","start":111,"end":112,"id":22},{"text":"rank","start":112,"end":116,"id":23},{"text":"representation","start":117,"end":131,"id":24},{"text":"problem","start":132,"end":139,"id":25},{"text":".","start":139,"end":140,"id":26}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"Holmes and Adams (2002) focused on the performance of BKNN in terms of misclassification error but did not assess its ability to quantify uncertainty.","_input_hash":819004482,"_task_hash":-2084388308,"tokens":[{"text":"Holmes","start":0,"end":6,"id":0},{"text":"and","start":7,"end":10,"id":1},{"text":"Adams","start":11,"end":16,"id":2},{"text":"(","start":17,"end":18,"id":3},{"text":"2002","start":18,"end":22,"id":4},{"text":")","start":22,"end":23,"id":5},{"text":"focused","start":24,"end":31,"id":6},{"text":"on","start":32,"end":34,"id":7},{"text":"the","start":35,"end":38,"id":8},{"text":"performance","start":39,"end":50,"id":9},{"text":"of","start":51,"end":53,"id":10},{"text":"BKNN","start":54,"end":58,"id":11},{"text":"in","start":59,"end":61,"id":12},{"text":"terms","start":62,"end":67,"id":13},{"text":"of","start":68,"end":70,"id":14},{"text":"misclassification","start":71,"end":88,"id":15},{"text":"error","start":89,"end":94,"id":16},{"text":"but","start":95,"end":98,"id":17},{"text":"did","start":99,"end":102,"id":18},{"text":"not","start":103,"end":106,"id":19},{"text":"assess","start":107,"end":113,"id":20},{"text":"its","start":114,"end":117,"id":21},{"text":"ability","start":118,"end":125,"id":22},{"text":"to","start":126,"end":128,"id":23},{"text":"quantify","start":129,"end":137,"id":24},{"text":"uncertainty","start":138,"end":149,"id":25},{"text":".","start":149,"end":150,"id":26}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":54,"end":58,"token_start":11,"token_end":11,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Models that require simultaneous draws from a hierarchical Dirichlet process with restriction, such as infinite Hidden markov models (iHMM), were difficult to enjoy benefits of \\markerg{the} HCRP due to combinatorial explosion in calculating distributions of coupled draws.","_input_hash":1517795949,"_task_hash":1886505523,"tokens":[{"text":"Models","start":0,"end":6,"id":0},{"text":"that","start":7,"end":11,"id":1},{"text":"require","start":12,"end":19,"id":2},{"text":"simultaneous","start":20,"end":32,"id":3},{"text":"draws","start":33,"end":38,"id":4},{"text":"from","start":39,"end":43,"id":5},{"text":"a","start":44,"end":45,"id":6},{"text":"hierarchical","start":46,"end":58,"id":7},{"text":"Dirichlet","start":59,"end":68,"id":8},{"text":"process","start":69,"end":76,"id":9},{"text":"with","start":77,"end":81,"id":10},{"text":"restriction","start":82,"end":93,"id":11},{"text":",","start":93,"end":94,"id":12},{"text":"such","start":95,"end":99,"id":13},{"text":"as","start":100,"end":102,"id":14},{"text":"infinite","start":103,"end":111,"id":15},{"text":"Hidden","start":112,"end":118,"id":16},{"text":"markov","start":119,"end":125,"id":17},{"text":"models","start":126,"end":132,"id":18},{"text":"(","start":133,"end":134,"id":19},{"text":"iHMM","start":134,"end":138,"id":20},{"text":")","start":138,"end":139,"id":21},{"text":",","start":139,"end":140,"id":22},{"text":"were","start":141,"end":145,"id":23},{"text":"difficult","start":146,"end":155,"id":24},{"text":"to","start":156,"end":158,"id":25},{"text":"enjoy","start":159,"end":164,"id":26},{"text":"benefits","start":165,"end":173,"id":27},{"text":"of","start":174,"end":176,"id":28},{"text":"\\markerg{the","start":177,"end":189,"id":29},{"text":"}","start":189,"end":190,"id":30},{"text":"HCRP","start":191,"end":195,"id":31},{"text":"due","start":196,"end":199,"id":32},{"text":"to","start":200,"end":202,"id":33},{"text":"combinatorial","start":203,"end":216,"id":34},{"text":"explosion","start":217,"end":226,"id":35},{"text":"in","start":227,"end":229,"id":36},{"text":"calculating","start":230,"end":241,"id":37},{"text":"distributions","start":242,"end":255,"id":38},{"text":"of","start":256,"end":258,"id":39},{"text":"coupled","start":259,"end":266,"id":40},{"text":"draws","start":267,"end":272,"id":41},{"text":".","start":272,"end":273,"id":42}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":46,"end":68,"token_start":7,"token_end":8,"label":"ALGO","answer":"accept"},{"start":103,"end":125,"token_start":15,"token_end":17,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We then discuss relative bounds, comparing the generalization error of two classification rules, showing how the margin assumption of Mammen and Tsybakov can be replaced with some empirical measure of the covariance structure of the classification model.","_input_hash":-1789333198,"_task_hash":76277052,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"then","start":3,"end":7,"id":1},{"text":"discuss","start":8,"end":15,"id":2},{"text":"relative","start":16,"end":24,"id":3},{"text":"bounds","start":25,"end":31,"id":4},{"text":",","start":31,"end":32,"id":5},{"text":"comparing","start":33,"end":42,"id":6},{"text":"the","start":43,"end":46,"id":7},{"text":"generalization","start":47,"end":61,"id":8},{"text":"error","start":62,"end":67,"id":9},{"text":"of","start":68,"end":70,"id":10},{"text":"two","start":71,"end":74,"id":11},{"text":"classification","start":75,"end":89,"id":12},{"text":"rules","start":90,"end":95,"id":13},{"text":",","start":95,"end":96,"id":14},{"text":"showing","start":97,"end":104,"id":15},{"text":"how","start":105,"end":108,"id":16},{"text":"the","start":109,"end":112,"id":17},{"text":"margin","start":113,"end":119,"id":18},{"text":"assumption","start":120,"end":130,"id":19},{"text":"of","start":131,"end":133,"id":20},{"text":"Mammen","start":134,"end":140,"id":21},{"text":"and","start":141,"end":144,"id":22},{"text":"Tsybakov","start":145,"end":153,"id":23},{"text":"can","start":154,"end":157,"id":24},{"text":"be","start":158,"end":160,"id":25},{"text":"replaced","start":161,"end":169,"id":26},{"text":"with","start":170,"end":174,"id":27},{"text":"some","start":175,"end":179,"id":28},{"text":"empirical","start":180,"end":189,"id":29},{"text":"measure","start":190,"end":197,"id":30},{"text":"of","start":198,"end":200,"id":31},{"text":"the","start":201,"end":204,"id":32},{"text":"covariance","start":205,"end":215,"id":33},{"text":"structure","start":216,"end":225,"id":34},{"text":"of","start":226,"end":228,"id":35},{"text":"the","start":229,"end":232,"id":36},{"text":"classification","start":233,"end":247,"id":37},{"text":"model","start":248,"end":253,"id":38},{"text":".","start":253,"end":254,"id":39}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We utilize localization techniques to obtain the sharp learning rate.","_input_hash":1981004043,"_task_hash":-1162336533,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"utilize","start":3,"end":10,"id":1},{"text":"localization","start":11,"end":23,"id":2},{"text":"techniques","start":24,"end":34,"id":3},{"text":"to","start":35,"end":37,"id":4},{"text":"obtain","start":38,"end":44,"id":5},{"text":"the","start":45,"end":48,"id":6},{"text":"sharp","start":49,"end":54,"id":7},{"text":"learning","start":55,"end":63,"id":8},{"text":"rate","start":64,"end":68,"id":9},{"text":".","start":68,"end":69,"id":10}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"Unsupervised discovery of latent representations, in addition to being useful for density modeling, visualisation and exploratory data analysis, is also increasingly important for learning features relevant to discriminative tasks.","_input_hash":1642277009,"_task_hash":1299958983,"tokens":[{"text":"Unsupervised","start":0,"end":12,"id":0},{"text":"discovery","start":13,"end":22,"id":1},{"text":"of","start":23,"end":25,"id":2},{"text":"latent","start":26,"end":32,"id":3},{"text":"representations","start":33,"end":48,"id":4},{"text":",","start":48,"end":49,"id":5},{"text":"in","start":50,"end":52,"id":6},{"text":"addition","start":53,"end":61,"id":7},{"text":"to","start":62,"end":64,"id":8},{"text":"being","start":65,"end":70,"id":9},{"text":"useful","start":71,"end":77,"id":10},{"text":"for","start":78,"end":81,"id":11},{"text":"density","start":82,"end":89,"id":12},{"text":"modeling","start":90,"end":98,"id":13},{"text":",","start":98,"end":99,"id":14},{"text":"visualisation","start":100,"end":113,"id":15},{"text":"and","start":114,"end":117,"id":16},{"text":"exploratory","start":118,"end":129,"id":17},{"text":"data","start":130,"end":134,"id":18},{"text":"analysis","start":135,"end":143,"id":19},{"text":",","start":143,"end":144,"id":20},{"text":"is","start":145,"end":147,"id":21},{"text":"also","start":148,"end":152,"id":22},{"text":"increasingly","start":153,"end":165,"id":23},{"text":"important","start":166,"end":175,"id":24},{"text":"for","start":176,"end":179,"id":25},{"text":"learning","start":180,"end":188,"id":26},{"text":"features","start":189,"end":197,"id":27},{"text":"relevant","start":198,"end":206,"id":28},{"text":"to","start":207,"end":209,"id":29},{"text":"discriminative","start":210,"end":224,"id":30},{"text":"tasks","start":225,"end":230,"id":31},{"text":".","start":230,"end":231,"id":32}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":82,"end":89,"token_start":12,"token_end":12,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We are interested in the actual clustering, not only in the costs of the solution.","_input_hash":-137369651,"_task_hash":256525834,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"are","start":3,"end":6,"id":1},{"text":"interested","start":7,"end":17,"id":2},{"text":"in","start":18,"end":20,"id":3},{"text":"the","start":21,"end":24,"id":4},{"text":"actual","start":25,"end":31,"id":5},{"text":"clustering","start":32,"end":42,"id":6},{"text":",","start":42,"end":43,"id":7},{"text":"not","start":44,"end":47,"id":8},{"text":"only","start":48,"end":52,"id":9},{"text":"in","start":53,"end":55,"id":10},{"text":"the","start":56,"end":59,"id":11},{"text":"costs","start":60,"end":65,"id":12},{"text":"of","start":66,"end":68,"id":13},{"text":"the","start":69,"end":72,"id":14},{"text":"solution","start":73,"end":81,"id":15},{"text":".","start":81,"end":82,"id":16}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"2002).","_input_hash":-1673416215,"_task_hash":-808929096,"tokens":[{"text":"2002","start":0,"end":4,"id":0},{"text":")","start":4,"end":5,"id":1},{"text":".","start":5,"end":6,"id":2}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"A state-of-the-art CDOM method employs a kernel-based dependency measure, but it has a drawback that the kernel parameter needs to be determined manually.","_input_hash":835533890,"_task_hash":652017344,"tokens":[{"text":"A","start":0,"end":1,"id":0},{"text":"state","start":2,"end":7,"id":1},{"text":"-","start":7,"end":8,"id":2},{"text":"of","start":8,"end":10,"id":3},{"text":"-","start":10,"end":11,"id":4},{"text":"the","start":11,"end":14,"id":5},{"text":"-","start":14,"end":15,"id":6},{"text":"art","start":15,"end":18,"id":7},{"text":"CDOM","start":19,"end":23,"id":8},{"text":"method","start":24,"end":30,"id":9},{"text":"employs","start":31,"end":38,"id":10},{"text":"a","start":39,"end":40,"id":11},{"text":"kernel","start":41,"end":47,"id":12},{"text":"-","start":47,"end":48,"id":13},{"text":"based","start":48,"end":53,"id":14},{"text":"dependency","start":54,"end":64,"id":15},{"text":"measure","start":65,"end":72,"id":16},{"text":",","start":72,"end":73,"id":17},{"text":"but","start":74,"end":77,"id":18},{"text":"it","start":78,"end":80,"id":19},{"text":"has","start":81,"end":84,"id":20},{"text":"a","start":85,"end":86,"id":21},{"text":"drawback","start":87,"end":95,"id":22},{"text":"that","start":96,"end":100,"id":23},{"text":"the","start":101,"end":104,"id":24},{"text":"kernel","start":105,"end":111,"id":25},{"text":"parameter","start":112,"end":121,"id":26},{"text":"needs","start":122,"end":127,"id":27},{"text":"to","start":128,"end":130,"id":28},{"text":"be","start":131,"end":133,"id":29},{"text":"determined","start":134,"end":144,"id":30},{"text":"manually","start":145,"end":153,"id":31},{"text":".","start":153,"end":154,"id":32}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"To deal with large-scale data sets, we also develop an online inference algorithm for DILN and compare with online HDP and online LDA on the Nature magazine, which contains approximately 350,000 articles.","_input_hash":-577497111,"_task_hash":1965679948,"tokens":[{"text":"To","start":0,"end":2,"id":0},{"text":"deal","start":3,"end":7,"id":1},{"text":"with","start":8,"end":12,"id":2},{"text":"large","start":13,"end":18,"id":3},{"text":"-","start":18,"end":19,"id":4},{"text":"scale","start":19,"end":24,"id":5},{"text":"data","start":25,"end":29,"id":6},{"text":"sets","start":30,"end":34,"id":7},{"text":",","start":34,"end":35,"id":8},{"text":"we","start":36,"end":38,"id":9},{"text":"also","start":39,"end":43,"id":10},{"text":"develop","start":44,"end":51,"id":11},{"text":"an","start":52,"end":54,"id":12},{"text":"online","start":55,"end":61,"id":13},{"text":"inference","start":62,"end":71,"id":14},{"text":"algorithm","start":72,"end":81,"id":15},{"text":"for","start":82,"end":85,"id":16},{"text":"DILN","start":86,"end":90,"id":17},{"text":"and","start":91,"end":94,"id":18},{"text":"compare","start":95,"end":102,"id":19},{"text":"with","start":103,"end":107,"id":20},{"text":"online","start":108,"end":114,"id":21},{"text":"HDP","start":115,"end":118,"id":22},{"text":"and","start":119,"end":122,"id":23},{"text":"online","start":123,"end":129,"id":24},{"text":"LDA","start":130,"end":133,"id":25},{"text":"on","start":134,"end":136,"id":26},{"text":"the","start":137,"end":140,"id":27},{"text":"Nature","start":141,"end":147,"id":28},{"text":"magazine","start":148,"end":156,"id":29},{"text":",","start":156,"end":157,"id":30},{"text":"which","start":158,"end":163,"id":31},{"text":"contains","start":164,"end":172,"id":32},{"text":"approximately","start":173,"end":186,"id":33},{"text":"350,000","start":187,"end":194,"id":34},{"text":"articles","start":195,"end":203,"id":35},{"text":".","start":203,"end":204,"id":36}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":55,"end":71,"token_start":13,"token_end":14,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"the data may be related to the function ${\\mu}$ in another way, the sum of squares may be replaced by a more suitable expression, or the penalty, $\\int_a^b [{\\mu}\"(t)]^2 dt$, might take a different form.","_input_hash":201896631,"_task_hash":-2120934874,"tokens":[{"text":"the","start":0,"end":3,"id":0},{"text":"data","start":4,"end":8,"id":1},{"text":"may","start":9,"end":12,"id":2},{"text":"be","start":13,"end":15,"id":3},{"text":"related","start":16,"end":23,"id":4},{"text":"to","start":24,"end":26,"id":5},{"text":"the","start":27,"end":30,"id":6},{"text":"function","start":31,"end":39,"id":7},{"text":"$","start":40,"end":41,"id":8},{"text":"{","start":41,"end":42,"id":9},{"text":"\\mu}$","start":42,"end":47,"id":10},{"text":"in","start":48,"end":50,"id":11},{"text":"another","start":51,"end":58,"id":12},{"text":"way","start":59,"end":62,"id":13},{"text":",","start":62,"end":63,"id":14},{"text":"the","start":64,"end":67,"id":15},{"text":"sum","start":68,"end":71,"id":16},{"text":"of","start":72,"end":74,"id":17},{"text":"squares","start":75,"end":82,"id":18},{"text":"may","start":83,"end":86,"id":19},{"text":"be","start":87,"end":89,"id":20},{"text":"replaced","start":90,"end":98,"id":21},{"text":"by","start":99,"end":101,"id":22},{"text":"a","start":102,"end":103,"id":23},{"text":"more","start":104,"end":108,"id":24},{"text":"suitable","start":109,"end":117,"id":25},{"text":"expression","start":118,"end":128,"id":26},{"text":",","start":128,"end":129,"id":27},{"text":"or","start":130,"end":132,"id":28},{"text":"the","start":133,"end":136,"id":29},{"text":"penalty","start":137,"end":144,"id":30},{"text":",","start":144,"end":145,"id":31},{"text":"$","start":146,"end":147,"id":32},{"text":"\\int_a^b","start":147,"end":155,"id":33},{"text":"[","start":156,"end":157,"id":34},{"text":"{","start":157,"end":158,"id":35},{"text":"\\mu}\"(t)]^2","start":158,"end":169,"id":36},{"text":"dt$","start":170,"end":173,"id":37},{"text":",","start":173,"end":174,"id":38},{"text":"might","start":175,"end":180,"id":39},{"text":"take","start":181,"end":185,"id":40},{"text":"a","start":186,"end":187,"id":41},{"text":"different","start":188,"end":197,"id":42},{"text":"form","start":198,"end":202,"id":43},{"text":".","start":202,"end":203,"id":44}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We propose a method to infer causal structures containing both discrete and continuous variables.","_input_hash":494398072,"_task_hash":669962654,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"propose","start":3,"end":10,"id":1},{"text":"a","start":11,"end":12,"id":2},{"text":"method","start":13,"end":19,"id":3},{"text":"to","start":20,"end":22,"id":4},{"text":"infer","start":23,"end":28,"id":5},{"text":"causal","start":29,"end":35,"id":6},{"text":"structures","start":36,"end":46,"id":7},{"text":"containing","start":47,"end":57,"id":8},{"text":"both","start":58,"end":62,"id":9},{"text":"discrete","start":63,"end":71,"id":10},{"text":"and","start":72,"end":75,"id":11},{"text":"continuous","start":76,"end":86,"id":12},{"text":"variables","start":87,"end":96,"id":13},{"text":".","start":96,"end":97,"id":14}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"The target field of application are CFD problems, where objective function are extremely expensive to evaluate, but the theory can be also used in other fields.","_input_hash":-863819772,"_task_hash":536865355,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"target","start":4,"end":10,"id":1},{"text":"field","start":11,"end":16,"id":2},{"text":"of","start":17,"end":19,"id":3},{"text":"application","start":20,"end":31,"id":4},{"text":"are","start":32,"end":35,"id":5},{"text":"CFD","start":36,"end":39,"id":6},{"text":"problems","start":40,"end":48,"id":7},{"text":",","start":48,"end":49,"id":8},{"text":"where","start":50,"end":55,"id":9},{"text":"objective","start":56,"end":65,"id":10},{"text":"function","start":66,"end":74,"id":11},{"text":"are","start":75,"end":78,"id":12},{"text":"extremely","start":79,"end":88,"id":13},{"text":"expensive","start":89,"end":98,"id":14},{"text":"to","start":99,"end":101,"id":15},{"text":"evaluate","start":102,"end":110,"id":16},{"text":",","start":110,"end":111,"id":17},{"text":"but","start":112,"end":115,"id":18},{"text":"the","start":116,"end":119,"id":19},{"text":"theory","start":120,"end":126,"id":20},{"text":"can","start":127,"end":130,"id":21},{"text":"be","start":131,"end":133,"id":22},{"text":"also","start":134,"end":138,"id":23},{"text":"used","start":139,"end":143,"id":24},{"text":"in","start":144,"end":146,"id":25},{"text":"other","start":147,"end":152,"id":26},{"text":"fields","start":153,"end":159,"id":27},{"text":".","start":159,"end":160,"id":28}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"This paper re-examines the issues in two parts.","_input_hash":-2035294479,"_task_hash":965280369,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"paper","start":5,"end":10,"id":1},{"text":"re","start":11,"end":13,"id":2},{"text":"-","start":13,"end":14,"id":3},{"text":"examines","start":14,"end":22,"id":4},{"text":"the","start":23,"end":26,"id":5},{"text":"issues","start":27,"end":33,"id":6},{"text":"in","start":34,"end":36,"id":7},{"text":"two","start":37,"end":40,"id":8},{"text":"parts","start":41,"end":46,"id":9},{"text":".","start":46,"end":47,"id":10}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We present a new online boosting algorithm for adapting the weights of a boosted classifier, which yields a closer approximation to Freund and Schapire's AdaBoost algorithm than previous online boosting algorithms.","_input_hash":199535872,"_task_hash":-155511390,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"present","start":3,"end":10,"id":1},{"text":"a","start":11,"end":12,"id":2},{"text":"new","start":13,"end":16,"id":3},{"text":"online","start":17,"end":23,"id":4},{"text":"boosting","start":24,"end":32,"id":5},{"text":"algorithm","start":33,"end":42,"id":6},{"text":"for","start":43,"end":46,"id":7},{"text":"adapting","start":47,"end":55,"id":8},{"text":"the","start":56,"end":59,"id":9},{"text":"weights","start":60,"end":67,"id":10},{"text":"of","start":68,"end":70,"id":11},{"text":"a","start":71,"end":72,"id":12},{"text":"boosted","start":73,"end":80,"id":13},{"text":"classifier","start":81,"end":91,"id":14},{"text":",","start":91,"end":92,"id":15},{"text":"which","start":93,"end":98,"id":16},{"text":"yields","start":99,"end":105,"id":17},{"text":"a","start":106,"end":107,"id":18},{"text":"closer","start":108,"end":114,"id":19},{"text":"approximation","start":115,"end":128,"id":20},{"text":"to","start":129,"end":131,"id":21},{"text":"Freund","start":132,"end":138,"id":22},{"text":"and","start":139,"end":142,"id":23},{"text":"Schapire","start":143,"end":151,"id":24},{"text":"'s","start":151,"end":153,"id":25},{"text":"AdaBoost","start":154,"end":162,"id":26},{"text":"algorithm","start":163,"end":172,"id":27},{"text":"than","start":173,"end":177,"id":28},{"text":"previous","start":178,"end":186,"id":29},{"text":"online","start":187,"end":193,"id":30},{"text":"boosting","start":194,"end":202,"id":31},{"text":"algorithms","start":203,"end":213,"id":32},{"text":".","start":213,"end":214,"id":33}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":17,"end":32,"token_start":4,"token_end":5,"label":"ALGO","answer":"accept"},{"start":154,"end":162,"token_start":26,"token_end":26,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Similar results are obtained for function learning in terms of fat-shattering dimension modulo countable sets, but, just like in the classical distribution-free case, the finiteness of this parameter is sufficient but not necessary for PAC learnability under non-atomic measures.","_input_hash":-356720325,"_task_hash":240698764,"tokens":[{"text":"Similar","start":0,"end":7,"id":0},{"text":"results","start":8,"end":15,"id":1},{"text":"are","start":16,"end":19,"id":2},{"text":"obtained","start":20,"end":28,"id":3},{"text":"for","start":29,"end":32,"id":4},{"text":"function","start":33,"end":41,"id":5},{"text":"learning","start":42,"end":50,"id":6},{"text":"in","start":51,"end":53,"id":7},{"text":"terms","start":54,"end":59,"id":8},{"text":"of","start":60,"end":62,"id":9},{"text":"fat","start":63,"end":66,"id":10},{"text":"-","start":66,"end":67,"id":11},{"text":"shattering","start":67,"end":77,"id":12},{"text":"dimension","start":78,"end":87,"id":13},{"text":"modulo","start":88,"end":94,"id":14},{"text":"countable","start":95,"end":104,"id":15},{"text":"sets","start":105,"end":109,"id":16},{"text":",","start":109,"end":110,"id":17},{"text":"but","start":111,"end":114,"id":18},{"text":",","start":114,"end":115,"id":19},{"text":"just","start":116,"end":120,"id":20},{"text":"like","start":121,"end":125,"id":21},{"text":"in","start":126,"end":128,"id":22},{"text":"the","start":129,"end":132,"id":23},{"text":"classical","start":133,"end":142,"id":24},{"text":"distribution","start":143,"end":155,"id":25},{"text":"-","start":155,"end":156,"id":26},{"text":"free","start":156,"end":160,"id":27},{"text":"case","start":161,"end":165,"id":28},{"text":",","start":165,"end":166,"id":29},{"text":"the","start":167,"end":170,"id":30},{"text":"finiteness","start":171,"end":181,"id":31},{"text":"of","start":182,"end":184,"id":32},{"text":"this","start":185,"end":189,"id":33},{"text":"parameter","start":190,"end":199,"id":34},{"text":"is","start":200,"end":202,"id":35},{"text":"sufficient","start":203,"end":213,"id":36},{"text":"but","start":214,"end":217,"id":37},{"text":"not","start":218,"end":221,"id":38},{"text":"necessary","start":222,"end":231,"id":39},{"text":"for","start":232,"end":235,"id":40},{"text":"PAC","start":236,"end":239,"id":41},{"text":"learnability","start":240,"end":252,"id":42},{"text":"under","start":253,"end":258,"id":43},{"text":"non","start":259,"end":262,"id":44},{"text":"-","start":262,"end":263,"id":45},{"text":"atomic","start":263,"end":269,"id":46},{"text":"measures","start":270,"end":278,"id":47},{"text":".","start":278,"end":279,"id":48}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":236,"end":239,"token_start":41,"token_end":41,"label":"ALGO","answer":"reject"}],"answer":"reject"}
{"text":"We propose a novel method of introducing structure into existing machine learning techniques by developing structure-based similarity and distance measures.","_input_hash":-1619866776,"_task_hash":-1675763522,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"propose","start":3,"end":10,"id":1},{"text":"a","start":11,"end":12,"id":2},{"text":"novel","start":13,"end":18,"id":3},{"text":"method","start":19,"end":25,"id":4},{"text":"of","start":26,"end":28,"id":5},{"text":"introducing","start":29,"end":40,"id":6},{"text":"structure","start":41,"end":50,"id":7},{"text":"into","start":51,"end":55,"id":8},{"text":"existing","start":56,"end":64,"id":9},{"text":"machine","start":65,"end":72,"id":10},{"text":"learning","start":73,"end":81,"id":11},{"text":"techniques","start":82,"end":92,"id":12},{"text":"by","start":93,"end":95,"id":13},{"text":"developing","start":96,"end":106,"id":14},{"text":"structure","start":107,"end":116,"id":15},{"text":"-","start":116,"end":117,"id":16},{"text":"based","start":117,"end":122,"id":17},{"text":"similarity","start":123,"end":133,"id":18},{"text":"and","start":134,"end":137,"id":19},{"text":"distance","start":138,"end":146,"id":20},{"text":"measures","start":147,"end":155,"id":21},{"text":".","start":155,"end":156,"id":22}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"The model accommodates a variety of response types.","_input_hash":-768690754,"_task_hash":435747910,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"model","start":4,"end":9,"id":1},{"text":"accommodates","start":10,"end":22,"id":2},{"text":"a","start":23,"end":24,"id":3},{"text":"variety","start":25,"end":32,"id":4},{"text":"of","start":33,"end":35,"id":5},{"text":"response","start":36,"end":44,"id":6},{"text":"types","start":45,"end":50,"id":7},{"text":".","start":50,"end":51,"id":8}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"In particular, we consider the scenario in which the model evolves in a piece-wise constant fashion.","_input_hash":91655820,"_task_hash":-314779701,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"particular","start":3,"end":13,"id":1},{"text":",","start":13,"end":14,"id":2},{"text":"we","start":15,"end":17,"id":3},{"text":"consider","start":18,"end":26,"id":4},{"text":"the","start":27,"end":30,"id":5},{"text":"scenario","start":31,"end":39,"id":6},{"text":"in","start":40,"end":42,"id":7},{"text":"which","start":43,"end":48,"id":8},{"text":"the","start":49,"end":52,"id":9},{"text":"model","start":53,"end":58,"id":10},{"text":"evolves","start":59,"end":66,"id":11},{"text":"in","start":67,"end":69,"id":12},{"text":"a","start":70,"end":71,"id":13},{"text":"piece","start":72,"end":77,"id":14},{"text":"-","start":77,"end":78,"id":15},{"text":"wise","start":78,"end":82,"id":16},{"text":"constant","start":83,"end":91,"id":17},{"text":"fashion","start":92,"end":99,"id":18},{"text":".","start":99,"end":100,"id":19}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We also propose a modification for the case when only integer-valued allocations are possible.","_input_hash":675297407,"_task_hash":-696232248,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"also","start":3,"end":7,"id":1},{"text":"propose","start":8,"end":15,"id":2},{"text":"a","start":16,"end":17,"id":3},{"text":"modification","start":18,"end":30,"id":4},{"text":"for","start":31,"end":34,"id":5},{"text":"the","start":35,"end":38,"id":6},{"text":"case","start":39,"end":43,"id":7},{"text":"when","start":44,"end":48,"id":8},{"text":"only","start":49,"end":53,"id":9},{"text":"integer","start":54,"end":61,"id":10},{"text":"-","start":61,"end":62,"id":11},{"text":"valued","start":62,"end":68,"id":12},{"text":"allocations","start":69,"end":80,"id":13},{"text":"are","start":81,"end":84,"id":14},{"text":"possible","start":85,"end":93,"id":15},{"text":".","start":93,"end":94,"id":16}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":54,"end":68,"token_start":10,"token_end":12,"label":"ALGO","answer":"reject"}],"answer":"reject"}
{"text":"We propose a new training strategy and obtain improved generalisation performance and better density estimates in comparative evaluations on several benchmark data sets.","_input_hash":-234269161,"_task_hash":-341524968,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"propose","start":3,"end":10,"id":1},{"text":"a","start":11,"end":12,"id":2},{"text":"new","start":13,"end":16,"id":3},{"text":"training","start":17,"end":25,"id":4},{"text":"strategy","start":26,"end":34,"id":5},{"text":"and","start":35,"end":38,"id":6},{"text":"obtain","start":39,"end":45,"id":7},{"text":"improved","start":46,"end":54,"id":8},{"text":"generalisation","start":55,"end":69,"id":9},{"text":"performance","start":70,"end":81,"id":10},{"text":"and","start":82,"end":85,"id":11},{"text":"better","start":86,"end":92,"id":12},{"text":"density","start":93,"end":100,"id":13},{"text":"estimates","start":101,"end":110,"id":14},{"text":"in","start":111,"end":113,"id":15},{"text":"comparative","start":114,"end":125,"id":16},{"text":"evaluations","start":126,"end":137,"id":17},{"text":"on","start":138,"end":140,"id":18},{"text":"several","start":141,"end":148,"id":19},{"text":"benchmark","start":149,"end":158,"id":20},{"text":"data","start":159,"end":163,"id":21},{"text":"sets","start":164,"end":168,"id":22},{"text":".","start":168,"end":169,"id":23}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Nonnegative matrix factorization (NMF) is now a common tool for audio source separation.","_input_hash":1708946146,"_task_hash":-1834038906,"tokens":[{"text":"Nonnegative","start":0,"end":11,"id":0},{"text":"matrix","start":12,"end":18,"id":1},{"text":"factorization","start":19,"end":32,"id":2},{"text":"(","start":33,"end":34,"id":3},{"text":"NMF","start":34,"end":37,"id":4},{"text":")","start":37,"end":38,"id":5},{"text":"is","start":39,"end":41,"id":6},{"text":"now","start":42,"end":45,"id":7},{"text":"a","start":46,"end":47,"id":8},{"text":"common","start":48,"end":54,"id":9},{"text":"tool","start":55,"end":59,"id":10},{"text":"for","start":60,"end":63,"id":11},{"text":"audio","start":64,"end":69,"id":12},{"text":"source","start":70,"end":76,"id":13},{"text":"separation","start":77,"end":87,"id":14},{"text":".","start":87,"end":88,"id":15}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":0,"end":32,"token_start":0,"token_end":2,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"In this paper, we consider a sparse hierarchical structured regularization.","_input_hash":-1593076490,"_task_hash":296908094,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"this","start":3,"end":7,"id":1},{"text":"paper","start":8,"end":13,"id":2},{"text":",","start":13,"end":14,"id":3},{"text":"we","start":15,"end":17,"id":4},{"text":"consider","start":18,"end":26,"id":5},{"text":"a","start":27,"end":28,"id":6},{"text":"sparse","start":29,"end":35,"id":7},{"text":"hierarchical","start":36,"end":48,"id":8},{"text":"structured","start":49,"end":59,"id":9},{"text":"regularization","start":60,"end":74,"id":10},{"text":".","start":74,"end":75,"id":11}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"With the present revival of interest in bistatic radar systems, research in that area has gained momentum.","_input_hash":793808710,"_task_hash":-1301613672,"tokens":[{"text":"With","start":0,"end":4,"id":0},{"text":"the","start":5,"end":8,"id":1},{"text":"present","start":9,"end":16,"id":2},{"text":"revival","start":17,"end":24,"id":3},{"text":"of","start":25,"end":27,"id":4},{"text":"interest","start":28,"end":36,"id":5},{"text":"in","start":37,"end":39,"id":6},{"text":"bistatic","start":40,"end":48,"id":7},{"text":"radar","start":49,"end":54,"id":8},{"text":"systems","start":55,"end":62,"id":9},{"text":",","start":62,"end":63,"id":10},{"text":"research","start":64,"end":72,"id":11},{"text":"in","start":73,"end":75,"id":12},{"text":"that","start":76,"end":80,"id":13},{"text":"area","start":81,"end":85,"id":14},{"text":"has","start":86,"end":89,"id":15},{"text":"gained","start":90,"end":96,"id":16},{"text":"momentum","start":97,"end":105,"id":17},{"text":".","start":105,"end":106,"id":18}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"The main assumption is that the mean value is piecewise constant in time, and the task is to estimate the change times and the mean values within the segments.","_input_hash":-72051538,"_task_hash":-620396599,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"main","start":4,"end":8,"id":1},{"text":"assumption","start":9,"end":19,"id":2},{"text":"is","start":20,"end":22,"id":3},{"text":"that","start":23,"end":27,"id":4},{"text":"the","start":28,"end":31,"id":5},{"text":"mean","start":32,"end":36,"id":6},{"text":"value","start":37,"end":42,"id":7},{"text":"is","start":43,"end":45,"id":8},{"text":"piecewise","start":46,"end":55,"id":9},{"text":"constant","start":56,"end":64,"id":10},{"text":"in","start":65,"end":67,"id":11},{"text":"time","start":68,"end":72,"id":12},{"text":",","start":72,"end":73,"id":13},{"text":"and","start":74,"end":77,"id":14},{"text":"the","start":78,"end":81,"id":15},{"text":"task","start":82,"end":86,"id":16},{"text":"is","start":87,"end":89,"id":17},{"text":"to","start":90,"end":92,"id":18},{"text":"estimate","start":93,"end":101,"id":19},{"text":"the","start":102,"end":105,"id":20},{"text":"change","start":106,"end":112,"id":21},{"text":"times","start":113,"end":118,"id":22},{"text":"and","start":119,"end":122,"id":23},{"text":"the","start":123,"end":126,"id":24},{"text":"mean","start":127,"end":131,"id":25},{"text":"values","start":132,"end":138,"id":26},{"text":"within","start":139,"end":145,"id":27},{"text":"the","start":146,"end":149,"id":28},{"text":"segments","start":150,"end":158,"id":29},{"text":".","start":158,"end":159,"id":30}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We consider principal component analysis (PCA) in decomposable Gaussian graphical models.","_input_hash":874169289,"_task_hash":-1642414478,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"consider","start":3,"end":11,"id":1},{"text":"principal","start":12,"end":21,"id":2},{"text":"component","start":22,"end":31,"id":3},{"text":"analysis","start":32,"end":40,"id":4},{"text":"(","start":41,"end":42,"id":5},{"text":"PCA","start":42,"end":45,"id":6},{"text":")","start":45,"end":46,"id":7},{"text":"in","start":47,"end":49,"id":8},{"text":"decomposable","start":50,"end":62,"id":9},{"text":"Gaussian","start":63,"end":71,"id":10},{"text":"graphical","start":72,"end":81,"id":11},{"text":"models","start":82,"end":88,"id":12},{"text":".","start":88,"end":89,"id":13}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":12,"end":40,"token_start":2,"token_end":4,"label":"ALGO","answer":"accept"},{"start":50,"end":81,"token_start":9,"token_end":11,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We also show a lower bound that shows that the bound is tight, and derive consequences regarding excess loss, namely fast convergence rates of the order $O(n^{-\\frac{\\alpha}{1+\\alpha}})$, where $\\alpha$ is the minimum eigenvalue decay rate of the individual kernels.","_input_hash":-792363590,"_task_hash":-1332267459,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"also","start":3,"end":7,"id":1},{"text":"show","start":8,"end":12,"id":2},{"text":"a","start":13,"end":14,"id":3},{"text":"lower","start":15,"end":20,"id":4},{"text":"bound","start":21,"end":26,"id":5},{"text":"that","start":27,"end":31,"id":6},{"text":"shows","start":32,"end":37,"id":7},{"text":"that","start":38,"end":42,"id":8},{"text":"the","start":43,"end":46,"id":9},{"text":"bound","start":47,"end":52,"id":10},{"text":"is","start":53,"end":55,"id":11},{"text":"tight","start":56,"end":61,"id":12},{"text":",","start":61,"end":62,"id":13},{"text":"and","start":63,"end":66,"id":14},{"text":"derive","start":67,"end":73,"id":15},{"text":"consequences","start":74,"end":86,"id":16},{"text":"regarding","start":87,"end":96,"id":17},{"text":"excess","start":97,"end":103,"id":18},{"text":"loss","start":104,"end":108,"id":19},{"text":",","start":108,"end":109,"id":20},{"text":"namely","start":110,"end":116,"id":21},{"text":"fast","start":117,"end":121,"id":22},{"text":"convergence","start":122,"end":133,"id":23},{"text":"rates","start":134,"end":139,"id":24},{"text":"of","start":140,"end":142,"id":25},{"text":"the","start":143,"end":146,"id":26},{"text":"order","start":147,"end":152,"id":27},{"text":"$","start":153,"end":154,"id":28},{"text":"O(n^{-\\frac{\\alpha}{1+\\alpha}})$","start":154,"end":186,"id":29},{"text":",","start":186,"end":187,"id":30},{"text":"where","start":188,"end":193,"id":31},{"text":"$","start":194,"end":195,"id":32},{"text":"\\alpha$","start":195,"end":202,"id":33},{"text":"is","start":203,"end":205,"id":34},{"text":"the","start":206,"end":209,"id":35},{"text":"minimum","start":210,"end":217,"id":36},{"text":"eigenvalue","start":218,"end":228,"id":37},{"text":"decay","start":229,"end":234,"id":38},{"text":"rate","start":235,"end":239,"id":39},{"text":"of","start":240,"end":242,"id":40},{"text":"the","start":243,"end":246,"id":41},{"text":"individual","start":247,"end":257,"id":42},{"text":"kernels","start":258,"end":265,"id":43},{"text":".","start":265,"end":266,"id":44}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"A group may correspond to one view of the same set of objects, one of many data sets tied by co-occurrence, or a set of alternative variables collected from statistics tables to measure one property of interest.","_input_hash":-1867347353,"_task_hash":-1480565661,"tokens":[{"text":"A","start":0,"end":1,"id":0},{"text":"group","start":2,"end":7,"id":1},{"text":"may","start":8,"end":11,"id":2},{"text":"correspond","start":12,"end":22,"id":3},{"text":"to","start":23,"end":25,"id":4},{"text":"one","start":26,"end":29,"id":5},{"text":"view","start":30,"end":34,"id":6},{"text":"of","start":35,"end":37,"id":7},{"text":"the","start":38,"end":41,"id":8},{"text":"same","start":42,"end":46,"id":9},{"text":"set","start":47,"end":50,"id":10},{"text":"of","start":51,"end":53,"id":11},{"text":"objects","start":54,"end":61,"id":12},{"text":",","start":61,"end":62,"id":13},{"text":"one","start":63,"end":66,"id":14},{"text":"of","start":67,"end":69,"id":15},{"text":"many","start":70,"end":74,"id":16},{"text":"data","start":75,"end":79,"id":17},{"text":"sets","start":80,"end":84,"id":18},{"text":"tied","start":85,"end":89,"id":19},{"text":"by","start":90,"end":92,"id":20},{"text":"co","start":93,"end":95,"id":21},{"text":"-","start":95,"end":96,"id":22},{"text":"occurrence","start":96,"end":106,"id":23},{"text":",","start":106,"end":107,"id":24},{"text":"or","start":108,"end":110,"id":25},{"text":"a","start":111,"end":112,"id":26},{"text":"set","start":113,"end":116,"id":27},{"text":"of","start":117,"end":119,"id":28},{"text":"alternative","start":120,"end":131,"id":29},{"text":"variables","start":132,"end":141,"id":30},{"text":"collected","start":142,"end":151,"id":31},{"text":"from","start":152,"end":156,"id":32},{"text":"statistics","start":157,"end":167,"id":33},{"text":"tables","start":168,"end":174,"id":34},{"text":"to","start":175,"end":177,"id":35},{"text":"measure","start":178,"end":185,"id":36},{"text":"one","start":186,"end":189,"id":37},{"text":"property","start":190,"end":198,"id":38},{"text":"of","start":199,"end":201,"id":39},{"text":"interest","start":202,"end":210,"id":40},{"text":".","start":210,"end":211,"id":41}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Our results show that this strategy results in unexpected theoretical consequences for the procedure.","_input_hash":-1274876520,"_task_hash":-1417842616,"tokens":[{"text":"Our","start":0,"end":3,"id":0},{"text":"results","start":4,"end":11,"id":1},{"text":"show","start":12,"end":16,"id":2},{"text":"that","start":17,"end":21,"id":3},{"text":"this","start":22,"end":26,"id":4},{"text":"strategy","start":27,"end":35,"id":5},{"text":"results","start":36,"end":43,"id":6},{"text":"in","start":44,"end":46,"id":7},{"text":"unexpected","start":47,"end":57,"id":8},{"text":"theoretical","start":58,"end":69,"id":9},{"text":"consequences","start":70,"end":82,"id":10},{"text":"for","start":83,"end":86,"id":11},{"text":"the","start":87,"end":90,"id":12},{"text":"procedure","start":91,"end":100,"id":13},{"text":".","start":100,"end":101,"id":14}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"\\bigskip","_input_hash":608020576,"_task_hash":-128423785,"tokens":[{"text":"\\bigskip","start":0,"end":8,"id":0}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"from the small NORB object recognition dataset and yields state-of-the-art performance for a single layer, non-convolutional network.","_input_hash":740279243,"_task_hash":-239515801,"tokens":[{"text":"from","start":0,"end":4,"id":0},{"text":"the","start":5,"end":8,"id":1},{"text":"small","start":9,"end":14,"id":2},{"text":"NORB","start":15,"end":19,"id":3},{"text":"object","start":20,"end":26,"id":4},{"text":"recognition","start":27,"end":38,"id":5},{"text":"dataset","start":39,"end":46,"id":6},{"text":"and","start":47,"end":50,"id":7},{"text":"yields","start":51,"end":57,"id":8},{"text":"state","start":58,"end":63,"id":9},{"text":"-","start":63,"end":64,"id":10},{"text":"of","start":64,"end":66,"id":11},{"text":"-","start":66,"end":67,"id":12},{"text":"the","start":67,"end":70,"id":13},{"text":"-","start":70,"end":71,"id":14},{"text":"art","start":71,"end":74,"id":15},{"text":"performance","start":75,"end":86,"id":16},{"text":"for","start":87,"end":90,"id":17},{"text":"a","start":91,"end":92,"id":18},{"text":"single","start":93,"end":99,"id":19},{"text":"layer","start":100,"end":105,"id":20},{"text":",","start":105,"end":106,"id":21},{"text":"non","start":107,"end":110,"id":22},{"text":"-","start":110,"end":111,"id":23},{"text":"convolutional","start":111,"end":124,"id":24},{"text":"network","start":125,"end":132,"id":25},{"text":".","start":132,"end":133,"id":26}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":107,"end":132,"token_start":22,"token_end":25,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"It is well known that a linear regression can benefit from knowledge that the underlying regression vector is sparse.","_input_hash":1438029261,"_task_hash":358579888,"tokens":[{"text":"It","start":0,"end":2,"id":0},{"text":"is","start":3,"end":5,"id":1},{"text":"well","start":6,"end":10,"id":2},{"text":"known","start":11,"end":16,"id":3},{"text":"that","start":17,"end":21,"id":4},{"text":"a","start":22,"end":23,"id":5},{"text":"linear","start":24,"end":30,"id":6},{"text":"regression","start":31,"end":41,"id":7},{"text":"can","start":42,"end":45,"id":8},{"text":"benefit","start":46,"end":53,"id":9},{"text":"from","start":54,"end":58,"id":10},{"text":"knowledge","start":59,"end":68,"id":11},{"text":"that","start":69,"end":73,"id":12},{"text":"the","start":74,"end":77,"id":13},{"text":"underlying","start":78,"end":88,"id":14},{"text":"regression","start":89,"end":99,"id":15},{"text":"vector","start":100,"end":106,"id":16},{"text":"is","start":107,"end":109,"id":17},{"text":"sparse","start":110,"end":116,"id":18},{"text":".","start":116,"end":117,"id":19}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":24,"end":41,"token_start":6,"token_end":7,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"First we consider the closure of $k$-dimensional exponential families of distribution with discrete base measure and polyhedral convex support $\\mathrm{P}$. We show that the normal fan of $\\mathrm{P}$ is a geometric object that plays a fundamental role in deriving the statistical and geometric properties of the corresponding extended exponential families.","_input_hash":1504062085,"_task_hash":1397639665,"tokens":[{"text":"First","start":0,"end":5,"id":0},{"text":"we","start":6,"end":8,"id":1},{"text":"consider","start":9,"end":17,"id":2},{"text":"the","start":18,"end":21,"id":3},{"text":"closure","start":22,"end":29,"id":4},{"text":"of","start":30,"end":32,"id":5},{"text":"$","start":33,"end":34,"id":6},{"text":"k$-dimensional","start":34,"end":48,"id":7},{"text":"exponential","start":49,"end":60,"id":8},{"text":"families","start":61,"end":69,"id":9},{"text":"of","start":70,"end":72,"id":10},{"text":"distribution","start":73,"end":85,"id":11},{"text":"with","start":86,"end":90,"id":12},{"text":"discrete","start":91,"end":99,"id":13},{"text":"base","start":100,"end":104,"id":14},{"text":"measure","start":105,"end":112,"id":15},{"text":"and","start":113,"end":116,"id":16},{"text":"polyhedral","start":117,"end":127,"id":17},{"text":"convex","start":128,"end":134,"id":18},{"text":"support","start":135,"end":142,"id":19},{"text":"$","start":143,"end":144,"id":20},{"text":"\\mathrm{P}$.","start":144,"end":156,"id":21},{"text":"We","start":157,"end":159,"id":22},{"text":"show","start":160,"end":164,"id":23},{"text":"that","start":165,"end":169,"id":24},{"text":"the","start":170,"end":173,"id":25},{"text":"normal","start":174,"end":180,"id":26},{"text":"fan","start":181,"end":184,"id":27},{"text":"of","start":185,"end":187,"id":28},{"text":"$","start":188,"end":189,"id":29},{"text":"\\mathrm{P}$","start":189,"end":200,"id":30},{"text":"is","start":201,"end":203,"id":31},{"text":"a","start":204,"end":205,"id":32},{"text":"geometric","start":206,"end":215,"id":33},{"text":"object","start":216,"end":222,"id":34},{"text":"that","start":223,"end":227,"id":35},{"text":"plays","start":228,"end":233,"id":36},{"text":"a","start":234,"end":235,"id":37},{"text":"fundamental","start":236,"end":247,"id":38},{"text":"role","start":248,"end":252,"id":39},{"text":"in","start":253,"end":255,"id":40},{"text":"deriving","start":256,"end":264,"id":41},{"text":"the","start":265,"end":268,"id":42},{"text":"statistical","start":269,"end":280,"id":43},{"text":"and","start":281,"end":284,"id":44},{"text":"geometric","start":285,"end":294,"id":45},{"text":"properties","start":295,"end":305,"id":46},{"text":"of","start":306,"end":308,"id":47},{"text":"the","start":309,"end":312,"id":48},{"text":"corresponding","start":313,"end":326,"id":49},{"text":"extended","start":327,"end":335,"id":50},{"text":"exponential","start":336,"end":347,"id":51},{"text":"families","start":348,"end":356,"id":52},{"text":".","start":356,"end":357,"id":53}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"However, in many application domains, data are obtained under different conditions, that is, multiple datasets are obtained rather than a single dataset.","_input_hash":-278697848,"_task_hash":1082613974,"tokens":[{"text":"However","start":0,"end":7,"id":0},{"text":",","start":7,"end":8,"id":1},{"text":"in","start":9,"end":11,"id":2},{"text":"many","start":12,"end":16,"id":3},{"text":"application","start":17,"end":28,"id":4},{"text":"domains","start":29,"end":36,"id":5},{"text":",","start":36,"end":37,"id":6},{"text":"data","start":38,"end":42,"id":7},{"text":"are","start":43,"end":46,"id":8},{"text":"obtained","start":47,"end":55,"id":9},{"text":"under","start":56,"end":61,"id":10},{"text":"different","start":62,"end":71,"id":11},{"text":"conditions","start":72,"end":82,"id":12},{"text":",","start":82,"end":83,"id":13},{"text":"that","start":84,"end":88,"id":14},{"text":"is","start":89,"end":91,"id":15},{"text":",","start":91,"end":92,"id":16},{"text":"multiple","start":93,"end":101,"id":17},{"text":"datasets","start":102,"end":110,"id":18},{"text":"are","start":111,"end":114,"id":19},{"text":"obtained","start":115,"end":123,"id":20},{"text":"rather","start":124,"end":130,"id":21},{"text":"than","start":131,"end":135,"id":22},{"text":"a","start":136,"end":137,"id":23},{"text":"single","start":138,"end":144,"id":24},{"text":"dataset","start":145,"end":152,"id":25},{"text":".","start":152,"end":153,"id":26}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"In simulations, the new method estimates the models more accurately than estimating them separately.","_input_hash":-647759663,"_task_hash":1561468029,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"simulations","start":3,"end":14,"id":1},{"text":",","start":14,"end":15,"id":2},{"text":"the","start":16,"end":19,"id":3},{"text":"new","start":20,"end":23,"id":4},{"text":"method","start":24,"end":30,"id":5},{"text":"estimates","start":31,"end":40,"id":6},{"text":"the","start":41,"end":44,"id":7},{"text":"models","start":45,"end":51,"id":8},{"text":"more","start":52,"end":56,"id":9},{"text":"accurately","start":57,"end":67,"id":10},{"text":"than","start":68,"end":72,"id":11},{"text":"estimating","start":73,"end":83,"id":12},{"text":"them","start":84,"end":88,"id":13},{"text":"separately","start":89,"end":99,"id":14},{"text":".","start":99,"end":100,"id":15}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We characterize and study variable importance (VIMP) and pairwise variable associations in binary regression trees.","_input_hash":-1220427477,"_task_hash":-1404360791,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"characterize","start":3,"end":15,"id":1},{"text":"and","start":16,"end":19,"id":2},{"text":"study","start":20,"end":25,"id":3},{"text":"variable","start":26,"end":34,"id":4},{"text":"importance","start":35,"end":45,"id":5},{"text":"(","start":46,"end":47,"id":6},{"text":"VIMP","start":47,"end":51,"id":7},{"text":")","start":51,"end":52,"id":8},{"text":"and","start":53,"end":56,"id":9},{"text":"pairwise","start":57,"end":65,"id":10},{"text":"variable","start":66,"end":74,"id":11},{"text":"associations","start":75,"end":87,"id":12},{"text":"in","start":88,"end":90,"id":13},{"text":"binary","start":91,"end":97,"id":14},{"text":"regression","start":98,"end":108,"id":15},{"text":"trees","start":109,"end":114,"id":16},{"text":".","start":114,"end":115,"id":17}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":91,"end":114,"token_start":14,"token_end":16,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We attribute this mainly to the stochastic search strategy used, and to parsimony (sparsity and identifiability), which is an explicit part of the model.","_input_hash":-181767219,"_task_hash":-1659760119,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"attribute","start":3,"end":12,"id":1},{"text":"this","start":13,"end":17,"id":2},{"text":"mainly","start":18,"end":24,"id":3},{"text":"to","start":25,"end":27,"id":4},{"text":"the","start":28,"end":31,"id":5},{"text":"stochastic","start":32,"end":42,"id":6},{"text":"search","start":43,"end":49,"id":7},{"text":"strategy","start":50,"end":58,"id":8},{"text":"used","start":59,"end":63,"id":9},{"text":",","start":63,"end":64,"id":10},{"text":"and","start":65,"end":68,"id":11},{"text":"to","start":69,"end":71,"id":12},{"text":"parsimony","start":72,"end":81,"id":13},{"text":"(","start":82,"end":83,"id":14},{"text":"sparsity","start":83,"end":91,"id":15},{"text":"and","start":92,"end":95,"id":16},{"text":"identifiability","start":96,"end":111,"id":17},{"text":")","start":111,"end":112,"id":18},{"text":",","start":112,"end":113,"id":19},{"text":"which","start":114,"end":119,"id":20},{"text":"is","start":120,"end":122,"id":21},{"text":"an","start":123,"end":125,"id":22},{"text":"explicit","start":126,"end":134,"id":23},{"text":"part","start":135,"end":139,"id":24},{"text":"of","start":140,"end":142,"id":25},{"text":"the","start":143,"end":146,"id":26},{"text":"model","start":147,"end":152,"id":27},{"text":".","start":152,"end":153,"id":28}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We present the nested Chinese restaurant process (nCRP), a stochastic process which assigns probability distributions to infinitely-deep, infinitely-branching trees.","_input_hash":1329355179,"_task_hash":403223978,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"present","start":3,"end":10,"id":1},{"text":"the","start":11,"end":14,"id":2},{"text":"nested","start":15,"end":21,"id":3},{"text":"Chinese","start":22,"end":29,"id":4},{"text":"restaurant","start":30,"end":40,"id":5},{"text":"process","start":41,"end":48,"id":6},{"text":"(","start":49,"end":50,"id":7},{"text":"nCRP","start":50,"end":54,"id":8},{"text":")","start":54,"end":55,"id":9},{"text":",","start":55,"end":56,"id":10},{"text":"a","start":57,"end":58,"id":11},{"text":"stochastic","start":59,"end":69,"id":12},{"text":"process","start":70,"end":77,"id":13},{"text":"which","start":78,"end":83,"id":14},{"text":"assigns","start":84,"end":91,"id":15},{"text":"probability","start":92,"end":103,"id":16},{"text":"distributions","start":104,"end":117,"id":17},{"text":"to","start":118,"end":120,"id":18},{"text":"infinitely","start":121,"end":131,"id":19},{"text":"-","start":131,"end":132,"id":20},{"text":"deep","start":132,"end":136,"id":21},{"text":",","start":136,"end":137,"id":22},{"text":"infinitely","start":138,"end":148,"id":23},{"text":"-","start":148,"end":149,"id":24},{"text":"branching","start":149,"end":158,"id":25},{"text":"trees","start":159,"end":164,"id":26},{"text":".","start":164,"end":165,"id":27}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":15,"end":40,"token_start":3,"token_end":5,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We compare the risk of ridge regression to a simple variant of ordinary least squares, in which one simply projects the data onto a finite dimensional subspace (as specified by a Principal Component Analysis) and then performs an ordinary (un-regularized) least squares regression in this subspace.","_input_hash":1478296110,"_task_hash":-1192658596,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"compare","start":3,"end":10,"id":1},{"text":"the","start":11,"end":14,"id":2},{"text":"risk","start":15,"end":19,"id":3},{"text":"of","start":20,"end":22,"id":4},{"text":"ridge","start":23,"end":28,"id":5},{"text":"regression","start":29,"end":39,"id":6},{"text":"to","start":40,"end":42,"id":7},{"text":"a","start":43,"end":44,"id":8},{"text":"simple","start":45,"end":51,"id":9},{"text":"variant","start":52,"end":59,"id":10},{"text":"of","start":60,"end":62,"id":11},{"text":"ordinary","start":63,"end":71,"id":12},{"text":"least","start":72,"end":77,"id":13},{"text":"squares","start":78,"end":85,"id":14},{"text":",","start":85,"end":86,"id":15},{"text":"in","start":87,"end":89,"id":16},{"text":"which","start":90,"end":95,"id":17},{"text":"one","start":96,"end":99,"id":18},{"text":"simply","start":100,"end":106,"id":19},{"text":"projects","start":107,"end":115,"id":20},{"text":"the","start":116,"end":119,"id":21},{"text":"data","start":120,"end":124,"id":22},{"text":"onto","start":125,"end":129,"id":23},{"text":"a","start":130,"end":131,"id":24},{"text":"finite","start":132,"end":138,"id":25},{"text":"dimensional","start":139,"end":150,"id":26},{"text":"subspace","start":151,"end":159,"id":27},{"text":"(","start":160,"end":161,"id":28},{"text":"as","start":161,"end":163,"id":29},{"text":"specified","start":164,"end":173,"id":30},{"text":"by","start":174,"end":176,"id":31},{"text":"a","start":177,"end":178,"id":32},{"text":"Principal","start":179,"end":188,"id":33},{"text":"Component","start":189,"end":198,"id":34},{"text":"Analysis","start":199,"end":207,"id":35},{"text":")","start":207,"end":208,"id":36},{"text":"and","start":209,"end":212,"id":37},{"text":"then","start":213,"end":217,"id":38},{"text":"performs","start":218,"end":226,"id":39},{"text":"an","start":227,"end":229,"id":40},{"text":"ordinary","start":230,"end":238,"id":41},{"text":"(","start":239,"end":240,"id":42},{"text":"un","start":240,"end":242,"id":43},{"text":"-","start":242,"end":243,"id":44},{"text":"regularized","start":243,"end":254,"id":45},{"text":")","start":254,"end":255,"id":46},{"text":"least","start":256,"end":261,"id":47},{"text":"squares","start":262,"end":269,"id":48},{"text":"regression","start":270,"end":280,"id":49},{"text":"in","start":281,"end":283,"id":50},{"text":"this","start":284,"end":288,"id":51},{"text":"subspace","start":289,"end":297,"id":52},{"text":".","start":297,"end":298,"id":53}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":23,"end":39,"token_start":5,"token_end":6,"label":"ALGO","answer":"accept"},{"start":179,"end":207,"token_start":33,"token_end":35,"label":"ALGO","answer":"accept"},{"start":256,"end":280,"token_start":47,"token_end":49,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We prove an oracle inequality on the excess risk of the resulting estimator relative to the risk of the best forest.","_input_hash":-1656166332,"_task_hash":-129100090,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"prove","start":3,"end":8,"id":1},{"text":"an","start":9,"end":11,"id":2},{"text":"oracle","start":12,"end":18,"id":3},{"text":"inequality","start":19,"end":29,"id":4},{"text":"on","start":30,"end":32,"id":5},{"text":"the","start":33,"end":36,"id":6},{"text":"excess","start":37,"end":43,"id":7},{"text":"risk","start":44,"end":48,"id":8},{"text":"of","start":49,"end":51,"id":9},{"text":"the","start":52,"end":55,"id":10},{"text":"resulting","start":56,"end":65,"id":11},{"text":"estimator","start":66,"end":75,"id":12},{"text":"relative","start":76,"end":84,"id":13},{"text":"to","start":85,"end":87,"id":14},{"text":"the","start":88,"end":91,"id":15},{"text":"risk","start":92,"end":96,"id":16},{"text":"of","start":97,"end":99,"id":17},{"text":"the","start":100,"end":103,"id":18},{"text":"best","start":104,"end":108,"id":19},{"text":"forest","start":109,"end":115,"id":20},{"text":".","start":115,"end":116,"id":21}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":109,"end":115,"token_start":20,"token_end":20,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Multi-task learning;","_input_hash":1279509745,"_task_hash":1341006846,"tokens":[{"text":"Multi","start":0,"end":5,"id":0},{"text":"-","start":5,"end":6,"id":1},{"text":"task","start":6,"end":10,"id":2},{"text":"learning","start":11,"end":19,"id":3},{"text":";","start":19,"end":20,"id":4}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":0,"end":19,"token_start":0,"token_end":3,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We show experimentally the superiority of our algorithms over others, and demonstrate our approach in document clustering and phylolinguistics.","_input_hash":540279485,"_task_hash":606675593,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"show","start":3,"end":7,"id":1},{"text":"experimentally","start":8,"end":22,"id":2},{"text":"the","start":23,"end":26,"id":3},{"text":"superiority","start":27,"end":38,"id":4},{"text":"of","start":39,"end":41,"id":5},{"text":"our","start":42,"end":45,"id":6},{"text":"algorithms","start":46,"end":56,"id":7},{"text":"over","start":57,"end":61,"id":8},{"text":"others","start":62,"end":68,"id":9},{"text":",","start":68,"end":69,"id":10},{"text":"and","start":70,"end":73,"id":11},{"text":"demonstrate","start":74,"end":85,"id":12},{"text":"our","start":86,"end":89,"id":13},{"text":"approach","start":90,"end":98,"id":14},{"text":"in","start":99,"end":101,"id":15},{"text":"document","start":102,"end":110,"id":16},{"text":"clustering","start":111,"end":121,"id":17},{"text":"and","start":122,"end":125,"id":18},{"text":"phylolinguistics","start":126,"end":142,"id":19},{"text":".","start":142,"end":143,"id":20}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"screening for variables that have persistently large auto-correlations over two treatments (persistent-correlation screening).","_input_hash":428042678,"_task_hash":1357434149,"tokens":[{"text":"screening","start":0,"end":9,"id":0},{"text":"for","start":10,"end":13,"id":1},{"text":"variables","start":14,"end":23,"id":2},{"text":"that","start":24,"end":28,"id":3},{"text":"have","start":29,"end":33,"id":4},{"text":"persistently","start":34,"end":46,"id":5},{"text":"large","start":47,"end":52,"id":6},{"text":"auto","start":53,"end":57,"id":7},{"text":"-","start":57,"end":58,"id":8},{"text":"correlations","start":58,"end":70,"id":9},{"text":"over","start":71,"end":75,"id":10},{"text":"two","start":76,"end":79,"id":11},{"text":"treatments","start":80,"end":90,"id":12},{"text":"(","start":91,"end":92,"id":13},{"text":"persistent","start":92,"end":102,"id":14},{"text":"-","start":102,"end":103,"id":15},{"text":"correlation","start":103,"end":114,"id":16},{"text":"screening","start":115,"end":124,"id":17},{"text":")","start":124,"end":125,"id":18},{"text":".","start":125,"end":126,"id":19}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"This method is in practice difficult, because it requires a global optimization of a complicated function, the joint distribution by fixed input variables.","_input_hash":-2105218453,"_task_hash":-1582819205,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"method","start":5,"end":11,"id":1},{"text":"is","start":12,"end":14,"id":2},{"text":"in","start":15,"end":17,"id":3},{"text":"practice","start":18,"end":26,"id":4},{"text":"difficult","start":27,"end":36,"id":5},{"text":",","start":36,"end":37,"id":6},{"text":"because","start":38,"end":45,"id":7},{"text":"it","start":46,"end":48,"id":8},{"text":"requires","start":49,"end":57,"id":9},{"text":"a","start":58,"end":59,"id":10},{"text":"global","start":60,"end":66,"id":11},{"text":"optimization","start":67,"end":79,"id":12},{"text":"of","start":80,"end":82,"id":13},{"text":"a","start":83,"end":84,"id":14},{"text":"complicated","start":85,"end":96,"id":15},{"text":"function","start":97,"end":105,"id":16},{"text":",","start":105,"end":106,"id":17},{"text":"the","start":107,"end":110,"id":18},{"text":"joint","start":111,"end":116,"id":19},{"text":"distribution","start":117,"end":129,"id":20},{"text":"by","start":130,"end":132,"id":21},{"text":"fixed","start":133,"end":138,"id":22},{"text":"input","start":139,"end":144,"id":23},{"text":"variables","start":145,"end":154,"id":24},{"text":".","start":154,"end":155,"id":25}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Feature path realizations also reveal important non-linear correlations among features that prove useful in determining a subset of significant variables.","_input_hash":-1968131365,"_task_hash":283339050,"tokens":[{"text":"Feature","start":0,"end":7,"id":0},{"text":"path","start":8,"end":12,"id":1},{"text":"realizations","start":13,"end":25,"id":2},{"text":"also","start":26,"end":30,"id":3},{"text":"reveal","start":31,"end":37,"id":4},{"text":"important","start":38,"end":47,"id":5},{"text":"non","start":48,"end":51,"id":6},{"text":"-","start":51,"end":52,"id":7},{"text":"linear","start":52,"end":58,"id":8},{"text":"correlations","start":59,"end":71,"id":9},{"text":"among","start":72,"end":77,"id":10},{"text":"features","start":78,"end":86,"id":11},{"text":"that","start":87,"end":91,"id":12},{"text":"prove","start":92,"end":97,"id":13},{"text":"useful","start":98,"end":104,"id":14},{"text":"in","start":105,"end":107,"id":15},{"text":"determining","start":108,"end":119,"id":16},{"text":"a","start":120,"end":121,"id":17},{"text":"subset","start":122,"end":128,"id":18},{"text":"of","start":129,"end":131,"id":19},{"text":"significant","start":132,"end":143,"id":20},{"text":"variables","start":144,"end":153,"id":21},{"text":".","start":153,"end":154,"id":22}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Our proof captures the essence of existing proofs based on approachability (e.g., the proof by Foster, 1999 in case of binary outcomes) and highlights the intrinsic connection between approachability and calibration.","_input_hash":459132159,"_task_hash":-490034561,"tokens":[{"text":"Our","start":0,"end":3,"id":0},{"text":"proof","start":4,"end":9,"id":1},{"text":"captures","start":10,"end":18,"id":2},{"text":"the","start":19,"end":22,"id":3},{"text":"essence","start":23,"end":30,"id":4},{"text":"of","start":31,"end":33,"id":5},{"text":"existing","start":34,"end":42,"id":6},{"text":"proofs","start":43,"end":49,"id":7},{"text":"based","start":50,"end":55,"id":8},{"text":"on","start":56,"end":58,"id":9},{"text":"approachability","start":59,"end":74,"id":10},{"text":"(","start":75,"end":76,"id":11},{"text":"e.g.","start":76,"end":80,"id":12},{"text":",","start":80,"end":81,"id":13},{"text":"the","start":82,"end":85,"id":14},{"text":"proof","start":86,"end":91,"id":15},{"text":"by","start":92,"end":94,"id":16},{"text":"Foster","start":95,"end":101,"id":17},{"text":",","start":101,"end":102,"id":18},{"text":"1999","start":103,"end":107,"id":19},{"text":"in","start":108,"end":110,"id":20},{"text":"case","start":111,"end":115,"id":21},{"text":"of","start":116,"end":118,"id":22},{"text":"binary","start":119,"end":125,"id":23},{"text":"outcomes","start":126,"end":134,"id":24},{"text":")","start":134,"end":135,"id":25},{"text":"and","start":136,"end":139,"id":26},{"text":"highlights","start":140,"end":150,"id":27},{"text":"the","start":151,"end":154,"id":28},{"text":"intrinsic","start":155,"end":164,"id":29},{"text":"connection","start":165,"end":175,"id":30},{"text":"between","start":176,"end":183,"id":31},{"text":"approachability","start":184,"end":199,"id":32},{"text":"and","start":200,"end":203,"id":33},{"text":"calibration","start":204,"end":215,"id":34},{"text":".","start":215,"end":216,"id":35}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"The main contribution is that a convex formulation of this variance estimation problem, where the parametrization is based on the inverse of the variance, can be formulated as a certain l_1 mean estimation problem.","_input_hash":-334347717,"_task_hash":1945916975,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"main","start":4,"end":8,"id":1},{"text":"contribution","start":9,"end":21,"id":2},{"text":"is","start":22,"end":24,"id":3},{"text":"that","start":25,"end":29,"id":4},{"text":"a","start":30,"end":31,"id":5},{"text":"convex","start":32,"end":38,"id":6},{"text":"formulation","start":39,"end":50,"id":7},{"text":"of","start":51,"end":53,"id":8},{"text":"this","start":54,"end":58,"id":9},{"text":"variance","start":59,"end":67,"id":10},{"text":"estimation","start":68,"end":78,"id":11},{"text":"problem","start":79,"end":86,"id":12},{"text":",","start":86,"end":87,"id":13},{"text":"where","start":88,"end":93,"id":14},{"text":"the","start":94,"end":97,"id":15},{"text":"parametrization","start":98,"end":113,"id":16},{"text":"is","start":114,"end":116,"id":17},{"text":"based","start":117,"end":122,"id":18},{"text":"on","start":123,"end":125,"id":19},{"text":"the","start":126,"end":129,"id":20},{"text":"inverse","start":130,"end":137,"id":21},{"text":"of","start":138,"end":140,"id":22},{"text":"the","start":141,"end":144,"id":23},{"text":"variance","start":145,"end":153,"id":24},{"text":",","start":153,"end":154,"id":25},{"text":"can","start":155,"end":158,"id":26},{"text":"be","start":159,"end":161,"id":27},{"text":"formulated","start":162,"end":172,"id":28},{"text":"as","start":173,"end":175,"id":29},{"text":"a","start":176,"end":177,"id":30},{"text":"certain","start":178,"end":185,"id":31},{"text":"l_1","start":186,"end":189,"id":32},{"text":"mean","start":190,"end":194,"id":33},{"text":"estimation","start":195,"end":205,"id":34},{"text":"problem","start":206,"end":213,"id":35},{"text":".","start":213,"end":214,"id":36}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Existing algorithms have trouble in scaling to dimensions larger than a thousand.","_input_hash":2071909264,"_task_hash":1358545302,"tokens":[{"text":"Existing","start":0,"end":8,"id":0},{"text":"algorithms","start":9,"end":19,"id":1},{"text":"have","start":20,"end":24,"id":2},{"text":"trouble","start":25,"end":32,"id":3},{"text":"in","start":33,"end":35,"id":4},{"text":"scaling","start":36,"end":43,"id":5},{"text":"to","start":44,"end":46,"id":6},{"text":"dimensions","start":47,"end":57,"id":7},{"text":"larger","start":58,"end":64,"id":8},{"text":"than","start":65,"end":69,"id":9},{"text":"a","start":70,"end":71,"id":10},{"text":"thousand","start":72,"end":80,"id":11},{"text":".","start":80,"end":81,"id":12}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"In this case, one can sometimes easily calculate the minimizer explicitly, using Green's functions.","_input_hash":571991141,"_task_hash":1070864626,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"this","start":3,"end":7,"id":1},{"text":"case","start":8,"end":12,"id":2},{"text":",","start":12,"end":13,"id":3},{"text":"one","start":14,"end":17,"id":4},{"text":"can","start":18,"end":21,"id":5},{"text":"sometimes","start":22,"end":31,"id":6},{"text":"easily","start":32,"end":38,"id":7},{"text":"calculate","start":39,"end":48,"id":8},{"text":"the","start":49,"end":52,"id":9},{"text":"minimizer","start":53,"end":62,"id":10},{"text":"explicitly","start":63,"end":73,"id":11},{"text":",","start":73,"end":74,"id":12},{"text":"using","start":75,"end":80,"id":13},{"text":"Green","start":81,"end":86,"id":14},{"text":"'s","start":86,"end":88,"id":15},{"text":"functions","start":89,"end":98,"id":16},{"text":".","start":98,"end":99,"id":17}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"It also overcomes some of the major hurdles faced by existing methods in the literature as correlation screening is naturally scalable to high dimension.","_input_hash":-2053670338,"_task_hash":1820747899,"tokens":[{"text":"It","start":0,"end":2,"id":0},{"text":"also","start":3,"end":7,"id":1},{"text":"overcomes","start":8,"end":17,"id":2},{"text":"some","start":18,"end":22,"id":3},{"text":"of","start":23,"end":25,"id":4},{"text":"the","start":26,"end":29,"id":5},{"text":"major","start":30,"end":35,"id":6},{"text":"hurdles","start":36,"end":43,"id":7},{"text":"faced","start":44,"end":49,"id":8},{"text":"by","start":50,"end":52,"id":9},{"text":"existing","start":53,"end":61,"id":10},{"text":"methods","start":62,"end":69,"id":11},{"text":"in","start":70,"end":72,"id":12},{"text":"the","start":73,"end":76,"id":13},{"text":"literature","start":77,"end":87,"id":14},{"text":"as","start":88,"end":90,"id":15},{"text":"correlation","start":91,"end":102,"id":16},{"text":"screening","start":103,"end":112,"id":17},{"text":"is","start":113,"end":115,"id":18},{"text":"naturally","start":116,"end":125,"id":19},{"text":"scalable","start":126,"end":134,"id":20},{"text":"to","start":135,"end":137,"id":21},{"text":"high","start":138,"end":142,"id":22},{"text":"dimension","start":143,"end":152,"id":23},{"text":".","start":152,"end":153,"id":24}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Tree ensembles, on the other hand, lack usually a formal way of variable selection and are difficult to visualize.","_input_hash":-2024684727,"_task_hash":410674121,"tokens":[{"text":"Tree","start":0,"end":4,"id":0},{"text":"ensembles","start":5,"end":14,"id":1},{"text":",","start":14,"end":15,"id":2},{"text":"on","start":16,"end":18,"id":3},{"text":"the","start":19,"end":22,"id":4},{"text":"other","start":23,"end":28,"id":5},{"text":"hand","start":29,"end":33,"id":6},{"text":",","start":33,"end":34,"id":7},{"text":"lack","start":35,"end":39,"id":8},{"text":"usually","start":40,"end":47,"id":9},{"text":"a","start":48,"end":49,"id":10},{"text":"formal","start":50,"end":56,"id":11},{"text":"way","start":57,"end":60,"id":12},{"text":"of","start":61,"end":63,"id":13},{"text":"variable","start":64,"end":72,"id":14},{"text":"selection","start":73,"end":82,"id":15},{"text":"and","start":83,"end":86,"id":16},{"text":"are","start":87,"end":90,"id":17},{"text":"difficult","start":91,"end":100,"id":18},{"text":"to","start":101,"end":103,"id":19},{"text":"visualize","start":104,"end":113,"id":20},{"text":".","start":113,"end":114,"id":21}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":0,"end":14,"token_start":0,"token_end":1,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We use the CIBP prior with the nonlinear Gaussian belief network so each unit can additionally vary its behavior between discrete and continuous representations.","_input_hash":-1633248066,"_task_hash":1283035382,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"use","start":3,"end":6,"id":1},{"text":"the","start":7,"end":10,"id":2},{"text":"CIBP","start":11,"end":15,"id":3},{"text":"prior","start":16,"end":21,"id":4},{"text":"with","start":22,"end":26,"id":5},{"text":"the","start":27,"end":30,"id":6},{"text":"nonlinear","start":31,"end":40,"id":7},{"text":"Gaussian","start":41,"end":49,"id":8},{"text":"belief","start":50,"end":56,"id":9},{"text":"network","start":57,"end":64,"id":10},{"text":"so","start":65,"end":67,"id":11},{"text":"each","start":68,"end":72,"id":12},{"text":"unit","start":73,"end":77,"id":13},{"text":"can","start":78,"end":81,"id":14},{"text":"additionally","start":82,"end":94,"id":15},{"text":"vary","start":95,"end":99,"id":16},{"text":"its","start":100,"end":103,"id":17},{"text":"behavior","start":104,"end":112,"id":18},{"text":"between","start":113,"end":120,"id":19},{"text":"discrete","start":121,"end":129,"id":20},{"text":"and","start":130,"end":133,"id":21},{"text":"continuous","start":134,"end":144,"id":22},{"text":"representations","start":145,"end":160,"id":23},{"text":".","start":160,"end":161,"id":24}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":31,"end":64,"token_start":7,"token_end":10,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"first, we consider fixed hierarchical dictionaries of wavelets to denoise natural images.","_input_hash":2020002736,"_task_hash":1235321103,"tokens":[{"text":"first","start":0,"end":5,"id":0},{"text":",","start":5,"end":6,"id":1},{"text":"we","start":7,"end":9,"id":2},{"text":"consider","start":10,"end":18,"id":3},{"text":"fixed","start":19,"end":24,"id":4},{"text":"hierarchical","start":25,"end":37,"id":5},{"text":"dictionaries","start":38,"end":50,"id":6},{"text":"of","start":51,"end":53,"id":7},{"text":"wavelets","start":54,"end":62,"id":8},{"text":"to","start":63,"end":65,"id":9},{"text":"denoise","start":66,"end":73,"id":10},{"text":"natural","start":74,"end":81,"id":11},{"text":"images","start":82,"end":88,"id":12},{"text":".","start":88,"end":89,"id":13}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"This paper addresses the problem of segmenting a time-series with respect to changes in the mean value or in the variance.","_input_hash":-1665729211,"_task_hash":762516953,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"paper","start":5,"end":10,"id":1},{"text":"addresses","start":11,"end":20,"id":2},{"text":"the","start":21,"end":24,"id":3},{"text":"problem","start":25,"end":32,"id":4},{"text":"of","start":33,"end":35,"id":5},{"text":"segmenting","start":36,"end":46,"id":6},{"text":"a","start":47,"end":48,"id":7},{"text":"time","start":49,"end":53,"id":8},{"text":"-","start":53,"end":54,"id":9},{"text":"series","start":54,"end":60,"id":10},{"text":"with","start":61,"end":65,"id":11},{"text":"respect","start":66,"end":73,"id":12},{"text":"to","start":74,"end":76,"id":13},{"text":"changes","start":77,"end":84,"id":14},{"text":"in","start":85,"end":87,"id":15},{"text":"the","start":88,"end":91,"id":16},{"text":"mean","start":92,"end":96,"id":17},{"text":"value","start":97,"end":102,"id":18},{"text":"or","start":103,"end":105,"id":19},{"text":"in","start":106,"end":108,"id":20},{"text":"the","start":109,"end":112,"id":21},{"text":"variance","start":113,"end":121,"id":22},{"text":".","start":121,"end":122,"id":23}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"First, it is valid for an arbitrary finite number of outcomes.","_input_hash":252638691,"_task_hash":2115868365,"tokens":[{"text":"First","start":0,"end":5,"id":0},{"text":",","start":5,"end":6,"id":1},{"text":"it","start":7,"end":9,"id":2},{"text":"is","start":10,"end":12,"id":3},{"text":"valid","start":13,"end":18,"id":4},{"text":"for","start":19,"end":22,"id":5},{"text":"an","start":23,"end":25,"id":6},{"text":"arbitrary","start":26,"end":35,"id":7},{"text":"finite","start":36,"end":42,"id":8},{"text":"number","start":43,"end":49,"id":9},{"text":"of","start":50,"end":52,"id":10},{"text":"outcomes","start":53,"end":61,"id":11},{"text":".","start":61,"end":62,"id":12}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"Exogenous variables work as triggers that activate a causal chain in the model, and their identification leads to more efficient experimental designs and better understanding of the causal mechanism.","_input_hash":848702007,"_task_hash":328942004,"tokens":[{"text":"Exogenous","start":0,"end":9,"id":0},{"text":"variables","start":10,"end":19,"id":1},{"text":"work","start":20,"end":24,"id":2},{"text":"as","start":25,"end":27,"id":3},{"text":"triggers","start":28,"end":36,"id":4},{"text":"that","start":37,"end":41,"id":5},{"text":"activate","start":42,"end":50,"id":6},{"text":"a","start":51,"end":52,"id":7},{"text":"causal","start":53,"end":59,"id":8},{"text":"chain","start":60,"end":65,"id":9},{"text":"in","start":66,"end":68,"id":10},{"text":"the","start":69,"end":72,"id":11},{"text":"model","start":73,"end":78,"id":12},{"text":",","start":78,"end":79,"id":13},{"text":"and","start":80,"end":83,"id":14},{"text":"their","start":84,"end":89,"id":15},{"text":"identification","start":90,"end":104,"id":16},{"text":"leads","start":105,"end":110,"id":17},{"text":"to","start":111,"end":113,"id":18},{"text":"more","start":114,"end":118,"id":19},{"text":"efficient","start":119,"end":128,"id":20},{"text":"experimental","start":129,"end":141,"id":21},{"text":"designs","start":142,"end":149,"id":22},{"text":"and","start":150,"end":153,"id":23},{"text":"better","start":154,"end":160,"id":24},{"text":"understanding","start":161,"end":174,"id":25},{"text":"of","start":175,"end":177,"id":26},{"text":"the","start":178,"end":181,"id":27},{"text":"causal","start":182,"end":188,"id":28},{"text":"mechanism","start":189,"end":198,"id":29},{"text":".","start":198,"end":199,"id":30}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Our work builds upon the vector autoregressive (VAR) model for multivariate stochastic processes:","_input_hash":-1123274190,"_task_hash":1609125814,"tokens":[{"text":"Our","start":0,"end":3,"id":0},{"text":"work","start":4,"end":8,"id":1},{"text":"builds","start":9,"end":15,"id":2},{"text":"upon","start":16,"end":20,"id":3},{"text":"the","start":21,"end":24,"id":4},{"text":"vector","start":25,"end":31,"id":5},{"text":"autoregressive","start":32,"end":46,"id":6},{"text":"(","start":47,"end":48,"id":7},{"text":"VAR","start":48,"end":51,"id":8},{"text":")","start":51,"end":52,"id":9},{"text":"model","start":53,"end":58,"id":10},{"text":"for","start":59,"end":62,"id":11},{"text":"multivariate","start":63,"end":75,"id":12},{"text":"stochastic","start":76,"end":86,"id":13},{"text":"processes","start":87,"end":96,"id":14},{"text":":","start":96,"end":97,"id":15}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":25,"end":46,"token_start":5,"token_end":6,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We present and analyse three online algorithms for learning in discrete Hidden Markov Models (HMMs) and compare them with the Baldi-Chauvin Algorithm.","_input_hash":451561862,"_task_hash":-1315138872,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"present","start":3,"end":10,"id":1},{"text":"and","start":11,"end":14,"id":2},{"text":"analyse","start":15,"end":22,"id":3},{"text":"three","start":23,"end":28,"id":4},{"text":"online","start":29,"end":35,"id":5},{"text":"algorithms","start":36,"end":46,"id":6},{"text":"for","start":47,"end":50,"id":7},{"text":"learning","start":51,"end":59,"id":8},{"text":"in","start":60,"end":62,"id":9},{"text":"discrete","start":63,"end":71,"id":10},{"text":"Hidden","start":72,"end":78,"id":11},{"text":"Markov","start":79,"end":85,"id":12},{"text":"Models","start":86,"end":92,"id":13},{"text":"(","start":93,"end":94,"id":14},{"text":"HMMs","start":94,"end":98,"id":15},{"text":")","start":98,"end":99,"id":16},{"text":"and","start":100,"end":103,"id":17},{"text":"compare","start":104,"end":111,"id":18},{"text":"them","start":112,"end":116,"id":19},{"text":"with","start":117,"end":121,"id":20},{"text":"the","start":122,"end":125,"id":21},{"text":"Baldi","start":126,"end":131,"id":22},{"text":"-","start":131,"end":132,"id":23},{"text":"Chauvin","start":132,"end":139,"id":24},{"text":"Algorithm","start":140,"end":149,"id":25},{"text":".","start":149,"end":150,"id":26}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":72,"end":85,"token_start":11,"token_end":12,"label":"ALGO","answer":"accept"},{"start":126,"end":139,"token_start":22,"token_end":24,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We state conditions under which the method admits the sure screening property within a general class of single-index hazard rate models with ultra-high dimensional features.","_input_hash":-1440299395,"_task_hash":-397972462,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"state","start":3,"end":8,"id":1},{"text":"conditions","start":9,"end":19,"id":2},{"text":"under","start":20,"end":25,"id":3},{"text":"which","start":26,"end":31,"id":4},{"text":"the","start":32,"end":35,"id":5},{"text":"method","start":36,"end":42,"id":6},{"text":"admits","start":43,"end":49,"id":7},{"text":"the","start":50,"end":53,"id":8},{"text":"sure","start":54,"end":58,"id":9},{"text":"screening","start":59,"end":68,"id":10},{"text":"property","start":69,"end":77,"id":11},{"text":"within","start":78,"end":84,"id":12},{"text":"a","start":85,"end":86,"id":13},{"text":"general","start":87,"end":94,"id":14},{"text":"class","start":95,"end":100,"id":15},{"text":"of","start":101,"end":103,"id":16},{"text":"single","start":104,"end":110,"id":17},{"text":"-","start":110,"end":111,"id":18},{"text":"index","start":111,"end":116,"id":19},{"text":"hazard","start":117,"end":123,"id":20},{"text":"rate","start":124,"end":128,"id":21},{"text":"models","start":129,"end":135,"id":22},{"text":"with","start":136,"end":140,"id":23},{"text":"ultra","start":141,"end":146,"id":24},{"text":"-","start":146,"end":147,"id":25},{"text":"high","start":147,"end":151,"id":26},{"text":"dimensional","start":152,"end":163,"id":27},{"text":"features","start":164,"end":172,"id":28},{"text":".","start":172,"end":173,"id":29}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":104,"end":128,"token_start":17,"token_end":21,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Whereas training a LapSVM in the dual requires two steps, using the primal form allows us to collapse training to a single step.","_input_hash":251554342,"_task_hash":-1518713479,"tokens":[{"text":"Whereas","start":0,"end":7,"id":0},{"text":"training","start":8,"end":16,"id":1},{"text":"a","start":17,"end":18,"id":2},{"text":"LapSVM","start":19,"end":25,"id":3},{"text":"in","start":26,"end":28,"id":4},{"text":"the","start":29,"end":32,"id":5},{"text":"dual","start":33,"end":37,"id":6},{"text":"requires","start":38,"end":46,"id":7},{"text":"two","start":47,"end":50,"id":8},{"text":"steps","start":51,"end":56,"id":9},{"text":",","start":56,"end":57,"id":10},{"text":"using","start":58,"end":63,"id":11},{"text":"the","start":64,"end":67,"id":12},{"text":"primal","start":68,"end":74,"id":13},{"text":"form","start":75,"end":79,"id":14},{"text":"allows","start":80,"end":86,"id":15},{"text":"us","start":87,"end":89,"id":16},{"text":"to","start":90,"end":92,"id":17},{"text":"collapse","start":93,"end":101,"id":18},{"text":"training","start":102,"end":110,"id":19},{"text":"to","start":111,"end":113,"id":20},{"text":"a","start":114,"end":115,"id":21},{"text":"single","start":116,"end":122,"id":22},{"text":"step","start":123,"end":127,"id":23},{"text":".","start":127,"end":128,"id":24}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":19,"end":25,"token_start":3,"token_end":3,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We show that the reconstruction weights computed by LLE capture the high-dimensional structure of the neighborhoods, and not the low-dimensional manifold structure.","_input_hash":-1196170794,"_task_hash":17773513,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"show","start":3,"end":7,"id":1},{"text":"that","start":8,"end":12,"id":2},{"text":"the","start":13,"end":16,"id":3},{"text":"reconstruction","start":17,"end":31,"id":4},{"text":"weights","start":32,"end":39,"id":5},{"text":"computed","start":40,"end":48,"id":6},{"text":"by","start":49,"end":51,"id":7},{"text":"LLE","start":52,"end":55,"id":8},{"text":"capture","start":56,"end":63,"id":9},{"text":"the","start":64,"end":67,"id":10},{"text":"high","start":68,"end":72,"id":11},{"text":"-","start":72,"end":73,"id":12},{"text":"dimensional","start":73,"end":84,"id":13},{"text":"structure","start":85,"end":94,"id":14},{"text":"of","start":95,"end":97,"id":15},{"text":"the","start":98,"end":101,"id":16},{"text":"neighborhoods","start":102,"end":115,"id":17},{"text":",","start":115,"end":116,"id":18},{"text":"and","start":117,"end":120,"id":19},{"text":"not","start":121,"end":124,"id":20},{"text":"the","start":125,"end":128,"id":21},{"text":"low","start":129,"end":132,"id":22},{"text":"-","start":132,"end":133,"id":23},{"text":"dimensional","start":133,"end":144,"id":24},{"text":"manifold","start":145,"end":153,"id":25},{"text":"structure","start":154,"end":163,"id":26},{"text":".","start":163,"end":164,"id":27}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":52,"end":55,"token_start":8,"token_end":8,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"The active set update rules rely on the ability of the predictive distributions of a Gaussian process classifier to estimate the relative contribution of a datapoint when being either included or removed from the model.","_input_hash":-44225571,"_task_hash":1688186557,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"active","start":4,"end":10,"id":1},{"text":"set","start":11,"end":14,"id":2},{"text":"update","start":15,"end":21,"id":3},{"text":"rules","start":22,"end":27,"id":4},{"text":"rely","start":28,"end":32,"id":5},{"text":"on","start":33,"end":35,"id":6},{"text":"the","start":36,"end":39,"id":7},{"text":"ability","start":40,"end":47,"id":8},{"text":"of","start":48,"end":50,"id":9},{"text":"the","start":51,"end":54,"id":10},{"text":"predictive","start":55,"end":65,"id":11},{"text":"distributions","start":66,"end":79,"id":12},{"text":"of","start":80,"end":82,"id":13},{"text":"a","start":83,"end":84,"id":14},{"text":"Gaussian","start":85,"end":93,"id":15},{"text":"process","start":94,"end":101,"id":16},{"text":"classifier","start":102,"end":112,"id":17},{"text":"to","start":113,"end":115,"id":18},{"text":"estimate","start":116,"end":124,"id":19},{"text":"the","start":125,"end":128,"id":20},{"text":"relative","start":129,"end":137,"id":21},{"text":"contribution","start":138,"end":150,"id":22},{"text":"of","start":151,"end":153,"id":23},{"text":"a","start":154,"end":155,"id":24},{"text":"datapoint","start":156,"end":165,"id":25},{"text":"when","start":166,"end":170,"id":26},{"text":"being","start":171,"end":176,"id":27},{"text":"either","start":177,"end":183,"id":28},{"text":"included","start":184,"end":192,"id":29},{"text":"or","start":193,"end":195,"id":30},{"text":"removed","start":196,"end":203,"id":31},{"text":"from","start":204,"end":208,"id":32},{"text":"the","start":209,"end":212,"id":33},{"text":"model","start":213,"end":218,"id":34},{"text":".","start":218,"end":219,"id":35}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"In the astrophysics application, we also compare the method with the spectral clustering algorithms.","_input_hash":309712950,"_task_hash":-127151558,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"the","start":3,"end":6,"id":1},{"text":"astrophysics","start":7,"end":19,"id":2},{"text":"application","start":20,"end":31,"id":3},{"text":",","start":31,"end":32,"id":4},{"text":"we","start":33,"end":35,"id":5},{"text":"also","start":36,"end":40,"id":6},{"text":"compare","start":41,"end":48,"id":7},{"text":"the","start":49,"end":52,"id":8},{"text":"method","start":53,"end":59,"id":9},{"text":"with","start":60,"end":64,"id":10},{"text":"the","start":65,"end":68,"id":11},{"text":"spectral","start":69,"end":77,"id":12},{"text":"clustering","start":78,"end":88,"id":13},{"text":"algorithms","start":89,"end":99,"id":14},{"text":".","start":99,"end":100,"id":15}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":69,"end":88,"token_start":12,"token_end":13,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Independent component analysis (ICA) aims at decomposing an observed random vector into statistically independent variables.","_input_hash":-1584514654,"_task_hash":-366951719,"tokens":[{"text":"Independent","start":0,"end":11,"id":0},{"text":"component","start":12,"end":21,"id":1},{"text":"analysis","start":22,"end":30,"id":2},{"text":"(","start":31,"end":32,"id":3},{"text":"ICA","start":32,"end":35,"id":4},{"text":")","start":35,"end":36,"id":5},{"text":"aims","start":37,"end":41,"id":6},{"text":"at","start":42,"end":44,"id":7},{"text":"decomposing","start":45,"end":56,"id":8},{"text":"an","start":57,"end":59,"id":9},{"text":"observed","start":60,"end":68,"id":10},{"text":"random","start":69,"end":75,"id":11},{"text":"vector","start":76,"end":82,"id":12},{"text":"into","start":83,"end":87,"id":13},{"text":"statistically","start":88,"end":101,"id":14},{"text":"independent","start":102,"end":113,"id":15},{"text":"variables","start":114,"end":123,"id":16},{"text":".","start":123,"end":124,"id":17}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":0,"end":30,"token_start":0,"token_end":2,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Both sets of conditions are comparable to results for the homoscedastic model, showing that when a measure of the signal to noise ratio is large, the Lasso performs well on both Poisson-like data and homoscedastic data.","_input_hash":-947439,"_task_hash":-1454391631,"tokens":[{"text":"Both","start":0,"end":4,"id":0},{"text":"sets","start":5,"end":9,"id":1},{"text":"of","start":10,"end":12,"id":2},{"text":"conditions","start":13,"end":23,"id":3},{"text":"are","start":24,"end":27,"id":4},{"text":"comparable","start":28,"end":38,"id":5},{"text":"to","start":39,"end":41,"id":6},{"text":"results","start":42,"end":49,"id":7},{"text":"for","start":50,"end":53,"id":8},{"text":"the","start":54,"end":57,"id":9},{"text":"homoscedastic","start":58,"end":71,"id":10},{"text":"model","start":72,"end":77,"id":11},{"text":",","start":77,"end":78,"id":12},{"text":"showing","start":79,"end":86,"id":13},{"text":"that","start":87,"end":91,"id":14},{"text":"when","start":92,"end":96,"id":15},{"text":"a","start":97,"end":98,"id":16},{"text":"measure","start":99,"end":106,"id":17},{"text":"of","start":107,"end":109,"id":18},{"text":"the","start":110,"end":113,"id":19},{"text":"signal","start":114,"end":120,"id":20},{"text":"to","start":121,"end":123,"id":21},{"text":"noise","start":124,"end":129,"id":22},{"text":"ratio","start":130,"end":135,"id":23},{"text":"is","start":136,"end":138,"id":24},{"text":"large","start":139,"end":144,"id":25},{"text":",","start":144,"end":145,"id":26},{"text":"the","start":146,"end":149,"id":27},{"text":"Lasso","start":150,"end":155,"id":28},{"text":"performs","start":156,"end":164,"id":29},{"text":"well","start":165,"end":169,"id":30},{"text":"on","start":170,"end":172,"id":31},{"text":"both","start":173,"end":177,"id":32},{"text":"Poisson","start":178,"end":185,"id":33},{"text":"-","start":185,"end":186,"id":34},{"text":"like","start":186,"end":190,"id":35},{"text":"data","start":191,"end":195,"id":36},{"text":"and","start":196,"end":199,"id":37},{"text":"homoscedastic","start":200,"end":213,"id":38},{"text":"data","start":214,"end":218,"id":39},{"text":".","start":218,"end":219,"id":40}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":58,"end":71,"token_start":10,"token_end":10,"label":"ALGO","answer":"accept"},{"start":150,"end":155,"token_start":28,"token_end":28,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"It consists of a fully Bayesian hierarchy for sparse models using slab and spike priors (two-component delta-function and continuous mixtures), non-Gaussian latent factors and a stochastic search over the ordering of the variables.","_input_hash":603829078,"_task_hash":952221448,"tokens":[{"text":"It","start":0,"end":2,"id":0},{"text":"consists","start":3,"end":11,"id":1},{"text":"of","start":12,"end":14,"id":2},{"text":"a","start":15,"end":16,"id":3},{"text":"fully","start":17,"end":22,"id":4},{"text":"Bayesian","start":23,"end":31,"id":5},{"text":"hierarchy","start":32,"end":41,"id":6},{"text":"for","start":42,"end":45,"id":7},{"text":"sparse","start":46,"end":52,"id":8},{"text":"models","start":53,"end":59,"id":9},{"text":"using","start":60,"end":65,"id":10},{"text":"slab","start":66,"end":70,"id":11},{"text":"and","start":71,"end":74,"id":12},{"text":"spike","start":75,"end":80,"id":13},{"text":"priors","start":81,"end":87,"id":14},{"text":"(","start":88,"end":89,"id":15},{"text":"two","start":89,"end":92,"id":16},{"text":"-","start":92,"end":93,"id":17},{"text":"component","start":93,"end":102,"id":18},{"text":"delta","start":103,"end":108,"id":19},{"text":"-","start":108,"end":109,"id":20},{"text":"function","start":109,"end":117,"id":21},{"text":"and","start":118,"end":121,"id":22},{"text":"continuous","start":122,"end":132,"id":23},{"text":"mixtures","start":133,"end":141,"id":24},{"text":")","start":141,"end":142,"id":25},{"text":",","start":142,"end":143,"id":26},{"text":"non","start":144,"end":147,"id":27},{"text":"-","start":147,"end":148,"id":28},{"text":"Gaussian","start":148,"end":156,"id":29},{"text":"latent","start":157,"end":163,"id":30},{"text":"factors","start":164,"end":171,"id":31},{"text":"and","start":172,"end":175,"id":32},{"text":"a","start":176,"end":177,"id":33},{"text":"stochastic","start":178,"end":188,"id":34},{"text":"search","start":189,"end":195,"id":35},{"text":"over","start":196,"end":200,"id":36},{"text":"the","start":201,"end":204,"id":37},{"text":"ordering","start":205,"end":213,"id":38},{"text":"of","start":214,"end":216,"id":39},{"text":"the","start":217,"end":220,"id":40},{"text":"variables","start":221,"end":230,"id":41},{"text":".","start":230,"end":231,"id":42}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"A computer program is available that implements the proposed shrinkage estimator.","_input_hash":-701394453,"_task_hash":-179084809,"tokens":[{"text":"A","start":0,"end":1,"id":0},{"text":"computer","start":2,"end":10,"id":1},{"text":"program","start":11,"end":18,"id":2},{"text":"is","start":19,"end":21,"id":3},{"text":"available","start":22,"end":31,"id":4},{"text":"that","start":32,"end":36,"id":5},{"text":"implements","start":37,"end":47,"id":6},{"text":"the","start":48,"end":51,"id":7},{"text":"proposed","start":52,"end":60,"id":8},{"text":"shrinkage","start":61,"end":70,"id":9},{"text":"estimator","start":71,"end":80,"id":10},{"text":".","start":80,"end":81,"id":11}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"Unbalanced data arises in many learning tasks such as clustering of multi-class data, hierarchical divisive clustering and semisupervised learning.","_input_hash":853218599,"_task_hash":-114363679,"tokens":[{"text":"Unbalanced","start":0,"end":10,"id":0},{"text":"data","start":11,"end":15,"id":1},{"text":"arises","start":16,"end":22,"id":2},{"text":"in","start":23,"end":25,"id":3},{"text":"many","start":26,"end":30,"id":4},{"text":"learning","start":31,"end":39,"id":5},{"text":"tasks","start":40,"end":45,"id":6},{"text":"such","start":46,"end":50,"id":7},{"text":"as","start":51,"end":53,"id":8},{"text":"clustering","start":54,"end":64,"id":9},{"text":"of","start":65,"end":67,"id":10},{"text":"multi","start":68,"end":73,"id":11},{"text":"-","start":73,"end":74,"id":12},{"text":"class","start":74,"end":79,"id":13},{"text":"data","start":80,"end":84,"id":14},{"text":",","start":84,"end":85,"id":15},{"text":"hierarchical","start":86,"end":98,"id":16},{"text":"divisive","start":99,"end":107,"id":17},{"text":"clustering","start":108,"end":118,"id":18},{"text":"and","start":119,"end":122,"id":19},{"text":"semisupervised","start":123,"end":137,"id":20},{"text":"learning","start":138,"end":146,"id":21},{"text":".","start":146,"end":147,"id":22}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":86,"end":118,"token_start":16,"token_end":18,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We closely follow the formalism introduced by \\cite{DUD03} to cover a large variety of cross-validation procedures including leave-one-out cross-validation, $k$% -fold cross-validation, hold-out cross-validation (or split sample), and the leave-$\\upsilon$-out cross-validation.","_input_hash":-288228580,"_task_hash":1681403414,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"closely","start":3,"end":10,"id":1},{"text":"follow","start":11,"end":17,"id":2},{"text":"the","start":18,"end":21,"id":3},{"text":"formalism","start":22,"end":31,"id":4},{"text":"introduced","start":32,"end":42,"id":5},{"text":"by","start":43,"end":45,"id":6},{"text":"\\cite{DUD03","start":46,"end":57,"id":7},{"text":"}","start":57,"end":58,"id":8},{"text":"to","start":59,"end":61,"id":9},{"text":"cover","start":62,"end":67,"id":10},{"text":"a","start":68,"end":69,"id":11},{"text":"large","start":70,"end":75,"id":12},{"text":"variety","start":76,"end":83,"id":13},{"text":"of","start":84,"end":86,"id":14},{"text":"cross","start":87,"end":92,"id":15},{"text":"-","start":92,"end":93,"id":16},{"text":"validation","start":93,"end":103,"id":17},{"text":"procedures","start":104,"end":114,"id":18},{"text":"including","start":115,"end":124,"id":19},{"text":"leave","start":125,"end":130,"id":20},{"text":"-","start":130,"end":131,"id":21},{"text":"one","start":131,"end":134,"id":22},{"text":"-","start":134,"end":135,"id":23},{"text":"out","start":135,"end":138,"id":24},{"text":"cross","start":139,"end":144,"id":25},{"text":"-","start":144,"end":145,"id":26},{"text":"validation","start":145,"end":155,"id":27},{"text":",","start":155,"end":156,"id":28},{"text":"$","start":157,"end":158,"id":29},{"text":"k$%","start":158,"end":161,"id":30},{"text":"-fold","start":162,"end":167,"id":31},{"text":"cross","start":168,"end":173,"id":32},{"text":"-","start":173,"end":174,"id":33},{"text":"validation","start":174,"end":184,"id":34},{"text":",","start":184,"end":185,"id":35},{"text":"hold","start":186,"end":190,"id":36},{"text":"-","start":190,"end":191,"id":37},{"text":"out","start":191,"end":194,"id":38},{"text":"cross","start":195,"end":200,"id":39},{"text":"-","start":200,"end":201,"id":40},{"text":"validation","start":201,"end":211,"id":41},{"text":"(","start":212,"end":213,"id":42},{"text":"or","start":213,"end":215,"id":43},{"text":"split","start":216,"end":221,"id":44},{"text":"sample","start":222,"end":228,"id":45},{"text":")","start":228,"end":229,"id":46},{"text":",","start":229,"end":230,"id":47},{"text":"and","start":231,"end":234,"id":48},{"text":"the","start":235,"end":238,"id":49},{"text":"leave-$\\upsilon$-out","start":239,"end":259,"id":50},{"text":"cross","start":260,"end":265,"id":51},{"text":"-","start":265,"end":266,"id":52},{"text":"validation","start":266,"end":276,"id":53},{"text":".","start":276,"end":277,"id":54}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":125,"end":155,"token_start":20,"token_end":27,"label":"ALGO","answer":"accept"},{"start":157,"end":184,"token_start":29,"token_end":34,"label":"ALGO","answer":"accept"},{"start":186,"end":211,"token_start":36,"token_end":41,"label":"ALGO","answer":"accept"},{"start":239,"end":276,"token_start":50,"token_end":53,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We assess the models for future prediction and compare their performance to a VMM, which is the current state of the art in melody generation.","_input_hash":-485895485,"_task_hash":-1207091744,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"assess","start":3,"end":9,"id":1},{"text":"the","start":10,"end":13,"id":2},{"text":"models","start":14,"end":20,"id":3},{"text":"for","start":21,"end":24,"id":4},{"text":"future","start":25,"end":31,"id":5},{"text":"prediction","start":32,"end":42,"id":6},{"text":"and","start":43,"end":46,"id":7},{"text":"compare","start":47,"end":54,"id":8},{"text":"their","start":55,"end":60,"id":9},{"text":"performance","start":61,"end":72,"id":10},{"text":"to","start":73,"end":75,"id":11},{"text":"a","start":76,"end":77,"id":12},{"text":"VMM","start":78,"end":81,"id":13},{"text":",","start":81,"end":82,"id":14},{"text":"which","start":83,"end":88,"id":15},{"text":"is","start":89,"end":91,"id":16},{"text":"the","start":92,"end":95,"id":17},{"text":"current","start":96,"end":103,"id":18},{"text":"state","start":104,"end":109,"id":19},{"text":"of","start":110,"end":112,"id":20},{"text":"the","start":113,"end":116,"id":21},{"text":"art","start":117,"end":120,"id":22},{"text":"in","start":121,"end":123,"id":23},{"text":"melody","start":124,"end":130,"id":24},{"text":"generation","start":131,"end":141,"id":25},{"text":".","start":141,"end":142,"id":26}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"At the end, we calculate the topology of the hidden part of a dendrogram.","_input_hash":1928286706,"_task_hash":1441321189,"tokens":[{"text":"At","start":0,"end":2,"id":0},{"text":"the","start":3,"end":6,"id":1},{"text":"end","start":7,"end":10,"id":2},{"text":",","start":10,"end":11,"id":3},{"text":"we","start":12,"end":14,"id":4},{"text":"calculate","start":15,"end":24,"id":5},{"text":"the","start":25,"end":28,"id":6},{"text":"topology","start":29,"end":37,"id":7},{"text":"of","start":38,"end":40,"id":8},{"text":"the","start":41,"end":44,"id":9},{"text":"hidden","start":45,"end":51,"id":10},{"text":"part","start":52,"end":56,"id":11},{"text":"of","start":57,"end":59,"id":12},{"text":"a","start":60,"end":61,"id":13},{"text":"dendrogram","start":62,"end":72,"id":14},{"text":".","start":72,"end":73,"id":15}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"This example highlights the strengths of the PVM in yielding a low-error, highly interpretable model.","_input_hash":5530812,"_task_hash":-924366592,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"example","start":5,"end":12,"id":1},{"text":"highlights","start":13,"end":23,"id":2},{"text":"the","start":24,"end":27,"id":3},{"text":"strengths","start":28,"end":37,"id":4},{"text":"of","start":38,"end":40,"id":5},{"text":"the","start":41,"end":44,"id":6},{"text":"PVM","start":45,"end":48,"id":7},{"text":"in","start":49,"end":51,"id":8},{"text":"yielding","start":52,"end":60,"id":9},{"text":"a","start":61,"end":62,"id":10},{"text":"low","start":63,"end":66,"id":11},{"text":"-","start":66,"end":67,"id":12},{"text":"error","start":67,"end":72,"id":13},{"text":",","start":72,"end":73,"id":14},{"text":"highly","start":74,"end":80,"id":15},{"text":"interpretable","start":81,"end":94,"id":16},{"text":"model","start":95,"end":100,"id":17},{"text":".","start":100,"end":101,"id":18}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":45,"end":48,"token_start":7,"token_end":7,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We derive the optimal estimator of f-divergence in the sense of the asymptotic variance, and then investigate the relation between the proposed test procedure and the existing score test based on empirical likelihood estimator.","_input_hash":1058532734,"_task_hash":-1757208693,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"derive","start":3,"end":9,"id":1},{"text":"the","start":10,"end":13,"id":2},{"text":"optimal","start":14,"end":21,"id":3},{"text":"estimator","start":22,"end":31,"id":4},{"text":"of","start":32,"end":34,"id":5},{"text":"f","start":35,"end":36,"id":6},{"text":"-","start":36,"end":37,"id":7},{"text":"divergence","start":37,"end":47,"id":8},{"text":"in","start":48,"end":50,"id":9},{"text":"the","start":51,"end":54,"id":10},{"text":"sense","start":55,"end":60,"id":11},{"text":"of","start":61,"end":63,"id":12},{"text":"the","start":64,"end":67,"id":13},{"text":"asymptotic","start":68,"end":78,"id":14},{"text":"variance","start":79,"end":87,"id":15},{"text":",","start":87,"end":88,"id":16},{"text":"and","start":89,"end":92,"id":17},{"text":"then","start":93,"end":97,"id":18},{"text":"investigate","start":98,"end":109,"id":19},{"text":"the","start":110,"end":113,"id":20},{"text":"relation","start":114,"end":122,"id":21},{"text":"between","start":123,"end":130,"id":22},{"text":"the","start":131,"end":134,"id":23},{"text":"proposed","start":135,"end":143,"id":24},{"text":"test","start":144,"end":148,"id":25},{"text":"procedure","start":149,"end":158,"id":26},{"text":"and","start":159,"end":162,"id":27},{"text":"the","start":163,"end":166,"id":28},{"text":"existing","start":167,"end":175,"id":29},{"text":"score","start":176,"end":181,"id":30},{"text":"test","start":182,"end":186,"id":31},{"text":"based","start":187,"end":192,"id":32},{"text":"on","start":193,"end":195,"id":33},{"text":"empirical","start":196,"end":205,"id":34},{"text":"likelihood","start":206,"end":216,"id":35},{"text":"estimator","start":217,"end":226,"id":36},{"text":".","start":226,"end":227,"id":37}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"In this paper, we propose a novel causal inference algorithm called least-squares independence regression (LSIR).","_input_hash":-319865186,"_task_hash":2063598205,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"this","start":3,"end":7,"id":1},{"text":"paper","start":8,"end":13,"id":2},{"text":",","start":13,"end":14,"id":3},{"text":"we","start":15,"end":17,"id":4},{"text":"propose","start":18,"end":25,"id":5},{"text":"a","start":26,"end":27,"id":6},{"text":"novel","start":28,"end":33,"id":7},{"text":"causal","start":34,"end":40,"id":8},{"text":"inference","start":41,"end":50,"id":9},{"text":"algorithm","start":51,"end":60,"id":10},{"text":"called","start":61,"end":67,"id":11},{"text":"least","start":68,"end":73,"id":12},{"text":"-","start":73,"end":74,"id":13},{"text":"squares","start":74,"end":81,"id":14},{"text":"independence","start":82,"end":94,"id":15},{"text":"regression","start":95,"end":105,"id":16},{"text":"(","start":106,"end":107,"id":17},{"text":"LSIR","start":107,"end":111,"id":18},{"text":")","start":111,"end":112,"id":19},{"text":".","start":112,"end":113,"id":20}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":68,"end":105,"token_start":12,"token_end":16,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Although it is used very commonly, this procedure will make the response appear more predictable than it actually is.","_input_hash":2035627713,"_task_hash":-838434883,"tokens":[{"text":"Although","start":0,"end":8,"id":0},{"text":"it","start":9,"end":11,"id":1},{"text":"is","start":12,"end":14,"id":2},{"text":"used","start":15,"end":19,"id":3},{"text":"very","start":20,"end":24,"id":4},{"text":"commonly","start":25,"end":33,"id":5},{"text":",","start":33,"end":34,"id":6},{"text":"this","start":35,"end":39,"id":7},{"text":"procedure","start":40,"end":49,"id":8},{"text":"will","start":50,"end":54,"id":9},{"text":"make","start":55,"end":59,"id":10},{"text":"the","start":60,"end":63,"id":11},{"text":"response","start":64,"end":72,"id":12},{"text":"appear","start":73,"end":79,"id":13},{"text":"more","start":80,"end":84,"id":14},{"text":"predictable","start":85,"end":96,"id":15},{"text":"than","start":97,"end":101,"id":16},{"text":"it","start":102,"end":104,"id":17},{"text":"actually","start":105,"end":113,"id":18},{"text":"is","start":114,"end":116,"id":19},{"text":".","start":116,"end":117,"id":20}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"The methods are evaluated through simulation studies and through application to a real gene expression dataset.","_input_hash":569388529,"_task_hash":989671975,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"methods","start":4,"end":11,"id":1},{"text":"are","start":12,"end":15,"id":2},{"text":"evaluated","start":16,"end":25,"id":3},{"text":"through","start":26,"end":33,"id":4},{"text":"simulation","start":34,"end":44,"id":5},{"text":"studies","start":45,"end":52,"id":6},{"text":"and","start":53,"end":56,"id":7},{"text":"through","start":57,"end":64,"id":8},{"text":"application","start":65,"end":76,"id":9},{"text":"to","start":77,"end":79,"id":10},{"text":"a","start":80,"end":81,"id":11},{"text":"real","start":82,"end":86,"id":12},{"text":"gene","start":87,"end":91,"id":13},{"text":"expression","start":92,"end":102,"id":14},{"text":"dataset","start":103,"end":110,"id":15},{"text":".","start":110,"end":111,"id":16}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"First, relying on a recent result by Cerou and Guyader (2006), we prove the consistency of the k-NN classifier for functional data whose distribution belongs to a broad family of Gaussian processes with triangular covariance functions.","_input_hash":1795532205,"_task_hash":-1455676496,"tokens":[{"text":"First","start":0,"end":5,"id":0},{"text":",","start":5,"end":6,"id":1},{"text":"relying","start":7,"end":14,"id":2},{"text":"on","start":15,"end":17,"id":3},{"text":"a","start":18,"end":19,"id":4},{"text":"recent","start":20,"end":26,"id":5},{"text":"result","start":27,"end":33,"id":6},{"text":"by","start":34,"end":36,"id":7},{"text":"Cerou","start":37,"end":42,"id":8},{"text":"and","start":43,"end":46,"id":9},{"text":"Guyader","start":47,"end":54,"id":10},{"text":"(","start":55,"end":56,"id":11},{"text":"2006","start":56,"end":60,"id":12},{"text":")","start":60,"end":61,"id":13},{"text":",","start":61,"end":62,"id":14},{"text":"we","start":63,"end":65,"id":15},{"text":"prove","start":66,"end":71,"id":16},{"text":"the","start":72,"end":75,"id":17},{"text":"consistency","start":76,"end":87,"id":18},{"text":"of","start":88,"end":90,"id":19},{"text":"the","start":91,"end":94,"id":20},{"text":"k","start":95,"end":96,"id":21},{"text":"-","start":96,"end":97,"id":22},{"text":"NN","start":97,"end":99,"id":23},{"text":"classifier","start":100,"end":110,"id":24},{"text":"for","start":111,"end":114,"id":25},{"text":"functional","start":115,"end":125,"id":26},{"text":"data","start":126,"end":130,"id":27},{"text":"whose","start":131,"end":136,"id":28},{"text":"distribution","start":137,"end":149,"id":29},{"text":"belongs","start":150,"end":157,"id":30},{"text":"to","start":158,"end":160,"id":31},{"text":"a","start":161,"end":162,"id":32},{"text":"broad","start":163,"end":168,"id":33},{"text":"family","start":169,"end":175,"id":34},{"text":"of","start":176,"end":178,"id":35},{"text":"Gaussian","start":179,"end":187,"id":36},{"text":"processes","start":188,"end":197,"id":37},{"text":"with","start":198,"end":202,"id":38},{"text":"triangular","start":203,"end":213,"id":39},{"text":"covariance","start":214,"end":224,"id":40},{"text":"functions","start":225,"end":234,"id":41},{"text":".","start":234,"end":235,"id":42}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":95,"end":99,"token_start":21,"token_end":23,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"By constructing a proposal of seating arrangements (partitioning) and stochastically accepts the proposal by the Metropolis-Hastings algorithm, the RCD sampler makes accurate sampling for complex combination of draws while retaining efficiency of HCRP representation.","_input_hash":-1578838596,"_task_hash":355389760,"tokens":[{"text":"By","start":0,"end":2,"id":0},{"text":"constructing","start":3,"end":15,"id":1},{"text":"a","start":16,"end":17,"id":2},{"text":"proposal","start":18,"end":26,"id":3},{"text":"of","start":27,"end":29,"id":4},{"text":"seating","start":30,"end":37,"id":5},{"text":"arrangements","start":38,"end":50,"id":6},{"text":"(","start":51,"end":52,"id":7},{"text":"partitioning","start":52,"end":64,"id":8},{"text":")","start":64,"end":65,"id":9},{"text":"and","start":66,"end":69,"id":10},{"text":"stochastically","start":70,"end":84,"id":11},{"text":"accepts","start":85,"end":92,"id":12},{"text":"the","start":93,"end":96,"id":13},{"text":"proposal","start":97,"end":105,"id":14},{"text":"by","start":106,"end":108,"id":15},{"text":"the","start":109,"end":112,"id":16},{"text":"Metropolis","start":113,"end":123,"id":17},{"text":"-","start":123,"end":124,"id":18},{"text":"Hastings","start":124,"end":132,"id":19},{"text":"algorithm","start":133,"end":142,"id":20},{"text":",","start":142,"end":143,"id":21},{"text":"the","start":144,"end":147,"id":22},{"text":"RCD","start":148,"end":151,"id":23},{"text":"sampler","start":152,"end":159,"id":24},{"text":"makes","start":160,"end":165,"id":25},{"text":"accurate","start":166,"end":174,"id":26},{"text":"sampling","start":175,"end":183,"id":27},{"text":"for","start":184,"end":187,"id":28},{"text":"complex","start":188,"end":195,"id":29},{"text":"combination","start":196,"end":207,"id":30},{"text":"of","start":208,"end":210,"id":31},{"text":"draws","start":211,"end":216,"id":32},{"text":"while","start":217,"end":222,"id":33},{"text":"retaining","start":223,"end":232,"id":34},{"text":"efficiency","start":233,"end":243,"id":35},{"text":"of","start":244,"end":246,"id":36},{"text":"HCRP","start":247,"end":251,"id":37},{"text":"representation","start":252,"end":266,"id":38},{"text":".","start":266,"end":267,"id":39}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":113,"end":132,"token_start":17,"token_end":19,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Our algorithm, like its predecessors, maximizes an $\\ell_1$-norm penalized log-likelihood and has the same per iteration arithmetic complexity as the best methods in its class.","_input_hash":1807416866,"_task_hash":-665092835,"tokens":[{"text":"Our","start":0,"end":3,"id":0},{"text":"algorithm","start":4,"end":13,"id":1},{"text":",","start":13,"end":14,"id":2},{"text":"like","start":15,"end":19,"id":3},{"text":"its","start":20,"end":23,"id":4},{"text":"predecessors","start":24,"end":36,"id":5},{"text":",","start":36,"end":37,"id":6},{"text":"maximizes","start":38,"end":47,"id":7},{"text":"an","start":48,"end":50,"id":8},{"text":"$","start":51,"end":52,"id":9},{"text":"\\ell_1$-norm","start":52,"end":64,"id":10},{"text":"penalized","start":65,"end":74,"id":11},{"text":"log","start":75,"end":78,"id":12},{"text":"-","start":78,"end":79,"id":13},{"text":"likelihood","start":79,"end":89,"id":14},{"text":"and","start":90,"end":93,"id":15},{"text":"has","start":94,"end":97,"id":16},{"text":"the","start":98,"end":101,"id":17},{"text":"same","start":102,"end":106,"id":18},{"text":"per","start":107,"end":110,"id":19},{"text":"iteration","start":111,"end":120,"id":20},{"text":"arithmetic","start":121,"end":131,"id":21},{"text":"complexity","start":132,"end":142,"id":22},{"text":"as","start":143,"end":145,"id":23},{"text":"the","start":146,"end":149,"id":24},{"text":"best","start":150,"end":154,"id":25},{"text":"methods","start":155,"end":162,"id":26},{"text":"in","start":163,"end":165,"id":27},{"text":"its","start":166,"end":169,"id":28},{"text":"class","start":170,"end":175,"id":29},{"text":".","start":175,"end":176,"id":30}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"The posterior is likewise an RKHS mean of a weighted sample.","_input_hash":1556689185,"_task_hash":3684682,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"posterior","start":4,"end":13,"id":1},{"text":"is","start":14,"end":16,"id":2},{"text":"likewise","start":17,"end":25,"id":3},{"text":"an","start":26,"end":28,"id":4},{"text":"RKHS","start":29,"end":33,"id":5},{"text":"mean","start":34,"end":38,"id":6},{"text":"of","start":39,"end":41,"id":7},{"text":"a","start":42,"end":43,"id":8},{"text":"weighted","start":44,"end":52,"id":9},{"text":"sample","start":53,"end":59,"id":10},{"text":".","start":59,"end":60,"id":11}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"An iterative variant is also described which combines screening with penalized regression in order to handle more complex feature covariance structures.","_input_hash":-1135706676,"_task_hash":1013373511,"tokens":[{"text":"An","start":0,"end":2,"id":0},{"text":"iterative","start":3,"end":12,"id":1},{"text":"variant","start":13,"end":20,"id":2},{"text":"is","start":21,"end":23,"id":3},{"text":"also","start":24,"end":28,"id":4},{"text":"described","start":29,"end":38,"id":5},{"text":"which","start":39,"end":44,"id":6},{"text":"combines","start":45,"end":53,"id":7},{"text":"screening","start":54,"end":63,"id":8},{"text":"with","start":64,"end":68,"id":9},{"text":"penalized","start":69,"end":78,"id":10},{"text":"regression","start":79,"end":89,"id":11},{"text":"in","start":90,"end":92,"id":12},{"text":"order","start":93,"end":98,"id":13},{"text":"to","start":99,"end":101,"id":14},{"text":"handle","start":102,"end":108,"id":15},{"text":"more","start":109,"end":113,"id":16},{"text":"complex","start":114,"end":121,"id":17},{"text":"feature","start":122,"end":129,"id":18},{"text":"covariance","start":130,"end":140,"id":19},{"text":"structures","start":141,"end":151,"id":20},{"text":".","start":151,"end":152,"id":21}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":69,"end":89,"token_start":10,"token_end":11,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We compare our method to Laplacian spectral clustering through analysis of simulated data and a graph derived from Wikipedia documents.","_input_hash":877900994,"_task_hash":82516543,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"compare","start":3,"end":10,"id":1},{"text":"our","start":11,"end":14,"id":2},{"text":"method","start":15,"end":21,"id":3},{"text":"to","start":22,"end":24,"id":4},{"text":"Laplacian","start":25,"end":34,"id":5},{"text":"spectral","start":35,"end":43,"id":6},{"text":"clustering","start":44,"end":54,"id":7},{"text":"through","start":55,"end":62,"id":8},{"text":"analysis","start":63,"end":71,"id":9},{"text":"of","start":72,"end":74,"id":10},{"text":"simulated","start":75,"end":84,"id":11},{"text":"data","start":85,"end":89,"id":12},{"text":"and","start":90,"end":93,"id":13},{"text":"a","start":94,"end":95,"id":14},{"text":"graph","start":96,"end":101,"id":15},{"text":"derived","start":102,"end":109,"id":16},{"text":"from","start":110,"end":114,"id":17},{"text":"Wikipedia","start":115,"end":124,"id":18},{"text":"documents","start":125,"end":134,"id":19},{"text":".","start":134,"end":135,"id":20}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":25,"end":54,"token_start":5,"token_end":7,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"From this representation, similarity between observations based on non-linear structure is computed and can be incorporated into existing feature transformations, dimensionality reduction techniques, and machine learning methods.","_input_hash":-932808717,"_task_hash":608056758,"tokens":[{"text":"From","start":0,"end":4,"id":0},{"text":"this","start":5,"end":9,"id":1},{"text":"representation","start":10,"end":24,"id":2},{"text":",","start":24,"end":25,"id":3},{"text":"similarity","start":26,"end":36,"id":4},{"text":"between","start":37,"end":44,"id":5},{"text":"observations","start":45,"end":57,"id":6},{"text":"based","start":58,"end":63,"id":7},{"text":"on","start":64,"end":66,"id":8},{"text":"non","start":67,"end":70,"id":9},{"text":"-","start":70,"end":71,"id":10},{"text":"linear","start":71,"end":77,"id":11},{"text":"structure","start":78,"end":87,"id":12},{"text":"is","start":88,"end":90,"id":13},{"text":"computed","start":91,"end":99,"id":14},{"text":"and","start":100,"end":103,"id":15},{"text":"can","start":104,"end":107,"id":16},{"text":"be","start":108,"end":110,"id":17},{"text":"incorporated","start":111,"end":123,"id":18},{"text":"into","start":124,"end":128,"id":19},{"text":"existing","start":129,"end":137,"id":20},{"text":"feature","start":138,"end":145,"id":21},{"text":"transformations","start":146,"end":161,"id":22},{"text":",","start":161,"end":162,"id":23},{"text":"dimensionality","start":163,"end":177,"id":24},{"text":"reduction","start":178,"end":187,"id":25},{"text":"techniques","start":188,"end":198,"id":26},{"text":",","start":198,"end":199,"id":27},{"text":"and","start":200,"end":203,"id":28},{"text":"machine","start":204,"end":211,"id":29},{"text":"learning","start":212,"end":220,"id":30},{"text":"methods","start":221,"end":228,"id":31},{"text":".","start":228,"end":229,"id":32}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"Usually the Euclidean distance metric is assumed or a Mahalanobis distance metric is optimized to improve the NN performance.","_input_hash":184187675,"_task_hash":1331312961,"tokens":[{"text":"Usually","start":0,"end":7,"id":0},{"text":"the","start":8,"end":11,"id":1},{"text":"Euclidean","start":12,"end":21,"id":2},{"text":"distance","start":22,"end":30,"id":3},{"text":"metric","start":31,"end":37,"id":4},{"text":"is","start":38,"end":40,"id":5},{"text":"assumed","start":41,"end":48,"id":6},{"text":"or","start":49,"end":51,"id":7},{"text":"a","start":52,"end":53,"id":8},{"text":"Mahalanobis","start":54,"end":65,"id":9},{"text":"distance","start":66,"end":74,"id":10},{"text":"metric","start":75,"end":81,"id":11},{"text":"is","start":82,"end":84,"id":12},{"text":"optimized","start":85,"end":94,"id":13},{"text":"to","start":95,"end":97,"id":14},{"text":"improve","start":98,"end":105,"id":15},{"text":"the","start":106,"end":109,"id":16},{"text":"NN","start":110,"end":112,"id":17},{"text":"performance","start":113,"end":124,"id":18},{"text":".","start":124,"end":125,"id":19}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":110,"end":112,"token_start":17,"token_end":17,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"The issues associated with these difficulties relate to the broader structure of discrete exponential families.","_input_hash":-1951682925,"_task_hash":-212268159,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"issues","start":4,"end":10,"id":1},{"text":"associated","start":11,"end":21,"id":2},{"text":"with","start":22,"end":26,"id":3},{"text":"these","start":27,"end":32,"id":4},{"text":"difficulties","start":33,"end":45,"id":5},{"text":"relate","start":46,"end":52,"id":6},{"text":"to","start":53,"end":55,"id":7},{"text":"the","start":56,"end":59,"id":8},{"text":"broader","start":60,"end":67,"id":9},{"text":"structure","start":68,"end":77,"id":10},{"text":"of","start":78,"end":80,"id":11},{"text":"discrete","start":81,"end":89,"id":12},{"text":"exponential","start":90,"end":101,"id":13},{"text":"families","start":102,"end":110,"id":14},{"text":".","start":110,"end":111,"id":15}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We present a probabilistic viewpoint to multiple kernel learning unifying well-known regularised risk approaches and recent advances in approximate Bayesian inference relaxations.","_input_hash":-164296711,"_task_hash":170596614,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"present","start":3,"end":10,"id":1},{"text":"a","start":11,"end":12,"id":2},{"text":"probabilistic","start":13,"end":26,"id":3},{"text":"viewpoint","start":27,"end":36,"id":4},{"text":"to","start":37,"end":39,"id":5},{"text":"multiple","start":40,"end":48,"id":6},{"text":"kernel","start":49,"end":55,"id":7},{"text":"learning","start":56,"end":64,"id":8},{"text":"unifying","start":65,"end":73,"id":9},{"text":"well","start":74,"end":78,"id":10},{"text":"-","start":78,"end":79,"id":11},{"text":"known","start":79,"end":84,"id":12},{"text":"regularised","start":85,"end":96,"id":13},{"text":"risk","start":97,"end":101,"id":14},{"text":"approaches","start":102,"end":112,"id":15},{"text":"and","start":113,"end":116,"id":16},{"text":"recent","start":117,"end":123,"id":17},{"text":"advances","start":124,"end":132,"id":18},{"text":"in","start":133,"end":135,"id":19},{"text":"approximate","start":136,"end":147,"id":20},{"text":"Bayesian","start":148,"end":156,"id":21},{"text":"inference","start":157,"end":166,"id":22},{"text":"relaxations","start":167,"end":178,"id":23},{"text":".","start":178,"end":179,"id":24}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":40,"end":64,"token_start":6,"token_end":8,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"The \"distance\" here is actually a kernel defining the similarity between two images.","_input_hash":-897828985,"_task_hash":-101705564,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"\"","start":4,"end":5,"id":1},{"text":"distance","start":5,"end":13,"id":2},{"text":"\"","start":13,"end":14,"id":3},{"text":"here","start":15,"end":19,"id":4},{"text":"is","start":20,"end":22,"id":5},{"text":"actually","start":23,"end":31,"id":6},{"text":"a","start":32,"end":33,"id":7},{"text":"kernel","start":34,"end":40,"id":8},{"text":"defining","start":41,"end":49,"id":9},{"text":"the","start":50,"end":53,"id":10},{"text":"similarity","start":54,"end":64,"id":11},{"text":"between","start":65,"end":72,"id":12},{"text":"two","start":73,"end":76,"id":13},{"text":"images","start":77,"end":83,"id":14},{"text":".","start":83,"end":84,"id":15}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"A simple and computationally efficient scheme for tree-structured vector quantization is presented.","_input_hash":240911377,"_task_hash":689423020,"tokens":[{"text":"A","start":0,"end":1,"id":0},{"text":"simple","start":2,"end":8,"id":1},{"text":"and","start":9,"end":12,"id":2},{"text":"computationally","start":13,"end":28,"id":3},{"text":"efficient","start":29,"end":38,"id":4},{"text":"scheme","start":39,"end":45,"id":5},{"text":"for","start":46,"end":49,"id":6},{"text":"tree","start":50,"end":54,"id":7},{"text":"-","start":54,"end":55,"id":8},{"text":"structured","start":55,"end":65,"id":9},{"text":"vector","start":66,"end":72,"id":10},{"text":"quantization","start":73,"end":85,"id":11},{"text":"is","start":86,"end":88,"id":12},{"text":"presented","start":89,"end":98,"id":13},{"text":".","start":98,"end":99,"id":14}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"In this report, we derive a non-negative series expansion for the Jensen-Shannon divergence (JSD) between two probability distributions.","_input_hash":-379261663,"_task_hash":-1352715429,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"this","start":3,"end":7,"id":1},{"text":"report","start":8,"end":14,"id":2},{"text":",","start":14,"end":15,"id":3},{"text":"we","start":16,"end":18,"id":4},{"text":"derive","start":19,"end":25,"id":5},{"text":"a","start":26,"end":27,"id":6},{"text":"non","start":28,"end":31,"id":7},{"text":"-","start":31,"end":32,"id":8},{"text":"negative","start":32,"end":40,"id":9},{"text":"series","start":41,"end":47,"id":10},{"text":"expansion","start":48,"end":57,"id":11},{"text":"for","start":58,"end":61,"id":12},{"text":"the","start":62,"end":65,"id":13},{"text":"Jensen","start":66,"end":72,"id":14},{"text":"-","start":72,"end":73,"id":15},{"text":"Shannon","start":73,"end":80,"id":16},{"text":"divergence","start":81,"end":91,"id":17},{"text":"(","start":92,"end":93,"id":18},{"text":"JSD","start":93,"end":96,"id":19},{"text":")","start":96,"end":97,"id":20},{"text":"between","start":98,"end":105,"id":21},{"text":"two","start":106,"end":109,"id":22},{"text":"probability","start":110,"end":121,"id":23},{"text":"distributions","start":122,"end":135,"id":24},{"text":".","start":135,"end":136,"id":25}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"It can also be generalized to time series taking values in arbitrary state spaces, as long as the state space itself is endowed with a kernel \\kappa.","_input_hash":562828014,"_task_hash":-161797488,"tokens":[{"text":"It","start":0,"end":2,"id":0},{"text":"can","start":3,"end":6,"id":1},{"text":"also","start":7,"end":11,"id":2},{"text":"be","start":12,"end":14,"id":3},{"text":"generalized","start":15,"end":26,"id":4},{"text":"to","start":27,"end":29,"id":5},{"text":"time","start":30,"end":34,"id":6},{"text":"series","start":35,"end":41,"id":7},{"text":"taking","start":42,"end":48,"id":8},{"text":"values","start":49,"end":55,"id":9},{"text":"in","start":56,"end":58,"id":10},{"text":"arbitrary","start":59,"end":68,"id":11},{"text":"state","start":69,"end":74,"id":12},{"text":"spaces","start":75,"end":81,"id":13},{"text":",","start":81,"end":82,"id":14},{"text":"as","start":83,"end":85,"id":15},{"text":"long","start":86,"end":90,"id":16},{"text":"as","start":91,"end":93,"id":17},{"text":"the","start":94,"end":97,"id":18},{"text":"state","start":98,"end":103,"id":19},{"text":"space","start":104,"end":109,"id":20},{"text":"itself","start":110,"end":116,"id":21},{"text":"is","start":117,"end":119,"id":22},{"text":"endowed","start":120,"end":127,"id":23},{"text":"with","start":128,"end":132,"id":24},{"text":"a","start":133,"end":134,"id":25},{"text":"kernel","start":135,"end":141,"id":26},{"text":"\\kappa","start":142,"end":148,"id":27},{"text":".","start":148,"end":149,"id":28}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Among other practical benefits, RobustICA can avoid prewhitening and deals with real- and complex-valued mixtures of possibly noncircular sources alike.","_input_hash":918322072,"_task_hash":1687500086,"tokens":[{"text":"Among","start":0,"end":5,"id":0},{"text":"other","start":6,"end":11,"id":1},{"text":"practical","start":12,"end":21,"id":2},{"text":"benefits","start":22,"end":30,"id":3},{"text":",","start":30,"end":31,"id":4},{"text":"RobustICA","start":32,"end":41,"id":5},{"text":"can","start":42,"end":45,"id":6},{"text":"avoid","start":46,"end":51,"id":7},{"text":"prewhitening","start":52,"end":64,"id":8},{"text":"and","start":65,"end":68,"id":9},{"text":"deals","start":69,"end":74,"id":10},{"text":"with","start":75,"end":79,"id":11},{"text":"real-","start":80,"end":85,"id":12},{"text":"and","start":86,"end":89,"id":13},{"text":"complex","start":90,"end":97,"id":14},{"text":"-","start":97,"end":98,"id":15},{"text":"valued","start":98,"end":104,"id":16},{"text":"mixtures","start":105,"end":113,"id":17},{"text":"of","start":114,"end":116,"id":18},{"text":"possibly","start":117,"end":125,"id":19},{"text":"noncircular","start":126,"end":137,"id":20},{"text":"sources","start":138,"end":145,"id":21},{"text":"alike","start":146,"end":151,"id":22},{"text":".","start":151,"end":152,"id":23}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":32,"end":41,"token_start":5,"token_end":5,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Both the finite sample case and the limit case are analyzed.","_input_hash":-711154741,"_task_hash":-1814561714,"tokens":[{"text":"Both","start":0,"end":4,"id":0},{"text":"the","start":5,"end":8,"id":1},{"text":"finite","start":9,"end":15,"id":2},{"text":"sample","start":16,"end":22,"id":3},{"text":"case","start":23,"end":27,"id":4},{"text":"and","start":28,"end":31,"id":5},{"text":"the","start":32,"end":35,"id":6},{"text":"limit","start":36,"end":41,"id":7},{"text":"case","start":42,"end":46,"id":8},{"text":"are","start":47,"end":50,"id":9},{"text":"analyzed","start":51,"end":59,"id":10},{"text":".","start":59,"end":60,"id":11}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"When learning NMF on large audio databases, one major drawback is that the complexity in time is O(FKN) when updating the dictionary (where (F;N) is the dimension of the input power spectrograms, and K the number of basis spectra), thus forbidding its application on signals longer than an hour.","_input_hash":859286477,"_task_hash":-29603425,"tokens":[{"text":"When","start":0,"end":4,"id":0},{"text":"learning","start":5,"end":13,"id":1},{"text":"NMF","start":14,"end":17,"id":2},{"text":"on","start":18,"end":20,"id":3},{"text":"large","start":21,"end":26,"id":4},{"text":"audio","start":27,"end":32,"id":5},{"text":"databases","start":33,"end":42,"id":6},{"text":",","start":42,"end":43,"id":7},{"text":"one","start":44,"end":47,"id":8},{"text":"major","start":48,"end":53,"id":9},{"text":"drawback","start":54,"end":62,"id":10},{"text":"is","start":63,"end":65,"id":11},{"text":"that","start":66,"end":70,"id":12},{"text":"the","start":71,"end":74,"id":13},{"text":"complexity","start":75,"end":85,"id":14},{"text":"in","start":86,"end":88,"id":15},{"text":"time","start":89,"end":93,"id":16},{"text":"is","start":94,"end":96,"id":17},{"text":"O(FKN","start":97,"end":102,"id":18},{"text":")","start":102,"end":103,"id":19},{"text":"when","start":104,"end":108,"id":20},{"text":"updating","start":109,"end":117,"id":21},{"text":"the","start":118,"end":121,"id":22},{"text":"dictionary","start":122,"end":132,"id":23},{"text":"(","start":133,"end":134,"id":24},{"text":"where","start":134,"end":139,"id":25},{"text":"(","start":140,"end":141,"id":26},{"text":"F;N","start":141,"end":144,"id":27},{"text":")","start":144,"end":145,"id":28},{"text":"is","start":146,"end":148,"id":29},{"text":"the","start":149,"end":152,"id":30},{"text":"dimension","start":153,"end":162,"id":31},{"text":"of","start":163,"end":165,"id":32},{"text":"the","start":166,"end":169,"id":33},{"text":"input","start":170,"end":175,"id":34},{"text":"power","start":176,"end":181,"id":35},{"text":"spectrograms","start":182,"end":194,"id":36},{"text":",","start":194,"end":195,"id":37},{"text":"and","start":196,"end":199,"id":38},{"text":"K","start":200,"end":201,"id":39},{"text":"the","start":202,"end":205,"id":40},{"text":"number","start":206,"end":212,"id":41},{"text":"of","start":213,"end":215,"id":42},{"text":"basis","start":216,"end":221,"id":43},{"text":"spectra","start":222,"end":229,"id":44},{"text":")","start":229,"end":230,"id":45},{"text":",","start":230,"end":231,"id":46},{"text":"thus","start":232,"end":236,"id":47},{"text":"forbidding","start":237,"end":247,"id":48},{"text":"its","start":248,"end":251,"id":49},{"text":"application","start":252,"end":263,"id":50},{"text":"on","start":264,"end":266,"id":51},{"text":"signals","start":267,"end":274,"id":52},{"text":"longer","start":275,"end":281,"id":53},{"text":"than","start":282,"end":286,"id":54},{"text":"an","start":287,"end":289,"id":55},{"text":"hour","start":290,"end":294,"id":56},{"text":".","start":294,"end":295,"id":57}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Indeed, we demonstrate on the much studied ZIP code data how the PVM can reap the benefits of a problem-specific metric.","_input_hash":811333583,"_task_hash":-2093621216,"tokens":[{"text":"Indeed","start":0,"end":6,"id":0},{"text":",","start":6,"end":7,"id":1},{"text":"we","start":8,"end":10,"id":2},{"text":"demonstrate","start":11,"end":22,"id":3},{"text":"on","start":23,"end":25,"id":4},{"text":"the","start":26,"end":29,"id":5},{"text":"much","start":30,"end":34,"id":6},{"text":"studied","start":35,"end":42,"id":7},{"text":"ZIP","start":43,"end":46,"id":8},{"text":"code","start":47,"end":51,"id":9},{"text":"data","start":52,"end":56,"id":10},{"text":"how","start":57,"end":60,"id":11},{"text":"the","start":61,"end":64,"id":12},{"text":"PVM","start":65,"end":68,"id":13},{"text":"can","start":69,"end":72,"id":14},{"text":"reap","start":73,"end":77,"id":15},{"text":"the","start":78,"end":81,"id":16},{"text":"benefits","start":82,"end":90,"id":17},{"text":"of","start":91,"end":93,"id":18},{"text":"a","start":94,"end":95,"id":19},{"text":"problem","start":96,"end":103,"id":20},{"text":"-","start":103,"end":104,"id":21},{"text":"specific","start":104,"end":112,"id":22},{"text":"metric","start":113,"end":119,"id":23},{"text":".","start":119,"end":120,"id":24}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":65,"end":68,"token_start":13,"token_end":13,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"One approach to the problem is to search for a lower dimensional manifold which captures the main characteristics of the data.","_input_hash":-503766215,"_task_hash":-1754268838,"tokens":[{"text":"One","start":0,"end":3,"id":0},{"text":"approach","start":4,"end":12,"id":1},{"text":"to","start":13,"end":15,"id":2},{"text":"the","start":16,"end":19,"id":3},{"text":"problem","start":20,"end":27,"id":4},{"text":"is","start":28,"end":30,"id":5},{"text":"to","start":31,"end":33,"id":6},{"text":"search","start":34,"end":40,"id":7},{"text":"for","start":41,"end":44,"id":8},{"text":"a","start":45,"end":46,"id":9},{"text":"lower","start":47,"end":52,"id":10},{"text":"dimensional","start":53,"end":64,"id":11},{"text":"manifold","start":65,"end":73,"id":12},{"text":"which","start":74,"end":79,"id":13},{"text":"captures","start":80,"end":88,"id":14},{"text":"the","start":89,"end":92,"id":15},{"text":"main","start":93,"end":97,"id":16},{"text":"characteristics","start":98,"end":113,"id":17},{"text":"of","start":114,"end":116,"id":18},{"text":"the","start":117,"end":120,"id":19},{"text":"data","start":121,"end":125,"id":20},{"text":".","start":125,"end":126,"id":21}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"a collapsed MCMC sampler which allows us to model uncertainty over tree structures, and a computationally efficient greedy Bayesian EM search algorithm.","_input_hash":2124162306,"_task_hash":767747053,"tokens":[{"text":"a","start":0,"end":1,"id":0},{"text":"collapsed","start":2,"end":11,"id":1},{"text":"MCMC","start":12,"end":16,"id":2},{"text":"sampler","start":17,"end":24,"id":3},{"text":"which","start":25,"end":30,"id":4},{"text":"allows","start":31,"end":37,"id":5},{"text":"us","start":38,"end":40,"id":6},{"text":"to","start":41,"end":43,"id":7},{"text":"model","start":44,"end":49,"id":8},{"text":"uncertainty","start":50,"end":61,"id":9},{"text":"over","start":62,"end":66,"id":10},{"text":"tree","start":67,"end":71,"id":11},{"text":"structures","start":72,"end":82,"id":12},{"text":",","start":82,"end":83,"id":13},{"text":"and","start":84,"end":87,"id":14},{"text":"a","start":88,"end":89,"id":15},{"text":"computationally","start":90,"end":105,"id":16},{"text":"efficient","start":106,"end":115,"id":17},{"text":"greedy","start":116,"end":122,"id":18},{"text":"Bayesian","start":123,"end":131,"id":19},{"text":"EM","start":132,"end":134,"id":20},{"text":"search","start":135,"end":141,"id":21},{"text":"algorithm","start":142,"end":151,"id":22},{"text":".","start":151,"end":152,"id":23}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":12,"end":16,"token_start":2,"token_end":2,"label":"ALGO","answer":"accept"},{"start":123,"end":141,"token_start":19,"token_end":21,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"The former finds both disassortative and assortative structure, while the alternative assumes assortativity and finds community-like structures like the earlier methods motivated by physics.","_input_hash":251252844,"_task_hash":-328374854,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"former","start":4,"end":10,"id":1},{"text":"finds","start":11,"end":16,"id":2},{"text":"both","start":17,"end":21,"id":3},{"text":"disassortative","start":22,"end":36,"id":4},{"text":"and","start":37,"end":40,"id":5},{"text":"assortative","start":41,"end":52,"id":6},{"text":"structure","start":53,"end":62,"id":7},{"text":",","start":62,"end":63,"id":8},{"text":"while","start":64,"end":69,"id":9},{"text":"the","start":70,"end":73,"id":10},{"text":"alternative","start":74,"end":85,"id":11},{"text":"assumes","start":86,"end":93,"id":12},{"text":"assortativity","start":94,"end":107,"id":13},{"text":"and","start":108,"end":111,"id":14},{"text":"finds","start":112,"end":117,"id":15},{"text":"community","start":118,"end":127,"id":16},{"text":"-","start":127,"end":128,"id":17},{"text":"like","start":128,"end":132,"id":18},{"text":"structures","start":133,"end":143,"id":19},{"text":"like","start":144,"end":148,"id":20},{"text":"the","start":149,"end":152,"id":21},{"text":"earlier","start":153,"end":160,"id":22},{"text":"methods","start":161,"end":168,"id":23},{"text":"motivated","start":169,"end":178,"id":24},{"text":"by","start":179,"end":181,"id":25},{"text":"physics","start":182,"end":189,"id":26},{"text":".","start":189,"end":190,"id":27}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"In the general setting, we prove sanity-check bounds in the spirit of \\cite{KR99} \\textquotedblleft\\textit{bounds showing that the worst-case error of this estimate is not much worse that of training error estimate} \\textquotedblright .","_input_hash":-1841768082,"_task_hash":1562831002,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"the","start":3,"end":6,"id":1},{"text":"general","start":7,"end":14,"id":2},{"text":"setting","start":15,"end":22,"id":3},{"text":",","start":22,"end":23,"id":4},{"text":"we","start":24,"end":26,"id":5},{"text":"prove","start":27,"end":32,"id":6},{"text":"sanity","start":33,"end":39,"id":7},{"text":"-","start":39,"end":40,"id":8},{"text":"check","start":40,"end":45,"id":9},{"text":"bounds","start":46,"end":52,"id":10},{"text":"in","start":53,"end":55,"id":11},{"text":"the","start":56,"end":59,"id":12},{"text":"spirit","start":60,"end":66,"id":13},{"text":"of","start":67,"end":69,"id":14},{"text":"\\cite{KR99","start":70,"end":80,"id":15},{"text":"}","start":80,"end":81,"id":16},{"text":"\\textquotedblleft\\textit{bounds","start":82,"end":113,"id":17},{"text":"showing","start":114,"end":121,"id":18},{"text":"that","start":122,"end":126,"id":19},{"text":"the","start":127,"end":130,"id":20},{"text":"worst","start":131,"end":136,"id":21},{"text":"-","start":136,"end":137,"id":22},{"text":"case","start":137,"end":141,"id":23},{"text":"error","start":142,"end":147,"id":24},{"text":"of","start":148,"end":150,"id":25},{"text":"this","start":151,"end":155,"id":26},{"text":"estimate","start":156,"end":164,"id":27},{"text":"is","start":165,"end":167,"id":28},{"text":"not","start":168,"end":171,"id":29},{"text":"much","start":172,"end":176,"id":30},{"text":"worse","start":177,"end":182,"id":31},{"text":"that","start":183,"end":187,"id":32},{"text":"of","start":188,"end":190,"id":33},{"text":"training","start":191,"end":199,"id":34},{"text":"error","start":200,"end":205,"id":35},{"text":"estimate","start":206,"end":214,"id":36},{"text":"}","start":214,"end":215,"id":37},{"text":"\\textquotedblright","start":216,"end":234,"id":38},{"text":".","start":235,"end":236,"id":39}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Slow feature analysis (SFA) is a method for extracting slowly varying driving forces from quickly varying nonstationary time series.","_input_hash":-1287211239,"_task_hash":1643478642,"tokens":[{"text":"Slow","start":0,"end":4,"id":0},{"text":"feature","start":5,"end":12,"id":1},{"text":"analysis","start":13,"end":21,"id":2},{"text":"(","start":22,"end":23,"id":3},{"text":"SFA","start":23,"end":26,"id":4},{"text":")","start":26,"end":27,"id":5},{"text":"is","start":28,"end":30,"id":6},{"text":"a","start":31,"end":32,"id":7},{"text":"method","start":33,"end":39,"id":8},{"text":"for","start":40,"end":43,"id":9},{"text":"extracting","start":44,"end":54,"id":10},{"text":"slowly","start":55,"end":61,"id":11},{"text":"varying","start":62,"end":69,"id":12},{"text":"driving","start":70,"end":77,"id":13},{"text":"forces","start":78,"end":84,"id":14},{"text":"from","start":85,"end":89,"id":15},{"text":"quickly","start":90,"end":97,"id":16},{"text":"varying","start":98,"end":105,"id":17},{"text":"nonstationary","start":106,"end":119,"id":18},{"text":"time","start":120,"end":124,"id":19},{"text":"series","start":125,"end":131,"id":20},{"text":".","start":131,"end":132,"id":21}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":0,"end":21,"token_start":0,"token_end":2,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We show how a hierarchical clustering can be read directly from one pass through the data.","_input_hash":1636760805,"_task_hash":1970231389,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"show","start":3,"end":7,"id":1},{"text":"how","start":8,"end":11,"id":2},{"text":"a","start":12,"end":13,"id":3},{"text":"hierarchical","start":14,"end":26,"id":4},{"text":"clustering","start":27,"end":37,"id":5},{"text":"can","start":38,"end":41,"id":6},{"text":"be","start":42,"end":44,"id":7},{"text":"read","start":45,"end":49,"id":8},{"text":"directly","start":50,"end":58,"id":9},{"text":"from","start":59,"end":63,"id":10},{"text":"one","start":64,"end":67,"id":11},{"text":"pass","start":68,"end":72,"id":12},{"text":"through","start":73,"end":80,"id":13},{"text":"the","start":81,"end":84,"id":14},{"text":"data","start":85,"end":89,"id":15},{"text":".","start":89,"end":90,"id":16}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":14,"end":37,"token_start":4,"token_end":5,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We obtain an index of the complexity of a random sequence by allowing the role of the measure in classical probability theory to be played by a function we call the generating mechanism.","_input_hash":293701925,"_task_hash":-213590300,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"obtain","start":3,"end":9,"id":1},{"text":"an","start":10,"end":12,"id":2},{"text":"index","start":13,"end":18,"id":3},{"text":"of","start":19,"end":21,"id":4},{"text":"the","start":22,"end":25,"id":5},{"text":"complexity","start":26,"end":36,"id":6},{"text":"of","start":37,"end":39,"id":7},{"text":"a","start":40,"end":41,"id":8},{"text":"random","start":42,"end":48,"id":9},{"text":"sequence","start":49,"end":57,"id":10},{"text":"by","start":58,"end":60,"id":11},{"text":"allowing","start":61,"end":69,"id":12},{"text":"the","start":70,"end":73,"id":13},{"text":"role","start":74,"end":78,"id":14},{"text":"of","start":79,"end":81,"id":15},{"text":"the","start":82,"end":85,"id":16},{"text":"measure","start":86,"end":93,"id":17},{"text":"in","start":94,"end":96,"id":18},{"text":"classical","start":97,"end":106,"id":19},{"text":"probability","start":107,"end":118,"id":20},{"text":"theory","start":119,"end":125,"id":21},{"text":"to","start":126,"end":128,"id":22},{"text":"be","start":129,"end":131,"id":23},{"text":"played","start":132,"end":138,"id":24},{"text":"by","start":139,"end":141,"id":25},{"text":"a","start":142,"end":143,"id":26},{"text":"function","start":144,"end":152,"id":27},{"text":"we","start":153,"end":155,"id":28},{"text":"call","start":156,"end":160,"id":29},{"text":"the","start":161,"end":164,"id":30},{"text":"generating","start":165,"end":175,"id":31},{"text":"mechanism","start":176,"end":185,"id":32},{"text":".","start":185,"end":186,"id":33}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"Finally we review briefly the construction of Support Vector Machines and show how to derive generalization bounds for them, measuring the complexity either through the number of support vectors or through the value of the transductive or inductive margin.","_input_hash":1340001408,"_task_hash":1257872603,"tokens":[{"text":"Finally","start":0,"end":7,"id":0},{"text":"we","start":8,"end":10,"id":1},{"text":"review","start":11,"end":17,"id":2},{"text":"briefly","start":18,"end":25,"id":3},{"text":"the","start":26,"end":29,"id":4},{"text":"construction","start":30,"end":42,"id":5},{"text":"of","start":43,"end":45,"id":6},{"text":"Support","start":46,"end":53,"id":7},{"text":"Vector","start":54,"end":60,"id":8},{"text":"Machines","start":61,"end":69,"id":9},{"text":"and","start":70,"end":73,"id":10},{"text":"show","start":74,"end":78,"id":11},{"text":"how","start":79,"end":82,"id":12},{"text":"to","start":83,"end":85,"id":13},{"text":"derive","start":86,"end":92,"id":14},{"text":"generalization","start":93,"end":107,"id":15},{"text":"bounds","start":108,"end":114,"id":16},{"text":"for","start":115,"end":118,"id":17},{"text":"them","start":119,"end":123,"id":18},{"text":",","start":123,"end":124,"id":19},{"text":"measuring","start":125,"end":134,"id":20},{"text":"the","start":135,"end":138,"id":21},{"text":"complexity","start":139,"end":149,"id":22},{"text":"either","start":150,"end":156,"id":23},{"text":"through","start":157,"end":164,"id":24},{"text":"the","start":165,"end":168,"id":25},{"text":"number","start":169,"end":175,"id":26},{"text":"of","start":176,"end":178,"id":27},{"text":"support","start":179,"end":186,"id":28},{"text":"vectors","start":187,"end":194,"id":29},{"text":"or","start":195,"end":197,"id":30},{"text":"through","start":198,"end":205,"id":31},{"text":"the","start":206,"end":209,"id":32},{"text":"value","start":210,"end":215,"id":33},{"text":"of","start":216,"end":218,"id":34},{"text":"the","start":219,"end":222,"id":35},{"text":"transductive","start":223,"end":235,"id":36},{"text":"or","start":236,"end":238,"id":37},{"text":"inductive","start":239,"end":248,"id":38},{"text":"margin","start":249,"end":255,"id":39},{"text":".","start":255,"end":256,"id":40}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":46,"end":69,"token_start":7,"token_end":9,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We also give examples for when those conditions hold, including models for compactly supported continuous distributions and a model with continuous covariates and categorical response.","_input_hash":-98749212,"_task_hash":1798949911,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"also","start":3,"end":7,"id":1},{"text":"give","start":8,"end":12,"id":2},{"text":"examples","start":13,"end":21,"id":3},{"text":"for","start":22,"end":25,"id":4},{"text":"when","start":26,"end":30,"id":5},{"text":"those","start":31,"end":36,"id":6},{"text":"conditions","start":37,"end":47,"id":7},{"text":"hold","start":48,"end":52,"id":8},{"text":",","start":52,"end":53,"id":9},{"text":"including","start":54,"end":63,"id":10},{"text":"models","start":64,"end":70,"id":11},{"text":"for","start":71,"end":74,"id":12},{"text":"compactly","start":75,"end":84,"id":13},{"text":"supported","start":85,"end":94,"id":14},{"text":"continuous","start":95,"end":105,"id":15},{"text":"distributions","start":106,"end":119,"id":16},{"text":"and","start":120,"end":123,"id":17},{"text":"a","start":124,"end":125,"id":18},{"text":"model","start":126,"end":131,"id":19},{"text":"with","start":132,"end":136,"id":20},{"text":"continuous","start":137,"end":147,"id":21},{"text":"covariates","start":148,"end":158,"id":22},{"text":"and","start":159,"end":162,"id":23},{"text":"categorical","start":163,"end":174,"id":24},{"text":"response","start":175,"end":183,"id":25},{"text":".","start":183,"end":184,"id":26}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"For graphs generated from a stochastic algorithm designed to model community structure, the optimal information theoretic partition and the optimal min-cut partition are shown to be the same with high probability.","_input_hash":1434805796,"_task_hash":829451928,"tokens":[{"text":"For","start":0,"end":3,"id":0},{"text":"graphs","start":4,"end":10,"id":1},{"text":"generated","start":11,"end":20,"id":2},{"text":"from","start":21,"end":25,"id":3},{"text":"a","start":26,"end":27,"id":4},{"text":"stochastic","start":28,"end":38,"id":5},{"text":"algorithm","start":39,"end":48,"id":6},{"text":"designed","start":49,"end":57,"id":7},{"text":"to","start":58,"end":60,"id":8},{"text":"model","start":61,"end":66,"id":9},{"text":"community","start":67,"end":76,"id":10},{"text":"structure","start":77,"end":86,"id":11},{"text":",","start":86,"end":87,"id":12},{"text":"the","start":88,"end":91,"id":13},{"text":"optimal","start":92,"end":99,"id":14},{"text":"information","start":100,"end":111,"id":15},{"text":"theoretic","start":112,"end":121,"id":16},{"text":"partition","start":122,"end":131,"id":17},{"text":"and","start":132,"end":135,"id":18},{"text":"the","start":136,"end":139,"id":19},{"text":"optimal","start":140,"end":147,"id":20},{"text":"min","start":148,"end":151,"id":21},{"text":"-","start":151,"end":152,"id":22},{"text":"cut","start":152,"end":155,"id":23},{"text":"partition","start":156,"end":165,"id":24},{"text":"are","start":166,"end":169,"id":25},{"text":"shown","start":170,"end":175,"id":26},{"text":"to","start":176,"end":178,"id":27},{"text":"be","start":179,"end":181,"id":28},{"text":"the","start":182,"end":185,"id":29},{"text":"same","start":186,"end":190,"id":30},{"text":"with","start":191,"end":195,"id":31},{"text":"high","start":196,"end":200,"id":32},{"text":"probability","start":201,"end":212,"id":33},{"text":".","start":212,"end":213,"id":34}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"As our main theoretical contribution, we prove that, asymptotically, S-OMP is guaranteed to reduce an ultra-high number of variables to below the sample size without losing true relevant variables.","_input_hash":1623590736,"_task_hash":-170916391,"tokens":[{"text":"As","start":0,"end":2,"id":0},{"text":"our","start":3,"end":6,"id":1},{"text":"main","start":7,"end":11,"id":2},{"text":"theoretical","start":12,"end":23,"id":3},{"text":"contribution","start":24,"end":36,"id":4},{"text":",","start":36,"end":37,"id":5},{"text":"we","start":38,"end":40,"id":6},{"text":"prove","start":41,"end":46,"id":7},{"text":"that","start":47,"end":51,"id":8},{"text":",","start":51,"end":52,"id":9},{"text":"asymptotically","start":53,"end":67,"id":10},{"text":",","start":67,"end":68,"id":11},{"text":"S","start":69,"end":70,"id":12},{"text":"-","start":70,"end":71,"id":13},{"text":"OMP","start":71,"end":74,"id":14},{"text":"is","start":75,"end":77,"id":15},{"text":"guaranteed","start":78,"end":88,"id":16},{"text":"to","start":89,"end":91,"id":17},{"text":"reduce","start":92,"end":98,"id":18},{"text":"an","start":99,"end":101,"id":19},{"text":"ultra","start":102,"end":107,"id":20},{"text":"-","start":107,"end":108,"id":21},{"text":"high","start":108,"end":112,"id":22},{"text":"number","start":113,"end":119,"id":23},{"text":"of","start":120,"end":122,"id":24},{"text":"variables","start":123,"end":132,"id":25},{"text":"to","start":133,"end":135,"id":26},{"text":"below","start":136,"end":141,"id":27},{"text":"the","start":142,"end":145,"id":28},{"text":"sample","start":146,"end":152,"id":29},{"text":"size","start":153,"end":157,"id":30},{"text":"without","start":158,"end":165,"id":31},{"text":"losing","start":166,"end":172,"id":32},{"text":"true","start":173,"end":177,"id":33},{"text":"relevant","start":178,"end":186,"id":34},{"text":"variables","start":187,"end":196,"id":35},{"text":".","start":196,"end":197,"id":36}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Although the main focus of this paper is to present a general, theoretical framework for metric embedding in a NN setting, we demonstrate the performance of the proposed method on some benchmark datasets and show that it performs better than the Mahalanobis metric learning algorithm in terms of leave-one-out and generalization errors.","_input_hash":-1031192136,"_task_hash":1100255065,"tokens":[{"text":"Although","start":0,"end":8,"id":0},{"text":"the","start":9,"end":12,"id":1},{"text":"main","start":13,"end":17,"id":2},{"text":"focus","start":18,"end":23,"id":3},{"text":"of","start":24,"end":26,"id":4},{"text":"this","start":27,"end":31,"id":5},{"text":"paper","start":32,"end":37,"id":6},{"text":"is","start":38,"end":40,"id":7},{"text":"to","start":41,"end":43,"id":8},{"text":"present","start":44,"end":51,"id":9},{"text":"a","start":52,"end":53,"id":10},{"text":"general","start":54,"end":61,"id":11},{"text":",","start":61,"end":62,"id":12},{"text":"theoretical","start":63,"end":74,"id":13},{"text":"framework","start":75,"end":84,"id":14},{"text":"for","start":85,"end":88,"id":15},{"text":"metric","start":89,"end":95,"id":16},{"text":"embedding","start":96,"end":105,"id":17},{"text":"in","start":106,"end":108,"id":18},{"text":"a","start":109,"end":110,"id":19},{"text":"NN","start":111,"end":113,"id":20},{"text":"setting","start":114,"end":121,"id":21},{"text":",","start":121,"end":122,"id":22},{"text":"we","start":123,"end":125,"id":23},{"text":"demonstrate","start":126,"end":137,"id":24},{"text":"the","start":138,"end":141,"id":25},{"text":"performance","start":142,"end":153,"id":26},{"text":"of","start":154,"end":156,"id":27},{"text":"the","start":157,"end":160,"id":28},{"text":"proposed","start":161,"end":169,"id":29},{"text":"method","start":170,"end":176,"id":30},{"text":"on","start":177,"end":179,"id":31},{"text":"some","start":180,"end":184,"id":32},{"text":"benchmark","start":185,"end":194,"id":33},{"text":"datasets","start":195,"end":203,"id":34},{"text":"and","start":204,"end":207,"id":35},{"text":"show","start":208,"end":212,"id":36},{"text":"that","start":213,"end":217,"id":37},{"text":"it","start":218,"end":220,"id":38},{"text":"performs","start":221,"end":229,"id":39},{"text":"better","start":230,"end":236,"id":40},{"text":"than","start":237,"end":241,"id":41},{"text":"the","start":242,"end":245,"id":42},{"text":"Mahalanobis","start":246,"end":257,"id":43},{"text":"metric","start":258,"end":264,"id":44},{"text":"learning","start":265,"end":273,"id":45},{"text":"algorithm","start":274,"end":283,"id":46},{"text":"in","start":284,"end":286,"id":47},{"text":"terms","start":287,"end":292,"id":48},{"text":"of","start":293,"end":295,"id":49},{"text":"leave","start":296,"end":301,"id":50},{"text":"-","start":301,"end":302,"id":51},{"text":"one","start":302,"end":305,"id":52},{"text":"-","start":305,"end":306,"id":53},{"text":"out","start":306,"end":309,"id":54},{"text":"and","start":310,"end":313,"id":55},{"text":"generalization","start":314,"end":328,"id":56},{"text":"errors","start":329,"end":335,"id":57},{"text":".","start":335,"end":336,"id":58}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":111,"end":113,"token_start":20,"token_end":20,"label":"ALGO","answer":"accept"},{"start":246,"end":273,"token_start":43,"token_end":45,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We give an explicit construction of kernels - and thus of their RKHSs - which leads in combination with a Lipschitz continuous loss function to consistent and statistically robust SMVs for additive models.","_input_hash":1823417886,"_task_hash":-1752411678,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"give","start":3,"end":7,"id":1},{"text":"an","start":8,"end":10,"id":2},{"text":"explicit","start":11,"end":19,"id":3},{"text":"construction","start":20,"end":32,"id":4},{"text":"of","start":33,"end":35,"id":5},{"text":"kernels","start":36,"end":43,"id":6},{"text":"-","start":44,"end":45,"id":7},{"text":"and","start":46,"end":49,"id":8},{"text":"thus","start":50,"end":54,"id":9},{"text":"of","start":55,"end":57,"id":10},{"text":"their","start":58,"end":63,"id":11},{"text":"RKHSs","start":64,"end":69,"id":12},{"text":"-","start":70,"end":71,"id":13},{"text":"which","start":72,"end":77,"id":14},{"text":"leads","start":78,"end":83,"id":15},{"text":"in","start":84,"end":86,"id":16},{"text":"combination","start":87,"end":98,"id":17},{"text":"with","start":99,"end":103,"id":18},{"text":"a","start":104,"end":105,"id":19},{"text":"Lipschitz","start":106,"end":115,"id":20},{"text":"continuous","start":116,"end":126,"id":21},{"text":"loss","start":127,"end":131,"id":22},{"text":"function","start":132,"end":140,"id":23},{"text":"to","start":141,"end":143,"id":24},{"text":"consistent","start":144,"end":154,"id":25},{"text":"and","start":155,"end":158,"id":26},{"text":"statistically","start":159,"end":172,"id":27},{"text":"robust","start":173,"end":179,"id":28},{"text":"SMVs","start":180,"end":184,"id":29},{"text":"for","start":185,"end":188,"id":30},{"text":"additive","start":189,"end":197,"id":31},{"text":"models","start":198,"end":204,"id":32},{"text":".","start":204,"end":205,"id":33}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":180,"end":184,"token_start":29,"token_end":29,"label":"ALGO","answer":"accept"},{"start":189,"end":197,"token_start":31,"token_end":31,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Those proofs are provided in this paper.","_input_hash":1706121623,"_task_hash":-721906357,"tokens":[{"text":"Those","start":0,"end":5,"id":0},{"text":"proofs","start":6,"end":12,"id":1},{"text":"are","start":13,"end":16,"id":2},{"text":"provided","start":17,"end":25,"id":3},{"text":"in","start":26,"end":28,"id":4},{"text":"this","start":29,"end":33,"id":5},{"text":"paper","start":34,"end":39,"id":6},{"text":".","start":39,"end":40,"id":7}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We present numerical examples demonstrating both the perturbation and linear projection problems, and the improved outputs using the low-dimensional neighborhood representation.","_input_hash":1302518157,"_task_hash":-1290208708,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"present","start":3,"end":10,"id":1},{"text":"numerical","start":11,"end":20,"id":2},{"text":"examples","start":21,"end":29,"id":3},{"text":"demonstrating","start":30,"end":43,"id":4},{"text":"both","start":44,"end":48,"id":5},{"text":"the","start":49,"end":52,"id":6},{"text":"perturbation","start":53,"end":65,"id":7},{"text":"and","start":66,"end":69,"id":8},{"text":"linear","start":70,"end":76,"id":9},{"text":"projection","start":77,"end":87,"id":10},{"text":"problems","start":88,"end":96,"id":11},{"text":",","start":96,"end":97,"id":12},{"text":"and","start":98,"end":101,"id":13},{"text":"the","start":102,"end":105,"id":14},{"text":"improved","start":106,"end":114,"id":15},{"text":"outputs","start":115,"end":122,"id":16},{"text":"using","start":123,"end":128,"id":17},{"text":"the","start":129,"end":132,"id":18},{"text":"low","start":133,"end":136,"id":19},{"text":"-","start":136,"end":137,"id":20},{"text":"dimensional","start":137,"end":148,"id":21},{"text":"neighborhood","start":149,"end":161,"id":22},{"text":"representation","start":162,"end":176,"id":23},{"text":".","start":176,"end":177,"id":24}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We discuss its relevance to maximum likelihood estimation, both from a theoretical and computational standpoint.","_input_hash":-1688515034,"_task_hash":-1378774903,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"discuss","start":3,"end":10,"id":1},{"text":"its","start":11,"end":14,"id":2},{"text":"relevance","start":15,"end":24,"id":3},{"text":"to","start":25,"end":27,"id":4},{"text":"maximum","start":28,"end":35,"id":5},{"text":"likelihood","start":36,"end":46,"id":6},{"text":"estimation","start":47,"end":57,"id":7},{"text":",","start":57,"end":58,"id":8},{"text":"both","start":59,"end":63,"id":9},{"text":"from","start":64,"end":68,"id":10},{"text":"a","start":69,"end":70,"id":11},{"text":"theoretical","start":71,"end":82,"id":12},{"text":"and","start":83,"end":86,"id":13},{"text":"computational","start":87,"end":100,"id":14},{"text":"standpoint","start":101,"end":111,"id":15},{"text":".","start":111,"end":112,"id":16}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"The f-divergence estimator is then exploited for the two-sample homogeneity test.","_input_hash":-2038338754,"_task_hash":514887218,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"f","start":4,"end":5,"id":1},{"text":"-","start":5,"end":6,"id":2},{"text":"divergence","start":6,"end":16,"id":3},{"text":"estimator","start":17,"end":26,"id":4},{"text":"is","start":27,"end":29,"id":5},{"text":"then","start":30,"end":34,"id":6},{"text":"exploited","start":35,"end":44,"id":7},{"text":"for","start":45,"end":48,"id":8},{"text":"the","start":49,"end":52,"id":9},{"text":"two","start":53,"end":56,"id":10},{"text":"-","start":56,"end":57,"id":11},{"text":"sample","start":57,"end":63,"id":12},{"text":"homogeneity","start":64,"end":75,"id":13},{"text":"test","start":76,"end":80,"id":14},{"text":".","start":80,"end":81,"id":15}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"We prove some theoretical properties of the model and then present two inference methods:","_input_hash":-1464672162,"_task_hash":1651910424,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"prove","start":3,"end":8,"id":1},{"text":"some","start":9,"end":13,"id":2},{"text":"theoretical","start":14,"end":25,"id":3},{"text":"properties","start":26,"end":36,"id":4},{"text":"of","start":37,"end":39,"id":5},{"text":"the","start":40,"end":43,"id":6},{"text":"model","start":44,"end":49,"id":7},{"text":"and","start":50,"end":53,"id":8},{"text":"then","start":54,"end":58,"id":9},{"text":"present","start":59,"end":66,"id":10},{"text":"two","start":67,"end":70,"id":11},{"text":"inference","start":71,"end":80,"id":12},{"text":"methods","start":81,"end":88,"id":13},{"text":":","start":88,"end":89,"id":14}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We conclude that what is percieved as slow by SFA varies and that a more or less fast switching from one regime to the other occurs, perhaps showing some similarity to human perception.","_input_hash":1471156297,"_task_hash":775194311,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"conclude","start":3,"end":11,"id":1},{"text":"that","start":12,"end":16,"id":2},{"text":"what","start":17,"end":21,"id":3},{"text":"is","start":22,"end":24,"id":4},{"text":"percieved","start":25,"end":34,"id":5},{"text":"as","start":35,"end":37,"id":6},{"text":"slow","start":38,"end":42,"id":7},{"text":"by","start":43,"end":45,"id":8},{"text":"SFA","start":46,"end":49,"id":9},{"text":"varies","start":50,"end":56,"id":10},{"text":"and","start":57,"end":60,"id":11},{"text":"that","start":61,"end":65,"id":12},{"text":"a","start":66,"end":67,"id":13},{"text":"more","start":68,"end":72,"id":14},{"text":"or","start":73,"end":75,"id":15},{"text":"less","start":76,"end":80,"id":16},{"text":"fast","start":81,"end":85,"id":17},{"text":"switching","start":86,"end":95,"id":18},{"text":"from","start":96,"end":100,"id":19},{"text":"one","start":101,"end":104,"id":20},{"text":"regime","start":105,"end":111,"id":21},{"text":"to","start":112,"end":114,"id":22},{"text":"the","start":115,"end":118,"id":23},{"text":"other","start":119,"end":124,"id":24},{"text":"occurs","start":125,"end":131,"id":25},{"text":",","start":131,"end":132,"id":26},{"text":"perhaps","start":133,"end":140,"id":27},{"text":"showing","start":141,"end":148,"id":28},{"text":"some","start":149,"end":153,"id":29},{"text":"similarity","start":154,"end":164,"id":30},{"text":"to","start":165,"end":167,"id":31},{"text":"human","start":168,"end":173,"id":32},{"text":"perception","start":174,"end":184,"id":33},{"text":".","start":184,"end":185,"id":34}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":46,"end":49,"token_start":9,"token_end":9,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"The kernel density estimation based approaches are of interest due to the low time complexity of either O(n) or O(n*log(n)) for constructing a classifier, where n is the number of sampling instances.","_input_hash":-1025131444,"_task_hash":-2069335017,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"kernel","start":4,"end":10,"id":1},{"text":"density","start":11,"end":18,"id":2},{"text":"estimation","start":19,"end":29,"id":3},{"text":"based","start":30,"end":35,"id":4},{"text":"approaches","start":36,"end":46,"id":5},{"text":"are","start":47,"end":50,"id":6},{"text":"of","start":51,"end":53,"id":7},{"text":"interest","start":54,"end":62,"id":8},{"text":"due","start":63,"end":66,"id":9},{"text":"to","start":67,"end":69,"id":10},{"text":"the","start":70,"end":73,"id":11},{"text":"low","start":74,"end":77,"id":12},{"text":"time","start":78,"end":82,"id":13},{"text":"complexity","start":83,"end":93,"id":14},{"text":"of","start":94,"end":96,"id":15},{"text":"either","start":97,"end":103,"id":16},{"text":"O(n","start":104,"end":107,"id":17},{"text":")","start":107,"end":108,"id":18},{"text":"or","start":109,"end":111,"id":19},{"text":"O(n*log(n","start":112,"end":121,"id":20},{"text":")","start":121,"end":122,"id":21},{"text":")","start":122,"end":123,"id":22},{"text":"for","start":124,"end":127,"id":23},{"text":"constructing","start":128,"end":140,"id":24},{"text":"a","start":141,"end":142,"id":25},{"text":"classifier","start":143,"end":153,"id":26},{"text":",","start":153,"end":154,"id":27},{"text":"where","start":155,"end":160,"id":28},{"text":"n","start":161,"end":162,"id":29},{"text":"is","start":163,"end":165,"id":30},{"text":"the","start":166,"end":169,"id":31},{"text":"number","start":170,"end":176,"id":32},{"text":"of","start":177,"end":179,"id":33},{"text":"sampling","start":180,"end":188,"id":34},{"text":"instances","start":189,"end":198,"id":35},{"text":".","start":198,"end":199,"id":36}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"During the last years a great part of the statistical research on SVMs has concentrated on the question how to design SVMs such that they are universally consistent and statistically robust for nonparametric classification or nonparametric regression purposes.","_input_hash":931169385,"_task_hash":1393336917,"tokens":[{"text":"During","start":0,"end":6,"id":0},{"text":"the","start":7,"end":10,"id":1},{"text":"last","start":11,"end":15,"id":2},{"text":"years","start":16,"end":21,"id":3},{"text":"a","start":22,"end":23,"id":4},{"text":"great","start":24,"end":29,"id":5},{"text":"part","start":30,"end":34,"id":6},{"text":"of","start":35,"end":37,"id":7},{"text":"the","start":38,"end":41,"id":8},{"text":"statistical","start":42,"end":53,"id":9},{"text":"research","start":54,"end":62,"id":10},{"text":"on","start":63,"end":65,"id":11},{"text":"SVMs","start":66,"end":70,"id":12},{"text":"has","start":71,"end":74,"id":13},{"text":"concentrated","start":75,"end":87,"id":14},{"text":"on","start":88,"end":90,"id":15},{"text":"the","start":91,"end":94,"id":16},{"text":"question","start":95,"end":103,"id":17},{"text":"how","start":104,"end":107,"id":18},{"text":"to","start":108,"end":110,"id":19},{"text":"design","start":111,"end":117,"id":20},{"text":"SVMs","start":118,"end":122,"id":21},{"text":"such","start":123,"end":127,"id":22},{"text":"that","start":128,"end":132,"id":23},{"text":"they","start":133,"end":137,"id":24},{"text":"are","start":138,"end":141,"id":25},{"text":"universally","start":142,"end":153,"id":26},{"text":"consistent","start":154,"end":164,"id":27},{"text":"and","start":165,"end":168,"id":28},{"text":"statistically","start":169,"end":182,"id":29},{"text":"robust","start":183,"end":189,"id":30},{"text":"for","start":190,"end":193,"id":31},{"text":"nonparametric","start":194,"end":207,"id":32},{"text":"classification","start":208,"end":222,"id":33},{"text":"or","start":223,"end":225,"id":34},{"text":"nonparametric","start":226,"end":239,"id":35},{"text":"regression","start":240,"end":250,"id":36},{"text":"purposes","start":251,"end":259,"id":37},{"text":".","start":259,"end":260,"id":38}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":66,"end":70,"token_start":12,"token_end":12,"label":"ALGO","answer":"accept"},{"start":118,"end":122,"token_start":21,"token_end":21,"label":"ALGO","answer":"accept"},{"start":194,"end":222,"token_start":32,"token_end":33,"label":"ALGO","answer":"accept"},{"start":226,"end":250,"token_start":35,"token_end":36,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"To penalize variations in the estimated parameters, the l_1-norm of the time difference of the parameters is used as a regularization term.","_input_hash":-1177763408,"_task_hash":-243424940,"tokens":[{"text":"To","start":0,"end":2,"id":0},{"text":"penalize","start":3,"end":11,"id":1},{"text":"variations","start":12,"end":22,"id":2},{"text":"in","start":23,"end":25,"id":3},{"text":"the","start":26,"end":29,"id":4},{"text":"estimated","start":30,"end":39,"id":5},{"text":"parameters","start":40,"end":50,"id":6},{"text":",","start":50,"end":51,"id":7},{"text":"the","start":52,"end":55,"id":8},{"text":"l_1-norm","start":56,"end":64,"id":9},{"text":"of","start":65,"end":67,"id":10},{"text":"the","start":68,"end":71,"id":11},{"text":"time","start":72,"end":76,"id":12},{"text":"difference","start":77,"end":87,"id":13},{"text":"of","start":88,"end":90,"id":14},{"text":"the","start":91,"end":94,"id":15},{"text":"parameters","start":95,"end":105,"id":16},{"text":"is","start":106,"end":108,"id":17},{"text":"used","start":109,"end":113,"id":18},{"text":"as","start":114,"end":116,"id":19},{"text":"a","start":117,"end":118,"id":20},{"text":"regularization","start":119,"end":133,"id":21},{"text":"term","start":134,"end":138,"id":22},{"text":".","start":138,"end":139,"id":23}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"data from an invariant model widely considered in current literature of structural estimation.","_input_hash":-1876943304,"_task_hash":-1996752404,"tokens":[{"text":"data","start":0,"end":4,"id":0},{"text":"from","start":5,"end":9,"id":1},{"text":"an","start":10,"end":12,"id":2},{"text":"invariant","start":13,"end":22,"id":3},{"text":"model","start":23,"end":28,"id":4},{"text":"widely","start":29,"end":35,"id":5},{"text":"considered","start":36,"end":46,"id":6},{"text":"in","start":47,"end":49,"id":7},{"text":"current","start":50,"end":57,"id":8},{"text":"literature","start":58,"end":68,"id":9},{"text":"of","start":69,"end":71,"id":10},{"text":"structural","start":72,"end":82,"id":11},{"text":"estimation","start":83,"end":93,"id":12},{"text":".","start":93,"end":94,"id":13}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Second, we present a class of structured sparsity regularization called structured Lasso for which calculations can be readily performed under our theoretical framework.","_input_hash":-268796253,"_task_hash":-321408105,"tokens":[{"text":"Second","start":0,"end":6,"id":0},{"text":",","start":6,"end":7,"id":1},{"text":"we","start":8,"end":10,"id":2},{"text":"present","start":11,"end":18,"id":3},{"text":"a","start":19,"end":20,"id":4},{"text":"class","start":21,"end":26,"id":5},{"text":"of","start":27,"end":29,"id":6},{"text":"structured","start":30,"end":40,"id":7},{"text":"sparsity","start":41,"end":49,"id":8},{"text":"regularization","start":50,"end":64,"id":9},{"text":"called","start":65,"end":71,"id":10},{"text":"structured","start":72,"end":82,"id":11},{"text":"Lasso","start":83,"end":88,"id":12},{"text":"for","start":89,"end":92,"id":13},{"text":"which","start":93,"end":98,"id":14},{"text":"calculations","start":99,"end":111,"id":15},{"text":"can","start":112,"end":115,"id":16},{"text":"be","start":116,"end":118,"id":17},{"text":"readily","start":119,"end":126,"id":18},{"text":"performed","start":127,"end":136,"id":19},{"text":"under","start":137,"end":142,"id":20},{"text":"our","start":143,"end":146,"id":21},{"text":"theoretical","start":147,"end":158,"id":22},{"text":"framework","start":159,"end":168,"id":23},{"text":".","start":168,"end":169,"id":24}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":83,"end":88,"token_start":12,"token_end":12,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We speed up training by using an early stopping strategy based on the prediction on unlabeled data or, if available, on labeled validation examples.","_input_hash":-117015718,"_task_hash":-1906945971,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"speed","start":3,"end":8,"id":1},{"text":"up","start":9,"end":11,"id":2},{"text":"training","start":12,"end":20,"id":3},{"text":"by","start":21,"end":23,"id":4},{"text":"using","start":24,"end":29,"id":5},{"text":"an","start":30,"end":32,"id":6},{"text":"early","start":33,"end":38,"id":7},{"text":"stopping","start":39,"end":47,"id":8},{"text":"strategy","start":48,"end":56,"id":9},{"text":"based","start":57,"end":62,"id":10},{"text":"on","start":63,"end":65,"id":11},{"text":"the","start":66,"end":69,"id":12},{"text":"prediction","start":70,"end":80,"id":13},{"text":"on","start":81,"end":83,"id":14},{"text":"unlabeled","start":84,"end":93,"id":15},{"text":"data","start":94,"end":98,"id":16},{"text":"or","start":99,"end":101,"id":17},{"text":",","start":101,"end":102,"id":18},{"text":"if","start":103,"end":105,"id":19},{"text":"available","start":106,"end":115,"id":20},{"text":",","start":115,"end":116,"id":21},{"text":"on","start":117,"end":119,"id":22},{"text":"labeled","start":120,"end":127,"id":23},{"text":"validation","start":128,"end":138,"id":24},{"text":"examples","start":139,"end":147,"id":25},{"text":".","start":147,"end":148,"id":26}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"However, learning the structure of a belief network, particularly one with hidden units, is difficult.","_input_hash":-1921318332,"_task_hash":723144682,"tokens":[{"text":"However","start":0,"end":7,"id":0},{"text":",","start":7,"end":8,"id":1},{"text":"learning","start":9,"end":17,"id":2},{"text":"the","start":18,"end":21,"id":3},{"text":"structure","start":22,"end":31,"id":4},{"text":"of","start":32,"end":34,"id":5},{"text":"a","start":35,"end":36,"id":6},{"text":"belief","start":37,"end":43,"id":7},{"text":"network","start":44,"end":51,"id":8},{"text":",","start":51,"end":52,"id":9},{"text":"particularly","start":53,"end":65,"id":10},{"text":"one","start":66,"end":69,"id":11},{"text":"with","start":70,"end":74,"id":12},{"text":"hidden","start":75,"end":81,"id":13},{"text":"units","start":82,"end":87,"id":14},{"text":",","start":87,"end":88,"id":15},{"text":"is","start":89,"end":91,"id":16},{"text":"difficult","start":92,"end":101,"id":17},{"text":".","start":101,"end":102,"id":18}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":37,"end":51,"token_start":7,"token_end":8,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We show regret, convergence, and generalization bounds for the proposed method.","_input_hash":-1416708471,"_task_hash":1301231662,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"show","start":3,"end":7,"id":1},{"text":"regret","start":8,"end":14,"id":2},{"text":",","start":14,"end":15,"id":3},{"text":"convergence","start":16,"end":27,"id":4},{"text":",","start":27,"end":28,"id":5},{"text":"and","start":29,"end":32,"id":6},{"text":"generalization","start":33,"end":47,"id":7},{"text":"bounds","start":48,"end":54,"id":8},{"text":"for","start":55,"end":58,"id":9},{"text":"the","start":59,"end":62,"id":10},{"text":"proposed","start":63,"end":71,"id":11},{"text":"method","start":72,"end":78,"id":12},{"text":".","start":78,"end":79,"id":13}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"In Chapter 3, we propose a method for compressing a group of parameters into a single one, by exploiting the fact that many predictor variables derived from high-order interactions have the same values for all the training cases.","_input_hash":82553369,"_task_hash":-1207129772,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"Chapter","start":3,"end":10,"id":1},{"text":"3","start":11,"end":12,"id":2},{"text":",","start":12,"end":13,"id":3},{"text":"we","start":14,"end":16,"id":4},{"text":"propose","start":17,"end":24,"id":5},{"text":"a","start":25,"end":26,"id":6},{"text":"method","start":27,"end":33,"id":7},{"text":"for","start":34,"end":37,"id":8},{"text":"compressing","start":38,"end":49,"id":9},{"text":"a","start":50,"end":51,"id":10},{"text":"group","start":52,"end":57,"id":11},{"text":"of","start":58,"end":60,"id":12},{"text":"parameters","start":61,"end":71,"id":13},{"text":"into","start":72,"end":76,"id":14},{"text":"a","start":77,"end":78,"id":15},{"text":"single","start":79,"end":85,"id":16},{"text":"one","start":86,"end":89,"id":17},{"text":",","start":89,"end":90,"id":18},{"text":"by","start":91,"end":93,"id":19},{"text":"exploiting","start":94,"end":104,"id":20},{"text":"the","start":105,"end":108,"id":21},{"text":"fact","start":109,"end":113,"id":22},{"text":"that","start":114,"end":118,"id":23},{"text":"many","start":119,"end":123,"id":24},{"text":"predictor","start":124,"end":133,"id":25},{"text":"variables","start":134,"end":143,"id":26},{"text":"derived","start":144,"end":151,"id":27},{"text":"from","start":152,"end":156,"id":28},{"text":"high","start":157,"end":161,"id":29},{"text":"-","start":161,"end":162,"id":30},{"text":"order","start":162,"end":167,"id":31},{"text":"interactions","start":168,"end":180,"id":32},{"text":"have","start":181,"end":185,"id":33},{"text":"the","start":186,"end":189,"id":34},{"text":"same","start":190,"end":194,"id":35},{"text":"values","start":195,"end":201,"id":36},{"text":"for","start":202,"end":205,"id":37},{"text":"all","start":206,"end":209,"id":38},{"text":"the","start":210,"end":213,"id":39},{"text":"training","start":214,"end":222,"id":40},{"text":"cases","start":223,"end":228,"id":41},{"text":".","start":228,"end":229,"id":42}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"By using an iterative procedure of the dynamic logic process \"from vague-to-crisp,\" the new tracker overcomes combinatorial complexity of tracking in highly-cluttered scenarios and results in a significant improvement in signal-to-clutter ratio.","_input_hash":1253113625,"_task_hash":-1835798998,"tokens":[{"text":"By","start":0,"end":2,"id":0},{"text":"using","start":3,"end":8,"id":1},{"text":"an","start":9,"end":11,"id":2},{"text":"iterative","start":12,"end":21,"id":3},{"text":"procedure","start":22,"end":31,"id":4},{"text":"of","start":32,"end":34,"id":5},{"text":"the","start":35,"end":38,"id":6},{"text":"dynamic","start":39,"end":46,"id":7},{"text":"logic","start":47,"end":52,"id":8},{"text":"process","start":53,"end":60,"id":9},{"text":"\"","start":61,"end":62,"id":10},{"text":"from","start":62,"end":66,"id":11},{"text":"vague","start":67,"end":72,"id":12},{"text":"-","start":72,"end":73,"id":13},{"text":"to","start":73,"end":75,"id":14},{"text":"-","start":75,"end":76,"id":15},{"text":"crisp","start":76,"end":81,"id":16},{"text":",","start":81,"end":82,"id":17},{"text":"\"","start":82,"end":83,"id":18},{"text":"the","start":84,"end":87,"id":19},{"text":"new","start":88,"end":91,"id":20},{"text":"tracker","start":92,"end":99,"id":21},{"text":"overcomes","start":100,"end":109,"id":22},{"text":"combinatorial","start":110,"end":123,"id":23},{"text":"complexity","start":124,"end":134,"id":24},{"text":"of","start":135,"end":137,"id":25},{"text":"tracking","start":138,"end":146,"id":26},{"text":"in","start":147,"end":149,"id":27},{"text":"highly","start":150,"end":156,"id":28},{"text":"-","start":156,"end":157,"id":29},{"text":"cluttered","start":157,"end":166,"id":30},{"text":"scenarios","start":167,"end":176,"id":31},{"text":"and","start":177,"end":180,"id":32},{"text":"results","start":181,"end":188,"id":33},{"text":"in","start":189,"end":191,"id":34},{"text":"a","start":192,"end":193,"id":35},{"text":"significant","start":194,"end":205,"id":36},{"text":"improvement","start":206,"end":217,"id":37},{"text":"in","start":218,"end":220,"id":38},{"text":"signal","start":221,"end":227,"id":39},{"text":"-","start":227,"end":228,"id":40},{"text":"to","start":228,"end":230,"id":41},{"text":"-","start":230,"end":231,"id":42},{"text":"clutter","start":231,"end":238,"id":43},{"text":"ratio","start":239,"end":244,"id":44},{"text":".","start":244,"end":245,"id":45}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":12,"end":21,"token_start":3,"token_end":3,"label":"ALGO","answer":"reject"}],"answer":"reject"}
{"text":"More importantly, the directed model enables us to perform a principled evaluation.","_input_hash":1384223591,"_task_hash":-1879294105,"tokens":[{"text":"More","start":0,"end":4,"id":0},{"text":"importantly","start":5,"end":16,"id":1},{"text":",","start":16,"end":17,"id":2},{"text":"the","start":18,"end":21,"id":3},{"text":"directed","start":22,"end":30,"id":4},{"text":"model","start":31,"end":36,"id":5},{"text":"enables","start":37,"end":44,"id":6},{"text":"us","start":45,"end":47,"id":7},{"text":"to","start":48,"end":50,"id":8},{"text":"perform","start":51,"end":58,"id":9},{"text":"a","start":59,"end":60,"id":10},{"text":"principled","start":61,"end":71,"id":11},{"text":"evaluation","start":72,"end":82,"id":12},{"text":".","start":82,"end":83,"id":13}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":22,"end":30,"token_start":4,"token_end":4,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We propose a novel algebraic framework for treating probability distributions represented by their cumulants such as the mean and covariance matrix.","_input_hash":1734497419,"_task_hash":-1831029130,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"propose","start":3,"end":10,"id":1},{"text":"a","start":11,"end":12,"id":2},{"text":"novel","start":13,"end":18,"id":3},{"text":"algebraic","start":19,"end":28,"id":4},{"text":"framework","start":29,"end":38,"id":5},{"text":"for","start":39,"end":42,"id":6},{"text":"treating","start":43,"end":51,"id":7},{"text":"probability","start":52,"end":63,"id":8},{"text":"distributions","start":64,"end":77,"id":9},{"text":"represented","start":78,"end":89,"id":10},{"text":"by","start":90,"end":92,"id":11},{"text":"their","start":93,"end":98,"id":12},{"text":"cumulants","start":99,"end":108,"id":13},{"text":"such","start":109,"end":113,"id":14},{"text":"as","start":114,"end":116,"id":15},{"text":"the","start":117,"end":120,"id":16},{"text":"mean","start":121,"end":125,"id":17},{"text":"and","start":126,"end":129,"id":18},{"text":"covariance","start":130,"end":140,"id":19},{"text":"matrix","start":141,"end":147,"id":20},{"text":".","start":147,"end":148,"id":21}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"Second, the ATR per- formance does not deteriorate substantially with increasing bistatic angle.","_input_hash":2145880268,"_task_hash":2068741337,"tokens":[{"text":"Second","start":0,"end":6,"id":0},{"text":",","start":6,"end":7,"id":1},{"text":"the","start":8,"end":11,"id":2},{"text":"ATR","start":12,"end":15,"id":3},{"text":"per-","start":16,"end":20,"id":4},{"text":"formance","start":21,"end":29,"id":5},{"text":"does","start":30,"end":34,"id":6},{"text":"not","start":35,"end":38,"id":7},{"text":"deteriorate","start":39,"end":50,"id":8},{"text":"substantially","start":51,"end":64,"id":9},{"text":"with","start":65,"end":69,"id":10},{"text":"increasing","start":70,"end":80,"id":11},{"text":"bistatic","start":81,"end":89,"id":12},{"text":"angle","start":90,"end":95,"id":13},{"text":".","start":95,"end":96,"id":14}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"these vectors are clustered via minimization of a square error criterion.","_input_hash":-2005237879,"_task_hash":-1490625249,"tokens":[{"text":"these","start":0,"end":5,"id":0},{"text":"vectors","start":6,"end":13,"id":1},{"text":"are","start":14,"end":17,"id":2},{"text":"clustered","start":18,"end":27,"id":3},{"text":"via","start":28,"end":31,"id":4},{"text":"minimization","start":32,"end":44,"id":5},{"text":"of","start":45,"end":47,"id":6},{"text":"a","start":48,"end":49,"id":7},{"text":"square","start":50,"end":56,"id":8},{"text":"error","start":57,"end":62,"id":9},{"text":"criterion","start":63,"end":72,"id":10},{"text":".","start":72,"end":73,"id":11}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"The use of these informational divergences together with the proposed method leads to better results, compared to other clustering methods for the problem of astrophysical data processing.","_input_hash":-1974866019,"_task_hash":368956603,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"use","start":4,"end":7,"id":1},{"text":"of","start":8,"end":10,"id":2},{"text":"these","start":11,"end":16,"id":3},{"text":"informational","start":17,"end":30,"id":4},{"text":"divergences","start":31,"end":42,"id":5},{"text":"together","start":43,"end":51,"id":6},{"text":"with","start":52,"end":56,"id":7},{"text":"the","start":57,"end":60,"id":8},{"text":"proposed","start":61,"end":69,"id":9},{"text":"method","start":70,"end":76,"id":10},{"text":"leads","start":77,"end":82,"id":11},{"text":"to","start":83,"end":85,"id":12},{"text":"better","start":86,"end":92,"id":13},{"text":"results","start":93,"end":100,"id":14},{"text":",","start":100,"end":101,"id":15},{"text":"compared","start":102,"end":110,"id":16},{"text":"to","start":111,"end":113,"id":17},{"text":"other","start":114,"end":119,"id":18},{"text":"clustering","start":120,"end":130,"id":19},{"text":"methods","start":131,"end":138,"id":20},{"text":"for","start":139,"end":142,"id":21},{"text":"the","start":143,"end":146,"id":22},{"text":"problem","start":147,"end":154,"id":23},{"text":"of","start":155,"end":157,"id":24},{"text":"astrophysical","start":158,"end":171,"id":25},{"text":"data","start":172,"end":176,"id":26},{"text":"processing","start":177,"end":187,"id":27},{"text":".","start":187,"end":188,"id":28}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Some results are derived for evaluating the false positive rate of our cluster detection algorithm, with the help of approximations relevant in Euclidean spaces.","_input_hash":809253512,"_task_hash":2043502754,"tokens":[{"text":"Some","start":0,"end":4,"id":0},{"text":"results","start":5,"end":12,"id":1},{"text":"are","start":13,"end":16,"id":2},{"text":"derived","start":17,"end":24,"id":3},{"text":"for","start":25,"end":28,"id":4},{"text":"evaluating","start":29,"end":39,"id":5},{"text":"the","start":40,"end":43,"id":6},{"text":"false","start":44,"end":49,"id":7},{"text":"positive","start":50,"end":58,"id":8},{"text":"rate","start":59,"end":63,"id":9},{"text":"of","start":64,"end":66,"id":10},{"text":"our","start":67,"end":70,"id":11},{"text":"cluster","start":71,"end":78,"id":12},{"text":"detection","start":79,"end":88,"id":13},{"text":"algorithm","start":89,"end":98,"id":14},{"text":",","start":98,"end":99,"id":15},{"text":"with","start":100,"end":104,"id":16},{"text":"the","start":105,"end":108,"id":17},{"text":"help","start":109,"end":113,"id":18},{"text":"of","start":114,"end":116,"id":19},{"text":"approximations","start":117,"end":131,"id":20},{"text":"relevant","start":132,"end":140,"id":21},{"text":"in","start":141,"end":143,"id":22},{"text":"Euclidean","start":144,"end":153,"id":23},{"text":"spaces","start":154,"end":160,"id":24},{"text":".","start":160,"end":161,"id":25}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Both constraint-based and score-based algorithms are implemented, and can use the functionality provided by the snow package to improve their performance via parallel computing.","_input_hash":769924704,"_task_hash":1929571693,"tokens":[{"text":"Both","start":0,"end":4,"id":0},{"text":"constraint","start":5,"end":15,"id":1},{"text":"-","start":15,"end":16,"id":2},{"text":"based","start":16,"end":21,"id":3},{"text":"and","start":22,"end":25,"id":4},{"text":"score","start":26,"end":31,"id":5},{"text":"-","start":31,"end":32,"id":6},{"text":"based","start":32,"end":37,"id":7},{"text":"algorithms","start":38,"end":48,"id":8},{"text":"are","start":49,"end":52,"id":9},{"text":"implemented","start":53,"end":64,"id":10},{"text":",","start":64,"end":65,"id":11},{"text":"and","start":66,"end":69,"id":12},{"text":"can","start":70,"end":73,"id":13},{"text":"use","start":74,"end":77,"id":14},{"text":"the","start":78,"end":81,"id":15},{"text":"functionality","start":82,"end":95,"id":16},{"text":"provided","start":96,"end":104,"id":17},{"text":"by","start":105,"end":107,"id":18},{"text":"the","start":108,"end":111,"id":19},{"text":"snow","start":112,"end":116,"id":20},{"text":"package","start":117,"end":124,"id":21},{"text":"to","start":125,"end":127,"id":22},{"text":"improve","start":128,"end":135,"id":23},{"text":"their","start":136,"end":141,"id":24},{"text":"performance","start":142,"end":153,"id":25},{"text":"via","start":154,"end":157,"id":26},{"text":"parallel","start":158,"end":166,"id":27},{"text":"computing","start":167,"end":176,"id":28},{"text":".","start":176,"end":177,"id":29}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"the first component is a uniform distribution on that ball representing outliers and the other K components are uniform distributions along K d-dimensional linear subspaces restricted to that ball.","_input_hash":1359244882,"_task_hash":1340857627,"tokens":[{"text":"the","start":0,"end":3,"id":0},{"text":"first","start":4,"end":9,"id":1},{"text":"component","start":10,"end":19,"id":2},{"text":"is","start":20,"end":22,"id":3},{"text":"a","start":23,"end":24,"id":4},{"text":"uniform","start":25,"end":32,"id":5},{"text":"distribution","start":33,"end":45,"id":6},{"text":"on","start":46,"end":48,"id":7},{"text":"that","start":49,"end":53,"id":8},{"text":"ball","start":54,"end":58,"id":9},{"text":"representing","start":59,"end":71,"id":10},{"text":"outliers","start":72,"end":80,"id":11},{"text":"and","start":81,"end":84,"id":12},{"text":"the","start":85,"end":88,"id":13},{"text":"other","start":89,"end":94,"id":14},{"text":"K","start":95,"end":96,"id":15},{"text":"components","start":97,"end":107,"id":16},{"text":"are","start":108,"end":111,"id":17},{"text":"uniform","start":112,"end":119,"id":18},{"text":"distributions","start":120,"end":133,"id":19},{"text":"along","start":134,"end":139,"id":20},{"text":"K","start":140,"end":141,"id":21},{"text":"d","start":142,"end":143,"id":22},{"text":"-","start":143,"end":144,"id":23},{"text":"dimensional","start":144,"end":155,"id":24},{"text":"linear","start":156,"end":162,"id":25},{"text":"subspaces","start":163,"end":172,"id":26},{"text":"restricted","start":173,"end":183,"id":27},{"text":"to","start":184,"end":186,"id":28},{"text":"that","start":187,"end":191,"id":29},{"text":"ball","start":192,"end":196,"id":30},{"text":".","start":196,"end":197,"id":31}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"This analysis reveals that, by tuning the parameters of the mixing pdf different penalty functions are invoked depending on the estimation type used, the value of the noise variance, and whether real or complex signals are estimated.","_input_hash":572900240,"_task_hash":-1729733639,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"analysis","start":5,"end":13,"id":1},{"text":"reveals","start":14,"end":21,"id":2},{"text":"that","start":22,"end":26,"id":3},{"text":",","start":26,"end":27,"id":4},{"text":"by","start":28,"end":30,"id":5},{"text":"tuning","start":31,"end":37,"id":6},{"text":"the","start":38,"end":41,"id":7},{"text":"parameters","start":42,"end":52,"id":8},{"text":"of","start":53,"end":55,"id":9},{"text":"the","start":56,"end":59,"id":10},{"text":"mixing","start":60,"end":66,"id":11},{"text":"pdf","start":67,"end":70,"id":12},{"text":"different","start":71,"end":80,"id":13},{"text":"penalty","start":81,"end":88,"id":14},{"text":"functions","start":89,"end":98,"id":15},{"text":"are","start":99,"end":102,"id":16},{"text":"invoked","start":103,"end":110,"id":17},{"text":"depending","start":111,"end":120,"id":18},{"text":"on","start":121,"end":123,"id":19},{"text":"the","start":124,"end":127,"id":20},{"text":"estimation","start":128,"end":138,"id":21},{"text":"type","start":139,"end":143,"id":22},{"text":"used","start":144,"end":148,"id":23},{"text":",","start":148,"end":149,"id":24},{"text":"the","start":150,"end":153,"id":25},{"text":"value","start":154,"end":159,"id":26},{"text":"of","start":160,"end":162,"id":27},{"text":"the","start":163,"end":166,"id":28},{"text":"noise","start":167,"end":172,"id":29},{"text":"variance","start":173,"end":181,"id":30},{"text":",","start":181,"end":182,"id":31},{"text":"and","start":183,"end":186,"id":32},{"text":"whether","start":187,"end":194,"id":33},{"text":"real","start":195,"end":199,"id":34},{"text":"or","start":200,"end":202,"id":35},{"text":"complex","start":203,"end":210,"id":36},{"text":"signals","start":211,"end":218,"id":37},{"text":"are","start":219,"end":222,"id":38},{"text":"estimated","start":223,"end":232,"id":39},{"text":".","start":232,"end":233,"id":40}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"In recent years, kernel density estimation has been exploited by computer scientists to model machine learning problems.","_input_hash":-74833356,"_task_hash":-46186151,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"recent","start":3,"end":9,"id":1},{"text":"years","start":10,"end":15,"id":2},{"text":",","start":15,"end":16,"id":3},{"text":"kernel","start":17,"end":23,"id":4},{"text":"density","start":24,"end":31,"id":5},{"text":"estimation","start":32,"end":42,"id":6},{"text":"has","start":43,"end":46,"id":7},{"text":"been","start":47,"end":51,"id":8},{"text":"exploited","start":52,"end":61,"id":9},{"text":"by","start":62,"end":64,"id":10},{"text":"computer","start":65,"end":73,"id":11},{"text":"scientists","start":74,"end":84,"id":12},{"text":"to","start":85,"end":87,"id":13},{"text":"model","start":88,"end":93,"id":14},{"text":"machine","start":94,"end":101,"id":15},{"text":"learning","start":102,"end":110,"id":16},{"text":"problems","start":111,"end":119,"id":17},{"text":".","start":119,"end":120,"id":18}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":85,"end":87,"token_start":13,"token_end":13,"label":"ALGO","answer":"reject"},{"start":88,"end":93,"token_start":14,"token_end":14,"label":"ALGO","answer":"reject"},{"start":94,"end":110,"token_start":15,"token_end":16,"label":"ALGO","answer":"reject"},{"start":111,"end":119,"token_start":17,"token_end":17,"label":"ALGO","answer":"reject"}],"answer":"reject"}
{"text":"Despite its simplicity, we show that it outperforms eight other entropy estimation procedures across a diverse range of sampling scenarios and data-generating models, even in cases of severe undersampling.","_input_hash":1789338829,"_task_hash":-1307448222,"tokens":[{"text":"Despite","start":0,"end":7,"id":0},{"text":"its","start":8,"end":11,"id":1},{"text":"simplicity","start":12,"end":22,"id":2},{"text":",","start":22,"end":23,"id":3},{"text":"we","start":24,"end":26,"id":4},{"text":"show","start":27,"end":31,"id":5},{"text":"that","start":32,"end":36,"id":6},{"text":"it","start":37,"end":39,"id":7},{"text":"outperforms","start":40,"end":51,"id":8},{"text":"eight","start":52,"end":57,"id":9},{"text":"other","start":58,"end":63,"id":10},{"text":"entropy","start":64,"end":71,"id":11},{"text":"estimation","start":72,"end":82,"id":12},{"text":"procedures","start":83,"end":93,"id":13},{"text":"across","start":94,"end":100,"id":14},{"text":"a","start":101,"end":102,"id":15},{"text":"diverse","start":103,"end":110,"id":16},{"text":"range","start":111,"end":116,"id":17},{"text":"of","start":117,"end":119,"id":18},{"text":"sampling","start":120,"end":128,"id":19},{"text":"scenarios","start":129,"end":138,"id":20},{"text":"and","start":139,"end":142,"id":21},{"text":"data","start":143,"end":147,"id":22},{"text":"-","start":147,"end":148,"id":23},{"text":"generating","start":148,"end":158,"id":24},{"text":"models","start":159,"end":165,"id":25},{"text":",","start":165,"end":166,"id":26},{"text":"even","start":167,"end":171,"id":27},{"text":"in","start":172,"end":174,"id":28},{"text":"cases","start":175,"end":180,"id":29},{"text":"of","start":181,"end":183,"id":30},{"text":"severe","start":184,"end":190,"id":31},{"text":"undersampling","start":191,"end":204,"id":32},{"text":".","start":204,"end":205,"id":33}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"We consider applications to topic modeling and derive a variational inference algorithm for approximate posterior inference.","_input_hash":1322577803,"_task_hash":1410465952,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"consider","start":3,"end":11,"id":1},{"text":"applications","start":12,"end":24,"id":2},{"text":"to","start":25,"end":27,"id":3},{"text":"topic","start":28,"end":33,"id":4},{"text":"modeling","start":34,"end":42,"id":5},{"text":"and","start":43,"end":46,"id":6},{"text":"derive","start":47,"end":53,"id":7},{"text":"a","start":54,"end":55,"id":8},{"text":"variational","start":56,"end":67,"id":9},{"text":"inference","start":68,"end":77,"id":10},{"text":"algorithm","start":78,"end":87,"id":11},{"text":"for","start":88,"end":91,"id":12},{"text":"approximate","start":92,"end":103,"id":13},{"text":"posterior","start":104,"end":113,"id":14},{"text":"inference","start":114,"end":123,"id":15},{"text":".","start":123,"end":124,"id":16}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":28,"end":33,"token_start":4,"token_end":4,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"In addition to presenting the results in a slightly informal but accessible way, we relate them to each other and discuss their different implications.","_input_hash":149522558,"_task_hash":2025523720,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"addition","start":3,"end":11,"id":1},{"text":"to","start":12,"end":14,"id":2},{"text":"presenting","start":15,"end":25,"id":3},{"text":"the","start":26,"end":29,"id":4},{"text":"results","start":30,"end":37,"id":5},{"text":"in","start":38,"end":40,"id":6},{"text":"a","start":41,"end":42,"id":7},{"text":"slightly","start":43,"end":51,"id":8},{"text":"informal","start":52,"end":60,"id":9},{"text":"but","start":61,"end":64,"id":10},{"text":"accessible","start":65,"end":75,"id":11},{"text":"way","start":76,"end":79,"id":12},{"text":",","start":79,"end":80,"id":13},{"text":"we","start":81,"end":83,"id":14},{"text":"relate","start":84,"end":90,"id":15},{"text":"them","start":91,"end":95,"id":16},{"text":"to","start":96,"end":98,"id":17},{"text":"each","start":99,"end":103,"id":18},{"text":"other","start":104,"end":109,"id":19},{"text":"and","start":110,"end":113,"id":20},{"text":"discuss","start":114,"end":121,"id":21},{"text":"their","start":122,"end":127,"id":22},{"text":"different","start":128,"end":137,"id":23},{"text":"implications","start":138,"end":150,"id":24},{"text":".","start":150,"end":151,"id":25}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"  \\bigskip   \\noindent An interesting consequence is that the probability upper bound is bounded by the minimum of a Hoeffding-type bound and a Vapnik-type bounds, and thus is smaller than 1 even for small learning set.","_input_hash":-505626976,"_task_hash":1809966401,"tokens":[{"text":"  ","start":0,"end":2,"id":0},{"text":"\\bigskip","start":2,"end":10,"id":1},{"text":"  ","start":11,"end":13,"id":2},{"text":"\\noindent","start":13,"end":22,"id":3},{"text":"An","start":23,"end":25,"id":4},{"text":"interesting","start":26,"end":37,"id":5},{"text":"consequence","start":38,"end":49,"id":6},{"text":"is","start":50,"end":52,"id":7},{"text":"that","start":53,"end":57,"id":8},{"text":"the","start":58,"end":61,"id":9},{"text":"probability","start":62,"end":73,"id":10},{"text":"upper","start":74,"end":79,"id":11},{"text":"bound","start":80,"end":85,"id":12},{"text":"is","start":86,"end":88,"id":13},{"text":"bounded","start":89,"end":96,"id":14},{"text":"by","start":97,"end":99,"id":15},{"text":"the","start":100,"end":103,"id":16},{"text":"minimum","start":104,"end":111,"id":17},{"text":"of","start":112,"end":114,"id":18},{"text":"a","start":115,"end":116,"id":19},{"text":"Hoeffding","start":117,"end":126,"id":20},{"text":"-","start":126,"end":127,"id":21},{"text":"type","start":127,"end":131,"id":22},{"text":"bound","start":132,"end":137,"id":23},{"text":"and","start":138,"end":141,"id":24},{"text":"a","start":142,"end":143,"id":25},{"text":"Vapnik","start":144,"end":150,"id":26},{"text":"-","start":150,"end":151,"id":27},{"text":"type","start":151,"end":155,"id":28},{"text":"bounds","start":156,"end":162,"id":29},{"text":",","start":162,"end":163,"id":30},{"text":"and","start":164,"end":167,"id":31},{"text":"thus","start":168,"end":172,"id":32},{"text":"is","start":173,"end":175,"id":33},{"text":"smaller","start":176,"end":183,"id":34},{"text":"than","start":184,"end":188,"id":35},{"text":"1","start":189,"end":190,"id":36},{"text":"even","start":191,"end":195,"id":37},{"text":"for","start":196,"end":199,"id":38},{"text":"small","start":200,"end":205,"id":39},{"text":"learning","start":206,"end":214,"id":40},{"text":"set","start":215,"end":218,"id":41},{"text":".","start":218,"end":219,"id":42}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We prove that whenever the joint distribution $\\prob^{(X,Y)}$ admits such a model in one direction, e.g. $Y=f(X)+N, N \\independent X$, it does not admit the reversed model $X=g(Y)+\\tilde N, \\tilde N \\independent Y$ as long as the model is chosen in a generic way.","_input_hash":-987262579,"_task_hash":317098475,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"prove","start":3,"end":8,"id":1},{"text":"that","start":9,"end":13,"id":2},{"text":"whenever","start":14,"end":22,"id":3},{"text":"the","start":23,"end":26,"id":4},{"text":"joint","start":27,"end":32,"id":5},{"text":"distribution","start":33,"end":45,"id":6},{"text":"$","start":46,"end":47,"id":7},{"text":"\\prob^{(X","start":47,"end":56,"id":8},{"text":",","start":56,"end":57,"id":9},{"text":"Y)}$","start":57,"end":61,"id":10},{"text":"admits","start":62,"end":68,"id":11},{"text":"such","start":69,"end":73,"id":12},{"text":"a","start":74,"end":75,"id":13},{"text":"model","start":76,"end":81,"id":14},{"text":"in","start":82,"end":84,"id":15},{"text":"one","start":85,"end":88,"id":16},{"text":"direction","start":89,"end":98,"id":17},{"text":",","start":98,"end":99,"id":18},{"text":"e.g.","start":100,"end":104,"id":19},{"text":"$","start":105,"end":106,"id":20},{"text":"Y","start":106,"end":107,"id":21},{"text":"=","start":107,"end":108,"id":22},{"text":"f(X)+N","start":108,"end":114,"id":23},{"text":",","start":114,"end":115,"id":24},{"text":"N","start":116,"end":117,"id":25},{"text":"\\independent","start":118,"end":130,"id":26},{"text":"X$","start":131,"end":133,"id":27},{"text":",","start":133,"end":134,"id":28},{"text":"it","start":135,"end":137,"id":29},{"text":"does","start":138,"end":142,"id":30},{"text":"not","start":143,"end":146,"id":31},{"text":"admit","start":147,"end":152,"id":32},{"text":"the","start":153,"end":156,"id":33},{"text":"reversed","start":157,"end":165,"id":34},{"text":"model","start":166,"end":171,"id":35},{"text":"$","start":172,"end":173,"id":36},{"text":"X","start":173,"end":174,"id":37},{"text":"=","start":174,"end":175,"id":38},{"text":"g(Y)+\\tilde","start":175,"end":186,"id":39},{"text":"N","start":187,"end":188,"id":40},{"text":",","start":188,"end":189,"id":41},{"text":"\\tilde","start":190,"end":196,"id":42},{"text":"N","start":197,"end":198,"id":43},{"text":"\\independent","start":199,"end":211,"id":44},{"text":"Y$","start":212,"end":214,"id":45},{"text":"as","start":215,"end":217,"id":46},{"text":"long","start":218,"end":222,"id":47},{"text":"as","start":223,"end":225,"id":48},{"text":"the","start":226,"end":229,"id":49},{"text":"model","start":230,"end":235,"id":50},{"text":"is","start":236,"end":238,"id":51},{"text":"chosen","start":239,"end":245,"id":52},{"text":"in","start":246,"end":248,"id":53},{"text":"a","start":249,"end":250,"id":54},{"text":"generic","start":251,"end":258,"id":55},{"text":"way","start":259,"end":262,"id":56},{"text":".","start":262,"end":263,"id":57}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Simulation results demonstrate the utility of KNIFE for both kernel regression and support vector machines with a variety of kernels.","_input_hash":-111780878,"_task_hash":1756227829,"tokens":[{"text":"Simulation","start":0,"end":10,"id":0},{"text":"results","start":11,"end":18,"id":1},{"text":"demonstrate","start":19,"end":30,"id":2},{"text":"the","start":31,"end":34,"id":3},{"text":"utility","start":35,"end":42,"id":4},{"text":"of","start":43,"end":45,"id":5},{"text":"KNIFE","start":46,"end":51,"id":6},{"text":"for","start":52,"end":55,"id":7},{"text":"both","start":56,"end":60,"id":8},{"text":"kernel","start":61,"end":67,"id":9},{"text":"regression","start":68,"end":78,"id":10},{"text":"and","start":79,"end":82,"id":11},{"text":"support","start":83,"end":90,"id":12},{"text":"vector","start":91,"end":97,"id":13},{"text":"machines","start":98,"end":106,"id":14},{"text":"with","start":107,"end":111,"id":15},{"text":"a","start":112,"end":113,"id":16},{"text":"variety","start":114,"end":121,"id":17},{"text":"of","start":122,"end":124,"id":18},{"text":"kernels","start":125,"end":132,"id":19},{"text":".","start":132,"end":133,"id":20}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":46,"end":51,"token_start":6,"token_end":6,"label":"ALGO","answer":"accept"},{"start":61,"end":78,"token_start":9,"token_end":10,"label":"ALGO","answer":"accept"},{"start":83,"end":106,"token_start":12,"token_end":14,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"This norm leads to regularized problems that are difficult to optimize, and we propose in this paper efficient algorithms for solving them.","_input_hash":677465210,"_task_hash":-917694432,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"norm","start":5,"end":9,"id":1},{"text":"leads","start":10,"end":15,"id":2},{"text":"to","start":16,"end":18,"id":3},{"text":"regularized","start":19,"end":30,"id":4},{"text":"problems","start":31,"end":39,"id":5},{"text":"that","start":40,"end":44,"id":6},{"text":"are","start":45,"end":48,"id":7},{"text":"difficult","start":49,"end":58,"id":8},{"text":"to","start":59,"end":61,"id":9},{"text":"optimize","start":62,"end":70,"id":10},{"text":",","start":70,"end":71,"id":11},{"text":"and","start":72,"end":75,"id":12},{"text":"we","start":76,"end":78,"id":13},{"text":"propose","start":79,"end":86,"id":14},{"text":"in","start":87,"end":89,"id":15},{"text":"this","start":90,"end":94,"id":16},{"text":"paper","start":95,"end":100,"id":17},{"text":"efficient","start":101,"end":110,"id":18},{"text":"algorithms","start":111,"end":121,"id":19},{"text":"for","start":122,"end":125,"id":20},{"text":"solving","start":126,"end":133,"id":21},{"text":"them","start":134,"end":138,"id":22},{"text":".","start":138,"end":139,"id":23}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":95,"end":110,"token_start":17,"token_end":18,"label":"ALGO","answer":"reject"}],"answer":"reject"}
{"text":"Estimating a sparsity constrained inverse covariance matrix is a key component in Gaussian graphical model learning, but one that is numerically very challenging.","_input_hash":-745105841,"_task_hash":649789695,"tokens":[{"text":"Estimating","start":0,"end":10,"id":0},{"text":"a","start":11,"end":12,"id":1},{"text":"sparsity","start":13,"end":21,"id":2},{"text":"constrained","start":22,"end":33,"id":3},{"text":"inverse","start":34,"end":41,"id":4},{"text":"covariance","start":42,"end":52,"id":5},{"text":"matrix","start":53,"end":59,"id":6},{"text":"is","start":60,"end":62,"id":7},{"text":"a","start":63,"end":64,"id":8},{"text":"key","start":65,"end":68,"id":9},{"text":"component","start":69,"end":78,"id":10},{"text":"in","start":79,"end":81,"id":11},{"text":"Gaussian","start":82,"end":90,"id":12},{"text":"graphical","start":91,"end":100,"id":13},{"text":"model","start":101,"end":106,"id":14},{"text":"learning","start":107,"end":115,"id":15},{"text":",","start":115,"end":116,"id":16},{"text":"but","start":117,"end":120,"id":17},{"text":"one","start":121,"end":124,"id":18},{"text":"that","start":125,"end":129,"id":19},{"text":"is","start":130,"end":132,"id":20},{"text":"numerically","start":133,"end":144,"id":21},{"text":"very","start":145,"end":149,"id":22},{"text":"challenging","start":150,"end":161,"id":23},{"text":".","start":161,"end":162,"id":24}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":82,"end":100,"token_start":12,"token_end":13,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"While frequentist methods have yielded online filtering and prediction techniques, most Bayesian papers have focused on the retrospective segmentation problem.","_input_hash":-1258359525,"_task_hash":-716690733,"tokens":[{"text":"While","start":0,"end":5,"id":0},{"text":"frequentist","start":6,"end":17,"id":1},{"text":"methods","start":18,"end":25,"id":2},{"text":"have","start":26,"end":30,"id":3},{"text":"yielded","start":31,"end":38,"id":4},{"text":"online","start":39,"end":45,"id":5},{"text":"filtering","start":46,"end":55,"id":6},{"text":"and","start":56,"end":59,"id":7},{"text":"prediction","start":60,"end":70,"id":8},{"text":"techniques","start":71,"end":81,"id":9},{"text":",","start":81,"end":82,"id":10},{"text":"most","start":83,"end":87,"id":11},{"text":"Bayesian","start":88,"end":96,"id":12},{"text":"papers","start":97,"end":103,"id":13},{"text":"have","start":104,"end":108,"id":14},{"text":"focused","start":109,"end":116,"id":15},{"text":"on","start":117,"end":119,"id":16},{"text":"the","start":120,"end":123,"id":17},{"text":"retrospective","start":124,"end":137,"id":18},{"text":"segmentation","start":138,"end":150,"id":19},{"text":"problem","start":151,"end":158,"id":20},{"text":".","start":158,"end":159,"id":21}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":6,"end":17,"token_start":1,"token_end":1,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"The consistent block assignment makes possible consistent parameter estimation for a stochastic blockmodel.","_input_hash":-184342798,"_task_hash":1243546129,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"consistent","start":4,"end":14,"id":1},{"text":"block","start":15,"end":20,"id":2},{"text":"assignment","start":21,"end":31,"id":3},{"text":"makes","start":32,"end":37,"id":4},{"text":"possible","start":38,"end":46,"id":5},{"text":"consistent","start":47,"end":57,"id":6},{"text":"parameter","start":58,"end":67,"id":7},{"text":"estimation","start":68,"end":78,"id":8},{"text":"for","start":79,"end":82,"id":9},{"text":"a","start":83,"end":84,"id":10},{"text":"stochastic","start":85,"end":95,"id":11},{"text":"blockmodel","start":96,"end":106,"id":12},{"text":".","start":106,"end":107,"id":13}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"For density estimation, we do not assume the true distribution corresponds to a forest;","_input_hash":-1307895784,"_task_hash":638218156,"tokens":[{"text":"For","start":0,"end":3,"id":0},{"text":"density","start":4,"end":11,"id":1},{"text":"estimation","start":12,"end":22,"id":2},{"text":",","start":22,"end":23,"id":3},{"text":"we","start":24,"end":26,"id":4},{"text":"do","start":27,"end":29,"id":5},{"text":"not","start":30,"end":33,"id":6},{"text":"assume","start":34,"end":40,"id":7},{"text":"the","start":41,"end":44,"id":8},{"text":"true","start":45,"end":49,"id":9},{"text":"distribution","start":50,"end":62,"id":10},{"text":"corresponds","start":63,"end":74,"id":11},{"text":"to","start":75,"end":77,"id":12},{"text":"a","start":78,"end":79,"id":13},{"text":"forest","start":80,"end":86,"id":14},{"text":";","start":86,"end":87,"id":15}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":80,"end":86,"token_start":14,"token_end":14,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"  High dimensional features also arise when we consider high-order interactions.","_input_hash":-910918860,"_task_hash":852057726,"tokens":[{"text":"  ","start":0,"end":2,"id":0},{"text":"High","start":2,"end":6,"id":1},{"text":"dimensional","start":7,"end":18,"id":2},{"text":"features","start":19,"end":27,"id":3},{"text":"also","start":28,"end":32,"id":4},{"text":"arise","start":33,"end":38,"id":5},{"text":"when","start":39,"end":43,"id":6},{"text":"we","start":44,"end":46,"id":7},{"text":"consider","start":47,"end":55,"id":8},{"text":"high","start":56,"end":60,"id":9},{"text":"-","start":60,"end":61,"id":10},{"text":"order","start":61,"end":66,"id":11},{"text":"interactions","start":67,"end":79,"id":12},{"text":".","start":79,"end":80,"id":13}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"The cellular automata network simulates the higher level integration of information acquired from the independent learning trials.","_input_hash":-1149983967,"_task_hash":5255362,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"cellular","start":4,"end":12,"id":1},{"text":"automata","start":13,"end":21,"id":2},{"text":"network","start":22,"end":29,"id":3},{"text":"simulates","start":30,"end":39,"id":4},{"text":"the","start":40,"end":43,"id":5},{"text":"higher","start":44,"end":50,"id":6},{"text":"level","start":51,"end":56,"id":7},{"text":"integration","start":57,"end":68,"id":8},{"text":"of","start":69,"end":71,"id":9},{"text":"information","start":72,"end":83,"id":10},{"text":"acquired","start":84,"end":92,"id":11},{"text":"from","start":93,"end":97,"id":12},{"text":"the","start":98,"end":101,"id":13},{"text":"independent","start":102,"end":113,"id":14},{"text":"learning","start":114,"end":122,"id":15},{"text":"trials","start":123,"end":129,"id":16},{"text":".","start":129,"end":130,"id":17}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"However, the results are very technical and difficult to interpret for non-experts.","_input_hash":-1141677545,"_task_hash":-1391769287,"tokens":[{"text":"However","start":0,"end":7,"id":0},{"text":",","start":7,"end":8,"id":1},{"text":"the","start":9,"end":12,"id":2},{"text":"results","start":13,"end":20,"id":3},{"text":"are","start":21,"end":24,"id":4},{"text":"very","start":25,"end":29,"id":5},{"text":"technical","start":30,"end":39,"id":6},{"text":"and","start":40,"end":43,"id":7},{"text":"difficult","start":44,"end":53,"id":8},{"text":"to","start":54,"end":56,"id":9},{"text":"interpret","start":57,"end":66,"id":10},{"text":"for","start":67,"end":70,"id":11},{"text":"non","start":71,"end":74,"id":12},{"text":"-","start":74,"end":75,"id":13},{"text":"experts","start":75,"end":82,"id":14},{"text":".","start":82,"end":83,"id":15}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"Moreover, this causes LLE to converge to a linear projection of the input, as opposed to its non-linear embedding goal.","_input_hash":1855610389,"_task_hash":1917608877,"tokens":[{"text":"Moreover","start":0,"end":8,"id":0},{"text":",","start":8,"end":9,"id":1},{"text":"this","start":10,"end":14,"id":2},{"text":"causes","start":15,"end":21,"id":3},{"text":"LLE","start":22,"end":25,"id":4},{"text":"to","start":26,"end":28,"id":5},{"text":"converge","start":29,"end":37,"id":6},{"text":"to","start":38,"end":40,"id":7},{"text":"a","start":41,"end":42,"id":8},{"text":"linear","start":43,"end":49,"id":9},{"text":"projection","start":50,"end":60,"id":10},{"text":"of","start":61,"end":63,"id":11},{"text":"the","start":64,"end":67,"id":12},{"text":"input","start":68,"end":73,"id":13},{"text":",","start":73,"end":74,"id":14},{"text":"as","start":75,"end":77,"id":15},{"text":"opposed","start":78,"end":85,"id":16},{"text":"to","start":86,"end":88,"id":17},{"text":"its","start":89,"end":92,"id":18},{"text":"non","start":93,"end":96,"id":19},{"text":"-","start":96,"end":97,"id":20},{"text":"linear","start":97,"end":103,"id":21},{"text":"embedding","start":104,"end":113,"id":22},{"text":"goal","start":114,"end":118,"id":23},{"text":".","start":118,"end":119,"id":24}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":22,"end":25,"token_start":4,"token_end":4,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Autoencoders, in particular, have proven to be an effective way to learn latent codes that reflect meaningful variations in data.","_input_hash":1557197803,"_task_hash":1806751075,"tokens":[{"text":"Autoencoders","start":0,"end":12,"id":0},{"text":",","start":12,"end":13,"id":1},{"text":"in","start":14,"end":16,"id":2},{"text":"particular","start":17,"end":27,"id":3},{"text":",","start":27,"end":28,"id":4},{"text":"have","start":29,"end":33,"id":5},{"text":"proven","start":34,"end":40,"id":6},{"text":"to","start":41,"end":43,"id":7},{"text":"be","start":44,"end":46,"id":8},{"text":"an","start":47,"end":49,"id":9},{"text":"effective","start":50,"end":59,"id":10},{"text":"way","start":60,"end":63,"id":11},{"text":"to","start":64,"end":66,"id":12},{"text":"learn","start":67,"end":72,"id":13},{"text":"latent","start":73,"end":79,"id":14},{"text":"codes","start":80,"end":85,"id":15},{"text":"that","start":86,"end":90,"id":16},{"text":"reflect","start":91,"end":98,"id":17},{"text":"meaningful","start":99,"end":109,"id":18},{"text":"variations","start":110,"end":120,"id":19},{"text":"in","start":121,"end":123,"id":20},{"text":"data","start":124,"end":128,"id":21},{"text":".","start":128,"end":129,"id":22}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":0,"end":12,"token_start":0,"token_end":0,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"The finite sample performance of S-OMP is demonstrated on extensive simulation studies, and on a genetic association mapping problem.","_input_hash":1569308981,"_task_hash":-455559364,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"finite","start":4,"end":10,"id":1},{"text":"sample","start":11,"end":17,"id":2},{"text":"performance","start":18,"end":29,"id":3},{"text":"of","start":30,"end":32,"id":4},{"text":"S","start":33,"end":34,"id":5},{"text":"-","start":34,"end":35,"id":6},{"text":"OMP","start":35,"end":38,"id":7},{"text":"is","start":39,"end":41,"id":8},{"text":"demonstrated","start":42,"end":54,"id":9},{"text":"on","start":55,"end":57,"id":10},{"text":"extensive","start":58,"end":67,"id":11},{"text":"simulation","start":68,"end":78,"id":12},{"text":"studies","start":79,"end":86,"id":13},{"text":",","start":86,"end":87,"id":14},{"text":"and","start":88,"end":91,"id":15},{"text":"on","start":92,"end":94,"id":16},{"text":"a","start":95,"end":96,"id":17},{"text":"genetic","start":97,"end":104,"id":18},{"text":"association","start":105,"end":116,"id":19},{"text":"mapping","start":117,"end":124,"id":20},{"text":"problem","start":125,"end":132,"id":21},{"text":".","start":132,"end":133,"id":22}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We show some experimental results, which confirm the theory.","_input_hash":1008824299,"_task_hash":-1317059563,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"show","start":3,"end":7,"id":1},{"text":"some","start":8,"end":12,"id":2},{"text":"experimental","start":13,"end":25,"id":3},{"text":"results","start":26,"end":33,"id":4},{"text":",","start":33,"end":34,"id":5},{"text":"which","start":35,"end":40,"id":6},{"text":"confirm","start":41,"end":48,"id":7},{"text":"the","start":49,"end":52,"id":8},{"text":"theory","start":53,"end":59,"id":9},{"text":".","start":59,"end":60,"id":10}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"The main drawbacks of this approach are the associated computational and storage demands.","_input_hash":-1668893154,"_task_hash":-1659586906,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"main","start":4,"end":8,"id":1},{"text":"drawbacks","start":9,"end":18,"id":2},{"text":"of","start":19,"end":21,"id":3},{"text":"this","start":22,"end":26,"id":4},{"text":"approach","start":27,"end":35,"id":5},{"text":"are","start":36,"end":39,"id":6},{"text":"the","start":40,"end":43,"id":7},{"text":"associated","start":44,"end":54,"id":8},{"text":"computational","start":55,"end":68,"id":9},{"text":"and","start":69,"end":72,"id":10},{"text":"storage","start":73,"end":80,"id":11},{"text":"demands","start":81,"end":88,"id":12},{"text":".","start":88,"end":89,"id":13}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"The new parameter is obtained by \"thickening up\" single points in the definition of VC dimension to uncountable \"clusters\".","_input_hash":-608989572,"_task_hash":-968897345,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"new","start":4,"end":7,"id":1},{"text":"parameter","start":8,"end":17,"id":2},{"text":"is","start":18,"end":20,"id":3},{"text":"obtained","start":21,"end":29,"id":4},{"text":"by","start":30,"end":32,"id":5},{"text":"\"","start":33,"end":34,"id":6},{"text":"thickening","start":34,"end":44,"id":7},{"text":"up","start":45,"end":47,"id":8},{"text":"\"","start":47,"end":48,"id":9},{"text":"single","start":49,"end":55,"id":10},{"text":"points","start":56,"end":62,"id":11},{"text":"in","start":63,"end":65,"id":12},{"text":"the","start":66,"end":69,"id":13},{"text":"definition","start":70,"end":80,"id":14},{"text":"of","start":81,"end":83,"id":15},{"text":"VC","start":84,"end":86,"id":16},{"text":"dimension","start":87,"end":96,"id":17},{"text":"to","start":97,"end":99,"id":18},{"text":"uncountable","start":100,"end":111,"id":19},{"text":"\"","start":112,"end":113,"id":20},{"text":"clusters","start":113,"end":121,"id":21},{"text":"\"","start":121,"end":122,"id":22},{"text":".","start":122,"end":123,"id":23}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"Specifically, we develop a James-Stein-type shrinkage estimator, resulting in a procedure that is highly efficient statistically as well as computationally.","_input_hash":-358073182,"_task_hash":1281824272,"tokens":[{"text":"Specifically","start":0,"end":12,"id":0},{"text":",","start":12,"end":13,"id":1},{"text":"we","start":14,"end":16,"id":2},{"text":"develop","start":17,"end":24,"id":3},{"text":"a","start":25,"end":26,"id":4},{"text":"James","start":27,"end":32,"id":5},{"text":"-","start":32,"end":33,"id":6},{"text":"Stein","start":33,"end":38,"id":7},{"text":"-","start":38,"end":39,"id":8},{"text":"type","start":39,"end":43,"id":9},{"text":"shrinkage","start":44,"end":53,"id":10},{"text":"estimator","start":54,"end":63,"id":11},{"text":",","start":63,"end":64,"id":12},{"text":"resulting","start":65,"end":74,"id":13},{"text":"in","start":75,"end":77,"id":14},{"text":"a","start":78,"end":79,"id":15},{"text":"procedure","start":80,"end":89,"id":16},{"text":"that","start":90,"end":94,"id":17},{"text":"is","start":95,"end":97,"id":18},{"text":"highly","start":98,"end":104,"id":19},{"text":"efficient","start":105,"end":114,"id":20},{"text":"statistically","start":115,"end":128,"id":21},{"text":"as","start":129,"end":131,"id":22},{"text":"well","start":132,"end":136,"id":23},{"text":"as","start":137,"end":139,"id":24},{"text":"computationally","start":140,"end":155,"id":25},{"text":".","start":155,"end":156,"id":26}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"Specifically, the penalization we use is constructed from a tree that is obtained by spatially-constrained agglomerative clustering.","_input_hash":1483623304,"_task_hash":-2084755741,"tokens":[{"text":"Specifically","start":0,"end":12,"id":0},{"text":",","start":12,"end":13,"id":1},{"text":"the","start":14,"end":17,"id":2},{"text":"penalization","start":18,"end":30,"id":3},{"text":"we","start":31,"end":33,"id":4},{"text":"use","start":34,"end":37,"id":5},{"text":"is","start":38,"end":40,"id":6},{"text":"constructed","start":41,"end":52,"id":7},{"text":"from","start":53,"end":57,"id":8},{"text":"a","start":58,"end":59,"id":9},{"text":"tree","start":60,"end":64,"id":10},{"text":"that","start":65,"end":69,"id":11},{"text":"is","start":70,"end":72,"id":12},{"text":"obtained","start":73,"end":81,"id":13},{"text":"by","start":82,"end":84,"id":14},{"text":"spatially","start":85,"end":94,"id":15},{"text":"-","start":94,"end":95,"id":16},{"text":"constrained","start":95,"end":106,"id":17},{"text":"agglomerative","start":107,"end":120,"id":18},{"text":"clustering","start":121,"end":131,"id":19},{"text":".","start":131,"end":132,"id":20}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":85,"end":131,"token_start":15,"token_end":19,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We show that within this framework, one can prove a theorem analogous to one of J. Kleinberg, in which one obtains an existence and uniqueness theorem instead of a non-existence result.","_input_hash":-874321344,"_task_hash":-1692011917,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"show","start":3,"end":7,"id":1},{"text":"that","start":8,"end":12,"id":2},{"text":"within","start":13,"end":19,"id":3},{"text":"this","start":20,"end":24,"id":4},{"text":"framework","start":25,"end":34,"id":5},{"text":",","start":34,"end":35,"id":6},{"text":"one","start":36,"end":39,"id":7},{"text":"can","start":40,"end":43,"id":8},{"text":"prove","start":44,"end":49,"id":9},{"text":"a","start":50,"end":51,"id":10},{"text":"theorem","start":52,"end":59,"id":11},{"text":"analogous","start":60,"end":69,"id":12},{"text":"to","start":70,"end":72,"id":13},{"text":"one","start":73,"end":76,"id":14},{"text":"of","start":77,"end":79,"id":15},{"text":"J.","start":80,"end":82,"id":16},{"text":"Kleinberg","start":83,"end":92,"id":17},{"text":",","start":92,"end":93,"id":18},{"text":"in","start":94,"end":96,"id":19},{"text":"which","start":97,"end":102,"id":20},{"text":"one","start":103,"end":106,"id":21},{"text":"obtains","start":107,"end":114,"id":22},{"text":"an","start":115,"end":117,"id":23},{"text":"existence","start":118,"end":127,"id":24},{"text":"and","start":128,"end":131,"id":25},{"text":"uniqueness","start":132,"end":142,"id":26},{"text":"theorem","start":143,"end":150,"id":27},{"text":"instead","start":151,"end":158,"id":28},{"text":"of","start":159,"end":161,"id":29},{"text":"a","start":162,"end":163,"id":30},{"text":"non","start":164,"end":167,"id":31},{"text":"-","start":167,"end":168,"id":32},{"text":"existence","start":168,"end":177,"id":33},{"text":"result","start":178,"end":184,"id":34},{"text":".","start":184,"end":185,"id":35}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"However, the regular sequential form of EP (R-EP) may fail to converge in this model when the size of the training set is very small.","_input_hash":764369777,"_task_hash":220486071,"tokens":[{"text":"However","start":0,"end":7,"id":0},{"text":",","start":7,"end":8,"id":1},{"text":"the","start":9,"end":12,"id":2},{"text":"regular","start":13,"end":20,"id":3},{"text":"sequential","start":21,"end":31,"id":4},{"text":"form","start":32,"end":36,"id":5},{"text":"of","start":37,"end":39,"id":6},{"text":"EP","start":40,"end":42,"id":7},{"text":"(","start":43,"end":44,"id":8},{"text":"R","start":44,"end":45,"id":9},{"text":"-","start":45,"end":46,"id":10},{"text":"EP","start":46,"end":48,"id":11},{"text":")","start":48,"end":49,"id":12},{"text":"may","start":50,"end":53,"id":13},{"text":"fail","start":54,"end":58,"id":14},{"text":"to","start":59,"end":61,"id":15},{"text":"converge","start":62,"end":70,"id":16},{"text":"in","start":71,"end":73,"id":17},{"text":"this","start":74,"end":78,"id":18},{"text":"model","start":79,"end":84,"id":19},{"text":"when","start":85,"end":89,"id":20},{"text":"the","start":90,"end":93,"id":21},{"text":"size","start":94,"end":98,"id":22},{"text":"of","start":99,"end":101,"id":23},{"text":"the","start":102,"end":105,"id":24},{"text":"training","start":106,"end":114,"id":25},{"text":"set","start":115,"end":118,"id":26},{"text":"is","start":119,"end":121,"id":27},{"text":"very","start":122,"end":126,"id":28},{"text":"small","start":127,"end":132,"id":29},{"text":".","start":132,"end":133,"id":30}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Finding sparse solutions of underdetermined systems of linear equations is a fundamental problem in signal processing and statistics which has become a subject of interest in recent years.","_input_hash":1382224868,"_task_hash":1735348654,"tokens":[{"text":"Finding","start":0,"end":7,"id":0},{"text":"sparse","start":8,"end":14,"id":1},{"text":"solutions","start":15,"end":24,"id":2},{"text":"of","start":25,"end":27,"id":3},{"text":"underdetermined","start":28,"end":43,"id":4},{"text":"systems","start":44,"end":51,"id":5},{"text":"of","start":52,"end":54,"id":6},{"text":"linear","start":55,"end":61,"id":7},{"text":"equations","start":62,"end":71,"id":8},{"text":"is","start":72,"end":74,"id":9},{"text":"a","start":75,"end":76,"id":10},{"text":"fundamental","start":77,"end":88,"id":11},{"text":"problem","start":89,"end":96,"id":12},{"text":"in","start":97,"end":99,"id":13},{"text":"signal","start":100,"end":106,"id":14},{"text":"processing","start":107,"end":117,"id":15},{"text":"and","start":118,"end":121,"id":16},{"text":"statistics","start":122,"end":132,"id":17},{"text":"which","start":133,"end":138,"id":18},{"text":"has","start":139,"end":142,"id":19},{"text":"become","start":143,"end":149,"id":20},{"text":"a","start":150,"end":151,"id":21},{"text":"subject","start":152,"end":159,"id":22},{"text":"of","start":160,"end":162,"id":23},{"text":"interest","start":163,"end":171,"id":24},{"text":"in","start":172,"end":174,"id":25},{"text":"recent","start":175,"end":181,"id":26},{"text":"years","start":182,"end":187,"id":27},{"text":".","start":187,"end":188,"id":28}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"This problem is of particular interest when $F$ contains many irrelevant functions that should not appear in $\\tilde{f}$. We provide an exact oracle inequality for $\\tilde f$, where only two coefficients are non-zero, that entails $\\tilde f$ to be an optimal aggregation algorithm.","_input_hash":-671656956,"_task_hash":1325746409,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"problem","start":5,"end":12,"id":1},{"text":"is","start":13,"end":15,"id":2},{"text":"of","start":16,"end":18,"id":3},{"text":"particular","start":19,"end":29,"id":4},{"text":"interest","start":30,"end":38,"id":5},{"text":"when","start":39,"end":43,"id":6},{"text":"$","start":44,"end":45,"id":7},{"text":"F$","start":45,"end":47,"id":8},{"text":"contains","start":48,"end":56,"id":9},{"text":"many","start":57,"end":61,"id":10},{"text":"irrelevant","start":62,"end":72,"id":11},{"text":"functions","start":73,"end":82,"id":12},{"text":"that","start":83,"end":87,"id":13},{"text":"should","start":88,"end":94,"id":14},{"text":"not","start":95,"end":98,"id":15},{"text":"appear","start":99,"end":105,"id":16},{"text":"in","start":106,"end":108,"id":17},{"text":"$","start":109,"end":110,"id":18},{"text":"\\tilde{f}$.","start":110,"end":121,"id":19},{"text":"We","start":122,"end":124,"id":20},{"text":"provide","start":125,"end":132,"id":21},{"text":"an","start":133,"end":135,"id":22},{"text":"exact","start":136,"end":141,"id":23},{"text":"oracle","start":142,"end":148,"id":24},{"text":"inequality","start":149,"end":159,"id":25},{"text":"for","start":160,"end":163,"id":26},{"text":"$","start":164,"end":165,"id":27},{"text":"\\tilde","start":165,"end":171,"id":28},{"text":"f$","start":172,"end":174,"id":29},{"text":",","start":174,"end":175,"id":30},{"text":"where","start":176,"end":181,"id":31},{"text":"only","start":182,"end":186,"id":32},{"text":"two","start":187,"end":190,"id":33},{"text":"coefficients","start":191,"end":203,"id":34},{"text":"are","start":204,"end":207,"id":35},{"text":"non","start":208,"end":211,"id":36},{"text":"-","start":211,"end":212,"id":37},{"text":"zero","start":212,"end":216,"id":38},{"text":",","start":216,"end":217,"id":39},{"text":"that","start":218,"end":222,"id":40},{"text":"entails","start":223,"end":230,"id":41},{"text":"$","start":231,"end":232,"id":42},{"text":"\\tilde","start":232,"end":238,"id":43},{"text":"f$","start":239,"end":241,"id":44},{"text":"to","start":242,"end":244,"id":45},{"text":"be","start":245,"end":247,"id":46},{"text":"an","start":248,"end":250,"id":47},{"text":"optimal","start":251,"end":258,"id":48},{"text":"aggregation","start":259,"end":270,"id":49},{"text":"algorithm","start":271,"end":280,"id":50},{"text":".","start":280,"end":281,"id":51}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":251,"end":270,"token_start":48,"token_end":49,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"moreover, under suitable conditions, the global solution corresponds to the unique sparse local solution, which can be obtained via different numerical procedures.","_input_hash":2106837382,"_task_hash":-779453649,"tokens":[{"text":"moreover","start":0,"end":8,"id":0},{"text":",","start":8,"end":9,"id":1},{"text":"under","start":10,"end":15,"id":2},{"text":"suitable","start":16,"end":24,"id":3},{"text":"conditions","start":25,"end":35,"id":4},{"text":",","start":35,"end":36,"id":5},{"text":"the","start":37,"end":40,"id":6},{"text":"global","start":41,"end":47,"id":7},{"text":"solution","start":48,"end":56,"id":8},{"text":"corresponds","start":57,"end":68,"id":9},{"text":"to","start":69,"end":71,"id":10},{"text":"the","start":72,"end":75,"id":11},{"text":"unique","start":76,"end":82,"id":12},{"text":"sparse","start":83,"end":89,"id":13},{"text":"local","start":90,"end":95,"id":14},{"text":"solution","start":96,"end":104,"id":15},{"text":",","start":104,"end":105,"id":16},{"text":"which","start":106,"end":111,"id":17},{"text":"can","start":112,"end":115,"id":18},{"text":"be","start":116,"end":118,"id":19},{"text":"obtained","start":119,"end":127,"id":20},{"text":"via","start":128,"end":131,"id":21},{"text":"different","start":132,"end":141,"id":22},{"text":"numerical","start":142,"end":151,"id":23},{"text":"procedures","start":152,"end":162,"id":24},{"text":".","start":162,"end":163,"id":25}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We present different sparse approximations for dependent output Gaussian processes constructed through the convolution formalism.","_input_hash":-1711602771,"_task_hash":-1376056328,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"present","start":3,"end":10,"id":1},{"text":"different","start":11,"end":20,"id":2},{"text":"sparse","start":21,"end":27,"id":3},{"text":"approximations","start":28,"end":42,"id":4},{"text":"for","start":43,"end":46,"id":5},{"text":"dependent","start":47,"end":56,"id":6},{"text":"output","start":57,"end":63,"id":7},{"text":"Gaussian","start":64,"end":72,"id":8},{"text":"processes","start":73,"end":82,"id":9},{"text":"constructed","start":83,"end":94,"id":10},{"text":"through","start":95,"end":102,"id":11},{"text":"the","start":103,"end":106,"id":12},{"text":"convolution","start":107,"end":118,"id":13},{"text":"formalism","start":119,"end":128,"id":14},{"text":".","start":128,"end":129,"id":15}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"We propose a scalable, efficient and statistically motivated computational framework for Graphical Lasso (Friedman et al.,","_input_hash":-1869360253,"_task_hash":-861310901,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"propose","start":3,"end":10,"id":1},{"text":"a","start":11,"end":12,"id":2},{"text":"scalable","start":13,"end":21,"id":3},{"text":",","start":21,"end":22,"id":4},{"text":"efficient","start":23,"end":32,"id":5},{"text":"and","start":33,"end":36,"id":6},{"text":"statistically","start":37,"end":50,"id":7},{"text":"motivated","start":51,"end":60,"id":8},{"text":"computational","start":61,"end":74,"id":9},{"text":"framework","start":75,"end":84,"id":10},{"text":"for","start":85,"end":88,"id":11},{"text":"Graphical","start":89,"end":98,"id":12},{"text":"Lasso","start":99,"end":104,"id":13},{"text":"(","start":105,"end":106,"id":14},{"text":"Friedman","start":106,"end":114,"id":15},{"text":"et","start":115,"end":117,"id":16},{"text":"al","start":118,"end":120,"id":17},{"text":".","start":120,"end":121,"id":18},{"text":",","start":121,"end":122,"id":19}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":89,"end":104,"token_start":12,"token_end":13,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Equivalently, $\\VC(\\mathscr C\\modd\\omega_1)\\leq d$ if and only if every countable subclass of $\\mathscr C$ has VC dimension $\\leq d$ outside a countable subset of $\\Omega$. The new parameter can be also expressed as the classical VC dimension of $\\mathscr C$ calculated on a suitable subset of a compactification of $\\Omega$. We do not make any measurability assumptions on $\\mathscr C$, assuming instead the validity of Martin's Axiom (MA).","_input_hash":1578971403,"_task_hash":1805008153,"tokens":[{"text":"Equivalently","start":0,"end":12,"id":0},{"text":",","start":12,"end":13,"id":1},{"text":"$","start":14,"end":15,"id":2},{"text":"\\VC(\\mathscr","start":15,"end":27,"id":3},{"text":"C\\modd\\omega_1)\\leq","start":28,"end":47,"id":4},{"text":"d$","start":48,"end":50,"id":5},{"text":"if","start":51,"end":53,"id":6},{"text":"and","start":54,"end":57,"id":7},{"text":"only","start":58,"end":62,"id":8},{"text":"if","start":63,"end":65,"id":9},{"text":"every","start":66,"end":71,"id":10},{"text":"countable","start":72,"end":81,"id":11},{"text":"subclass","start":82,"end":90,"id":12},{"text":"of","start":91,"end":93,"id":13},{"text":"$","start":94,"end":95,"id":14},{"text":"\\mathscr","start":95,"end":103,"id":15},{"text":"C$","start":104,"end":106,"id":16},{"text":"has","start":107,"end":110,"id":17},{"text":"VC","start":111,"end":113,"id":18},{"text":"dimension","start":114,"end":123,"id":19},{"text":"$","start":124,"end":125,"id":20},{"text":"\\leq","start":125,"end":129,"id":21},{"text":"d$","start":130,"end":132,"id":22},{"text":"outside","start":133,"end":140,"id":23},{"text":"a","start":141,"end":142,"id":24},{"text":"countable","start":143,"end":152,"id":25},{"text":"subset","start":153,"end":159,"id":26},{"text":"of","start":160,"end":162,"id":27},{"text":"$","start":163,"end":164,"id":28},{"text":"\\Omega$.","start":164,"end":172,"id":29},{"text":"The","start":173,"end":176,"id":30},{"text":"new","start":177,"end":180,"id":31},{"text":"parameter","start":181,"end":190,"id":32},{"text":"can","start":191,"end":194,"id":33},{"text":"be","start":195,"end":197,"id":34},{"text":"also","start":198,"end":202,"id":35},{"text":"expressed","start":203,"end":212,"id":36},{"text":"as","start":213,"end":215,"id":37},{"text":"the","start":216,"end":219,"id":38},{"text":"classical","start":220,"end":229,"id":39},{"text":"VC","start":230,"end":232,"id":40},{"text":"dimension","start":233,"end":242,"id":41},{"text":"of","start":243,"end":245,"id":42},{"text":"$","start":246,"end":247,"id":43},{"text":"\\mathscr","start":247,"end":255,"id":44},{"text":"C$","start":256,"end":258,"id":45},{"text":"calculated","start":259,"end":269,"id":46},{"text":"on","start":270,"end":272,"id":47},{"text":"a","start":273,"end":274,"id":48},{"text":"suitable","start":275,"end":283,"id":49},{"text":"subset","start":284,"end":290,"id":50},{"text":"of","start":291,"end":293,"id":51},{"text":"a","start":294,"end":295,"id":52},{"text":"compactification","start":296,"end":312,"id":53},{"text":"of","start":313,"end":315,"id":54},{"text":"$","start":316,"end":317,"id":55},{"text":"\\Omega$.","start":317,"end":325,"id":56},{"text":"We","start":326,"end":328,"id":57},{"text":"do","start":329,"end":331,"id":58},{"text":"not","start":332,"end":335,"id":59},{"text":"make","start":336,"end":340,"id":60},{"text":"any","start":341,"end":344,"id":61},{"text":"measurability","start":345,"end":358,"id":62},{"text":"assumptions","start":359,"end":370,"id":63},{"text":"on","start":371,"end":373,"id":64},{"text":"$","start":374,"end":375,"id":65},{"text":"\\mathscr","start":375,"end":383,"id":66},{"text":"C$","start":384,"end":386,"id":67},{"text":",","start":386,"end":387,"id":68},{"text":"assuming","start":388,"end":396,"id":69},{"text":"instead","start":397,"end":404,"id":70},{"text":"the","start":405,"end":408,"id":71},{"text":"validity","start":409,"end":417,"id":72},{"text":"of","start":418,"end":420,"id":73},{"text":"Martin","start":421,"end":427,"id":74},{"text":"'s","start":427,"end":429,"id":75},{"text":"Axiom","start":430,"end":435,"id":76},{"text":"(","start":436,"end":437,"id":77},{"text":"MA","start":437,"end":439,"id":78},{"text":")","start":439,"end":440,"id":79},{"text":".","start":440,"end":441,"id":80}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":14,"end":50,"token_start":2,"token_end":5,"label":"ALGO","answer":"reject"},{"start":91,"end":106,"token_start":13,"token_end":16,"label":"ALGO","answer":"reject"},{"start":111,"end":123,"token_start":18,"token_end":19,"label":"ALGO","answer":"reject"},{"start":163,"end":172,"token_start":28,"token_end":29,"label":"ALGO","answer":"reject"},{"start":421,"end":435,"token_start":74,"token_end":76,"label":"ALGO","answer":"reject"}],"answer":"reject"}
{"text":"Recent efforts have shown that in the case of generalized linear models, independent screening may suffice to capture all relevant features with high probability, even in ultra-high dimension.","_input_hash":1921966750,"_task_hash":-1621019535,"tokens":[{"text":"Recent","start":0,"end":6,"id":0},{"text":"efforts","start":7,"end":14,"id":1},{"text":"have","start":15,"end":19,"id":2},{"text":"shown","start":20,"end":25,"id":3},{"text":"that","start":26,"end":30,"id":4},{"text":"in","start":31,"end":33,"id":5},{"text":"the","start":34,"end":37,"id":6},{"text":"case","start":38,"end":42,"id":7},{"text":"of","start":43,"end":45,"id":8},{"text":"generalized","start":46,"end":57,"id":9},{"text":"linear","start":58,"end":64,"id":10},{"text":"models","start":65,"end":71,"id":11},{"text":",","start":71,"end":72,"id":12},{"text":"independent","start":73,"end":84,"id":13},{"text":"screening","start":85,"end":94,"id":14},{"text":"may","start":95,"end":98,"id":15},{"text":"suffice","start":99,"end":106,"id":16},{"text":"to","start":107,"end":109,"id":17},{"text":"capture","start":110,"end":117,"id":18},{"text":"all","start":118,"end":121,"id":19},{"text":"relevant","start":122,"end":130,"id":20},{"text":"features","start":131,"end":139,"id":21},{"text":"with","start":140,"end":144,"id":22},{"text":"high","start":145,"end":149,"id":23},{"text":"probability","start":150,"end":161,"id":24},{"text":",","start":161,"end":162,"id":25},{"text":"even","start":163,"end":167,"id":26},{"text":"in","start":168,"end":170,"id":27},{"text":"ultra","start":171,"end":176,"id":28},{"text":"-","start":176,"end":177,"id":29},{"text":"high","start":177,"end":181,"id":30},{"text":"dimension","start":182,"end":191,"id":31},{"text":".","start":191,"end":192,"id":32}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":46,"end":64,"token_start":9,"token_end":10,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"In the currently popular field of network modeling, relatively little work has taken uncertainty of data seriously in the Bayesian sense, and component models have been introduced to the field only recently, by treating each node as a bag of out-going links.","_input_hash":-2045461051,"_task_hash":-317357642,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"the","start":3,"end":6,"id":1},{"text":"currently","start":7,"end":16,"id":2},{"text":"popular","start":17,"end":24,"id":3},{"text":"field","start":25,"end":30,"id":4},{"text":"of","start":31,"end":33,"id":5},{"text":"network","start":34,"end":41,"id":6},{"text":"modeling","start":42,"end":50,"id":7},{"text":",","start":50,"end":51,"id":8},{"text":"relatively","start":52,"end":62,"id":9},{"text":"little","start":63,"end":69,"id":10},{"text":"work","start":70,"end":74,"id":11},{"text":"has","start":75,"end":78,"id":12},{"text":"taken","start":79,"end":84,"id":13},{"text":"uncertainty","start":85,"end":96,"id":14},{"text":"of","start":97,"end":99,"id":15},{"text":"data","start":100,"end":104,"id":16},{"text":"seriously","start":105,"end":114,"id":17},{"text":"in","start":115,"end":117,"id":18},{"text":"the","start":118,"end":121,"id":19},{"text":"Bayesian","start":122,"end":130,"id":20},{"text":"sense","start":131,"end":136,"id":21},{"text":",","start":136,"end":137,"id":22},{"text":"and","start":138,"end":141,"id":23},{"text":"component","start":142,"end":151,"id":24},{"text":"models","start":152,"end":158,"id":25},{"text":"have","start":159,"end":163,"id":26},{"text":"been","start":164,"end":168,"id":27},{"text":"introduced","start":169,"end":179,"id":28},{"text":"to","start":180,"end":182,"id":29},{"text":"the","start":183,"end":186,"id":30},{"text":"field","start":187,"end":192,"id":31},{"text":"only","start":193,"end":197,"id":32},{"text":"recently","start":198,"end":206,"id":33},{"text":",","start":206,"end":207,"id":34},{"text":"by","start":208,"end":210,"id":35},{"text":"treating","start":211,"end":219,"id":36},{"text":"each","start":220,"end":224,"id":37},{"text":"node","start":225,"end":229,"id":38},{"text":"as","start":230,"end":232,"id":39},{"text":"a","start":233,"end":234,"id":40},{"text":"bag","start":235,"end":238,"id":41},{"text":"of","start":239,"end":241,"id":42},{"text":"out","start":242,"end":245,"id":43},{"text":"-","start":245,"end":246,"id":44},{"text":"going","start":246,"end":251,"id":45},{"text":"links","start":252,"end":257,"id":46},{"text":".","start":257,"end":258,"id":47}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"The transitions between these decisional states correspond to events that lead to a change of decision.","_input_hash":-752918440,"_task_hash":-1884347967,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"transitions","start":4,"end":15,"id":1},{"text":"between","start":16,"end":23,"id":2},{"text":"these","start":24,"end":29,"id":3},{"text":"decisional","start":30,"end":40,"id":4},{"text":"states","start":41,"end":47,"id":5},{"text":"correspond","start":48,"end":58,"id":6},{"text":"to","start":59,"end":61,"id":7},{"text":"events","start":62,"end":68,"id":8},{"text":"that","start":69,"end":73,"id":9},{"text":"lead","start":74,"end":78,"id":10},{"text":"to","start":79,"end":81,"id":11},{"text":"a","start":82,"end":83,"id":12},{"text":"change","start":84,"end":90,"id":13},{"text":"of","start":91,"end":93,"id":14},{"text":"decision","start":94,"end":102,"id":15},{"text":".","start":102,"end":103,"id":16}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Exact inference in the linear regression model with spike and slab priors is often intractable.","_input_hash":-365971898,"_task_hash":1784069257,"tokens":[{"text":"Exact","start":0,"end":5,"id":0},{"text":"inference","start":6,"end":15,"id":1},{"text":"in","start":16,"end":18,"id":2},{"text":"the","start":19,"end":22,"id":3},{"text":"linear","start":23,"end":29,"id":4},{"text":"regression","start":30,"end":40,"id":5},{"text":"model","start":41,"end":46,"id":6},{"text":"with","start":47,"end":51,"id":7},{"text":"spike","start":52,"end":57,"id":8},{"text":"and","start":58,"end":61,"id":9},{"text":"slab","start":62,"end":66,"id":10},{"text":"priors","start":67,"end":73,"id":11},{"text":"is","start":74,"end":76,"id":12},{"text":"often","start":77,"end":82,"id":13},{"text":"intractable","start":83,"end":94,"id":14},{"text":".","start":94,"end":95,"id":15}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":23,"end":40,"token_start":4,"token_end":5,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"The absence of prewhitening improves asymptotic performance.","_input_hash":-1575921533,"_task_hash":-838305515,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"absence","start":4,"end":11,"id":1},{"text":"of","start":12,"end":14,"id":2},{"text":"prewhitening","start":15,"end":27,"id":3},{"text":"improves","start":28,"end":36,"id":4},{"text":"asymptotic","start":37,"end":47,"id":5},{"text":"performance","start":48,"end":59,"id":6},{"text":".","start":59,"end":60,"id":7}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Through experiments on image matching, unpaired voice conversion, and photo album summarization tasks, the effectiveness of the proposed methods is demonstrated.","_input_hash":-1498825433,"_task_hash":1807602218,"tokens":[{"text":"Through","start":0,"end":7,"id":0},{"text":"experiments","start":8,"end":19,"id":1},{"text":"on","start":20,"end":22,"id":2},{"text":"image","start":23,"end":28,"id":3},{"text":"matching","start":29,"end":37,"id":4},{"text":",","start":37,"end":38,"id":5},{"text":"unpaired","start":39,"end":47,"id":6},{"text":"voice","start":48,"end":53,"id":7},{"text":"conversion","start":54,"end":64,"id":8},{"text":",","start":64,"end":65,"id":9},{"text":"and","start":66,"end":69,"id":10},{"text":"photo","start":70,"end":75,"id":11},{"text":"album","start":76,"end":81,"id":12},{"text":"summarization","start":82,"end":95,"id":13},{"text":"tasks","start":96,"end":101,"id":14},{"text":",","start":101,"end":102,"id":15},{"text":"the","start":103,"end":106,"id":16},{"text":"effectiveness","start":107,"end":120,"id":17},{"text":"of","start":121,"end":123,"id":18},{"text":"the","start":124,"end":127,"id":19},{"text":"proposed","start":128,"end":136,"id":20},{"text":"methods","start":137,"end":144,"id":21},{"text":"is","start":145,"end":147,"id":22},{"text":"demonstrated","start":148,"end":160,"id":23},{"text":".","start":160,"end":161,"id":24}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"This note shows that the risk of this ordinary least squares method is within a constant factor (namely 4) of the risk of ridge regression.","_input_hash":1198966536,"_task_hash":-1992365713,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"note","start":5,"end":9,"id":1},{"text":"shows","start":10,"end":15,"id":2},{"text":"that","start":16,"end":20,"id":3},{"text":"the","start":21,"end":24,"id":4},{"text":"risk","start":25,"end":29,"id":5},{"text":"of","start":30,"end":32,"id":6},{"text":"this","start":33,"end":37,"id":7},{"text":"ordinary","start":38,"end":46,"id":8},{"text":"least","start":47,"end":52,"id":9},{"text":"squares","start":53,"end":60,"id":10},{"text":"method","start":61,"end":67,"id":11},{"text":"is","start":68,"end":70,"id":12},{"text":"within","start":71,"end":77,"id":13},{"text":"a","start":78,"end":79,"id":14},{"text":"constant","start":80,"end":88,"id":15},{"text":"factor","start":89,"end":95,"id":16},{"text":"(","start":96,"end":97,"id":17},{"text":"namely","start":97,"end":103,"id":18},{"text":"4","start":104,"end":105,"id":19},{"text":")","start":105,"end":106,"id":20},{"text":"of","start":107,"end":109,"id":21},{"text":"the","start":110,"end":113,"id":22},{"text":"risk","start":114,"end":118,"id":23},{"text":"of","start":119,"end":121,"id":24},{"text":"ridge","start":122,"end":127,"id":25},{"text":"regression","start":128,"end":138,"id":26},{"text":".","start":138,"end":139,"id":27}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":47,"end":60,"token_start":9,"token_end":10,"label":"ALGO","answer":"accept"},{"start":122,"end":138,"token_start":25,"token_end":26,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"A key feature of the method is that, once a tree ensemble is fitted, no further tuning parameter needs to be selected.","_input_hash":35365933,"_task_hash":1367903609,"tokens":[{"text":"A","start":0,"end":1,"id":0},{"text":"key","start":2,"end":5,"id":1},{"text":"feature","start":6,"end":13,"id":2},{"text":"of","start":14,"end":16,"id":3},{"text":"the","start":17,"end":20,"id":4},{"text":"method","start":21,"end":27,"id":5},{"text":"is","start":28,"end":30,"id":6},{"text":"that","start":31,"end":35,"id":7},{"text":",","start":35,"end":36,"id":8},{"text":"once","start":37,"end":41,"id":9},{"text":"a","start":42,"end":43,"id":10},{"text":"tree","start":44,"end":48,"id":11},{"text":"ensemble","start":49,"end":57,"id":12},{"text":"is","start":58,"end":60,"id":13},{"text":"fitted","start":61,"end":67,"id":14},{"text":",","start":67,"end":68,"id":15},{"text":"no","start":69,"end":71,"id":16},{"text":"further","start":72,"end":79,"id":17},{"text":"tuning","start":80,"end":86,"id":18},{"text":"parameter","start":87,"end":96,"id":19},{"text":"needs","start":97,"end":102,"id":20},{"text":"to","start":103,"end":105,"id":21},{"text":"be","start":106,"end":108,"id":22},{"text":"selected","start":109,"end":117,"id":23},{"text":".","start":117,"end":118,"id":24}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We introduce a new general formulation of simulated annealing which allows one to guarantee finite-time performance in the optimization of functions of continuous variables.","_input_hash":2099441561,"_task_hash":-88008736,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"introduce","start":3,"end":12,"id":1},{"text":"a","start":13,"end":14,"id":2},{"text":"new","start":15,"end":18,"id":3},{"text":"general","start":19,"end":26,"id":4},{"text":"formulation","start":27,"end":38,"id":5},{"text":"of","start":39,"end":41,"id":6},{"text":"simulated","start":42,"end":51,"id":7},{"text":"annealing","start":52,"end":61,"id":8},{"text":"which","start":62,"end":67,"id":9},{"text":"allows","start":68,"end":74,"id":10},{"text":"one","start":75,"end":78,"id":11},{"text":"to","start":79,"end":81,"id":12},{"text":"guarantee","start":82,"end":91,"id":13},{"text":"finite","start":92,"end":98,"id":14},{"text":"-","start":98,"end":99,"id":15},{"text":"time","start":99,"end":103,"id":16},{"text":"performance","start":104,"end":115,"id":17},{"text":"in","start":116,"end":118,"id":18},{"text":"the","start":119,"end":122,"id":19},{"text":"optimization","start":123,"end":135,"id":20},{"text":"of","start":136,"end":138,"id":21},{"text":"functions","start":139,"end":148,"id":22},{"text":"of","start":149,"end":151,"id":23},{"text":"continuous","start":152,"end":162,"id":24},{"text":"variables","start":163,"end":172,"id":25},{"text":".","start":172,"end":173,"id":26}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":42,"end":61,"token_start":7,"token_end":8,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"In oder to get round the curse of dimensionality, a popular approach is to consider simplified models such as additive models.","_input_hash":339947542,"_task_hash":1560074120,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"oder","start":3,"end":7,"id":1},{"text":"to","start":8,"end":10,"id":2},{"text":"get","start":11,"end":14,"id":3},{"text":"round","start":15,"end":20,"id":4},{"text":"the","start":21,"end":24,"id":5},{"text":"curse","start":25,"end":30,"id":6},{"text":"of","start":31,"end":33,"id":7},{"text":"dimensionality","start":34,"end":48,"id":8},{"text":",","start":48,"end":49,"id":9},{"text":"a","start":50,"end":51,"id":10},{"text":"popular","start":52,"end":59,"id":11},{"text":"approach","start":60,"end":68,"id":12},{"text":"is","start":69,"end":71,"id":13},{"text":"to","start":72,"end":74,"id":14},{"text":"consider","start":75,"end":83,"id":15},{"text":"simplified","start":84,"end":94,"id":16},{"text":"models","start":95,"end":101,"id":17},{"text":"such","start":102,"end":106,"id":18},{"text":"as","start":107,"end":109,"id":19},{"text":"additive","start":110,"end":118,"id":20},{"text":"models","start":119,"end":125,"id":21},{"text":".","start":125,"end":126,"id":22}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":110,"end":118,"token_start":20,"token_end":20,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We also give an example of use of aggregation to achieve minimax adaptation over anisotropic Besov spaces, which was not previously known in minimax theory (in regression on a random design).","_input_hash":-1727036382,"_task_hash":1343226073,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"also","start":3,"end":7,"id":1},{"text":"give","start":8,"end":12,"id":2},{"text":"an","start":13,"end":15,"id":3},{"text":"example","start":16,"end":23,"id":4},{"text":"of","start":24,"end":26,"id":5},{"text":"use","start":27,"end":30,"id":6},{"text":"of","start":31,"end":33,"id":7},{"text":"aggregation","start":34,"end":45,"id":8},{"text":"to","start":46,"end":48,"id":9},{"text":"achieve","start":49,"end":56,"id":10},{"text":"minimax","start":57,"end":64,"id":11},{"text":"adaptation","start":65,"end":75,"id":12},{"text":"over","start":76,"end":80,"id":13},{"text":"anisotropic","start":81,"end":92,"id":14},{"text":"Besov","start":93,"end":98,"id":15},{"text":"spaces","start":99,"end":105,"id":16},{"text":",","start":105,"end":106,"id":17},{"text":"which","start":107,"end":112,"id":18},{"text":"was","start":113,"end":116,"id":19},{"text":"not","start":117,"end":120,"id":20},{"text":"previously","start":121,"end":131,"id":21},{"text":"known","start":132,"end":137,"id":22},{"text":"in","start":138,"end":140,"id":23},{"text":"minimax","start":141,"end":148,"id":24},{"text":"theory","start":149,"end":155,"id":25},{"text":"(","start":156,"end":157,"id":26},{"text":"in","start":157,"end":159,"id":27},{"text":"regression","start":160,"end":170,"id":28},{"text":"on","start":171,"end":173,"id":29},{"text":"a","start":174,"end":175,"id":30},{"text":"random","start":176,"end":182,"id":31},{"text":"design","start":183,"end":189,"id":32},{"text":")","start":189,"end":190,"id":33},{"text":".","start":190,"end":191,"id":34}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We present a simple proof for the oracle inequality for the excess risk of structural risk minimizers using a lasso type penalty.","_input_hash":1863823134,"_task_hash":1182958917,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"present","start":3,"end":10,"id":1},{"text":"a","start":11,"end":12,"id":2},{"text":"simple","start":13,"end":19,"id":3},{"text":"proof","start":20,"end":25,"id":4},{"text":"for","start":26,"end":29,"id":5},{"text":"the","start":30,"end":33,"id":6},{"text":"oracle","start":34,"end":40,"id":7},{"text":"inequality","start":41,"end":51,"id":8},{"text":"for","start":52,"end":55,"id":9},{"text":"the","start":56,"end":59,"id":10},{"text":"excess","start":60,"end":66,"id":11},{"text":"risk","start":67,"end":71,"id":12},{"text":"of","start":72,"end":74,"id":13},{"text":"structural","start":75,"end":85,"id":14},{"text":"risk","start":86,"end":90,"id":15},{"text":"minimizers","start":91,"end":101,"id":16},{"text":"using","start":102,"end":107,"id":17},{"text":"a","start":108,"end":109,"id":18},{"text":"lasso","start":110,"end":115,"id":19},{"text":"type","start":116,"end":120,"id":20},{"text":"penalty","start":121,"end":128,"id":21},{"text":".","start":128,"end":129,"id":22}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":110,"end":115,"token_start":19,"token_end":19,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We prove that finding a maximum weight spanning forest with restricted tree size is NP-hard, and develop an approximation algorithm for this problem.","_input_hash":-1043233372,"_task_hash":961878719,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"prove","start":3,"end":8,"id":1},{"text":"that","start":9,"end":13,"id":2},{"text":"finding","start":14,"end":21,"id":3},{"text":"a","start":22,"end":23,"id":4},{"text":"maximum","start":24,"end":31,"id":5},{"text":"weight","start":32,"end":38,"id":6},{"text":"spanning","start":39,"end":47,"id":7},{"text":"forest","start":48,"end":54,"id":8},{"text":"with","start":55,"end":59,"id":9},{"text":"restricted","start":60,"end":70,"id":10},{"text":"tree","start":71,"end":75,"id":11},{"text":"size","start":76,"end":80,"id":12},{"text":"is","start":81,"end":83,"id":13},{"text":"NP","start":84,"end":86,"id":14},{"text":"-","start":86,"end":87,"id":15},{"text":"hard","start":87,"end":91,"id":16},{"text":",","start":91,"end":92,"id":17},{"text":"and","start":93,"end":96,"id":18},{"text":"develop","start":97,"end":104,"id":19},{"text":"an","start":105,"end":107,"id":20},{"text":"approximation","start":108,"end":121,"id":21},{"text":"algorithm","start":122,"end":131,"id":22},{"text":"for","start":132,"end":135,"id":23},{"text":"this","start":136,"end":140,"id":24},{"text":"problem","start":141,"end":148,"id":25},{"text":".","start":148,"end":149,"id":26}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":48,"end":54,"token_start":8,"token_end":8,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Like all heteroscedastic models, the noise terms in this Poisson-like model are \\textit{not} independent of the design matrix.","_input_hash":-430726667,"_task_hash":-831121266,"tokens":[{"text":"Like","start":0,"end":4,"id":0},{"text":"all","start":5,"end":8,"id":1},{"text":"heteroscedastic","start":9,"end":24,"id":2},{"text":"models","start":25,"end":31,"id":3},{"text":",","start":31,"end":32,"id":4},{"text":"the","start":33,"end":36,"id":5},{"text":"noise","start":37,"end":42,"id":6},{"text":"terms","start":43,"end":48,"id":7},{"text":"in","start":49,"end":51,"id":8},{"text":"this","start":52,"end":56,"id":9},{"text":"Poisson","start":57,"end":64,"id":10},{"text":"-","start":64,"end":65,"id":11},{"text":"like","start":65,"end":69,"id":12},{"text":"model","start":70,"end":75,"id":13},{"text":"are","start":76,"end":79,"id":14},{"text":"\\textit{not","start":80,"end":91,"id":15},{"text":"}","start":91,"end":92,"id":16},{"text":"independent","start":93,"end":104,"id":17},{"text":"of","start":105,"end":107,"id":18},{"text":"the","start":108,"end":111,"id":19},{"text":"design","start":112,"end":118,"id":20},{"text":"matrix","start":119,"end":125,"id":21},{"text":".","start":125,"end":126,"id":22}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":9,"end":24,"token_start":2,"token_end":2,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Several network scores and conditional independence algorithms are available for both the learning algorithms and independent use.","_input_hash":1248275622,"_task_hash":171356993,"tokens":[{"text":"Several","start":0,"end":7,"id":0},{"text":"network","start":8,"end":15,"id":1},{"text":"scores","start":16,"end":22,"id":2},{"text":"and","start":23,"end":26,"id":3},{"text":"conditional","start":27,"end":38,"id":4},{"text":"independence","start":39,"end":51,"id":5},{"text":"algorithms","start":52,"end":62,"id":6},{"text":"are","start":63,"end":66,"id":7},{"text":"available","start":67,"end":76,"id":8},{"text":"for","start":77,"end":80,"id":9},{"text":"both","start":81,"end":85,"id":10},{"text":"the","start":86,"end":89,"id":11},{"text":"learning","start":90,"end":98,"id":12},{"text":"algorithms","start":99,"end":109,"id":13},{"text":"and","start":110,"end":113,"id":14},{"text":"independent","start":114,"end":125,"id":15},{"text":"use","start":126,"end":129,"id":16},{"text":".","start":129,"end":130,"id":17}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"We illustrate this modularity by demonstrating the algorithm on three different real-world data sets.","_input_hash":-995745589,"_task_hash":-439661184,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"illustrate","start":3,"end":13,"id":1},{"text":"this","start":14,"end":18,"id":2},{"text":"modularity","start":19,"end":29,"id":3},{"text":"by","start":30,"end":32,"id":4},{"text":"demonstrating","start":33,"end":46,"id":5},{"text":"the","start":47,"end":50,"id":6},{"text":"algorithm","start":51,"end":60,"id":7},{"text":"on","start":61,"end":63,"id":8},{"text":"three","start":64,"end":69,"id":9},{"text":"different","start":70,"end":79,"id":10},{"text":"real","start":80,"end":84,"id":11},{"text":"-","start":84,"end":85,"id":12},{"text":"world","start":85,"end":90,"id":13},{"text":"data","start":91,"end":95,"id":14},{"text":"sets","start":96,"end":100,"id":15},{"text":".","start":100,"end":101,"id":16}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"The impact of missing data is then investigated and decision forrests are found to improve the results.","_input_hash":-1496861163,"_task_hash":595301050,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"impact","start":4,"end":10,"id":1},{"text":"of","start":11,"end":13,"id":2},{"text":"missing","start":14,"end":21,"id":3},{"text":"data","start":22,"end":26,"id":4},{"text":"is","start":27,"end":29,"id":5},{"text":"then","start":30,"end":34,"id":6},{"text":"investigated","start":35,"end":47,"id":7},{"text":"and","start":48,"end":51,"id":8},{"text":"decision","start":52,"end":60,"id":9},{"text":"forrests","start":61,"end":69,"id":10},{"text":"are","start":70,"end":73,"id":11},{"text":"found","start":74,"end":79,"id":12},{"text":"to","start":80,"end":82,"id":13},{"text":"improve","start":83,"end":90,"id":14},{"text":"the","start":91,"end":94,"id":15},{"text":"results","start":95,"end":102,"id":16},{"text":".","start":102,"end":103,"id":17}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":52,"end":69,"token_start":9,"token_end":10,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We give conditions under which sample variance penalization is effective.","_input_hash":1848389840,"_task_hash":-1980547899,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"give","start":3,"end":7,"id":1},{"text":"conditions","start":8,"end":18,"id":2},{"text":"under","start":19,"end":24,"id":3},{"text":"which","start":25,"end":30,"id":4},{"text":"sample","start":31,"end":37,"id":5},{"text":"variance","start":38,"end":46,"id":6},{"text":"penalization","start":47,"end":59,"id":7},{"text":"is","start":60,"end":62,"id":8},{"text":"effective","start":63,"end":72,"id":9},{"text":".","start":72,"end":73,"id":10}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"This implies that results and methods for mean estimation can be applied to the challenging problem of variance segmentation/estimation.","_input_hash":557309164,"_task_hash":-156195635,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"implies","start":5,"end":12,"id":1},{"text":"that","start":13,"end":17,"id":2},{"text":"results","start":18,"end":25,"id":3},{"text":"and","start":26,"end":29,"id":4},{"text":"methods","start":30,"end":37,"id":5},{"text":"for","start":38,"end":41,"id":6},{"text":"mean","start":42,"end":46,"id":7},{"text":"estimation","start":47,"end":57,"id":8},{"text":"can","start":58,"end":61,"id":9},{"text":"be","start":62,"end":64,"id":10},{"text":"applied","start":65,"end":72,"id":11},{"text":"to","start":73,"end":75,"id":12},{"text":"the","start":76,"end":79,"id":13},{"text":"challenging","start":80,"end":91,"id":14},{"text":"problem","start":92,"end":99,"id":15},{"text":"of","start":100,"end":102,"id":16},{"text":"variance","start":103,"end":111,"id":17},{"text":"segmentation","start":112,"end":124,"id":18},{"text":"/","start":124,"end":125,"id":19},{"text":"estimation","start":125,"end":135,"id":20},{"text":".","start":135,"end":136,"id":21}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Probabilities are uniquely characterized by the mean of the canonical map to the RKHS.","_input_hash":826563427,"_task_hash":2108635876,"tokens":[{"text":"Probabilities","start":0,"end":13,"id":0},{"text":"are","start":14,"end":17,"id":1},{"text":"uniquely","start":18,"end":26,"id":2},{"text":"characterized","start":27,"end":40,"id":3},{"text":"by","start":41,"end":43,"id":4},{"text":"the","start":44,"end":47,"id":5},{"text":"mean","start":48,"end":52,"id":6},{"text":"of","start":53,"end":55,"id":7},{"text":"the","start":56,"end":59,"id":8},{"text":"canonical","start":60,"end":69,"id":9},{"text":"map","start":70,"end":73,"id":10},{"text":"to","start":74,"end":76,"id":11},{"text":"the","start":77,"end":80,"id":12},{"text":"RKHS","start":81,"end":85,"id":13},{"text":".","start":85,"end":86,"id":14}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We apply this compression method to logistic sequence prediction models and logistic classification models.","_input_hash":-1491980493,"_task_hash":-2135452672,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"apply","start":3,"end":8,"id":1},{"text":"this","start":9,"end":13,"id":2},{"text":"compression","start":14,"end":25,"id":3},{"text":"method","start":26,"end":32,"id":4},{"text":"to","start":33,"end":35,"id":5},{"text":"logistic","start":36,"end":44,"id":6},{"text":"sequence","start":45,"end":53,"id":7},{"text":"prediction","start":54,"end":64,"id":8},{"text":"models","start":65,"end":71,"id":9},{"text":"and","start":72,"end":75,"id":10},{"text":"logistic","start":76,"end":84,"id":11},{"text":"classification","start":85,"end":99,"id":12},{"text":"models","start":100,"end":106,"id":13},{"text":".","start":106,"end":107,"id":14}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":76,"end":99,"token_start":11,"token_end":12,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Although certain aspects of the dual certificate idea have already been used in some previous work, due to the lack of a general and coherent theory, the analysis has so far only been carried out in limited scopes for specific problems.","_input_hash":92202465,"_task_hash":1107931964,"tokens":[{"text":"Although","start":0,"end":8,"id":0},{"text":"certain","start":9,"end":16,"id":1},{"text":"aspects","start":17,"end":24,"id":2},{"text":"of","start":25,"end":27,"id":3},{"text":"the","start":28,"end":31,"id":4},{"text":"dual","start":32,"end":36,"id":5},{"text":"certificate","start":37,"end":48,"id":6},{"text":"idea","start":49,"end":53,"id":7},{"text":"have","start":54,"end":58,"id":8},{"text":"already","start":59,"end":66,"id":9},{"text":"been","start":67,"end":71,"id":10},{"text":"used","start":72,"end":76,"id":11},{"text":"in","start":77,"end":79,"id":12},{"text":"some","start":80,"end":84,"id":13},{"text":"previous","start":85,"end":93,"id":14},{"text":"work","start":94,"end":98,"id":15},{"text":",","start":98,"end":99,"id":16},{"text":"due","start":100,"end":103,"id":17},{"text":"to","start":104,"end":106,"id":18},{"text":"the","start":107,"end":110,"id":19},{"text":"lack","start":111,"end":115,"id":20},{"text":"of","start":116,"end":118,"id":21},{"text":"a","start":119,"end":120,"id":22},{"text":"general","start":121,"end":128,"id":23},{"text":"and","start":129,"end":132,"id":24},{"text":"coherent","start":133,"end":141,"id":25},{"text":"theory","start":142,"end":148,"id":26},{"text":",","start":148,"end":149,"id":27},{"text":"the","start":150,"end":153,"id":28},{"text":"analysis","start":154,"end":162,"id":29},{"text":"has","start":163,"end":166,"id":30},{"text":"so","start":167,"end":169,"id":31},{"text":"far","start":170,"end":173,"id":32},{"text":"only","start":174,"end":178,"id":33},{"text":"been","start":179,"end":183,"id":34},{"text":"carried","start":184,"end":191,"id":35},{"text":"out","start":192,"end":195,"id":36},{"text":"in","start":196,"end":198,"id":37},{"text":"limited","start":199,"end":206,"id":38},{"text":"scopes","start":207,"end":213,"id":39},{"text":"for","start":214,"end":217,"id":40},{"text":"specific","start":218,"end":226,"id":41},{"text":"problems","start":227,"end":235,"id":42},{"text":".","start":235,"end":236,"id":43}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Experimental results on both synthetic and real data sets show performance improvements for clustering, and anomaly detection through the use of structural similarity.","_input_hash":-1853744890,"_task_hash":-202682995,"tokens":[{"text":"Experimental","start":0,"end":12,"id":0},{"text":"results","start":13,"end":20,"id":1},{"text":"on","start":21,"end":23,"id":2},{"text":"both","start":24,"end":28,"id":3},{"text":"synthetic","start":29,"end":38,"id":4},{"text":"and","start":39,"end":42,"id":5},{"text":"real","start":43,"end":47,"id":6},{"text":"data","start":48,"end":52,"id":7},{"text":"sets","start":53,"end":57,"id":8},{"text":"show","start":58,"end":62,"id":9},{"text":"performance","start":63,"end":74,"id":10},{"text":"improvements","start":75,"end":87,"id":11},{"text":"for","start":88,"end":91,"id":12},{"text":"clustering","start":92,"end":102,"id":13},{"text":",","start":102,"end":103,"id":14},{"text":"and","start":104,"end":107,"id":15},{"text":"anomaly","start":108,"end":115,"id":16},{"text":"detection","start":116,"end":125,"id":17},{"text":"through","start":126,"end":133,"id":18},{"text":"the","start":134,"end":137,"id":19},{"text":"use","start":138,"end":141,"id":20},{"text":"of","start":142,"end":144,"id":21},{"text":"structural","start":145,"end":155,"id":22},{"text":"similarity","start":156,"end":166,"id":23},{"text":".","start":166,"end":167,"id":24}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"Both algorithms use message passing on the tree structure.","_input_hash":389420575,"_task_hash":-429325418,"tokens":[{"text":"Both","start":0,"end":4,"id":0},{"text":"algorithms","start":5,"end":15,"id":1},{"text":"use","start":16,"end":19,"id":2},{"text":"message","start":20,"end":27,"id":3},{"text":"passing","start":28,"end":35,"id":4},{"text":"on","start":36,"end":38,"id":5},{"text":"the","start":39,"end":42,"id":6},{"text":"tree","start":43,"end":47,"id":7},{"text":"structure","start":48,"end":57,"id":8},{"text":".","start":57,"end":58,"id":9}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We show that graph-based algorithms can fail for unbalanced data for many popular graphs such as k-NN, \\epsilon-neighborhood and full-RBF graphs.","_input_hash":-587006178,"_task_hash":-2146809696,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"show","start":3,"end":7,"id":1},{"text":"that","start":8,"end":12,"id":2},{"text":"graph","start":13,"end":18,"id":3},{"text":"-","start":18,"end":19,"id":4},{"text":"based","start":19,"end":24,"id":5},{"text":"algorithms","start":25,"end":35,"id":6},{"text":"can","start":36,"end":39,"id":7},{"text":"fail","start":40,"end":44,"id":8},{"text":"for","start":45,"end":48,"id":9},{"text":"unbalanced","start":49,"end":59,"id":10},{"text":"data","start":60,"end":64,"id":11},{"text":"for","start":65,"end":68,"id":12},{"text":"many","start":69,"end":73,"id":13},{"text":"popular","start":74,"end":81,"id":14},{"text":"graphs","start":82,"end":88,"id":15},{"text":"such","start":89,"end":93,"id":16},{"text":"as","start":94,"end":96,"id":17},{"text":"k","start":97,"end":98,"id":18},{"text":"-","start":98,"end":99,"id":19},{"text":"NN","start":99,"end":101,"id":20},{"text":",","start":101,"end":102,"id":21},{"text":"\\epsilon","start":103,"end":111,"id":22},{"text":"-","start":111,"end":112,"id":23},{"text":"neighborhood","start":112,"end":124,"id":24},{"text":"and","start":125,"end":128,"id":25},{"text":"full","start":129,"end":133,"id":26},{"text":"-","start":133,"end":134,"id":27},{"text":"RBF","start":134,"end":137,"id":28},{"text":"graphs","start":138,"end":144,"id":29},{"text":".","start":144,"end":145,"id":30}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":13,"end":24,"token_start":3,"token_end":5,"label":"ALGO","answer":"accept"},{"start":97,"end":101,"token_start":18,"token_end":20,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Only recently a few sparse recovery results have been established for some specific local solutions obtained via specialized numerical procedures.","_input_hash":-2111455498,"_task_hash":-1979987605,"tokens":[{"text":"Only","start":0,"end":4,"id":0},{"text":"recently","start":5,"end":13,"id":1},{"text":"a","start":14,"end":15,"id":2},{"text":"few","start":16,"end":19,"id":3},{"text":"sparse","start":20,"end":26,"id":4},{"text":"recovery","start":27,"end":35,"id":5},{"text":"results","start":36,"end":43,"id":6},{"text":"have","start":44,"end":48,"id":7},{"text":"been","start":49,"end":53,"id":8},{"text":"established","start":54,"end":65,"id":9},{"text":"for","start":66,"end":69,"id":10},{"text":"some","start":70,"end":74,"id":11},{"text":"specific","start":75,"end":83,"id":12},{"text":"local","start":84,"end":89,"id":13},{"text":"solutions","start":90,"end":99,"id":14},{"text":"obtained","start":100,"end":108,"id":15},{"text":"via","start":109,"end":112,"id":16},{"text":"specialized","start":113,"end":124,"id":17},{"text":"numerical","start":125,"end":134,"id":18},{"text":"procedures","start":135,"end":145,"id":19},{"text":".","start":145,"end":146,"id":20}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"A nonparametric kernel-based method for realizing Bayes' rule is proposed, based on representations of probabilities in reproducing kernel Hilbert spaces.","_input_hash":1289696069,"_task_hash":-1917567708,"tokens":[{"text":"A","start":0,"end":1,"id":0},{"text":"nonparametric","start":2,"end":15,"id":1},{"text":"kernel","start":16,"end":22,"id":2},{"text":"-","start":22,"end":23,"id":3},{"text":"based","start":23,"end":28,"id":4},{"text":"method","start":29,"end":35,"id":5},{"text":"for","start":36,"end":39,"id":6},{"text":"realizing","start":40,"end":49,"id":7},{"text":"Bayes","start":50,"end":55,"id":8},{"text":"'","start":55,"end":56,"id":9},{"text":"rule","start":57,"end":61,"id":10},{"text":"is","start":62,"end":64,"id":11},{"text":"proposed","start":65,"end":73,"id":12},{"text":",","start":73,"end":74,"id":13},{"text":"based","start":75,"end":80,"id":14},{"text":"on","start":81,"end":83,"id":15},{"text":"representations","start":84,"end":99,"id":16},{"text":"of","start":100,"end":102,"id":17},{"text":"probabilities","start":103,"end":116,"id":18},{"text":"in","start":117,"end":119,"id":19},{"text":"reproducing","start":120,"end":131,"id":20},{"text":"kernel","start":132,"end":138,"id":21},{"text":"Hilbert","start":139,"end":146,"id":22},{"text":"spaces","start":147,"end":153,"id":23},{"text":".","start":153,"end":154,"id":24}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":2,"end":28,"token_start":1,"token_end":4,"label":"ALGO","answer":"accept"},{"start":132,"end":153,"token_start":21,"token_end":23,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"The network is further optimized by implementing Decision Forest.","_input_hash":-191073578,"_task_hash":1861232210,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"network","start":4,"end":11,"id":1},{"text":"is","start":12,"end":14,"id":2},{"text":"further","start":15,"end":22,"id":3},{"text":"optimized","start":23,"end":32,"id":4},{"text":"by","start":33,"end":35,"id":5},{"text":"implementing","start":36,"end":48,"id":6},{"text":"Decision","start":49,"end":57,"id":7},{"text":"Forest","start":58,"end":64,"id":8},{"text":".","start":64,"end":65,"id":9}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":49,"end":64,"token_start":7,"token_end":8,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We also contribute a new way of deriving the online algorithm that ties together previous online boosting work.","_input_hash":-821263039,"_task_hash":330301346,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"also","start":3,"end":7,"id":1},{"text":"contribute","start":8,"end":18,"id":2},{"text":"a","start":19,"end":20,"id":3},{"text":"new","start":21,"end":24,"id":4},{"text":"way","start":25,"end":28,"id":5},{"text":"of","start":29,"end":31,"id":6},{"text":"deriving","start":32,"end":40,"id":7},{"text":"the","start":41,"end":44,"id":8},{"text":"online","start":45,"end":51,"id":9},{"text":"algorithm","start":52,"end":61,"id":10},{"text":"that","start":62,"end":66,"id":11},{"text":"ties","start":67,"end":71,"id":12},{"text":"together","start":72,"end":80,"id":13},{"text":"previous","start":81,"end":89,"id":14},{"text":"online","start":90,"end":96,"id":15},{"text":"boosting","start":97,"end":105,"id":16},{"text":"work","start":106,"end":110,"id":17},{"text":".","start":110,"end":111,"id":18}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":45,"end":51,"token_start":9,"token_end":9,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"The combinatorial problem of selecting the nonzero components of this vector can be \"relaxed\" by regularizing the squared error with a convex penalty function like the $\\ell_1$ norm.","_input_hash":1149404712,"_task_hash":433008329,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"combinatorial","start":4,"end":17,"id":1},{"text":"problem","start":18,"end":25,"id":2},{"text":"of","start":26,"end":28,"id":3},{"text":"selecting","start":29,"end":38,"id":4},{"text":"the","start":39,"end":42,"id":5},{"text":"nonzero","start":43,"end":50,"id":6},{"text":"components","start":51,"end":61,"id":7},{"text":"of","start":62,"end":64,"id":8},{"text":"this","start":65,"end":69,"id":9},{"text":"vector","start":70,"end":76,"id":10},{"text":"can","start":77,"end":80,"id":11},{"text":"be","start":81,"end":83,"id":12},{"text":"\"","start":84,"end":85,"id":13},{"text":"relaxed","start":85,"end":92,"id":14},{"text":"\"","start":92,"end":93,"id":15},{"text":"by","start":94,"end":96,"id":16},{"text":"regularizing","start":97,"end":109,"id":17},{"text":"the","start":110,"end":113,"id":18},{"text":"squared","start":114,"end":121,"id":19},{"text":"error","start":122,"end":127,"id":20},{"text":"with","start":128,"end":132,"id":21},{"text":"a","start":133,"end":134,"id":22},{"text":"convex","start":135,"end":141,"id":23},{"text":"penalty","start":142,"end":149,"id":24},{"text":"function","start":150,"end":158,"id":25},{"text":"like","start":159,"end":163,"id":26},{"text":"the","start":164,"end":167,"id":27},{"text":"$","start":168,"end":169,"id":28},{"text":"\\ell_1","start":169,"end":175,"id":29},{"text":"$","start":175,"end":176,"id":30},{"text":"norm","start":177,"end":181,"id":31},{"text":".","start":181,"end":182,"id":32}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Our method is considerably faster than the previously proposed ones, since its time complexity is linear in the number of training examples, the number of features in the original data set, and the desired size of the set of selected features.","_input_hash":-1271852373,"_task_hash":142642813,"tokens":[{"text":"Our","start":0,"end":3,"id":0},{"text":"method","start":4,"end":10,"id":1},{"text":"is","start":11,"end":13,"id":2},{"text":"considerably","start":14,"end":26,"id":3},{"text":"faster","start":27,"end":33,"id":4},{"text":"than","start":34,"end":38,"id":5},{"text":"the","start":39,"end":42,"id":6},{"text":"previously","start":43,"end":53,"id":7},{"text":"proposed","start":54,"end":62,"id":8},{"text":"ones","start":63,"end":67,"id":9},{"text":",","start":67,"end":68,"id":10},{"text":"since","start":69,"end":74,"id":11},{"text":"its","start":75,"end":78,"id":12},{"text":"time","start":79,"end":83,"id":13},{"text":"complexity","start":84,"end":94,"id":14},{"text":"is","start":95,"end":97,"id":15},{"text":"linear","start":98,"end":104,"id":16},{"text":"in","start":105,"end":107,"id":17},{"text":"the","start":108,"end":111,"id":18},{"text":"number","start":112,"end":118,"id":19},{"text":"of","start":119,"end":121,"id":20},{"text":"training","start":122,"end":130,"id":21},{"text":"examples","start":131,"end":139,"id":22},{"text":",","start":139,"end":140,"id":23},{"text":"the","start":141,"end":144,"id":24},{"text":"number","start":145,"end":151,"id":25},{"text":"of","start":152,"end":154,"id":26},{"text":"features","start":155,"end":163,"id":27},{"text":"in","start":164,"end":166,"id":28},{"text":"the","start":167,"end":170,"id":29},{"text":"original","start":171,"end":179,"id":30},{"text":"data","start":180,"end":184,"id":31},{"text":"set","start":185,"end":188,"id":32},{"text":",","start":188,"end":189,"id":33},{"text":"and","start":190,"end":193,"id":34},{"text":"the","start":194,"end":197,"id":35},{"text":"desired","start":198,"end":205,"id":36},{"text":"size","start":206,"end":210,"id":37},{"text":"of","start":211,"end":213,"id":38},{"text":"the","start":214,"end":217,"id":39},{"text":"set","start":218,"end":221,"id":40},{"text":"of","start":222,"end":224,"id":41},{"text":"selected","start":225,"end":233,"id":42},{"text":"features","start":234,"end":242,"id":43},{"text":".","start":242,"end":243,"id":44}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We then theoretically justify our method through limit cut analysis.","_input_hash":303266223,"_task_hash":-836181960,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"then","start":3,"end":7,"id":1},{"text":"theoretically","start":8,"end":21,"id":2},{"text":"justify","start":22,"end":29,"id":3},{"text":"our","start":30,"end":33,"id":4},{"text":"method","start":34,"end":40,"id":5},{"text":"through","start":41,"end":48,"id":6},{"text":"limit","start":49,"end":54,"id":7},{"text":"cut","start":55,"end":58,"id":8},{"text":"analysis","start":59,"end":67,"id":9},{"text":".","start":67,"end":68,"id":10}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We introduce an alternative, interaction component model for communities (ICMc), where the whole network is a bag of links, stemming from different components.","_input_hash":-485852851,"_task_hash":-501628116,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"introduce","start":3,"end":12,"id":1},{"text":"an","start":13,"end":15,"id":2},{"text":"alternative","start":16,"end":27,"id":3},{"text":",","start":27,"end":28,"id":4},{"text":"interaction","start":29,"end":40,"id":5},{"text":"component","start":41,"end":50,"id":6},{"text":"model","start":51,"end":56,"id":7},{"text":"for","start":57,"end":60,"id":8},{"text":"communities","start":61,"end":72,"id":9},{"text":"(","start":73,"end":74,"id":10},{"text":"ICMc","start":74,"end":78,"id":11},{"text":")","start":78,"end":79,"id":12},{"text":",","start":79,"end":80,"id":13},{"text":"where","start":81,"end":86,"id":14},{"text":"the","start":87,"end":90,"id":15},{"text":"whole","start":91,"end":96,"id":16},{"text":"network","start":97,"end":104,"id":17},{"text":"is","start":105,"end":107,"id":18},{"text":"a","start":108,"end":109,"id":19},{"text":"bag","start":110,"end":113,"id":20},{"text":"of","start":114,"end":116,"id":21},{"text":"links","start":117,"end":122,"id":22},{"text":",","start":122,"end":123,"id":23},{"text":"stemming","start":124,"end":132,"id":24},{"text":"from","start":133,"end":137,"id":25},{"text":"different","start":138,"end":147,"id":26},{"text":"components","start":148,"end":158,"id":27},{"text":".","start":158,"end":159,"id":28}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":29,"end":50,"token_start":5,"token_end":6,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"2009) exhibit a policy such that with probability at least 1-1/n, the regret of the policy is of order log(n).","_input_hash":190495516,"_task_hash":2029015817,"tokens":[{"text":"2009","start":0,"end":4,"id":0},{"text":")","start":4,"end":5,"id":1},{"text":"exhibit","start":6,"end":13,"id":2},{"text":"a","start":14,"end":15,"id":3},{"text":"policy","start":16,"end":22,"id":4},{"text":"such","start":23,"end":27,"id":5},{"text":"that","start":28,"end":32,"id":6},{"text":"with","start":33,"end":37,"id":7},{"text":"probability","start":38,"end":49,"id":8},{"text":"at","start":50,"end":52,"id":9},{"text":"least","start":53,"end":58,"id":10},{"text":"1","start":59,"end":60,"id":11},{"text":"-","start":60,"end":61,"id":12},{"text":"1","start":61,"end":62,"id":13},{"text":"/","start":62,"end":63,"id":14},{"text":"n","start":63,"end":64,"id":15},{"text":",","start":64,"end":65,"id":16},{"text":"the","start":66,"end":69,"id":17},{"text":"regret","start":70,"end":76,"id":18},{"text":"of","start":77,"end":79,"id":19},{"text":"the","start":80,"end":83,"id":20},{"text":"policy","start":84,"end":90,"id":21},{"text":"is","start":91,"end":93,"id":22},{"text":"of","start":94,"end":96,"id":23},{"text":"order","start":97,"end":102,"id":24},{"text":"log(n","start":103,"end":108,"id":25},{"text":")","start":108,"end":109,"id":26},{"text":".","start":109,"end":110,"id":27}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"We empirically demonstrate the relevance of the method on simulated and real data, where it performs at least as well as existing methods while being faster.","_input_hash":-1043606083,"_task_hash":-841274767,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"empirically","start":3,"end":14,"id":1},{"text":"demonstrate","start":15,"end":26,"id":2},{"text":"the","start":27,"end":30,"id":3},{"text":"relevance","start":31,"end":40,"id":4},{"text":"of","start":41,"end":43,"id":5},{"text":"the","start":44,"end":47,"id":6},{"text":"method","start":48,"end":54,"id":7},{"text":"on","start":55,"end":57,"id":8},{"text":"simulated","start":58,"end":67,"id":9},{"text":"and","start":68,"end":71,"id":10},{"text":"real","start":72,"end":76,"id":11},{"text":"data","start":77,"end":81,"id":12},{"text":",","start":81,"end":82,"id":13},{"text":"where","start":83,"end":88,"id":14},{"text":"it","start":89,"end":91,"id":15},{"text":"performs","start":92,"end":100,"id":16},{"text":"at","start":101,"end":103,"id":17},{"text":"least","start":104,"end":109,"id":18},{"text":"as","start":110,"end":112,"id":19},{"text":"well","start":113,"end":117,"id":20},{"text":"as","start":118,"end":120,"id":21},{"text":"existing","start":121,"end":129,"id":22},{"text":"methods","start":130,"end":137,"id":23},{"text":"while","start":138,"end":143,"id":24},{"text":"being","start":144,"end":149,"id":25},{"text":"faster","start":150,"end":156,"id":26},{"text":".","start":156,"end":157,"id":27}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"The purpose of sufficient dimension reduction (SDR) is to find the low-dimensional subspace of input features that is sufficient for predicting output values.","_input_hash":-1052019811,"_task_hash":2098944781,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"purpose","start":4,"end":11,"id":1},{"text":"of","start":12,"end":14,"id":2},{"text":"sufficient","start":15,"end":25,"id":3},{"text":"dimension","start":26,"end":35,"id":4},{"text":"reduction","start":36,"end":45,"id":5},{"text":"(","start":46,"end":47,"id":6},{"text":"SDR","start":47,"end":50,"id":7},{"text":")","start":50,"end":51,"id":8},{"text":"is","start":52,"end":54,"id":9},{"text":"to","start":55,"end":57,"id":10},{"text":"find","start":58,"end":62,"id":11},{"text":"the","start":63,"end":66,"id":12},{"text":"low","start":67,"end":70,"id":13},{"text":"-","start":70,"end":71,"id":14},{"text":"dimensional","start":71,"end":82,"id":15},{"text":"subspace","start":83,"end":91,"id":16},{"text":"of","start":92,"end":94,"id":17},{"text":"input","start":95,"end":100,"id":18},{"text":"features","start":101,"end":109,"id":19},{"text":"that","start":110,"end":114,"id":20},{"text":"is","start":115,"end":117,"id":21},{"text":"sufficient","start":118,"end":128,"id":22},{"text":"for","start":129,"end":132,"id":23},{"text":"predicting","start":133,"end":143,"id":24},{"text":"output","start":144,"end":150,"id":25},{"text":"values","start":151,"end":157,"id":26},{"text":".","start":157,"end":158,"id":27}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"The framework provides a way to use continuous directional probability densities and the methods developed thereof for establishing densities over permutations.","_input_hash":-1286580586,"_task_hash":1132838742,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"framework","start":4,"end":13,"id":1},{"text":"provides","start":14,"end":22,"id":2},{"text":"a","start":23,"end":24,"id":3},{"text":"way","start":25,"end":28,"id":4},{"text":"to","start":29,"end":31,"id":5},{"text":"use","start":32,"end":35,"id":6},{"text":"continuous","start":36,"end":46,"id":7},{"text":"directional","start":47,"end":58,"id":8},{"text":"probability","start":59,"end":70,"id":9},{"text":"densities","start":71,"end":80,"id":10},{"text":"and","start":81,"end":84,"id":11},{"text":"the","start":85,"end":88,"id":12},{"text":"methods","start":89,"end":96,"id":13},{"text":"developed","start":97,"end":106,"id":14},{"text":"thereof","start":107,"end":114,"id":15},{"text":"for","start":115,"end":118,"id":16},{"text":"establishing","start":119,"end":131,"id":17},{"text":"densities","start":132,"end":141,"id":18},{"text":"over","start":142,"end":146,"id":19},{"text":"permutations","start":147,"end":159,"id":20},{"text":".","start":159,"end":160,"id":21}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We focus on a sparse setting where the total number of kernels is large but the number of non-zero components of the ground truth is relatively small, and prove that elastic-net MKL achieves the minimax learning rate on the $\\ell_2$-mixed-norm ball.","_input_hash":98710250,"_task_hash":1355305837,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"focus","start":3,"end":8,"id":1},{"text":"on","start":9,"end":11,"id":2},{"text":"a","start":12,"end":13,"id":3},{"text":"sparse","start":14,"end":20,"id":4},{"text":"setting","start":21,"end":28,"id":5},{"text":"where","start":29,"end":34,"id":6},{"text":"the","start":35,"end":38,"id":7},{"text":"total","start":39,"end":44,"id":8},{"text":"number","start":45,"end":51,"id":9},{"text":"of","start":52,"end":54,"id":10},{"text":"kernels","start":55,"end":62,"id":11},{"text":"is","start":63,"end":65,"id":12},{"text":"large","start":66,"end":71,"id":13},{"text":"but","start":72,"end":75,"id":14},{"text":"the","start":76,"end":79,"id":15},{"text":"number","start":80,"end":86,"id":16},{"text":"of","start":87,"end":89,"id":17},{"text":"non","start":90,"end":93,"id":18},{"text":"-","start":93,"end":94,"id":19},{"text":"zero","start":94,"end":98,"id":20},{"text":"components","start":99,"end":109,"id":21},{"text":"of","start":110,"end":112,"id":22},{"text":"the","start":113,"end":116,"id":23},{"text":"ground","start":117,"end":123,"id":24},{"text":"truth","start":124,"end":129,"id":25},{"text":"is","start":130,"end":132,"id":26},{"text":"relatively","start":133,"end":143,"id":27},{"text":"small","start":144,"end":149,"id":28},{"text":",","start":149,"end":150,"id":29},{"text":"and","start":151,"end":154,"id":30},{"text":"prove","start":155,"end":160,"id":31},{"text":"that","start":161,"end":165,"id":32},{"text":"elastic","start":166,"end":173,"id":33},{"text":"-","start":173,"end":174,"id":34},{"text":"net","start":174,"end":177,"id":35},{"text":"MKL","start":178,"end":181,"id":36},{"text":"achieves","start":182,"end":190,"id":37},{"text":"the","start":191,"end":194,"id":38},{"text":"minimax","start":195,"end":202,"id":39},{"text":"learning","start":203,"end":211,"id":40},{"text":"rate","start":212,"end":216,"id":41},{"text":"on","start":217,"end":219,"id":42},{"text":"the","start":220,"end":223,"id":43},{"text":"$","start":224,"end":225,"id":44},{"text":"\\ell_2$-mixed","start":225,"end":238,"id":45},{"text":"-","start":238,"end":239,"id":46},{"text":"norm","start":239,"end":243,"id":47},{"text":"ball","start":244,"end":248,"id":48},{"text":".","start":248,"end":249,"id":49}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Simulated annealing is a popular method for approaching the solution of a global optimization problem.","_input_hash":1054415795,"_task_hash":-184384468,"tokens":[{"text":"Simulated","start":0,"end":9,"id":0},{"text":"annealing","start":10,"end":19,"id":1},{"text":"is","start":20,"end":22,"id":2},{"text":"a","start":23,"end":24,"id":3},{"text":"popular","start":25,"end":32,"id":4},{"text":"method","start":33,"end":39,"id":5},{"text":"for","start":40,"end":43,"id":6},{"text":"approaching","start":44,"end":55,"id":7},{"text":"the","start":56,"end":59,"id":8},{"text":"solution","start":60,"end":68,"id":9},{"text":"of","start":69,"end":71,"id":10},{"text":"a","start":72,"end":73,"id":11},{"text":"global","start":74,"end":80,"id":12},{"text":"optimization","start":81,"end":93,"id":13},{"text":"problem","start":94,"end":101,"id":14},{"text":".","start":101,"end":102,"id":15}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":0,"end":19,"token_start":0,"token_end":1,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"The properties of the Bessel K model are analyzed when it is applied to Type I and Type II estimation.","_input_hash":1459485058,"_task_hash":446374173,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"properties","start":4,"end":14,"id":1},{"text":"of","start":15,"end":17,"id":2},{"text":"the","start":18,"end":21,"id":3},{"text":"Bessel","start":22,"end":28,"id":4},{"text":"K","start":29,"end":30,"id":5},{"text":"model","start":31,"end":36,"id":6},{"text":"are","start":37,"end":40,"id":7},{"text":"analyzed","start":41,"end":49,"id":8},{"text":"when","start":50,"end":54,"id":9},{"text":"it","start":55,"end":57,"id":10},{"text":"is","start":58,"end":60,"id":11},{"text":"applied","start":61,"end":68,"id":12},{"text":"to","start":69,"end":71,"id":13},{"text":"Type","start":72,"end":76,"id":14},{"text":"I","start":77,"end":78,"id":15},{"text":"and","start":79,"end":82,"id":16},{"text":"Type","start":83,"end":87,"id":17},{"text":"II","start":88,"end":90,"id":18},{"text":"estimation","start":91,"end":101,"id":19},{"text":".","start":101,"end":102,"id":20}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":22,"end":30,"token_start":4,"token_end":5,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We prove theoretically that this straightforward and computationally simple modification of LLE reduces LLE's sensitivity to noise.","_input_hash":1337426058,"_task_hash":1796659381,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"prove","start":3,"end":8,"id":1},{"text":"theoretically","start":9,"end":22,"id":2},{"text":"that","start":23,"end":27,"id":3},{"text":"this","start":28,"end":32,"id":4},{"text":"straightforward","start":33,"end":48,"id":5},{"text":"and","start":49,"end":52,"id":6},{"text":"computationally","start":53,"end":68,"id":7},{"text":"simple","start":69,"end":75,"id":8},{"text":"modification","start":76,"end":88,"id":9},{"text":"of","start":89,"end":91,"id":10},{"text":"LLE","start":92,"end":95,"id":11},{"text":"reduces","start":96,"end":103,"id":12},{"text":"LLE","start":104,"end":107,"id":13},{"text":"'s","start":107,"end":109,"id":14},{"text":"sensitivity","start":110,"end":121,"id":15},{"text":"to","start":122,"end":124,"id":16},{"text":"noise","start":125,"end":130,"id":17},{"text":".","start":130,"end":131,"id":18}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":92,"end":95,"token_start":11,"token_end":11,"label":"ALGO","answer":"accept"},{"start":104,"end":107,"token_start":13,"token_end":13,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"However, modern datasets including gene expression data need high-dimensional causal modeling in challenging situations with orders of magnitude more variables than observations (p>>n).","_input_hash":402902850,"_task_hash":-67687126,"tokens":[{"text":"However","start":0,"end":7,"id":0},{"text":",","start":7,"end":8,"id":1},{"text":"modern","start":9,"end":15,"id":2},{"text":"datasets","start":16,"end":24,"id":3},{"text":"including","start":25,"end":34,"id":4},{"text":"gene","start":35,"end":39,"id":5},{"text":"expression","start":40,"end":50,"id":6},{"text":"data","start":51,"end":55,"id":7},{"text":"need","start":56,"end":60,"id":8},{"text":"high","start":61,"end":65,"id":9},{"text":"-","start":65,"end":66,"id":10},{"text":"dimensional","start":66,"end":77,"id":11},{"text":"causal","start":78,"end":84,"id":12},{"text":"modeling","start":85,"end":93,"id":13},{"text":"in","start":94,"end":96,"id":14},{"text":"challenging","start":97,"end":108,"id":15},{"text":"situations","start":109,"end":119,"id":16},{"text":"with","start":120,"end":124,"id":17},{"text":"orders","start":125,"end":131,"id":18},{"text":"of","start":132,"end":134,"id":19},{"text":"magnitude","start":135,"end":144,"id":20},{"text":"more","start":145,"end":149,"id":21},{"text":"variables","start":150,"end":159,"id":22},{"text":"than","start":160,"end":164,"id":23},{"text":"observations","start":165,"end":177,"id":24},{"text":"(","start":178,"end":179,"id":25},{"text":"p>>n","start":179,"end":183,"id":26},{"text":")","start":183,"end":184,"id":27},{"text":".","start":184,"end":185,"id":28}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":61,"end":84,"token_start":9,"token_end":12,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"The estimator for the expectation of a function of the posterior is derived, and rates of consistency are shown.","_input_hash":1763479016,"_task_hash":1598080471,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"estimator","start":4,"end":13,"id":1},{"text":"for","start":14,"end":17,"id":2},{"text":"the","start":18,"end":21,"id":3},{"text":"expectation","start":22,"end":33,"id":4},{"text":"of","start":34,"end":36,"id":5},{"text":"a","start":37,"end":38,"id":6},{"text":"function","start":39,"end":47,"id":7},{"text":"of","start":48,"end":50,"id":8},{"text":"the","start":51,"end":54,"id":9},{"text":"posterior","start":55,"end":64,"id":10},{"text":"is","start":65,"end":67,"id":11},{"text":"derived","start":68,"end":75,"id":12},{"text":",","start":75,"end":76,"id":13},{"text":"and","start":77,"end":80,"id":14},{"text":"rates","start":81,"end":86,"id":15},{"text":"of","start":87,"end":89,"id":16},{"text":"consistency","start":90,"end":101,"id":17},{"text":"are","start":102,"end":105,"id":18},{"text":"shown","start":106,"end":111,"id":19},{"text":".","start":111,"end":112,"id":20}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Consequently, the weight vectors are highly sensitive to noise.","_input_hash":-1963221515,"_task_hash":508945984,"tokens":[{"text":"Consequently","start":0,"end":12,"id":0},{"text":",","start":12,"end":13,"id":1},{"text":"the","start":14,"end":17,"id":2},{"text":"weight","start":18,"end":24,"id":3},{"text":"vectors","start":25,"end":32,"id":4},{"text":"are","start":33,"end":36,"id":5},{"text":"highly","start":37,"end":43,"id":6},{"text":"sensitive","start":44,"end":53,"id":7},{"text":"to","start":54,"end":56,"id":8},{"text":"noise","start":57,"end":62,"id":9},{"text":".","start":62,"end":63,"id":10}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We propose a new family of online proximal algorithms for MKL (as well as for group-lasso and variants thereof), which overcomes that drawback.","_input_hash":1216000582,"_task_hash":-1595381339,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"propose","start":3,"end":10,"id":1},{"text":"a","start":11,"end":12,"id":2},{"text":"new","start":13,"end":16,"id":3},{"text":"family","start":17,"end":23,"id":4},{"text":"of","start":24,"end":26,"id":5},{"text":"online","start":27,"end":33,"id":6},{"text":"proximal","start":34,"end":42,"id":7},{"text":"algorithms","start":43,"end":53,"id":8},{"text":"for","start":54,"end":57,"id":9},{"text":"MKL","start":58,"end":61,"id":10},{"text":"(","start":62,"end":63,"id":11},{"text":"as","start":63,"end":65,"id":12},{"text":"well","start":66,"end":70,"id":13},{"text":"as","start":71,"end":73,"id":14},{"text":"for","start":74,"end":77,"id":15},{"text":"group","start":78,"end":83,"id":16},{"text":"-","start":83,"end":84,"id":17},{"text":"lasso","start":84,"end":89,"id":18},{"text":"and","start":90,"end":93,"id":19},{"text":"variants","start":94,"end":102,"id":20},{"text":"thereof","start":103,"end":110,"id":21},{"text":")","start":110,"end":111,"id":22},{"text":",","start":111,"end":112,"id":23},{"text":"which","start":113,"end":118,"id":24},{"text":"overcomes","start":119,"end":128,"id":25},{"text":"that","start":129,"end":133,"id":26},{"text":"drawback","start":134,"end":142,"id":27},{"text":".","start":142,"end":143,"id":28}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":27,"end":42,"token_start":6,"token_end":7,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We propose a novel graph construction technique that encodes global statistical information into node degrees through a ranking scheme.","_input_hash":-111720194,"_task_hash":-452894370,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"propose","start":3,"end":10,"id":1},{"text":"a","start":11,"end":12,"id":2},{"text":"novel","start":13,"end":18,"id":3},{"text":"graph","start":19,"end":24,"id":4},{"text":"construction","start":25,"end":37,"id":5},{"text":"technique","start":38,"end":47,"id":6},{"text":"that","start":48,"end":52,"id":7},{"text":"encodes","start":53,"end":60,"id":8},{"text":"global","start":61,"end":67,"id":9},{"text":"statistical","start":68,"end":79,"id":10},{"text":"information","start":80,"end":91,"id":11},{"text":"into","start":92,"end":96,"id":12},{"text":"node","start":97,"end":101,"id":13},{"text":"degrees","start":102,"end":109,"id":14},{"text":"through","start":110,"end":117,"id":15},{"text":"a","start":118,"end":119,"id":16},{"text":"ranking","start":120,"end":127,"id":17},{"text":"scheme","start":128,"end":134,"id":18},{"text":".","start":134,"end":135,"id":19}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"The decisional states form a partition of the lower-level causal states that is defined according to the higher-level user's knowledge.","_input_hash":-90963145,"_task_hash":-1141820307,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"decisional","start":4,"end":14,"id":1},{"text":"states","start":15,"end":21,"id":2},{"text":"form","start":22,"end":26,"id":3},{"text":"a","start":27,"end":28,"id":4},{"text":"partition","start":29,"end":38,"id":5},{"text":"of","start":39,"end":41,"id":6},{"text":"the","start":42,"end":45,"id":7},{"text":"lower","start":46,"end":51,"id":8},{"text":"-","start":51,"end":52,"id":9},{"text":"level","start":52,"end":57,"id":10},{"text":"causal","start":58,"end":64,"id":11},{"text":"states","start":65,"end":71,"id":12},{"text":"that","start":72,"end":76,"id":13},{"text":"is","start":77,"end":79,"id":14},{"text":"defined","start":80,"end":87,"id":15},{"text":"according","start":88,"end":97,"id":16},{"text":"to","start":98,"end":100,"id":17},{"text":"the","start":101,"end":104,"id":18},{"text":"higher","start":105,"end":111,"id":19},{"text":"-","start":111,"end":112,"id":20},{"text":"level","start":112,"end":117,"id":21},{"text":"user","start":118,"end":122,"id":22},{"text":"'s","start":122,"end":124,"id":23},{"text":"knowledge","start":125,"end":134,"id":24},{"text":".","start":134,"end":135,"id":25}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Then, we apply our optimization tools in the context of dictionary learning, where learned dictionary elements naturally organize in a prespecified arborescent structure, leading to a better performance in reconstruction of natural image patches.","_input_hash":1548222526,"_task_hash":-1371923486,"tokens":[{"text":"Then","start":0,"end":4,"id":0},{"text":",","start":4,"end":5,"id":1},{"text":"we","start":6,"end":8,"id":2},{"text":"apply","start":9,"end":14,"id":3},{"text":"our","start":15,"end":18,"id":4},{"text":"optimization","start":19,"end":31,"id":5},{"text":"tools","start":32,"end":37,"id":6},{"text":"in","start":38,"end":40,"id":7},{"text":"the","start":41,"end":44,"id":8},{"text":"context","start":45,"end":52,"id":9},{"text":"of","start":53,"end":55,"id":10},{"text":"dictionary","start":56,"end":66,"id":11},{"text":"learning","start":67,"end":75,"id":12},{"text":",","start":75,"end":76,"id":13},{"text":"where","start":77,"end":82,"id":14},{"text":"learned","start":83,"end":90,"id":15},{"text":"dictionary","start":91,"end":101,"id":16},{"text":"elements","start":102,"end":110,"id":17},{"text":"naturally","start":111,"end":120,"id":18},{"text":"organize","start":121,"end":129,"id":19},{"text":"in","start":130,"end":132,"id":20},{"text":"a","start":133,"end":134,"id":21},{"text":"prespecified","start":135,"end":147,"id":22},{"text":"arborescent","start":148,"end":159,"id":23},{"text":"structure","start":160,"end":169,"id":24},{"text":",","start":169,"end":170,"id":25},{"text":"leading","start":171,"end":178,"id":26},{"text":"to","start":179,"end":181,"id":27},{"text":"a","start":182,"end":183,"id":28},{"text":"better","start":184,"end":190,"id":29},{"text":"performance","start":191,"end":202,"id":30},{"text":"in","start":203,"end":205,"id":31},{"text":"reconstruction","start":206,"end":220,"id":32},{"text":"of","start":221,"end":223,"id":33},{"text":"natural","start":224,"end":231,"id":34},{"text":"image","start":232,"end":237,"id":35},{"text":"patches","start":238,"end":245,"id":36},{"text":".","start":245,"end":246,"id":37}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":19,"end":31,"token_start":5,"token_end":5,"label":"ALGO","answer":"reject"}],"answer":"reject"}
{"text":"It contains 1-NN as a special case.","_input_hash":1500027186,"_task_hash":2092447590,"tokens":[{"text":"It","start":0,"end":2,"id":0},{"text":"contains","start":3,"end":11,"id":1},{"text":"1-NN","start":12,"end":16,"id":2},{"text":"as","start":17,"end":19,"id":3},{"text":"a","start":20,"end":21,"id":4},{"text":"special","start":22,"end":29,"id":5},{"text":"case","start":30,"end":34,"id":6},{"text":".","start":34,"end":35,"id":7}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"However, it relies on a prediction function that is plagued by the curse of dimensionality, since there are far more features than samples, i.e., more voxels than fMRI volumes.","_input_hash":2069458988,"_task_hash":185856509,"tokens":[{"text":"However","start":0,"end":7,"id":0},{"text":",","start":7,"end":8,"id":1},{"text":"it","start":9,"end":11,"id":2},{"text":"relies","start":12,"end":18,"id":3},{"text":"on","start":19,"end":21,"id":4},{"text":"a","start":22,"end":23,"id":5},{"text":"prediction","start":24,"end":34,"id":6},{"text":"function","start":35,"end":43,"id":7},{"text":"that","start":44,"end":48,"id":8},{"text":"is","start":49,"end":51,"id":9},{"text":"plagued","start":52,"end":59,"id":10},{"text":"by","start":60,"end":62,"id":11},{"text":"the","start":63,"end":66,"id":12},{"text":"curse","start":67,"end":72,"id":13},{"text":"of","start":73,"end":75,"id":14},{"text":"dimensionality","start":76,"end":90,"id":15},{"text":",","start":90,"end":91,"id":16},{"text":"since","start":92,"end":97,"id":17},{"text":"there","start":98,"end":103,"id":18},{"text":"are","start":104,"end":107,"id":19},{"text":"far","start":108,"end":111,"id":20},{"text":"more","start":112,"end":116,"id":21},{"text":"features","start":117,"end":125,"id":22},{"text":"than","start":126,"end":130,"id":23},{"text":"samples","start":131,"end":138,"id":24},{"text":",","start":138,"end":139,"id":25},{"text":"i.e.","start":140,"end":144,"id":26},{"text":",","start":144,"end":145,"id":27},{"text":"more","start":146,"end":150,"id":28},{"text":"voxels","start":151,"end":157,"id":29},{"text":"than","start":158,"end":162,"id":30},{"text":"fMRI","start":163,"end":167,"id":31},{"text":"volumes","start":168,"end":175,"id":32},{"text":".","start":175,"end":176,"id":33}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"  More specifically, this paper studies the sign consistency of the Lasso under a sparse Poisson-like model.","_input_hash":271050432,"_task_hash":-393739612,"tokens":[{"text":"  ","start":0,"end":2,"id":0},{"text":"More","start":2,"end":6,"id":1},{"text":"specifically","start":7,"end":19,"id":2},{"text":",","start":19,"end":20,"id":3},{"text":"this","start":21,"end":25,"id":4},{"text":"paper","start":26,"end":31,"id":5},{"text":"studies","start":32,"end":39,"id":6},{"text":"the","start":40,"end":43,"id":7},{"text":"sign","start":44,"end":48,"id":8},{"text":"consistency","start":49,"end":60,"id":9},{"text":"of","start":61,"end":63,"id":10},{"text":"the","start":64,"end":67,"id":11},{"text":"Lasso","start":68,"end":73,"id":12},{"text":"under","start":74,"end":79,"id":13},{"text":"a","start":80,"end":81,"id":14},{"text":"sparse","start":82,"end":88,"id":15},{"text":"Poisson","start":89,"end":96,"id":16},{"text":"-","start":96,"end":97,"id":17},{"text":"like","start":97,"end":101,"id":18},{"text":"model","start":102,"end":107,"id":19},{"text":".","start":107,"end":108,"id":20}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":82,"end":101,"token_start":15,"token_end":18,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"The fuzzy logic can be easily introduced into the system, even if learning agents are build from simple binary classification machine learning algorithms by calculating the percentage of agreeing agents.","_input_hash":-1774464216,"_task_hash":-1526623302,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"fuzzy","start":4,"end":9,"id":1},{"text":"logic","start":10,"end":15,"id":2},{"text":"can","start":16,"end":19,"id":3},{"text":"be","start":20,"end":22,"id":4},{"text":"easily","start":23,"end":29,"id":5},{"text":"introduced","start":30,"end":40,"id":6},{"text":"into","start":41,"end":45,"id":7},{"text":"the","start":46,"end":49,"id":8},{"text":"system","start":50,"end":56,"id":9},{"text":",","start":56,"end":57,"id":10},{"text":"even","start":58,"end":62,"id":11},{"text":"if","start":63,"end":65,"id":12},{"text":"learning","start":66,"end":74,"id":13},{"text":"agents","start":75,"end":81,"id":14},{"text":"are","start":82,"end":85,"id":15},{"text":"build","start":86,"end":91,"id":16},{"text":"from","start":92,"end":96,"id":17},{"text":"simple","start":97,"end":103,"id":18},{"text":"binary","start":104,"end":110,"id":19},{"text":"classification","start":111,"end":125,"id":20},{"text":"machine","start":126,"end":133,"id":21},{"text":"learning","start":134,"end":142,"id":22},{"text":"algorithms","start":143,"end":153,"id":23},{"text":"by","start":154,"end":156,"id":24},{"text":"calculating","start":157,"end":168,"id":25},{"text":"the","start":169,"end":172,"id":26},{"text":"percentage","start":173,"end":183,"id":27},{"text":"of","start":184,"end":186,"id":28},{"text":"agreeing","start":187,"end":195,"id":29},{"text":"agents","start":196,"end":202,"id":30},{"text":".","start":202,"end":203,"id":31}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":4,"end":15,"token_start":1,"token_end":2,"label":"ALGO","answer":"accept"},{"start":104,"end":125,"token_start":19,"token_end":20,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Many inference problems involving questions of optimality ask for the maximum or the minimum of a finite set of unknown quantities.","_input_hash":-1698370089,"_task_hash":-1547592385,"tokens":[{"text":"Many","start":0,"end":4,"id":0},{"text":"inference","start":5,"end":14,"id":1},{"text":"problems","start":15,"end":23,"id":2},{"text":"involving","start":24,"end":33,"id":3},{"text":"questions","start":34,"end":43,"id":4},{"text":"of","start":44,"end":46,"id":5},{"text":"optimality","start":47,"end":57,"id":6},{"text":"ask","start":58,"end":61,"id":7},{"text":"for","start":62,"end":65,"id":8},{"text":"the","start":66,"end":69,"id":9},{"text":"maximum","start":70,"end":77,"id":10},{"text":"or","start":78,"end":80,"id":11},{"text":"the","start":81,"end":84,"id":12},{"text":"minimum","start":85,"end":92,"id":13},{"text":"of","start":93,"end":95,"id":14},{"text":"a","start":96,"end":97,"id":15},{"text":"finite","start":98,"end":104,"id":16},{"text":"set","start":105,"end":108,"id":17},{"text":"of","start":109,"end":111,"id":18},{"text":"unknown","start":112,"end":119,"id":19},{"text":"quantities","start":120,"end":130,"id":20},{"text":".","start":130,"end":131,"id":21}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We introduce a multi-way latent variable model for this new task, by extending the generative model of Bayesian canonical correlation analysis (CCA) both to take multi-way covariate information into account as population priors, and by reducing the dimensionality by an integrated factor analysis that assumes the metabolites to come in correlated groups.","_input_hash":474078842,"_task_hash":981715196,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"introduce","start":3,"end":12,"id":1},{"text":"a","start":13,"end":14,"id":2},{"text":"multi","start":15,"end":20,"id":3},{"text":"-","start":20,"end":21,"id":4},{"text":"way","start":21,"end":24,"id":5},{"text":"latent","start":25,"end":31,"id":6},{"text":"variable","start":32,"end":40,"id":7},{"text":"model","start":41,"end":46,"id":8},{"text":"for","start":47,"end":50,"id":9},{"text":"this","start":51,"end":55,"id":10},{"text":"new","start":56,"end":59,"id":11},{"text":"task","start":60,"end":64,"id":12},{"text":",","start":64,"end":65,"id":13},{"text":"by","start":66,"end":68,"id":14},{"text":"extending","start":69,"end":78,"id":15},{"text":"the","start":79,"end":82,"id":16},{"text":"generative","start":83,"end":93,"id":17},{"text":"model","start":94,"end":99,"id":18},{"text":"of","start":100,"end":102,"id":19},{"text":"Bayesian","start":103,"end":111,"id":20},{"text":"canonical","start":112,"end":121,"id":21},{"text":"correlation","start":122,"end":133,"id":22},{"text":"analysis","start":134,"end":142,"id":23},{"text":"(","start":143,"end":144,"id":24},{"text":"CCA","start":144,"end":147,"id":25},{"text":")","start":147,"end":148,"id":26},{"text":"both","start":149,"end":153,"id":27},{"text":"to","start":154,"end":156,"id":28},{"text":"take","start":157,"end":161,"id":29},{"text":"multi","start":162,"end":167,"id":30},{"text":"-","start":167,"end":168,"id":31},{"text":"way","start":168,"end":171,"id":32},{"text":"covariate","start":172,"end":181,"id":33},{"text":"information","start":182,"end":193,"id":34},{"text":"into","start":194,"end":198,"id":35},{"text":"account","start":199,"end":206,"id":36},{"text":"as","start":207,"end":209,"id":37},{"text":"population","start":210,"end":220,"id":38},{"text":"priors","start":221,"end":227,"id":39},{"text":",","start":227,"end":228,"id":40},{"text":"and","start":229,"end":232,"id":41},{"text":"by","start":233,"end":235,"id":42},{"text":"reducing","start":236,"end":244,"id":43},{"text":"the","start":245,"end":248,"id":44},{"text":"dimensionality","start":249,"end":263,"id":45},{"text":"by","start":264,"end":266,"id":46},{"text":"an","start":267,"end":269,"id":47},{"text":"integrated","start":270,"end":280,"id":48},{"text":"factor","start":281,"end":287,"id":49},{"text":"analysis","start":288,"end":296,"id":50},{"text":"that","start":297,"end":301,"id":51},{"text":"assumes","start":302,"end":309,"id":52},{"text":"the","start":310,"end":313,"id":53},{"text":"metabolites","start":314,"end":325,"id":54},{"text":"to","start":326,"end":328,"id":55},{"text":"come","start":329,"end":333,"id":56},{"text":"in","start":334,"end":336,"id":57},{"text":"correlated","start":337,"end":347,"id":58},{"text":"groups","start":348,"end":354,"id":59},{"text":".","start":354,"end":355,"id":60}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":103,"end":142,"token_start":20,"token_end":23,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We prove that the criterion for Markov equivalence provided by Zhao et al. (","_input_hash":-235937920,"_task_hash":2126952125,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"prove","start":3,"end":8,"id":1},{"text":"that","start":9,"end":13,"id":2},{"text":"the","start":14,"end":17,"id":3},{"text":"criterion","start":18,"end":27,"id":4},{"text":"for","start":28,"end":31,"id":5},{"text":"Markov","start":32,"end":38,"id":6},{"text":"equivalence","start":39,"end":50,"id":7},{"text":"provided","start":51,"end":59,"id":8},{"text":"by","start":60,"end":62,"id":9},{"text":"Zhao","start":63,"end":67,"id":10},{"text":"et","start":68,"end":70,"id":11},{"text":"al","start":71,"end":73,"id":12},{"text":".","start":73,"end":74,"id":13},{"text":"(","start":75,"end":76,"id":14}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Modelling the real world complexity of music is a challenge for machine learning.","_input_hash":-395366981,"_task_hash":1723999042,"tokens":[{"text":"Modelling","start":0,"end":9,"id":0},{"text":"the","start":10,"end":13,"id":1},{"text":"real","start":14,"end":18,"id":2},{"text":"world","start":19,"end":24,"id":3},{"text":"complexity","start":25,"end":35,"id":4},{"text":"of","start":36,"end":38,"id":5},{"text":"music","start":39,"end":44,"id":6},{"text":"is","start":45,"end":47,"id":7},{"text":"a","start":48,"end":49,"id":8},{"text":"challenge","start":50,"end":59,"id":9},{"text":"for","start":60,"end":63,"id":10},{"text":"machine","start":64,"end":71,"id":11},{"text":"learning","start":72,"end":80,"id":12},{"text":".","start":80,"end":81,"id":13}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"This connection suggests efficient solvers based on convex relaxation, which lead naturally to a variational M-type estimator equivalent to the least-absolute shrinkage and selection operator (Lasso).","_input_hash":493126394,"_task_hash":1696848853,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"connection","start":5,"end":15,"id":1},{"text":"suggests","start":16,"end":24,"id":2},{"text":"efficient","start":25,"end":34,"id":3},{"text":"solvers","start":35,"end":42,"id":4},{"text":"based","start":43,"end":48,"id":5},{"text":"on","start":49,"end":51,"id":6},{"text":"convex","start":52,"end":58,"id":7},{"text":"relaxation","start":59,"end":69,"id":8},{"text":",","start":69,"end":70,"id":9},{"text":"which","start":71,"end":76,"id":10},{"text":"lead","start":77,"end":81,"id":11},{"text":"naturally","start":82,"end":91,"id":12},{"text":"to","start":92,"end":94,"id":13},{"text":"a","start":95,"end":96,"id":14},{"text":"variational","start":97,"end":108,"id":15},{"text":"M","start":109,"end":110,"id":16},{"text":"-","start":110,"end":111,"id":17},{"text":"type","start":111,"end":115,"id":18},{"text":"estimator","start":116,"end":125,"id":19},{"text":"equivalent","start":126,"end":136,"id":20},{"text":"to","start":137,"end":139,"id":21},{"text":"the","start":140,"end":143,"id":22},{"text":"least","start":144,"end":149,"id":23},{"text":"-","start":149,"end":150,"id":24},{"text":"absolute","start":150,"end":158,"id":25},{"text":"shrinkage","start":159,"end":168,"id":26},{"text":"and","start":169,"end":172,"id":27},{"text":"selection","start":173,"end":182,"id":28},{"text":"operator","start":183,"end":191,"id":29},{"text":"(","start":192,"end":193,"id":30},{"text":"Lasso","start":193,"end":198,"id":31},{"text":")","start":198,"end":199,"id":32},{"text":".","start":199,"end":200,"id":33}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":193,"end":198,"token_start":31,"token_end":31,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"In this article, we derive concentration inequalities for the cross-validation estimate of the generalization error for subagged estimators, both for classification and regressor.","_input_hash":621867797,"_task_hash":593266195,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"this","start":3,"end":7,"id":1},{"text":"article","start":8,"end":15,"id":2},{"text":",","start":15,"end":16,"id":3},{"text":"we","start":17,"end":19,"id":4},{"text":"derive","start":20,"end":26,"id":5},{"text":"concentration","start":27,"end":40,"id":6},{"text":"inequalities","start":41,"end":53,"id":7},{"text":"for","start":54,"end":57,"id":8},{"text":"the","start":58,"end":61,"id":9},{"text":"cross","start":62,"end":67,"id":10},{"text":"-","start":67,"end":68,"id":11},{"text":"validation","start":68,"end":78,"id":12},{"text":"estimate","start":79,"end":87,"id":13},{"text":"of","start":88,"end":90,"id":14},{"text":"the","start":91,"end":94,"id":15},{"text":"generalization","start":95,"end":109,"id":16},{"text":"error","start":110,"end":115,"id":17},{"text":"for","start":116,"end":119,"id":18},{"text":"subagged","start":120,"end":128,"id":19},{"text":"estimators","start":129,"end":139,"id":20},{"text":",","start":139,"end":140,"id":21},{"text":"both","start":141,"end":145,"id":22},{"text":"for","start":146,"end":149,"id":23},{"text":"classification","start":150,"end":164,"id":24},{"text":"and","start":165,"end":168,"id":25},{"text":"regressor","start":169,"end":178,"id":26},{"text":".","start":178,"end":179,"id":27}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We derive a well-principled set of codes for both parameters and error residuals along with smooth approximations to lengths of these codes as to allow gradient descent optimization of description length, and go on to show that sparsification and feature selection using our approach is faster than the LASSO on several datasets from the UCI and StatLib repositories, with favorable generalization accuracy, while being fully automatic, requiring neither cross-validation nor tuning of regularization hyper-parameters, allowing even for a nonlinear expansion of the feature set followed by sparsification.","_input_hash":-1916215688,"_task_hash":-524054101,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"derive","start":3,"end":9,"id":1},{"text":"a","start":10,"end":11,"id":2},{"text":"well","start":12,"end":16,"id":3},{"text":"-","start":16,"end":17,"id":4},{"text":"principled","start":17,"end":27,"id":5},{"text":"set","start":28,"end":31,"id":6},{"text":"of","start":32,"end":34,"id":7},{"text":"codes","start":35,"end":40,"id":8},{"text":"for","start":41,"end":44,"id":9},{"text":"both","start":45,"end":49,"id":10},{"text":"parameters","start":50,"end":60,"id":11},{"text":"and","start":61,"end":64,"id":12},{"text":"error","start":65,"end":70,"id":13},{"text":"residuals","start":71,"end":80,"id":14},{"text":"along","start":81,"end":86,"id":15},{"text":"with","start":87,"end":91,"id":16},{"text":"smooth","start":92,"end":98,"id":17},{"text":"approximations","start":99,"end":113,"id":18},{"text":"to","start":114,"end":116,"id":19},{"text":"lengths","start":117,"end":124,"id":20},{"text":"of","start":125,"end":127,"id":21},{"text":"these","start":128,"end":133,"id":22},{"text":"codes","start":134,"end":139,"id":23},{"text":"as","start":140,"end":142,"id":24},{"text":"to","start":143,"end":145,"id":25},{"text":"allow","start":146,"end":151,"id":26},{"text":"gradient","start":152,"end":160,"id":27},{"text":"descent","start":161,"end":168,"id":28},{"text":"optimization","start":169,"end":181,"id":29},{"text":"of","start":182,"end":184,"id":30},{"text":"description","start":185,"end":196,"id":31},{"text":"length","start":197,"end":203,"id":32},{"text":",","start":203,"end":204,"id":33},{"text":"and","start":205,"end":208,"id":34},{"text":"go","start":209,"end":211,"id":35},{"text":"on","start":212,"end":214,"id":36},{"text":"to","start":215,"end":217,"id":37},{"text":"show","start":218,"end":222,"id":38},{"text":"that","start":223,"end":227,"id":39},{"text":"sparsification","start":228,"end":242,"id":40},{"text":"and","start":243,"end":246,"id":41},{"text":"feature","start":247,"end":254,"id":42},{"text":"selection","start":255,"end":264,"id":43},{"text":"using","start":265,"end":270,"id":44},{"text":"our","start":271,"end":274,"id":45},{"text":"approach","start":275,"end":283,"id":46},{"text":"is","start":284,"end":286,"id":47},{"text":"faster","start":287,"end":293,"id":48},{"text":"than","start":294,"end":298,"id":49},{"text":"the","start":299,"end":302,"id":50},{"text":"LASSO","start":303,"end":308,"id":51},{"text":"on","start":309,"end":311,"id":52},{"text":"several","start":312,"end":319,"id":53},{"text":"datasets","start":320,"end":328,"id":54},{"text":"from","start":329,"end":333,"id":55},{"text":"the","start":334,"end":337,"id":56},{"text":"UCI","start":338,"end":341,"id":57},{"text":"and","start":342,"end":345,"id":58},{"text":"StatLib","start":346,"end":353,"id":59},{"text":"repositories","start":354,"end":366,"id":60},{"text":",","start":366,"end":367,"id":61},{"text":"with","start":368,"end":372,"id":62},{"text":"favorable","start":373,"end":382,"id":63},{"text":"generalization","start":383,"end":397,"id":64},{"text":"accuracy","start":398,"end":406,"id":65},{"text":",","start":406,"end":407,"id":66},{"text":"while","start":408,"end":413,"id":67},{"text":"being","start":414,"end":419,"id":68},{"text":"fully","start":420,"end":425,"id":69},{"text":"automatic","start":426,"end":435,"id":70},{"text":",","start":435,"end":436,"id":71},{"text":"requiring","start":437,"end":446,"id":72},{"text":"neither","start":447,"end":454,"id":73},{"text":"cross","start":455,"end":460,"id":74},{"text":"-","start":460,"end":461,"id":75},{"text":"validation","start":461,"end":471,"id":76},{"text":"nor","start":472,"end":475,"id":77},{"text":"tuning","start":476,"end":482,"id":78},{"text":"of","start":483,"end":485,"id":79},{"text":"regularization","start":486,"end":500,"id":80},{"text":"hyper","start":501,"end":506,"id":81},{"text":"-","start":506,"end":507,"id":82},{"text":"parameters","start":507,"end":517,"id":83},{"text":",","start":517,"end":518,"id":84},{"text":"allowing","start":519,"end":527,"id":85},{"text":"even","start":528,"end":532,"id":86},{"text":"for","start":533,"end":536,"id":87},{"text":"a","start":537,"end":538,"id":88},{"text":"nonlinear","start":539,"end":548,"id":89},{"text":"expansion","start":549,"end":558,"id":90},{"text":"of","start":559,"end":561,"id":91},{"text":"the","start":562,"end":565,"id":92},{"text":"feature","start":566,"end":573,"id":93},{"text":"set","start":574,"end":577,"id":94},{"text":"followed","start":578,"end":586,"id":95},{"text":"by","start":587,"end":589,"id":96},{"text":"sparsification","start":590,"end":604,"id":97},{"text":".","start":604,"end":605,"id":98}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":303,"end":308,"token_start":51,"token_end":51,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Min-cut clustering, based on minimizing one of two heuristic cost-functions proposed by Shi and Malik, has spawned tremendous research, both analytic and algorithmic, in the graph partitioning and image segmentation communities over the last decade.","_input_hash":-2126837843,"_task_hash":-888286132,"tokens":[{"text":"Min","start":0,"end":3,"id":0},{"text":"-","start":3,"end":4,"id":1},{"text":"cut","start":4,"end":7,"id":2},{"text":"clustering","start":8,"end":18,"id":3},{"text":",","start":18,"end":19,"id":4},{"text":"based","start":20,"end":25,"id":5},{"text":"on","start":26,"end":28,"id":6},{"text":"minimizing","start":29,"end":39,"id":7},{"text":"one","start":40,"end":43,"id":8},{"text":"of","start":44,"end":46,"id":9},{"text":"two","start":47,"end":50,"id":10},{"text":"heuristic","start":51,"end":60,"id":11},{"text":"cost","start":61,"end":65,"id":12},{"text":"-","start":65,"end":66,"id":13},{"text":"functions","start":66,"end":75,"id":14},{"text":"proposed","start":76,"end":84,"id":15},{"text":"by","start":85,"end":87,"id":16},{"text":"Shi","start":88,"end":91,"id":17},{"text":"and","start":92,"end":95,"id":18},{"text":"Malik","start":96,"end":101,"id":19},{"text":",","start":101,"end":102,"id":20},{"text":"has","start":103,"end":106,"id":21},{"text":"spawned","start":107,"end":114,"id":22},{"text":"tremendous","start":115,"end":125,"id":23},{"text":"research","start":126,"end":134,"id":24},{"text":",","start":134,"end":135,"id":25},{"text":"both","start":136,"end":140,"id":26},{"text":"analytic","start":141,"end":149,"id":27},{"text":"and","start":150,"end":153,"id":28},{"text":"algorithmic","start":154,"end":165,"id":29},{"text":",","start":165,"end":166,"id":30},{"text":"in","start":167,"end":169,"id":31},{"text":"the","start":170,"end":173,"id":32},{"text":"graph","start":174,"end":179,"id":33},{"text":"partitioning","start":180,"end":192,"id":34},{"text":"and","start":193,"end":196,"id":35},{"text":"image","start":197,"end":202,"id":36},{"text":"segmentation","start":203,"end":215,"id":37},{"text":"communities","start":216,"end":227,"id":38},{"text":"over","start":228,"end":232,"id":39},{"text":"the","start":233,"end":236,"id":40},{"text":"last","start":237,"end":241,"id":41},{"text":"decade","start":242,"end":248,"id":42},{"text":".","start":248,"end":249,"id":43}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":0,"end":18,"token_start":0,"token_end":3,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"It is applicable to Gaussian as well as non-Gaussian data.","_input_hash":-488972745,"_task_hash":1654397282,"tokens":[{"text":"It","start":0,"end":2,"id":0},{"text":"is","start":3,"end":5,"id":1},{"text":"applicable","start":6,"end":16,"id":2},{"text":"to","start":17,"end":19,"id":3},{"text":"Gaussian","start":20,"end":28,"id":4},{"text":"as","start":29,"end":31,"id":5},{"text":"well","start":32,"end":36,"id":6},{"text":"as","start":37,"end":39,"id":7},{"text":"non","start":40,"end":43,"id":8},{"text":"-","start":43,"end":44,"id":9},{"text":"Gaussian","start":44,"end":52,"id":10},{"text":"data","start":53,"end":57,"id":11},{"text":".","start":57,"end":58,"id":12}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"It is shown how this can be used to build a heuristic approximation to the maximum relationship over a finite set of Gaussian variables, allowing approximate inference by Expectation Propagation on such quantities.","_input_hash":1454766528,"_task_hash":-341878961,"tokens":[{"text":"It","start":0,"end":2,"id":0},{"text":"is","start":3,"end":5,"id":1},{"text":"shown","start":6,"end":11,"id":2},{"text":"how","start":12,"end":15,"id":3},{"text":"this","start":16,"end":20,"id":4},{"text":"can","start":21,"end":24,"id":5},{"text":"be","start":25,"end":27,"id":6},{"text":"used","start":28,"end":32,"id":7},{"text":"to","start":33,"end":35,"id":8},{"text":"build","start":36,"end":41,"id":9},{"text":"a","start":42,"end":43,"id":10},{"text":"heuristic","start":44,"end":53,"id":11},{"text":"approximation","start":54,"end":67,"id":12},{"text":"to","start":68,"end":70,"id":13},{"text":"the","start":71,"end":74,"id":14},{"text":"maximum","start":75,"end":82,"id":15},{"text":"relationship","start":83,"end":95,"id":16},{"text":"over","start":96,"end":100,"id":17},{"text":"a","start":101,"end":102,"id":18},{"text":"finite","start":103,"end":109,"id":19},{"text":"set","start":110,"end":113,"id":20},{"text":"of","start":114,"end":116,"id":21},{"text":"Gaussian","start":117,"end":125,"id":22},{"text":"variables","start":126,"end":135,"id":23},{"text":",","start":135,"end":136,"id":24},{"text":"allowing","start":137,"end":145,"id":25},{"text":"approximate","start":146,"end":157,"id":26},{"text":"inference","start":158,"end":167,"id":27},{"text":"by","start":168,"end":170,"id":28},{"text":"Expectation","start":171,"end":182,"id":29},{"text":"Propagation","start":183,"end":194,"id":30},{"text":"on","start":195,"end":197,"id":31},{"text":"such","start":198,"end":202,"id":32},{"text":"quantities","start":203,"end":213,"id":33},{"text":".","start":213,"end":214,"id":34}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":171,"end":194,"token_start":29,"token_end":30,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We perform a comparative analysis of two probabilistic models;","_input_hash":-2005477636,"_task_hash":-840859696,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"perform","start":3,"end":10,"id":1},{"text":"a","start":11,"end":12,"id":2},{"text":"comparative","start":13,"end":24,"id":3},{"text":"analysis","start":25,"end":33,"id":4},{"text":"of","start":34,"end":36,"id":5},{"text":"two","start":37,"end":40,"id":6},{"text":"probabilistic","start":41,"end":54,"id":7},{"text":"models","start":55,"end":61,"id":8},{"text":";","start":61,"end":62,"id":9}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We propose a simple kernel based nearest neighbor approach for handwritten digit classification.","_input_hash":-1778104689,"_task_hash":-975481991,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"propose","start":3,"end":10,"id":1},{"text":"a","start":11,"end":12,"id":2},{"text":"simple","start":13,"end":19,"id":3},{"text":"kernel","start":20,"end":26,"id":4},{"text":"based","start":27,"end":32,"id":5},{"text":"nearest","start":33,"end":40,"id":6},{"text":"neighbor","start":41,"end":49,"id":7},{"text":"approach","start":50,"end":58,"id":8},{"text":"for","start":59,"end":62,"id":9},{"text":"handwritten","start":63,"end":74,"id":10},{"text":"digit","start":75,"end":80,"id":11},{"text":"classification","start":81,"end":95,"id":12},{"text":".","start":95,"end":96,"id":13}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":20,"end":49,"token_start":4,"token_end":7,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"With Dirichlet Process priors and an efficient implementation the models are highly scalable, as demonstrated with a social network from the Last.fm web site, with 670,000 nodes and 1.89 million links.","_input_hash":913960638,"_task_hash":1436079070,"tokens":[{"text":"With","start":0,"end":4,"id":0},{"text":"Dirichlet","start":5,"end":14,"id":1},{"text":"Process","start":15,"end":22,"id":2},{"text":"priors","start":23,"end":29,"id":3},{"text":"and","start":30,"end":33,"id":4},{"text":"an","start":34,"end":36,"id":5},{"text":"efficient","start":37,"end":46,"id":6},{"text":"implementation","start":47,"end":61,"id":7},{"text":"the","start":62,"end":65,"id":8},{"text":"models","start":66,"end":72,"id":9},{"text":"are","start":73,"end":76,"id":10},{"text":"highly","start":77,"end":83,"id":11},{"text":"scalable","start":84,"end":92,"id":12},{"text":",","start":92,"end":93,"id":13},{"text":"as","start":94,"end":96,"id":14},{"text":"demonstrated","start":97,"end":109,"id":15},{"text":"with","start":110,"end":114,"id":16},{"text":"a","start":115,"end":116,"id":17},{"text":"social","start":117,"end":123,"id":18},{"text":"network","start":124,"end":131,"id":19},{"text":"from","start":132,"end":136,"id":20},{"text":"the","start":137,"end":140,"id":21},{"text":"Last.fm","start":141,"end":148,"id":22},{"text":"web","start":149,"end":152,"id":23},{"text":"site","start":153,"end":157,"id":24},{"text":",","start":157,"end":158,"id":25},{"text":"with","start":159,"end":163,"id":26},{"text":"670,000","start":164,"end":171,"id":27},{"text":"nodes","start":172,"end":177,"id":28},{"text":"and","start":178,"end":181,"id":29},{"text":"1.89","start":182,"end":186,"id":30},{"text":"million","start":187,"end":194,"id":31},{"text":"links","start":195,"end":200,"id":32},{"text":".","start":200,"end":201,"id":33}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":5,"end":22,"token_start":1,"token_end":2,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We show that it is able to generate images with interesting higher-order correlations when trained on natural images or samples from an occlusion based model.","_input_hash":379820961,"_task_hash":519287344,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"show","start":3,"end":7,"id":1},{"text":"that","start":8,"end":12,"id":2},{"text":"it","start":13,"end":15,"id":3},{"text":"is","start":16,"end":18,"id":4},{"text":"able","start":19,"end":23,"id":5},{"text":"to","start":24,"end":26,"id":6},{"text":"generate","start":27,"end":35,"id":7},{"text":"images","start":36,"end":42,"id":8},{"text":"with","start":43,"end":47,"id":9},{"text":"interesting","start":48,"end":59,"id":10},{"text":"higher","start":60,"end":66,"id":11},{"text":"-","start":66,"end":67,"id":12},{"text":"order","start":67,"end":72,"id":13},{"text":"correlations","start":73,"end":85,"id":14},{"text":"when","start":86,"end":90,"id":15},{"text":"trained","start":91,"end":98,"id":16},{"text":"on","start":99,"end":101,"id":17},{"text":"natural","start":102,"end":109,"id":18},{"text":"images","start":110,"end":116,"id":19},{"text":"or","start":117,"end":119,"id":20},{"text":"samples","start":120,"end":127,"id":21},{"text":"from","start":128,"end":132,"id":22},{"text":"an","start":133,"end":135,"id":23},{"text":"occlusion","start":136,"end":145,"id":24},{"text":"based","start":146,"end":151,"id":25},{"text":"model","start":152,"end":157,"id":26},{"text":".","start":157,"end":158,"id":27}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":136,"end":151,"token_start":24,"token_end":25,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We use this approximation to derive a finite sum-representation that converges almost surely to Ferguson's representation of the Dirichlet process based on arrivals of a homogeneous Poisson process.","_input_hash":-1714976526,"_task_hash":-1110803032,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"use","start":3,"end":6,"id":1},{"text":"this","start":7,"end":11,"id":2},{"text":"approximation","start":12,"end":25,"id":3},{"text":"to","start":26,"end":28,"id":4},{"text":"derive","start":29,"end":35,"id":5},{"text":"a","start":36,"end":37,"id":6},{"text":"finite","start":38,"end":44,"id":7},{"text":"sum","start":45,"end":48,"id":8},{"text":"-","start":48,"end":49,"id":9},{"text":"representation","start":49,"end":63,"id":10},{"text":"that","start":64,"end":68,"id":11},{"text":"converges","start":69,"end":78,"id":12},{"text":"almost","start":79,"end":85,"id":13},{"text":"surely","start":86,"end":92,"id":14},{"text":"to","start":93,"end":95,"id":15},{"text":"Ferguson","start":96,"end":104,"id":16},{"text":"'s","start":104,"end":106,"id":17},{"text":"representation","start":107,"end":121,"id":18},{"text":"of","start":122,"end":124,"id":19},{"text":"the","start":125,"end":128,"id":20},{"text":"Dirichlet","start":129,"end":138,"id":21},{"text":"process","start":139,"end":146,"id":22},{"text":"based","start":147,"end":152,"id":23},{"text":"on","start":153,"end":155,"id":24},{"text":"arrivals","start":156,"end":164,"id":25},{"text":"of","start":165,"end":167,"id":26},{"text":"a","start":168,"end":169,"id":27},{"text":"homogeneous","start":170,"end":181,"id":28},{"text":"Poisson","start":182,"end":189,"id":29},{"text":"process","start":190,"end":197,"id":30},{"text":".","start":197,"end":198,"id":31}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":129,"end":138,"token_start":21,"token_end":21,"label":"ALGO","answer":"accept"},{"start":170,"end":189,"token_start":28,"token_end":29,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Missing data is treated as Missing At Random mechanism by implementing maximum likelihood algorithm.","_input_hash":-445726575,"_task_hash":-452880343,"tokens":[{"text":"Missing","start":0,"end":7,"id":0},{"text":"data","start":8,"end":12,"id":1},{"text":"is","start":13,"end":15,"id":2},{"text":"treated","start":16,"end":23,"id":3},{"text":"as","start":24,"end":26,"id":4},{"text":"Missing","start":27,"end":34,"id":5},{"text":"At","start":35,"end":37,"id":6},{"text":"Random","start":38,"end":44,"id":7},{"text":"mechanism","start":45,"end":54,"id":8},{"text":"by","start":55,"end":57,"id":9},{"text":"implementing","start":58,"end":70,"id":10},{"text":"maximum","start":71,"end":78,"id":11},{"text":"likelihood","start":79,"end":89,"id":12},{"text":"algorithm","start":90,"end":99,"id":13},{"text":".","start":99,"end":100,"id":14}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":71,"end":89,"token_start":11,"token_end":12,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Still, the results are positive and make a positive case for the introduction of the bistatic configuration.","_input_hash":-112720826,"_task_hash":-909099500,"tokens":[{"text":"Still","start":0,"end":5,"id":0},{"text":",","start":5,"end":6,"id":1},{"text":"the","start":7,"end":10,"id":2},{"text":"results","start":11,"end":18,"id":3},{"text":"are","start":19,"end":22,"id":4},{"text":"positive","start":23,"end":31,"id":5},{"text":"and","start":32,"end":35,"id":6},{"text":"make","start":36,"end":40,"id":7},{"text":"a","start":41,"end":42,"id":8},{"text":"positive","start":43,"end":51,"id":9},{"text":"case","start":52,"end":56,"id":10},{"text":"for","start":57,"end":60,"id":11},{"text":"the","start":61,"end":64,"id":12},{"text":"introduction","start":65,"end":77,"id":13},{"text":"of","start":78,"end":80,"id":14},{"text":"the","start":81,"end":84,"id":15},{"text":"bistatic","start":85,"end":93,"id":16},{"text":"configuration","start":94,"end":107,"id":17},{"text":".","start":107,"end":108,"id":18}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"  We use both simulated data and real data to test our methods in both chapters.","_input_hash":-389650462,"_task_hash":67022977,"tokens":[{"text":"  ","start":0,"end":2,"id":0},{"text":"We","start":2,"end":4,"id":1},{"text":"use","start":5,"end":8,"id":2},{"text":"both","start":9,"end":13,"id":3},{"text":"simulated","start":14,"end":23,"id":4},{"text":"data","start":24,"end":28,"id":5},{"text":"and","start":29,"end":32,"id":6},{"text":"real","start":33,"end":37,"id":7},{"text":"data","start":38,"end":42,"id":8},{"text":"to","start":43,"end":45,"id":9},{"text":"test","start":46,"end":50,"id":10},{"text":"our","start":51,"end":54,"id":11},{"text":"methods","start":55,"end":62,"id":12},{"text":"in","start":63,"end":65,"id":13},{"text":"both","start":66,"end":70,"id":14},{"text":"chapters","start":71,"end":79,"id":15},{"text":".","start":79,"end":80,"id":16}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We show that by assuming group-wise sparse factors, active in a subset of the sets, the variation can be decomposed into factors explaining relationships between the sets and factors explaining away set-specific variation.","_input_hash":586675955,"_task_hash":1499123280,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"show","start":3,"end":7,"id":1},{"text":"that","start":8,"end":12,"id":2},{"text":"by","start":13,"end":15,"id":3},{"text":"assuming","start":16,"end":24,"id":4},{"text":"group","start":25,"end":30,"id":5},{"text":"-","start":30,"end":31,"id":6},{"text":"wise","start":31,"end":35,"id":7},{"text":"sparse","start":36,"end":42,"id":8},{"text":"factors","start":43,"end":50,"id":9},{"text":",","start":50,"end":51,"id":10},{"text":"active","start":52,"end":58,"id":11},{"text":"in","start":59,"end":61,"id":12},{"text":"a","start":62,"end":63,"id":13},{"text":"subset","start":64,"end":70,"id":14},{"text":"of","start":71,"end":73,"id":15},{"text":"the","start":74,"end":77,"id":16},{"text":"sets","start":78,"end":82,"id":17},{"text":",","start":82,"end":83,"id":18},{"text":"the","start":84,"end":87,"id":19},{"text":"variation","start":88,"end":97,"id":20},{"text":"can","start":98,"end":101,"id":21},{"text":"be","start":102,"end":104,"id":22},{"text":"decomposed","start":105,"end":115,"id":23},{"text":"into","start":116,"end":120,"id":24},{"text":"factors","start":121,"end":128,"id":25},{"text":"explaining","start":129,"end":139,"id":26},{"text":"relationships","start":140,"end":153,"id":27},{"text":"between","start":154,"end":161,"id":28},{"text":"the","start":162,"end":165,"id":29},{"text":"sets","start":166,"end":170,"id":30},{"text":"and","start":171,"end":174,"id":31},{"text":"factors","start":175,"end":182,"id":32},{"text":"explaining","start":183,"end":193,"id":33},{"text":"away","start":194,"end":198,"id":34},{"text":"set","start":199,"end":202,"id":35},{"text":"-","start":202,"end":203,"id":36},{"text":"specific","start":203,"end":211,"id":37},{"text":"variation","start":212,"end":221,"id":38},{"text":".","start":221,"end":222,"id":39}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"The framework, which we call SLIM (Sparse Linear Identifiable Multivariate modeling), is validated and bench-marked on artificial and real biological data sets.","_input_hash":-841058220,"_task_hash":3814561,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"framework","start":4,"end":13,"id":1},{"text":",","start":13,"end":14,"id":2},{"text":"which","start":15,"end":20,"id":3},{"text":"we","start":21,"end":23,"id":4},{"text":"call","start":24,"end":28,"id":5},{"text":"SLIM","start":29,"end":33,"id":6},{"text":"(","start":34,"end":35,"id":7},{"text":"Sparse","start":35,"end":41,"id":8},{"text":"Linear","start":42,"end":48,"id":9},{"text":"Identifiable","start":49,"end":61,"id":10},{"text":"Multivariate","start":62,"end":74,"id":11},{"text":"modeling","start":75,"end":83,"id":12},{"text":")","start":83,"end":84,"id":13},{"text":",","start":84,"end":85,"id":14},{"text":"is","start":86,"end":88,"id":15},{"text":"validated","start":89,"end":98,"id":16},{"text":"and","start":99,"end":102,"id":17},{"text":"bench","start":103,"end":108,"id":18},{"text":"-","start":108,"end":109,"id":19},{"text":"marked","start":109,"end":115,"id":20},{"text":"on","start":116,"end":118,"id":21},{"text":"artificial","start":119,"end":129,"id":22},{"text":"and","start":130,"end":133,"id":23},{"text":"real","start":134,"end":138,"id":24},{"text":"biological","start":139,"end":149,"id":25},{"text":"data","start":150,"end":154,"id":26},{"text":"sets","start":155,"end":159,"id":27},{"text":".","start":159,"end":160,"id":28}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":35,"end":74,"token_start":8,"token_end":11,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"This approach encodes the spatial structure of the data at different scales into the regularization, which makes the overall prediction procedure more robust to inter-subject variability.","_input_hash":1001344514,"_task_hash":-1364284199,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"approach","start":5,"end":13,"id":1},{"text":"encodes","start":14,"end":21,"id":2},{"text":"the","start":22,"end":25,"id":3},{"text":"spatial","start":26,"end":33,"id":4},{"text":"structure","start":34,"end":43,"id":5},{"text":"of","start":44,"end":46,"id":6},{"text":"the","start":47,"end":50,"id":7},{"text":"data","start":51,"end":55,"id":8},{"text":"at","start":56,"end":58,"id":9},{"text":"different","start":59,"end":68,"id":10},{"text":"scales","start":69,"end":75,"id":11},{"text":"into","start":76,"end":80,"id":12},{"text":"the","start":81,"end":84,"id":13},{"text":"regularization","start":85,"end":99,"id":14},{"text":",","start":99,"end":100,"id":15},{"text":"which","start":101,"end":106,"id":16},{"text":"makes","start":107,"end":112,"id":17},{"text":"the","start":113,"end":116,"id":18},{"text":"overall","start":117,"end":124,"id":19},{"text":"prediction","start":125,"end":135,"id":20},{"text":"procedure","start":136,"end":145,"id":21},{"text":"more","start":146,"end":150,"id":22},{"text":"robust","start":151,"end":157,"id":23},{"text":"to","start":158,"end":160,"id":24},{"text":"inter","start":161,"end":166,"id":25},{"text":"-","start":166,"end":167,"id":26},{"text":"subject","start":167,"end":174,"id":27},{"text":"variability","start":175,"end":186,"id":28},{"text":".","start":186,"end":187,"id":29}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"This allows the algorithm to quickly compute approximate solutions with roughly the same classification accuracy as the optimal ones, considerably reducing the training time.","_input_hash":2017069756,"_task_hash":1674284664,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"allows","start":5,"end":11,"id":1},{"text":"the","start":12,"end":15,"id":2},{"text":"algorithm","start":16,"end":25,"id":3},{"text":"to","start":26,"end":28,"id":4},{"text":"quickly","start":29,"end":36,"id":5},{"text":"compute","start":37,"end":44,"id":6},{"text":"approximate","start":45,"end":56,"id":7},{"text":"solutions","start":57,"end":66,"id":8},{"text":"with","start":67,"end":71,"id":9},{"text":"roughly","start":72,"end":79,"id":10},{"text":"the","start":80,"end":83,"id":11},{"text":"same","start":84,"end":88,"id":12},{"text":"classification","start":89,"end":103,"id":13},{"text":"accuracy","start":104,"end":112,"id":14},{"text":"as","start":113,"end":115,"id":15},{"text":"the","start":116,"end":119,"id":16},{"text":"optimal","start":120,"end":127,"id":17},{"text":"ones","start":128,"end":132,"id":18},{"text":",","start":132,"end":133,"id":19},{"text":"considerably","start":134,"end":146,"id":20},{"text":"reducing","start":147,"end":155,"id":21},{"text":"the","start":156,"end":159,"id":22},{"text":"training","start":160,"end":168,"id":23},{"text":"time","start":169,"end":173,"id":24},{"text":".","start":173,"end":174,"id":25}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We derive a representation of DILN as a normalized collection of gamma-distributed random variables, and study its statistical properties.","_input_hash":-1564794614,"_task_hash":-1625035094,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"derive","start":3,"end":9,"id":1},{"text":"a","start":10,"end":11,"id":2},{"text":"representation","start":12,"end":26,"id":3},{"text":"of","start":27,"end":29,"id":4},{"text":"DILN","start":30,"end":34,"id":5},{"text":"as","start":35,"end":37,"id":6},{"text":"a","start":38,"end":39,"id":7},{"text":"normalized","start":40,"end":50,"id":8},{"text":"collection","start":51,"end":61,"id":9},{"text":"of","start":62,"end":64,"id":10},{"text":"gamma","start":65,"end":70,"id":11},{"text":"-","start":70,"end":71,"id":12},{"text":"distributed","start":71,"end":82,"id":13},{"text":"random","start":83,"end":89,"id":14},{"text":"variables","start":90,"end":99,"id":15},{"text":",","start":99,"end":100,"id":16},{"text":"and","start":101,"end":104,"id":17},{"text":"study","start":105,"end":110,"id":18},{"text":"its","start":111,"end":114,"id":19},{"text":"statistical","start":115,"end":126,"id":20},{"text":"properties","start":127,"end":137,"id":21},{"text":".","start":137,"end":138,"id":22}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We study Bahadur's type exponential bounds on the minimax accuracy confidence function based on the excess risk.","_input_hash":660460808,"_task_hash":-1576826462,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"study","start":3,"end":8,"id":1},{"text":"Bahadur","start":9,"end":16,"id":2},{"text":"'s","start":16,"end":18,"id":3},{"text":"type","start":19,"end":23,"id":4},{"text":"exponential","start":24,"end":35,"id":5},{"text":"bounds","start":36,"end":42,"id":6},{"text":"on","start":43,"end":45,"id":7},{"text":"the","start":46,"end":49,"id":8},{"text":"minimax","start":50,"end":57,"id":9},{"text":"accuracy","start":58,"end":66,"id":10},{"text":"confidence","start":67,"end":77,"id":11},{"text":"function","start":78,"end":86,"id":12},{"text":"based","start":87,"end":92,"id":13},{"text":"on","start":93,"end":95,"id":14},{"text":"the","start":96,"end":99,"id":15},{"text":"excess","start":100,"end":106,"id":16},{"text":"risk","start":107,"end":111,"id":17},{"text":".","start":111,"end":112,"id":18}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"In this article, it is shown that with the proposed kernel function it is feasible to make the pointwise MSE of the density estimator converge at O(n^-2/3) regardless of the dimension of the vector space, provided that the probability density function at the point of interest meets certain conditions.","_input_hash":-521693369,"_task_hash":-1250994352,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"this","start":3,"end":7,"id":1},{"text":"article","start":8,"end":15,"id":2},{"text":",","start":15,"end":16,"id":3},{"text":"it","start":17,"end":19,"id":4},{"text":"is","start":20,"end":22,"id":5},{"text":"shown","start":23,"end":28,"id":6},{"text":"that","start":29,"end":33,"id":7},{"text":"with","start":34,"end":38,"id":8},{"text":"the","start":39,"end":42,"id":9},{"text":"proposed","start":43,"end":51,"id":10},{"text":"kernel","start":52,"end":58,"id":11},{"text":"function","start":59,"end":67,"id":12},{"text":"it","start":68,"end":70,"id":13},{"text":"is","start":71,"end":73,"id":14},{"text":"feasible","start":74,"end":82,"id":15},{"text":"to","start":83,"end":85,"id":16},{"text":"make","start":86,"end":90,"id":17},{"text":"the","start":91,"end":94,"id":18},{"text":"pointwise","start":95,"end":104,"id":19},{"text":"MSE","start":105,"end":108,"id":20},{"text":"of","start":109,"end":111,"id":21},{"text":"the","start":112,"end":115,"id":22},{"text":"density","start":116,"end":123,"id":23},{"text":"estimator","start":124,"end":133,"id":24},{"text":"converge","start":134,"end":142,"id":25},{"text":"at","start":143,"end":145,"id":26},{"text":"O(n^-2/3","start":146,"end":154,"id":27},{"text":")","start":154,"end":155,"id":28},{"text":"regardless","start":156,"end":166,"id":29},{"text":"of","start":167,"end":169,"id":30},{"text":"the","start":170,"end":173,"id":31},{"text":"dimension","start":174,"end":183,"id":32},{"text":"of","start":184,"end":186,"id":33},{"text":"the","start":187,"end":190,"id":34},{"text":"vector","start":191,"end":197,"id":35},{"text":"space","start":198,"end":203,"id":36},{"text":",","start":203,"end":204,"id":37},{"text":"provided","start":205,"end":213,"id":38},{"text":"that","start":214,"end":218,"id":39},{"text":"the","start":219,"end":222,"id":40},{"text":"probability","start":223,"end":234,"id":41},{"text":"density","start":235,"end":242,"id":42},{"text":"function","start":243,"end":251,"id":43},{"text":"at","start":252,"end":254,"id":44},{"text":"the","start":255,"end":258,"id":45},{"text":"point","start":259,"end":264,"id":46},{"text":"of","start":265,"end":267,"id":47},{"text":"interest","start":268,"end":276,"id":48},{"text":"meets","start":277,"end":282,"id":49},{"text":"certain","start":283,"end":290,"id":50},{"text":"conditions","start":291,"end":301,"id":51},{"text":".","start":301,"end":302,"id":52}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"General loss functions and class of predictors with finite VC-dimension are considered.","_input_hash":1006505184,"_task_hash":413456921,"tokens":[{"text":"General","start":0,"end":7,"id":0},{"text":"loss","start":8,"end":12,"id":1},{"text":"functions","start":13,"end":22,"id":2},{"text":"and","start":23,"end":26,"id":3},{"text":"class","start":27,"end":32,"id":4},{"text":"of","start":33,"end":35,"id":5},{"text":"predictors","start":36,"end":46,"id":6},{"text":"with","start":47,"end":51,"id":7},{"text":"finite","start":52,"end":58,"id":8},{"text":"VC","start":59,"end":61,"id":9},{"text":"-","start":61,"end":62,"id":10},{"text":"dimension","start":62,"end":71,"id":11},{"text":"are","start":72,"end":75,"id":12},{"text":"considered","start":76,"end":86,"id":13},{"text":".","start":86,"end":87,"id":14}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"This paper develops a general theoretical framework to analyze structured sparse recovery problems using the notation of dual certificate.","_input_hash":-261352855,"_task_hash":1404242428,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"paper","start":5,"end":10,"id":1},{"text":"develops","start":11,"end":19,"id":2},{"text":"a","start":20,"end":21,"id":3},{"text":"general","start":22,"end":29,"id":4},{"text":"theoretical","start":30,"end":41,"id":5},{"text":"framework","start":42,"end":51,"id":6},{"text":"to","start":52,"end":54,"id":7},{"text":"analyze","start":55,"end":62,"id":8},{"text":"structured","start":63,"end":73,"id":9},{"text":"sparse","start":74,"end":80,"id":10},{"text":"recovery","start":81,"end":89,"id":11},{"text":"problems","start":90,"end":98,"id":12},{"text":"using","start":99,"end":104,"id":13},{"text":"the","start":105,"end":108,"id":14},{"text":"notation","start":109,"end":117,"id":15},{"text":"of","start":118,"end":120,"id":16},{"text":"dual","start":121,"end":125,"id":17},{"text":"certificate","start":126,"end":137,"id":18},{"text":".","start":137,"end":138,"id":19}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"With only a few nearest neighbors (or most similar images) to vote, the test set error rate on MNIST database could reach about 1.5%-2.0%, which is very close to many advanced models.","_input_hash":-2131117482,"_task_hash":1899270129,"tokens":[{"text":"With","start":0,"end":4,"id":0},{"text":"only","start":5,"end":9,"id":1},{"text":"a","start":10,"end":11,"id":2},{"text":"few","start":12,"end":15,"id":3},{"text":"nearest","start":16,"end":23,"id":4},{"text":"neighbors","start":24,"end":33,"id":5},{"text":"(","start":34,"end":35,"id":6},{"text":"or","start":35,"end":37,"id":7},{"text":"most","start":38,"end":42,"id":8},{"text":"similar","start":43,"end":50,"id":9},{"text":"images","start":51,"end":57,"id":10},{"text":")","start":57,"end":58,"id":11},{"text":"to","start":59,"end":61,"id":12},{"text":"vote","start":62,"end":66,"id":13},{"text":",","start":66,"end":67,"id":14},{"text":"the","start":68,"end":71,"id":15},{"text":"test","start":72,"end":76,"id":16},{"text":"set","start":77,"end":80,"id":17},{"text":"error","start":81,"end":86,"id":18},{"text":"rate","start":87,"end":91,"id":19},{"text":"on","start":92,"end":94,"id":20},{"text":"MNIST","start":95,"end":100,"id":21},{"text":"database","start":101,"end":109,"id":22},{"text":"could","start":110,"end":115,"id":23},{"text":"reach","start":116,"end":121,"id":24},{"text":"about","start":122,"end":127,"id":25},{"text":"1.5%-2.0","start":128,"end":136,"id":26},{"text":"%","start":136,"end":137,"id":27},{"text":",","start":137,"end":138,"id":28},{"text":"which","start":139,"end":144,"id":29},{"text":"is","start":145,"end":147,"id":30},{"text":"very","start":148,"end":152,"id":31},{"text":"close","start":153,"end":158,"id":32},{"text":"to","start":159,"end":161,"id":33},{"text":"many","start":162,"end":166,"id":34},{"text":"advanced","start":167,"end":175,"id":35},{"text":"models","start":176,"end":182,"id":36},{"text":".","start":182,"end":183,"id":37}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We propose a novel algorithm for greedy forward feature selection for regularized least-squares (RLS) regression and classification, also known as the least-squares support vector machine or ridge regression.","_input_hash":385711376,"_task_hash":43146309,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"propose","start":3,"end":10,"id":1},{"text":"a","start":11,"end":12,"id":2},{"text":"novel","start":13,"end":18,"id":3},{"text":"algorithm","start":19,"end":28,"id":4},{"text":"for","start":29,"end":32,"id":5},{"text":"greedy","start":33,"end":39,"id":6},{"text":"forward","start":40,"end":47,"id":7},{"text":"feature","start":48,"end":55,"id":8},{"text":"selection","start":56,"end":65,"id":9},{"text":"for","start":66,"end":69,"id":10},{"text":"regularized","start":70,"end":81,"id":11},{"text":"least","start":82,"end":87,"id":12},{"text":"-","start":87,"end":88,"id":13},{"text":"squares","start":88,"end":95,"id":14},{"text":"(","start":96,"end":97,"id":15},{"text":"RLS","start":97,"end":100,"id":16},{"text":")","start":100,"end":101,"id":17},{"text":"regression","start":102,"end":112,"id":18},{"text":"and","start":113,"end":116,"id":19},{"text":"classification","start":117,"end":131,"id":20},{"text":",","start":131,"end":132,"id":21},{"text":"also","start":133,"end":137,"id":22},{"text":"known","start":138,"end":143,"id":23},{"text":"as","start":144,"end":146,"id":24},{"text":"the","start":147,"end":150,"id":25},{"text":"least","start":151,"end":156,"id":26},{"text":"-","start":156,"end":157,"id":27},{"text":"squares","start":157,"end":164,"id":28},{"text":"support","start":165,"end":172,"id":29},{"text":"vector","start":173,"end":179,"id":30},{"text":"machine","start":180,"end":187,"id":31},{"text":"or","start":188,"end":190,"id":32},{"text":"ridge","start":191,"end":196,"id":33},{"text":"regression","start":197,"end":207,"id":34},{"text":".","start":207,"end":208,"id":35}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":70,"end":95,"token_start":11,"token_end":14,"label":"ALGO","answer":"accept"},{"start":151,"end":187,"token_start":26,"token_end":31,"label":"ALGO","answer":"accept"},{"start":191,"end":207,"token_start":33,"token_end":34,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"It arises from a combinatorial optimization problem which we cast as a variant of the set cover problem.","_input_hash":-2021799529,"_task_hash":105061103,"tokens":[{"text":"It","start":0,"end":2,"id":0},{"text":"arises","start":3,"end":9,"id":1},{"text":"from","start":10,"end":14,"id":2},{"text":"a","start":15,"end":16,"id":3},{"text":"combinatorial","start":17,"end":30,"id":4},{"text":"optimization","start":31,"end":43,"id":5},{"text":"problem","start":44,"end":51,"id":6},{"text":"which","start":52,"end":57,"id":7},{"text":"we","start":58,"end":60,"id":8},{"text":"cast","start":61,"end":65,"id":9},{"text":"as","start":66,"end":68,"id":10},{"text":"a","start":69,"end":70,"id":11},{"text":"variant","start":71,"end":78,"id":12},{"text":"of","start":79,"end":81,"id":13},{"text":"the","start":82,"end":85,"id":14},{"text":"set","start":86,"end":89,"id":15},{"text":"cover","start":90,"end":95,"id":16},{"text":"problem","start":96,"end":103,"id":17},{"text":".","start":103,"end":104,"id":18}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We consider an extension of this framework where the atoms are further assumed to be embedded in a tree.","_input_hash":1553095004,"_task_hash":859226698,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"consider","start":3,"end":11,"id":1},{"text":"an","start":12,"end":14,"id":2},{"text":"extension","start":15,"end":24,"id":3},{"text":"of","start":25,"end":27,"id":4},{"text":"this","start":28,"end":32,"id":5},{"text":"framework","start":33,"end":42,"id":6},{"text":"where","start":43,"end":48,"id":7},{"text":"the","start":49,"end":52,"id":8},{"text":"atoms","start":53,"end":58,"id":9},{"text":"are","start":59,"end":62,"id":10},{"text":"further","start":63,"end":70,"id":11},{"text":"assumed","start":71,"end":78,"id":12},{"text":"to","start":79,"end":81,"id":13},{"text":"be","start":82,"end":84,"id":14},{"text":"embedded","start":85,"end":93,"id":15},{"text":"in","start":94,"end":96,"id":16},{"text":"a","start":97,"end":98,"id":17},{"text":"tree","start":99,"end":103,"id":18},{"text":".","start":103,"end":104,"id":19}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":85,"end":93,"token_start":15,"token_end":15,"label":"ALGO","answer":"reject"}],"answer":"reject"}
{"text":"screening for variables with large cross-correlations over two treatments (cross-correlation screening);","_input_hash":385600213,"_task_hash":-1176093385,"tokens":[{"text":"screening","start":0,"end":9,"id":0},{"text":"for","start":10,"end":13,"id":1},{"text":"variables","start":14,"end":23,"id":2},{"text":"with","start":24,"end":28,"id":3},{"text":"large","start":29,"end":34,"id":4},{"text":"cross","start":35,"end":40,"id":5},{"text":"-","start":40,"end":41,"id":6},{"text":"correlations","start":41,"end":53,"id":7},{"text":"over","start":54,"end":58,"id":8},{"text":"two","start":59,"end":62,"id":9},{"text":"treatments","start":63,"end":73,"id":10},{"text":"(","start":74,"end":75,"id":11},{"text":"cross","start":75,"end":80,"id":12},{"text":"-","start":80,"end":81,"id":13},{"text":"correlation","start":81,"end":92,"id":14},{"text":"screening","start":93,"end":102,"id":15},{"text":")","start":102,"end":103,"id":16},{"text":";","start":103,"end":104,"id":17}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"They have also shown that such a property is not shared by the popular ucb1 policy of Auer et al. (","_input_hash":539636592,"_task_hash":1597649918,"tokens":[{"text":"They","start":0,"end":4,"id":0},{"text":"have","start":5,"end":9,"id":1},{"text":"also","start":10,"end":14,"id":2},{"text":"shown","start":15,"end":20,"id":3},{"text":"that","start":21,"end":25,"id":4},{"text":"such","start":26,"end":30,"id":5},{"text":"a","start":31,"end":32,"id":6},{"text":"property","start":33,"end":41,"id":7},{"text":"is","start":42,"end":44,"id":8},{"text":"not","start":45,"end":48,"id":9},{"text":"shared","start":49,"end":55,"id":10},{"text":"by","start":56,"end":58,"id":11},{"text":"the","start":59,"end":62,"id":12},{"text":"popular","start":63,"end":70,"id":13},{"text":"ucb1","start":71,"end":75,"id":14},{"text":"policy","start":76,"end":82,"id":15},{"text":"of","start":83,"end":85,"id":16},{"text":"Auer","start":86,"end":90,"id":17},{"text":"et","start":91,"end":93,"id":18},{"text":"al","start":94,"end":96,"id":19},{"text":".","start":96,"end":97,"id":20},{"text":"(","start":98,"end":99,"id":21}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"In general, these systems have infinitely many solutions.","_input_hash":-339633159,"_task_hash":-345873301,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"general","start":3,"end":10,"id":1},{"text":",","start":10,"end":11,"id":2},{"text":"these","start":12,"end":17,"id":3},{"text":"systems","start":18,"end":25,"id":4},{"text":"have","start":26,"end":30,"id":5},{"text":"infinitely","start":31,"end":41,"id":6},{"text":"many","start":42,"end":46,"id":7},{"text":"solutions","start":47,"end":56,"id":8},{"text":".","start":56,"end":57,"id":9}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Greedy forward regression;","_input_hash":157488121,"_task_hash":-517304874,"tokens":[{"text":"Greedy","start":0,"end":6,"id":0},{"text":"forward","start":7,"end":14,"id":1},{"text":"regression","start":15,"end":25,"id":2},{"text":";","start":25,"end":26,"id":3}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":0,"end":25,"token_start":0,"token_end":2,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We point out the interest of each cross-validation procedure in terms of rate of convergence.","_input_hash":-791264148,"_task_hash":-395434608,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"point","start":3,"end":8,"id":1},{"text":"out","start":9,"end":12,"id":2},{"text":"the","start":13,"end":16,"id":3},{"text":"interest","start":17,"end":25,"id":4},{"text":"of","start":26,"end":28,"id":5},{"text":"each","start":29,"end":33,"id":6},{"text":"cross","start":34,"end":39,"id":7},{"text":"-","start":39,"end":40,"id":8},{"text":"validation","start":40,"end":50,"id":9},{"text":"procedure","start":51,"end":60,"id":10},{"text":"in","start":61,"end":63,"id":11},{"text":"terms","start":64,"end":69,"id":12},{"text":"of","start":70,"end":72,"id":13},{"text":"rate","start":73,"end":77,"id":14},{"text":"of","start":78,"end":80,"id":15},{"text":"convergence","start":81,"end":92,"id":16},{"text":".","start":92,"end":93,"id":17}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We provide self-contained proof of a theorem relating probabilistic coherence of forecasts to their non-domination by rival forecasts with respect to any proper scoring rule.","_input_hash":419135333,"_task_hash":954605435,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"provide","start":3,"end":10,"id":1},{"text":"self","start":11,"end":15,"id":2},{"text":"-","start":15,"end":16,"id":3},{"text":"contained","start":16,"end":25,"id":4},{"text":"proof","start":26,"end":31,"id":5},{"text":"of","start":32,"end":34,"id":6},{"text":"a","start":35,"end":36,"id":7},{"text":"theorem","start":37,"end":44,"id":8},{"text":"relating","start":45,"end":53,"id":9},{"text":"probabilistic","start":54,"end":67,"id":10},{"text":"coherence","start":68,"end":77,"id":11},{"text":"of","start":78,"end":80,"id":12},{"text":"forecasts","start":81,"end":90,"id":13},{"text":"to","start":91,"end":93,"id":14},{"text":"their","start":94,"end":99,"id":15},{"text":"non","start":100,"end":103,"id":16},{"text":"-","start":103,"end":104,"id":17},{"text":"domination","start":104,"end":114,"id":18},{"text":"by","start":115,"end":117,"id":19},{"text":"rival","start":118,"end":123,"id":20},{"text":"forecasts","start":124,"end":133,"id":21},{"text":"with","start":134,"end":138,"id":22},{"text":"respect","start":139,"end":146,"id":23},{"text":"to","start":147,"end":149,"id":24},{"text":"any","start":150,"end":153,"id":25},{"text":"proper","start":154,"end":160,"id":26},{"text":"scoring","start":161,"end":168,"id":27},{"text":"rule","start":169,"end":173,"id":28},{"text":".","start":173,"end":174,"id":29}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"In this paper, we establish conditions under which the proposed method consistently recovers the structure of a time-varying network.","_input_hash":-1539491207,"_task_hash":1334134881,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"this","start":3,"end":7,"id":1},{"text":"paper","start":8,"end":13,"id":2},{"text":",","start":13,"end":14,"id":3},{"text":"we","start":15,"end":17,"id":4},{"text":"establish","start":18,"end":27,"id":5},{"text":"conditions","start":28,"end":38,"id":6},{"text":"under","start":39,"end":44,"id":7},{"text":"which","start":45,"end":50,"id":8},{"text":"the","start":51,"end":54,"id":9},{"text":"proposed","start":55,"end":63,"id":10},{"text":"method","start":64,"end":70,"id":11},{"text":"consistently","start":71,"end":83,"id":12},{"text":"recovers","start":84,"end":92,"id":13},{"text":"the","start":93,"end":96,"id":14},{"text":"structure","start":97,"end":106,"id":15},{"text":"of","start":107,"end":109,"id":16},{"text":"a","start":110,"end":111,"id":17},{"text":"time","start":112,"end":116,"id":18},{"text":"-","start":116,"end":117,"id":19},{"text":"varying","start":117,"end":124,"id":20},{"text":"network","start":125,"end":132,"id":21},{"text":".","start":132,"end":133,"id":22}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"and indicates whether a data sample is close to valleys/modes.","_input_hash":1906035395,"_task_hash":-1123343480,"tokens":[{"text":"and","start":0,"end":3,"id":0},{"text":"indicates","start":4,"end":13,"id":1},{"text":"whether","start":14,"end":21,"id":2},{"text":"a","start":22,"end":23,"id":3},{"text":"data","start":24,"end":28,"id":4},{"text":"sample","start":29,"end":35,"id":5},{"text":"is","start":36,"end":38,"id":6},{"text":"close","start":39,"end":44,"id":7},{"text":"to","start":45,"end":47,"id":8},{"text":"valleys","start":48,"end":55,"id":9},{"text":"/","start":55,"end":56,"id":10},{"text":"modes","start":56,"end":61,"id":11},{"text":".","start":61,"end":62,"id":12}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We demonstrate the approach with applications.","_input_hash":-958605827,"_task_hash":-58402604,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"demonstrate","start":3,"end":14,"id":1},{"text":"the","start":15,"end":18,"id":2},{"text":"approach","start":19,"end":27,"id":3},{"text":"with","start":28,"end":32,"id":4},{"text":"applications","start":33,"end":45,"id":5},{"text":".","start":45,"end":46,"id":6}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We show that both models perform significantly better than the VMM, with the Dirichlet-VMM marginally outperforming the TC-RBM.","_input_hash":857454766,"_task_hash":1379462910,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"show","start":3,"end":7,"id":1},{"text":"that","start":8,"end":12,"id":2},{"text":"both","start":13,"end":17,"id":3},{"text":"models","start":18,"end":24,"id":4},{"text":"perform","start":25,"end":32,"id":5},{"text":"significantly","start":33,"end":46,"id":6},{"text":"better","start":47,"end":53,"id":7},{"text":"than","start":54,"end":58,"id":8},{"text":"the","start":59,"end":62,"id":9},{"text":"VMM","start":63,"end":66,"id":10},{"text":",","start":66,"end":67,"id":11},{"text":"with","start":68,"end":72,"id":12},{"text":"the","start":73,"end":76,"id":13},{"text":"Dirichlet","start":77,"end":86,"id":14},{"text":"-","start":86,"end":87,"id":15},{"text":"VMM","start":87,"end":90,"id":16},{"text":"marginally","start":91,"end":101,"id":17},{"text":"outperforming","start":102,"end":115,"id":18},{"text":"the","start":116,"end":119,"id":19},{"text":"TC","start":120,"end":122,"id":20},{"text":"-","start":122,"end":123,"id":21},{"text":"RBM","start":123,"end":126,"id":22},{"text":".","start":126,"end":127,"id":23}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"The goal of cross-domain object matching (CDOM) is to find correspondence between two sets of objects in different domains in an unsupervised way.","_input_hash":-1566704414,"_task_hash":2137509668,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"goal","start":4,"end":8,"id":1},{"text":"of","start":9,"end":11,"id":2},{"text":"cross","start":12,"end":17,"id":3},{"text":"-","start":17,"end":18,"id":4},{"text":"domain","start":18,"end":24,"id":5},{"text":"object","start":25,"end":31,"id":6},{"text":"matching","start":32,"end":40,"id":7},{"text":"(","start":41,"end":42,"id":8},{"text":"CDOM","start":42,"end":46,"id":9},{"text":")","start":46,"end":47,"id":10},{"text":"is","start":48,"end":50,"id":11},{"text":"to","start":51,"end":53,"id":12},{"text":"find","start":54,"end":58,"id":13},{"text":"correspondence","start":59,"end":73,"id":14},{"text":"between","start":74,"end":81,"id":15},{"text":"two","start":82,"end":85,"id":16},{"text":"sets","start":86,"end":90,"id":17},{"text":"of","start":91,"end":93,"id":18},{"text":"objects","start":94,"end":101,"id":19},{"text":"in","start":102,"end":104,"id":20},{"text":"different","start":105,"end":114,"id":21},{"text":"domains","start":115,"end":122,"id":22},{"text":"in","start":123,"end":125,"id":23},{"text":"an","start":126,"end":128,"id":24},{"text":"unsupervised","start":129,"end":141,"id":25},{"text":"way","start":142,"end":145,"id":26},{"text":".","start":145,"end":146,"id":27}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"In this paper we address these issues.","_input_hash":-1631506436,"_task_hash":2017089648,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"this","start":3,"end":7,"id":1},{"text":"paper","start":8,"end":13,"id":2},{"text":"we","start":14,"end":16,"id":3},{"text":"address","start":17,"end":24,"id":4},{"text":"these","start":25,"end":30,"id":5},{"text":"issues","start":31,"end":37,"id":6},{"text":".","start":37,"end":38,"id":7}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"A key component involves the node mean squared error for a quantity we refer to as a maximal subtree.","_input_hash":-1629405522,"_task_hash":-1010331814,"tokens":[{"text":"A","start":0,"end":1,"id":0},{"text":"key","start":2,"end":5,"id":1},{"text":"component","start":6,"end":15,"id":2},{"text":"involves","start":16,"end":24,"id":3},{"text":"the","start":25,"end":28,"id":4},{"text":"node","start":29,"end":33,"id":5},{"text":"mean","start":34,"end":38,"id":6},{"text":"squared","start":39,"end":46,"id":7},{"text":"error","start":47,"end":52,"id":8},{"text":"for","start":53,"end":56,"id":9},{"text":"a","start":57,"end":58,"id":10},{"text":"quantity","start":59,"end":67,"id":11},{"text":"we","start":68,"end":70,"id":12},{"text":"refer","start":71,"end":76,"id":13},{"text":"to","start":77,"end":79,"id":14},{"text":"as","start":80,"end":82,"id":15},{"text":"a","start":83,"end":84,"id":16},{"text":"maximal","start":85,"end":92,"id":17},{"text":"subtree","start":93,"end":100,"id":18},{"text":".","start":100,"end":101,"id":19}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"  In this paper we mainly address the question how to design SVMs by choosing the reproducing kernel Hilbert space (RKHS) or its corresponding kernel to obtain consistent and statistically robust estimators in additive models.","_input_hash":1285429061,"_task_hash":762136714,"tokens":[{"text":"  ","start":0,"end":2,"id":0},{"text":"In","start":2,"end":4,"id":1},{"text":"this","start":5,"end":9,"id":2},{"text":"paper","start":10,"end":15,"id":3},{"text":"we","start":16,"end":18,"id":4},{"text":"mainly","start":19,"end":25,"id":5},{"text":"address","start":26,"end":33,"id":6},{"text":"the","start":34,"end":37,"id":7},{"text":"question","start":38,"end":46,"id":8},{"text":"how","start":47,"end":50,"id":9},{"text":"to","start":51,"end":53,"id":10},{"text":"design","start":54,"end":60,"id":11},{"text":"SVMs","start":61,"end":65,"id":12},{"text":"by","start":66,"end":68,"id":13},{"text":"choosing","start":69,"end":77,"id":14},{"text":"the","start":78,"end":81,"id":15},{"text":"reproducing","start":82,"end":93,"id":16},{"text":"kernel","start":94,"end":100,"id":17},{"text":"Hilbert","start":101,"end":108,"id":18},{"text":"space","start":109,"end":114,"id":19},{"text":"(","start":115,"end":116,"id":20},{"text":"RKHS","start":116,"end":120,"id":21},{"text":")","start":120,"end":121,"id":22},{"text":"or","start":122,"end":124,"id":23},{"text":"its","start":125,"end":128,"id":24},{"text":"corresponding","start":129,"end":142,"id":25},{"text":"kernel","start":143,"end":149,"id":26},{"text":"to","start":150,"end":152,"id":27},{"text":"obtain","start":153,"end":159,"id":28},{"text":"consistent","start":160,"end":170,"id":29},{"text":"and","start":171,"end":174,"id":30},{"text":"statistically","start":175,"end":188,"id":31},{"text":"robust","start":189,"end":195,"id":32},{"text":"estimators","start":196,"end":206,"id":33},{"text":"in","start":207,"end":209,"id":34},{"text":"additive","start":210,"end":218,"id":35},{"text":"models","start":219,"end":225,"id":36},{"text":".","start":225,"end":226,"id":37}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":61,"end":65,"token_start":12,"token_end":12,"label":"ALGO","answer":"accept"},{"start":94,"end":114,"token_start":17,"token_end":19,"label":"ALGO","answer":"accept"},{"start":210,"end":218,"token_start":35,"token_end":35,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Examples are quantile regression based on the pinball loss function, regression based on the epsilon-insensitive loss function, and classification based on the hinge loss function.","_input_hash":1506673819,"_task_hash":355988414,"tokens":[{"text":"Examples","start":0,"end":8,"id":0},{"text":"are","start":9,"end":12,"id":1},{"text":"quantile","start":13,"end":21,"id":2},{"text":"regression","start":22,"end":32,"id":3},{"text":"based","start":33,"end":38,"id":4},{"text":"on","start":39,"end":41,"id":5},{"text":"the","start":42,"end":45,"id":6},{"text":"pinball","start":46,"end":53,"id":7},{"text":"loss","start":54,"end":58,"id":8},{"text":"function","start":59,"end":67,"id":9},{"text":",","start":67,"end":68,"id":10},{"text":"regression","start":69,"end":79,"id":11},{"text":"based","start":80,"end":85,"id":12},{"text":"on","start":86,"end":88,"id":13},{"text":"the","start":89,"end":92,"id":14},{"text":"epsilon","start":93,"end":100,"id":15},{"text":"-","start":100,"end":101,"id":16},{"text":"insensitive","start":101,"end":112,"id":17},{"text":"loss","start":113,"end":117,"id":18},{"text":"function","start":118,"end":126,"id":19},{"text":",","start":126,"end":127,"id":20},{"text":"and","start":128,"end":131,"id":21},{"text":"classification","start":132,"end":146,"id":22},{"text":"based","start":147,"end":152,"id":23},{"text":"on","start":153,"end":155,"id":24},{"text":"the","start":156,"end":159,"id":25},{"text":"hinge","start":160,"end":165,"id":26},{"text":"loss","start":166,"end":170,"id":27},{"text":"function","start":171,"end":179,"id":28},{"text":".","start":179,"end":180,"id":29}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":13,"end":32,"token_start":2,"token_end":3,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"By contrast, when R-EP converges, both methods perform similarly.","_input_hash":-375359492,"_task_hash":687021647,"tokens":[{"text":"By","start":0,"end":2,"id":0},{"text":"contrast","start":3,"end":11,"id":1},{"text":",","start":11,"end":12,"id":2},{"text":"when","start":13,"end":17,"id":3},{"text":"R","start":18,"end":19,"id":4},{"text":"-","start":19,"end":20,"id":5},{"text":"EP","start":20,"end":22,"id":6},{"text":"converges","start":23,"end":32,"id":7},{"text":",","start":32,"end":33,"id":8},{"text":"both","start":34,"end":38,"id":9},{"text":"methods","start":39,"end":46,"id":10},{"text":"perform","start":47,"end":54,"id":11},{"text":"similarly","start":55,"end":64,"id":12},{"text":".","start":64,"end":65,"id":13}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Further relaxations are also discussed.","_input_hash":1994490729,"_task_hash":1846884363,"tokens":[{"text":"Further","start":0,"end":7,"id":0},{"text":"relaxations","start":8,"end":19,"id":1},{"text":"are","start":20,"end":23,"id":2},{"text":"also","start":24,"end":28,"id":3},{"text":"discussed","start":29,"end":38,"id":4},{"text":".","start":38,"end":39,"id":5}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"We extend the previous work on this problem to adversarial scenarios, while also improving on their results in the iid setup.","_input_hash":-2014859766,"_task_hash":1841005240,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"extend","start":3,"end":9,"id":1},{"text":"the","start":10,"end":13,"id":2},{"text":"previous","start":14,"end":22,"id":3},{"text":"work","start":23,"end":27,"id":4},{"text":"on","start":28,"end":30,"id":5},{"text":"this","start":31,"end":35,"id":6},{"text":"problem","start":36,"end":43,"id":7},{"text":"to","start":44,"end":46,"id":8},{"text":"adversarial","start":47,"end":58,"id":9},{"text":"scenarios","start":59,"end":68,"id":10},{"text":",","start":68,"end":69,"id":11},{"text":"while","start":70,"end":75,"id":12},{"text":"also","start":76,"end":80,"id":13},{"text":"improving","start":81,"end":90,"id":14},{"text":"on","start":91,"end":93,"id":15},{"text":"their","start":94,"end":99,"id":16},{"text":"results","start":100,"end":107,"id":17},{"text":"in","start":108,"end":110,"id":18},{"text":"the","start":111,"end":114,"id":19},{"text":"iid","start":115,"end":118,"id":20},{"text":"setup","start":119,"end":124,"id":21},{"text":".","start":124,"end":125,"id":22}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"We propose an extension of the concept of Expected Improvement criterion commonly used in Kriging based optimization.","_input_hash":1073976132,"_task_hash":-813542588,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"propose","start":3,"end":10,"id":1},{"text":"an","start":11,"end":13,"id":2},{"text":"extension","start":14,"end":23,"id":3},{"text":"of","start":24,"end":26,"id":4},{"text":"the","start":27,"end":30,"id":5},{"text":"concept","start":31,"end":38,"id":6},{"text":"of","start":39,"end":41,"id":7},{"text":"Expected","start":42,"end":50,"id":8},{"text":"Improvement","start":51,"end":62,"id":9},{"text":"criterion","start":63,"end":72,"id":10},{"text":"commonly","start":73,"end":81,"id":11},{"text":"used","start":82,"end":86,"id":12},{"text":"in","start":87,"end":89,"id":13},{"text":"Kriging","start":90,"end":97,"id":14},{"text":"based","start":98,"end":103,"id":15},{"text":"optimization","start":104,"end":116,"id":16},{"text":".","start":116,"end":117,"id":17}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"In many applications, some qualitative prior knowledge of the distribution P or of the unknown function f to be estimated is present or the prediction function with a good interpretability is desired, such that a semiparametric model or an additive model is of interest.","_input_hash":931512951,"_task_hash":-42104417,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"many","start":3,"end":7,"id":1},{"text":"applications","start":8,"end":20,"id":2},{"text":",","start":20,"end":21,"id":3},{"text":"some","start":22,"end":26,"id":4},{"text":"qualitative","start":27,"end":38,"id":5},{"text":"prior","start":39,"end":44,"id":6},{"text":"knowledge","start":45,"end":54,"id":7},{"text":"of","start":55,"end":57,"id":8},{"text":"the","start":58,"end":61,"id":9},{"text":"distribution","start":62,"end":74,"id":10},{"text":"P","start":75,"end":76,"id":11},{"text":"or","start":77,"end":79,"id":12},{"text":"of","start":80,"end":82,"id":13},{"text":"the","start":83,"end":86,"id":14},{"text":"unknown","start":87,"end":94,"id":15},{"text":"function","start":95,"end":103,"id":16},{"text":"f","start":104,"end":105,"id":17},{"text":"to","start":106,"end":108,"id":18},{"text":"be","start":109,"end":111,"id":19},{"text":"estimated","start":112,"end":121,"id":20},{"text":"is","start":122,"end":124,"id":21},{"text":"present","start":125,"end":132,"id":22},{"text":"or","start":133,"end":135,"id":23},{"text":"the","start":136,"end":139,"id":24},{"text":"prediction","start":140,"end":150,"id":25},{"text":"function","start":151,"end":159,"id":26},{"text":"with","start":160,"end":164,"id":27},{"text":"a","start":165,"end":166,"id":28},{"text":"good","start":167,"end":171,"id":29},{"text":"interpretability","start":172,"end":188,"id":30},{"text":"is","start":189,"end":191,"id":31},{"text":"desired","start":192,"end":199,"id":32},{"text":",","start":199,"end":200,"id":33},{"text":"such","start":201,"end":205,"id":34},{"text":"that","start":206,"end":210,"id":35},{"text":"a","start":211,"end":212,"id":36},{"text":"semiparametric","start":213,"end":227,"id":37},{"text":"model","start":228,"end":233,"id":38},{"text":"or","start":234,"end":236,"id":39},{"text":"an","start":237,"end":239,"id":40},{"text":"additive","start":240,"end":248,"id":41},{"text":"model","start":249,"end":254,"id":42},{"text":"is","start":255,"end":257,"id":43},{"text":"of","start":258,"end":260,"id":44},{"text":"interest","start":261,"end":269,"id":45},{"text":".","start":269,"end":270,"id":46}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":213,"end":227,"token_start":37,"token_end":37,"label":"ALGO","answer":"accept"},{"start":240,"end":248,"token_start":41,"token_end":41,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"  In particular, we focus on proving the consistency of the various cross-validation procedures.","_input_hash":-1032825242,"_task_hash":1620179809,"tokens":[{"text":"  ","start":0,"end":2,"id":0},{"text":"In","start":2,"end":4,"id":1},{"text":"particular","start":5,"end":15,"id":2},{"text":",","start":15,"end":16,"id":3},{"text":"we","start":17,"end":19,"id":4},{"text":"focus","start":20,"end":25,"id":5},{"text":"on","start":26,"end":28,"id":6},{"text":"proving","start":29,"end":36,"id":7},{"text":"the","start":37,"end":40,"id":8},{"text":"consistency","start":41,"end":52,"id":9},{"text":"of","start":53,"end":55,"id":10},{"text":"the","start":56,"end":59,"id":11},{"text":"various","start":60,"end":67,"id":12},{"text":"cross","start":68,"end":73,"id":13},{"text":"-","start":73,"end":74,"id":14},{"text":"validation","start":74,"end":84,"id":15},{"text":"procedures","start":85,"end":95,"id":16},{"text":".","start":95,"end":96,"id":17}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"The rank of a data sample is an estimate of its p-value and is proportional to the total number of data samples with smaller density.","_input_hash":-1686625809,"_task_hash":1099487724,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"rank","start":4,"end":8,"id":1},{"text":"of","start":9,"end":11,"id":2},{"text":"a","start":12,"end":13,"id":3},{"text":"data","start":14,"end":18,"id":4},{"text":"sample","start":19,"end":25,"id":5},{"text":"is","start":26,"end":28,"id":6},{"text":"an","start":29,"end":31,"id":7},{"text":"estimate","start":32,"end":40,"id":8},{"text":"of","start":41,"end":43,"id":9},{"text":"its","start":44,"end":47,"id":10},{"text":"p","start":48,"end":49,"id":11},{"text":"-","start":49,"end":50,"id":12},{"text":"value","start":50,"end":55,"id":13},{"text":"and","start":56,"end":59,"id":14},{"text":"is","start":60,"end":62,"id":15},{"text":"proportional","start":63,"end":75,"id":16},{"text":"to","start":76,"end":78,"id":17},{"text":"the","start":79,"end":82,"id":18},{"text":"total","start":83,"end":88,"id":19},{"text":"number","start":89,"end":95,"id":20},{"text":"of","start":96,"end":98,"id":21},{"text":"data","start":99,"end":103,"id":22},{"text":"samples","start":104,"end":111,"id":23},{"text":"with","start":112,"end":116,"id":24},{"text":"smaller","start":117,"end":124,"id":25},{"text":"density","start":125,"end":132,"id":26},{"text":".","start":132,"end":133,"id":27}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"These regressors thus escape the curse of dimension when high-dimensional data has low intrinsic dimension (e.g. a manifold).","_input_hash":-1831991119,"_task_hash":-487006171,"tokens":[{"text":"These","start":0,"end":5,"id":0},{"text":"regressors","start":6,"end":16,"id":1},{"text":"thus","start":17,"end":21,"id":2},{"text":"escape","start":22,"end":28,"id":3},{"text":"the","start":29,"end":32,"id":4},{"text":"curse","start":33,"end":38,"id":5},{"text":"of","start":39,"end":41,"id":6},{"text":"dimension","start":42,"end":51,"id":7},{"text":"when","start":52,"end":56,"id":8},{"text":"high","start":57,"end":61,"id":9},{"text":"-","start":61,"end":62,"id":10},{"text":"dimensional","start":62,"end":73,"id":11},{"text":"data","start":74,"end":78,"id":12},{"text":"has","start":79,"end":82,"id":13},{"text":"low","start":83,"end":86,"id":14},{"text":"intrinsic","start":87,"end":96,"id":15},{"text":"dimension","start":97,"end":106,"id":16},{"text":"(","start":107,"end":108,"id":17},{"text":"e.g.","start":108,"end":112,"id":18},{"text":"a","start":113,"end":114,"id":19},{"text":"manifold","start":115,"end":123,"id":20},{"text":")","start":123,"end":124,"id":21},{"text":".","start":124,"end":125,"id":22}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Our method is also computationally feasible even for very large graphs.","_input_hash":-2064667142,"_task_hash":-1506464669,"tokens":[{"text":"Our","start":0,"end":3,"id":0},{"text":"method","start":4,"end":10,"id":1},{"text":"is","start":11,"end":13,"id":2},{"text":"also","start":14,"end":18,"id":3},{"text":"computationally","start":19,"end":34,"id":4},{"text":"feasible","start":35,"end":43,"id":5},{"text":"even","start":44,"end":48,"id":6},{"text":"for","start":49,"end":52,"id":7},{"text":"very","start":53,"end":57,"id":8},{"text":"large","start":58,"end":63,"id":9},{"text":"graphs","start":64,"end":70,"id":10},{"text":".","start":70,"end":71,"id":11}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We address this challenge by developing a new adaptive gradient-based method that carefully combines gradient information with an adaptive step-scaling strategy, which results in a scalable, highly competitive method.","_input_hash":-2113047668,"_task_hash":-584797897,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"address","start":3,"end":10,"id":1},{"text":"this","start":11,"end":15,"id":2},{"text":"challenge","start":16,"end":25,"id":3},{"text":"by","start":26,"end":28,"id":4},{"text":"developing","start":29,"end":39,"id":5},{"text":"a","start":40,"end":41,"id":6},{"text":"new","start":42,"end":45,"id":7},{"text":"adaptive","start":46,"end":54,"id":8},{"text":"gradient","start":55,"end":63,"id":9},{"text":"-","start":63,"end":64,"id":10},{"text":"based","start":64,"end":69,"id":11},{"text":"method","start":70,"end":76,"id":12},{"text":"that","start":77,"end":81,"id":13},{"text":"carefully","start":82,"end":91,"id":14},{"text":"combines","start":92,"end":100,"id":15},{"text":"gradient","start":101,"end":109,"id":16},{"text":"information","start":110,"end":121,"id":17},{"text":"with","start":122,"end":126,"id":18},{"text":"an","start":127,"end":129,"id":19},{"text":"adaptive","start":130,"end":138,"id":20},{"text":"step","start":139,"end":143,"id":21},{"text":"-","start":143,"end":144,"id":22},{"text":"scaling","start":144,"end":151,"id":23},{"text":"strategy","start":152,"end":160,"id":24},{"text":",","start":160,"end":161,"id":25},{"text":"which","start":162,"end":167,"id":26},{"text":"results","start":168,"end":175,"id":27},{"text":"in","start":176,"end":178,"id":28},{"text":"a","start":179,"end":180,"id":29},{"text":"scalable","start":181,"end":189,"id":30},{"text":",","start":189,"end":190,"id":31},{"text":"highly","start":191,"end":197,"id":32},{"text":"competitive","start":198,"end":209,"id":33},{"text":"method","start":210,"end":216,"id":34},{"text":".","start":216,"end":217,"id":35}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":46,"end":69,"token_start":8,"token_end":11,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"The first case is when the time data is modeled as a sequence of independent and normal distributed random variables with unknown, possibly changing, mean value but fixed variance.","_input_hash":1624795738,"_task_hash":-1487566859,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"first","start":4,"end":9,"id":1},{"text":"case","start":10,"end":14,"id":2},{"text":"is","start":15,"end":17,"id":3},{"text":"when","start":18,"end":22,"id":4},{"text":"the","start":23,"end":26,"id":5},{"text":"time","start":27,"end":31,"id":6},{"text":"data","start":32,"end":36,"id":7},{"text":"is","start":37,"end":39,"id":8},{"text":"modeled","start":40,"end":47,"id":9},{"text":"as","start":48,"end":50,"id":10},{"text":"a","start":51,"end":52,"id":11},{"text":"sequence","start":53,"end":61,"id":12},{"text":"of","start":62,"end":64,"id":13},{"text":"independent","start":65,"end":76,"id":14},{"text":"and","start":77,"end":80,"id":15},{"text":"normal","start":81,"end":87,"id":16},{"text":"distributed","start":88,"end":99,"id":17},{"text":"random","start":100,"end":106,"id":18},{"text":"variables","start":107,"end":116,"id":19},{"text":"with","start":117,"end":121,"id":20},{"text":"unknown","start":122,"end":129,"id":21},{"text":",","start":129,"end":130,"id":22},{"text":"possibly","start":131,"end":139,"id":23},{"text":"changing","start":140,"end":148,"id":24},{"text":",","start":148,"end":149,"id":25},{"text":"mean","start":150,"end":154,"id":26},{"text":"value","start":155,"end":160,"id":27},{"text":"but","start":161,"end":164,"id":28},{"text":"fixed","start":165,"end":170,"id":29},{"text":"variance","start":171,"end":179,"id":30},{"text":".","start":179,"end":180,"id":31}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Recently, the Gaussian Process Latent Variable Model (GPLVM) has successfully been used to find low dimensional manifolds in a variety of complex data.","_input_hash":572638255,"_task_hash":-1931285124,"tokens":[{"text":"Recently","start":0,"end":8,"id":0},{"text":",","start":8,"end":9,"id":1},{"text":"the","start":10,"end":13,"id":2},{"text":"Gaussian","start":14,"end":22,"id":3},{"text":"Process","start":23,"end":30,"id":4},{"text":"Latent","start":31,"end":37,"id":5},{"text":"Variable","start":38,"end":46,"id":6},{"text":"Model","start":47,"end":52,"id":7},{"text":"(","start":53,"end":54,"id":8},{"text":"GPLVM","start":54,"end":59,"id":9},{"text":")","start":59,"end":60,"id":10},{"text":"has","start":61,"end":64,"id":11},{"text":"successfully","start":65,"end":77,"id":12},{"text":"been","start":78,"end":82,"id":13},{"text":"used","start":83,"end":87,"id":14},{"text":"to","start":88,"end":90,"id":15},{"text":"find","start":91,"end":95,"id":16},{"text":"low","start":96,"end":99,"id":17},{"text":"dimensional","start":100,"end":111,"id":18},{"text":"manifolds","start":112,"end":121,"id":19},{"text":"in","start":122,"end":124,"id":20},{"text":"a","start":125,"end":126,"id":21},{"text":"variety","start":127,"end":134,"id":22},{"text":"of","start":135,"end":137,"id":23},{"text":"complex","start":138,"end":145,"id":24},{"text":"data","start":146,"end":150,"id":25},{"text":".","start":150,"end":151,"id":26}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":14,"end":46,"token_start":3,"token_end":6,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Typically, this generating mechanism will be a finite automata.","_input_hash":-1179024221,"_task_hash":-1078314631,"tokens":[{"text":"Typically","start":0,"end":9,"id":0},{"text":",","start":9,"end":10,"id":1},{"text":"this","start":11,"end":15,"id":2},{"text":"generating","start":16,"end":26,"id":3},{"text":"mechanism","start":27,"end":36,"id":4},{"text":"will","start":37,"end":41,"id":5},{"text":"be","start":42,"end":44,"id":6},{"text":"a","start":45,"end":46,"id":7},{"text":"finite","start":47,"end":53,"id":8},{"text":"automata","start":54,"end":62,"id":9},{"text":".","start":62,"end":63,"id":10}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"Comparison with the results obtained in the literature shows that the bound presented here provides an additional $36-40\\%$ reduction.","_input_hash":-1095710320,"_task_hash":502475128,"tokens":[{"text":"Comparison","start":0,"end":10,"id":0},{"text":"with","start":11,"end":15,"id":1},{"text":"the","start":16,"end":19,"id":2},{"text":"results","start":20,"end":27,"id":3},{"text":"obtained","start":28,"end":36,"id":4},{"text":"in","start":37,"end":39,"id":5},{"text":"the","start":40,"end":43,"id":6},{"text":"literature","start":44,"end":54,"id":7},{"text":"shows","start":55,"end":60,"id":8},{"text":"that","start":61,"end":65,"id":9},{"text":"the","start":66,"end":69,"id":10},{"text":"bound","start":70,"end":75,"id":11},{"text":"presented","start":76,"end":85,"id":12},{"text":"here","start":86,"end":90,"id":13},{"text":"provides","start":91,"end":99,"id":14},{"text":"an","start":100,"end":102,"id":15},{"text":"additional","start":103,"end":113,"id":16},{"text":"$","start":114,"end":115,"id":17},{"text":"36","start":115,"end":117,"id":18},{"text":"-","start":117,"end":118,"id":19},{"text":"40\\%$","start":118,"end":123,"id":20},{"text":"reduction","start":124,"end":133,"id":21},{"text":".","start":133,"end":134,"id":22}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"Gaussian process models -also called Kriging models- are often used as mathematical approximations of expensive experiments.","_input_hash":924732873,"_task_hash":-1742205899,"tokens":[{"text":"Gaussian","start":0,"end":8,"id":0},{"text":"process","start":9,"end":16,"id":1},{"text":"models","start":17,"end":23,"id":2},{"text":"-also","start":24,"end":29,"id":3},{"text":"called","start":30,"end":36,"id":4},{"text":"Kriging","start":37,"end":44,"id":5},{"text":"models-","start":45,"end":52,"id":6},{"text":"are","start":53,"end":56,"id":7},{"text":"often","start":57,"end":62,"id":8},{"text":"used","start":63,"end":67,"id":9},{"text":"as","start":68,"end":70,"id":10},{"text":"mathematical","start":71,"end":83,"id":11},{"text":"approximations","start":84,"end":98,"id":12},{"text":"of","start":99,"end":101,"id":13},{"text":"expensive","start":102,"end":111,"id":14},{"text":"experiments","start":112,"end":123,"id":15},{"text":".","start":123,"end":124,"id":16}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":0,"end":16,"token_start":0,"token_end":1,"label":"ALGO","answer":"accept"},{"start":37,"end":44,"token_start":5,"token_end":5,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Outliers are identified by judiciously tuning regularization parameters, which amounts to controlling the sparsity of the outlier vector along the whole robustification path of Lasso solutions.","_input_hash":-255543272,"_task_hash":1622401712,"tokens":[{"text":"Outliers","start":0,"end":8,"id":0},{"text":"are","start":9,"end":12,"id":1},{"text":"identified","start":13,"end":23,"id":2},{"text":"by","start":24,"end":26,"id":3},{"text":"judiciously","start":27,"end":38,"id":4},{"text":"tuning","start":39,"end":45,"id":5},{"text":"regularization","start":46,"end":60,"id":6},{"text":"parameters","start":61,"end":71,"id":7},{"text":",","start":71,"end":72,"id":8},{"text":"which","start":73,"end":78,"id":9},{"text":"amounts","start":79,"end":86,"id":10},{"text":"to","start":87,"end":89,"id":11},{"text":"controlling","start":90,"end":101,"id":12},{"text":"the","start":102,"end":105,"id":13},{"text":"sparsity","start":106,"end":114,"id":14},{"text":"of","start":115,"end":117,"id":15},{"text":"the","start":118,"end":121,"id":16},{"text":"outlier","start":122,"end":129,"id":17},{"text":"vector","start":130,"end":136,"id":18},{"text":"along","start":137,"end":142,"id":19},{"text":"the","start":143,"end":146,"id":20},{"text":"whole","start":147,"end":152,"id":21},{"text":"robustification","start":153,"end":168,"id":22},{"text":"path","start":169,"end":173,"id":23},{"text":"of","start":174,"end":176,"id":24},{"text":"Lasso","start":177,"end":182,"id":25},{"text":"solutions","start":183,"end":192,"id":26},{"text":".","start":192,"end":193,"id":27}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":177,"end":182,"token_start":25,"token_end":25,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"In this context, the fresh look advocated here permeates benefits from variable selection and compressive sampling, to robustify nonparametric regression against outliers - that is, data markedly deviating from the postulated models.","_input_hash":-1600959469,"_task_hash":66330434,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"this","start":3,"end":7,"id":1},{"text":"context","start":8,"end":15,"id":2},{"text":",","start":15,"end":16,"id":3},{"text":"the","start":17,"end":20,"id":4},{"text":"fresh","start":21,"end":26,"id":5},{"text":"look","start":27,"end":31,"id":6},{"text":"advocated","start":32,"end":41,"id":7},{"text":"here","start":42,"end":46,"id":8},{"text":"permeates","start":47,"end":56,"id":9},{"text":"benefits","start":57,"end":65,"id":10},{"text":"from","start":66,"end":70,"id":11},{"text":"variable","start":71,"end":79,"id":12},{"text":"selection","start":80,"end":89,"id":13},{"text":"and","start":90,"end":93,"id":14},{"text":"compressive","start":94,"end":105,"id":15},{"text":"sampling","start":106,"end":114,"id":16},{"text":",","start":114,"end":115,"id":17},{"text":"to","start":116,"end":118,"id":18},{"text":"robustify","start":119,"end":128,"id":19},{"text":"nonparametric","start":129,"end":142,"id":20},{"text":"regression","start":143,"end":153,"id":21},{"text":"against","start":154,"end":161,"id":22},{"text":"outliers","start":162,"end":170,"id":23},{"text":"-","start":171,"end":172,"id":24},{"text":"that","start":173,"end":177,"id":25},{"text":"is","start":178,"end":180,"id":26},{"text":",","start":180,"end":181,"id":27},{"text":"data","start":182,"end":186,"id":28},{"text":"markedly","start":187,"end":195,"id":29},{"text":"deviating","start":196,"end":205,"id":30},{"text":"from","start":206,"end":210,"id":31},{"text":"the","start":211,"end":214,"id":32},{"text":"postulated","start":215,"end":225,"id":33},{"text":"models","start":226,"end":232,"id":34},{"text":".","start":232,"end":233,"id":35}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":129,"end":153,"token_start":20,"token_end":21,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"The predictive power is maintained at least at the same level as the original tree ensemble.","_input_hash":1495663278,"_task_hash":-595697248,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"predictive","start":4,"end":14,"id":1},{"text":"power","start":15,"end":20,"id":2},{"text":"is","start":21,"end":23,"id":3},{"text":"maintained","start":24,"end":34,"id":4},{"text":"at","start":35,"end":37,"id":5},{"text":"least","start":38,"end":43,"id":6},{"text":"at","start":44,"end":46,"id":7},{"text":"the","start":47,"end":50,"id":8},{"text":"same","start":51,"end":55,"id":9},{"text":"level","start":56,"end":61,"id":10},{"text":"as","start":62,"end":64,"id":11},{"text":"the","start":65,"end":68,"id":12},{"text":"original","start":69,"end":77,"id":13},{"text":"tree","start":78,"end":82,"id":14},{"text":"ensemble","start":83,"end":91,"id":15},{"text":".","start":91,"end":92,"id":16}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":78,"end":91,"token_start":14,"token_end":15,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"The runtime for Kernel Partial Least Squares (KPLS) to compute the fit is quadratic in the number of examples.","_input_hash":-1440221630,"_task_hash":925807616,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"runtime","start":4,"end":11,"id":1},{"text":"for","start":12,"end":15,"id":2},{"text":"Kernel","start":16,"end":22,"id":3},{"text":"Partial","start":23,"end":30,"id":4},{"text":"Least","start":31,"end":36,"id":5},{"text":"Squares","start":37,"end":44,"id":6},{"text":"(","start":45,"end":46,"id":7},{"text":"KPLS","start":46,"end":50,"id":8},{"text":")","start":50,"end":51,"id":9},{"text":"to","start":52,"end":54,"id":10},{"text":"compute","start":55,"end":62,"id":11},{"text":"the","start":63,"end":66,"id":12},{"text":"fit","start":67,"end":70,"id":13},{"text":"is","start":71,"end":73,"id":14},{"text":"quadratic","start":74,"end":83,"id":15},{"text":"in","start":84,"end":86,"id":16},{"text":"the","start":87,"end":90,"id":17},{"text":"number","start":91,"end":97,"id":18},{"text":"of","start":98,"end":100,"id":19},{"text":"examples","start":101,"end":109,"id":20},{"text":".","start":109,"end":110,"id":21}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"This article addresses the modeling of reverberant recording environments in the context of under-determined convolutive blind source separation.","_input_hash":1679859491,"_task_hash":1344595225,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"article","start":5,"end":12,"id":1},{"text":"addresses","start":13,"end":22,"id":2},{"text":"the","start":23,"end":26,"id":3},{"text":"modeling","start":27,"end":35,"id":4},{"text":"of","start":36,"end":38,"id":5},{"text":"reverberant","start":39,"end":50,"id":6},{"text":"recording","start":51,"end":60,"id":7},{"text":"environments","start":61,"end":73,"id":8},{"text":"in","start":74,"end":76,"id":9},{"text":"the","start":77,"end":80,"id":10},{"text":"context","start":81,"end":88,"id":11},{"text":"of","start":89,"end":91,"id":12},{"text":"under","start":92,"end":97,"id":13},{"text":"-","start":97,"end":98,"id":14},{"text":"determined","start":98,"end":108,"id":15},{"text":"convolutive","start":109,"end":120,"id":16},{"text":"blind","start":121,"end":126,"id":17},{"text":"source","start":127,"end":133,"id":18},{"text":"separation","start":134,"end":144,"id":19},{"text":".","start":144,"end":145,"id":20}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"The first encodes the idea that the output of a clustering scheme should carry a multiresolution structure, the second the idea that one should be able to compare the results of clustering algorithms as one varies the data set, for example by adding points or by applying functions to it.","_input_hash":-595449267,"_task_hash":-340209862,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"first","start":4,"end":9,"id":1},{"text":"encodes","start":10,"end":17,"id":2},{"text":"the","start":18,"end":21,"id":3},{"text":"idea","start":22,"end":26,"id":4},{"text":"that","start":27,"end":31,"id":5},{"text":"the","start":32,"end":35,"id":6},{"text":"output","start":36,"end":42,"id":7},{"text":"of","start":43,"end":45,"id":8},{"text":"a","start":46,"end":47,"id":9},{"text":"clustering","start":48,"end":58,"id":10},{"text":"scheme","start":59,"end":65,"id":11},{"text":"should","start":66,"end":72,"id":12},{"text":"carry","start":73,"end":78,"id":13},{"text":"a","start":79,"end":80,"id":14},{"text":"multiresolution","start":81,"end":96,"id":15},{"text":"structure","start":97,"end":106,"id":16},{"text":",","start":106,"end":107,"id":17},{"text":"the","start":108,"end":111,"id":18},{"text":"second","start":112,"end":118,"id":19},{"text":"the","start":119,"end":122,"id":20},{"text":"idea","start":123,"end":127,"id":21},{"text":"that","start":128,"end":132,"id":22},{"text":"one","start":133,"end":136,"id":23},{"text":"should","start":137,"end":143,"id":24},{"text":"be","start":144,"end":146,"id":25},{"text":"able","start":147,"end":151,"id":26},{"text":"to","start":152,"end":154,"id":27},{"text":"compare","start":155,"end":162,"id":28},{"text":"the","start":163,"end":166,"id":29},{"text":"results","start":167,"end":174,"id":30},{"text":"of","start":175,"end":177,"id":31},{"text":"clustering","start":178,"end":188,"id":32},{"text":"algorithms","start":189,"end":199,"id":33},{"text":"as","start":200,"end":202,"id":34},{"text":"one","start":203,"end":206,"id":35},{"text":"varies","start":207,"end":213,"id":36},{"text":"the","start":214,"end":217,"id":37},{"text":"data","start":218,"end":222,"id":38},{"text":"set","start":223,"end":226,"id":39},{"text":",","start":226,"end":227,"id":40},{"text":"for","start":228,"end":231,"id":41},{"text":"example","start":232,"end":239,"id":42},{"text":"by","start":240,"end":242,"id":43},{"text":"adding","start":243,"end":249,"id":44},{"text":"points","start":250,"end":256,"id":45},{"text":"or","start":257,"end":259,"id":46},{"text":"by","start":260,"end":262,"id":47},{"text":"applying","start":263,"end":271,"id":48},{"text":"functions","start":272,"end":281,"id":49},{"text":"to","start":282,"end":284,"id":50},{"text":"it","start":285,"end":287,"id":51},{"text":".","start":287,"end":288,"id":52}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":178,"end":188,"token_start":32,"token_end":32,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We present two sets of theoretical results on the grouped lasso with overlap of Jacob, Obozinski and Vert (2009) in the linear regression setting.","_input_hash":-965908647,"_task_hash":-148412056,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"present","start":3,"end":10,"id":1},{"text":"two","start":11,"end":14,"id":2},{"text":"sets","start":15,"end":19,"id":3},{"text":"of","start":20,"end":22,"id":4},{"text":"theoretical","start":23,"end":34,"id":5},{"text":"results","start":35,"end":42,"id":6},{"text":"on","start":43,"end":45,"id":7},{"text":"the","start":46,"end":49,"id":8},{"text":"grouped","start":50,"end":57,"id":9},{"text":"lasso","start":58,"end":63,"id":10},{"text":"with","start":64,"end":68,"id":11},{"text":"overlap","start":69,"end":76,"id":12},{"text":"of","start":77,"end":79,"id":13},{"text":"Jacob","start":80,"end":85,"id":14},{"text":",","start":85,"end":86,"id":15},{"text":"Obozinski","start":87,"end":96,"id":16},{"text":"and","start":97,"end":100,"id":17},{"text":"Vert","start":101,"end":105,"id":18},{"text":"(","start":106,"end":107,"id":19},{"text":"2009","start":107,"end":111,"id":20},{"text":")","start":111,"end":112,"id":21},{"text":"in","start":113,"end":115,"id":22},{"text":"the","start":116,"end":119,"id":23},{"text":"linear","start":120,"end":126,"id":24},{"text":"regression","start":127,"end":137,"id":25},{"text":"setting","start":138,"end":145,"id":26},{"text":".","start":145,"end":146,"id":27}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":58,"end":63,"token_start":10,"token_end":10,"label":"ALGO","answer":"accept"},{"start":120,"end":137,"token_start":24,"token_end":25,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We propose a novel application of the Simultaneous Orthogonal Matching Pursuit (S-OMP) procedure for sparsistant variable selection in ultra-high dimensional multi-task regression problems.","_input_hash":1557680419,"_task_hash":2046051461,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"propose","start":3,"end":10,"id":1},{"text":"a","start":11,"end":12,"id":2},{"text":"novel","start":13,"end":18,"id":3},{"text":"application","start":19,"end":30,"id":4},{"text":"of","start":31,"end":33,"id":5},{"text":"the","start":34,"end":37,"id":6},{"text":"Simultaneous","start":38,"end":50,"id":7},{"text":"Orthogonal","start":51,"end":61,"id":8},{"text":"Matching","start":62,"end":70,"id":9},{"text":"Pursuit","start":71,"end":78,"id":10},{"text":"(","start":79,"end":80,"id":11},{"text":"S","start":80,"end":81,"id":12},{"text":"-","start":81,"end":82,"id":13},{"text":"OMP","start":82,"end":85,"id":14},{"text":")","start":85,"end":86,"id":15},{"text":"procedure","start":87,"end":96,"id":16},{"text":"for","start":97,"end":100,"id":17},{"text":"sparsistant","start":101,"end":112,"id":18},{"text":"variable","start":113,"end":121,"id":19},{"text":"selection","start":122,"end":131,"id":20},{"text":"in","start":132,"end":134,"id":21},{"text":"ultra","start":135,"end":140,"id":22},{"text":"-","start":140,"end":141,"id":23},{"text":"high","start":141,"end":145,"id":24},{"text":"dimensional","start":146,"end":157,"id":25},{"text":"multi","start":158,"end":163,"id":26},{"text":"-","start":163,"end":164,"id":27},{"text":"task","start":164,"end":168,"id":28},{"text":"regression","start":169,"end":179,"id":29},{"text":"problems","start":180,"end":188,"id":30},{"text":".","start":188,"end":189,"id":31}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":38,"end":78,"token_start":7,"token_end":10,"label":"ALGO","answer":"accept"},{"start":158,"end":179,"token_start":26,"token_end":29,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We study the problem of estimating a temporally varying coefficient and varying structure (VCVS) graphical model underlying nonstationary time series data, such as social states of interacting individuals or microarray expression profiles of gene networks, as opposed to i.i.d.","_input_hash":-56443354,"_task_hash":-468375978,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"study","start":3,"end":8,"id":1},{"text":"the","start":9,"end":12,"id":2},{"text":"problem","start":13,"end":20,"id":3},{"text":"of","start":21,"end":23,"id":4},{"text":"estimating","start":24,"end":34,"id":5},{"text":"a","start":35,"end":36,"id":6},{"text":"temporally","start":37,"end":47,"id":7},{"text":"varying","start":48,"end":55,"id":8},{"text":"coefficient","start":56,"end":67,"id":9},{"text":"and","start":68,"end":71,"id":10},{"text":"varying","start":72,"end":79,"id":11},{"text":"structure","start":80,"end":89,"id":12},{"text":"(","start":90,"end":91,"id":13},{"text":"VCVS","start":91,"end":95,"id":14},{"text":")","start":95,"end":96,"id":15},{"text":"graphical","start":97,"end":106,"id":16},{"text":"model","start":107,"end":112,"id":17},{"text":"underlying","start":113,"end":123,"id":18},{"text":"nonstationary","start":124,"end":137,"id":19},{"text":"time","start":138,"end":142,"id":20},{"text":"series","start":143,"end":149,"id":21},{"text":"data","start":150,"end":154,"id":22},{"text":",","start":154,"end":155,"id":23},{"text":"such","start":156,"end":160,"id":24},{"text":"as","start":161,"end":163,"id":25},{"text":"social","start":164,"end":170,"id":26},{"text":"states","start":171,"end":177,"id":27},{"text":"of","start":178,"end":180,"id":28},{"text":"interacting","start":181,"end":192,"id":29},{"text":"individuals","start":193,"end":204,"id":30},{"text":"or","start":205,"end":207,"id":31},{"text":"microarray","start":208,"end":218,"id":32},{"text":"expression","start":219,"end":229,"id":33},{"text":"profiles","start":230,"end":238,"id":34},{"text":"of","start":239,"end":241,"id":35},{"text":"gene","start":242,"end":246,"id":36},{"text":"networks","start":247,"end":255,"id":37},{"text":",","start":255,"end":256,"id":38},{"text":"as","start":257,"end":259,"id":39},{"text":"opposed","start":260,"end":267,"id":40},{"text":"to","start":268,"end":270,"id":41},{"text":"i.i.d","start":271,"end":276,"id":42},{"text":".","start":276,"end":277,"id":43}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":37,"end":89,"token_start":7,"token_end":12,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We study how this quantity depends on the complexity of the class of distributions characterized by exponents of entropies of the class of regression functions or of the class of Bayes classifiers corresponding to the distributions from the class.","_input_hash":1515898162,"_task_hash":996526672,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"study","start":3,"end":8,"id":1},{"text":"how","start":9,"end":12,"id":2},{"text":"this","start":13,"end":17,"id":3},{"text":"quantity","start":18,"end":26,"id":4},{"text":"depends","start":27,"end":34,"id":5},{"text":"on","start":35,"end":37,"id":6},{"text":"the","start":38,"end":41,"id":7},{"text":"complexity","start":42,"end":52,"id":8},{"text":"of","start":53,"end":55,"id":9},{"text":"the","start":56,"end":59,"id":10},{"text":"class","start":60,"end":65,"id":11},{"text":"of","start":66,"end":68,"id":12},{"text":"distributions","start":69,"end":82,"id":13},{"text":"characterized","start":83,"end":96,"id":14},{"text":"by","start":97,"end":99,"id":15},{"text":"exponents","start":100,"end":109,"id":16},{"text":"of","start":110,"end":112,"id":17},{"text":"entropies","start":113,"end":122,"id":18},{"text":"of","start":123,"end":125,"id":19},{"text":"the","start":126,"end":129,"id":20},{"text":"class","start":130,"end":135,"id":21},{"text":"of","start":136,"end":138,"id":22},{"text":"regression","start":139,"end":149,"id":23},{"text":"functions","start":150,"end":159,"id":24},{"text":"or","start":160,"end":162,"id":25},{"text":"of","start":163,"end":165,"id":26},{"text":"the","start":166,"end":169,"id":27},{"text":"class","start":170,"end":175,"id":28},{"text":"of","start":176,"end":178,"id":29},{"text":"Bayes","start":179,"end":184,"id":30},{"text":"classifiers","start":185,"end":196,"id":31},{"text":"corresponding","start":197,"end":210,"id":32},{"text":"to","start":211,"end":213,"id":33},{"text":"the","start":214,"end":217,"id":34},{"text":"distributions","start":218,"end":231,"id":35},{"text":"from","start":232,"end":236,"id":36},{"text":"the","start":237,"end":240,"id":37},{"text":"class","start":241,"end":246,"id":38},{"text":".","start":246,"end":247,"id":39}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":179,"end":196,"token_start":30,"token_end":31,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Experiments on handwritten digit classification, digital art identification, nonlinear inverse image problems, and compressed sensing demonstrate that our approach is effective in large-scale settings, and is well suited to supervised and semi-supervised classification, as well as regression tasks for data that admit sparse representations.","_input_hash":-1230483569,"_task_hash":1615488459,"tokens":[{"text":"Experiments","start":0,"end":11,"id":0},{"text":"on","start":12,"end":14,"id":1},{"text":"handwritten","start":15,"end":26,"id":2},{"text":"digit","start":27,"end":32,"id":3},{"text":"classification","start":33,"end":47,"id":4},{"text":",","start":47,"end":48,"id":5},{"text":"digital","start":49,"end":56,"id":6},{"text":"art","start":57,"end":60,"id":7},{"text":"identification","start":61,"end":75,"id":8},{"text":",","start":75,"end":76,"id":9},{"text":"nonlinear","start":77,"end":86,"id":10},{"text":"inverse","start":87,"end":94,"id":11},{"text":"image","start":95,"end":100,"id":12},{"text":"problems","start":101,"end":109,"id":13},{"text":",","start":109,"end":110,"id":14},{"text":"and","start":111,"end":114,"id":15},{"text":"compressed","start":115,"end":125,"id":16},{"text":"sensing","start":126,"end":133,"id":17},{"text":"demonstrate","start":134,"end":145,"id":18},{"text":"that","start":146,"end":150,"id":19},{"text":"our","start":151,"end":154,"id":20},{"text":"approach","start":155,"end":163,"id":21},{"text":"is","start":164,"end":166,"id":22},{"text":"effective","start":167,"end":176,"id":23},{"text":"in","start":177,"end":179,"id":24},{"text":"large","start":180,"end":185,"id":25},{"text":"-","start":185,"end":186,"id":26},{"text":"scale","start":186,"end":191,"id":27},{"text":"settings","start":192,"end":200,"id":28},{"text":",","start":200,"end":201,"id":29},{"text":"and","start":202,"end":205,"id":30},{"text":"is","start":206,"end":208,"id":31},{"text":"well","start":209,"end":213,"id":32},{"text":"suited","start":214,"end":220,"id":33},{"text":"to","start":221,"end":223,"id":34},{"text":"supervised","start":224,"end":234,"id":35},{"text":"and","start":235,"end":238,"id":36},{"text":"semi","start":239,"end":243,"id":37},{"text":"-","start":243,"end":244,"id":38},{"text":"supervised","start":244,"end":254,"id":39},{"text":"classification","start":255,"end":269,"id":40},{"text":",","start":269,"end":270,"id":41},{"text":"as","start":271,"end":273,"id":42},{"text":"well","start":274,"end":278,"id":43},{"text":"as","start":279,"end":281,"id":44},{"text":"regression","start":282,"end":292,"id":45},{"text":"tasks","start":293,"end":298,"id":46},{"text":"for","start":299,"end":302,"id":47},{"text":"data","start":303,"end":307,"id":48},{"text":"that","start":308,"end":312,"id":49},{"text":"admit","start":313,"end":318,"id":50},{"text":"sparse","start":319,"end":325,"id":51},{"text":"representations","start":326,"end":341,"id":52},{"text":".","start":341,"end":342,"id":53}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We introduce the Pitman Yor Diffusion Tree (PYDT) for hierarchical clustering, a generalization of the Dirichlet Diffusion Tree (Neal, 2001) which removes the restriction to binary branching structure.","_input_hash":-1684282837,"_task_hash":921577688,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"introduce","start":3,"end":12,"id":1},{"text":"the","start":13,"end":16,"id":2},{"text":"Pitman","start":17,"end":23,"id":3},{"text":"Yor","start":24,"end":27,"id":4},{"text":"Diffusion","start":28,"end":37,"id":5},{"text":"Tree","start":38,"end":42,"id":6},{"text":"(","start":43,"end":44,"id":7},{"text":"PYDT","start":44,"end":48,"id":8},{"text":")","start":48,"end":49,"id":9},{"text":"for","start":50,"end":53,"id":10},{"text":"hierarchical","start":54,"end":66,"id":11},{"text":"clustering","start":67,"end":77,"id":12},{"text":",","start":77,"end":78,"id":13},{"text":"a","start":79,"end":80,"id":14},{"text":"generalization","start":81,"end":95,"id":15},{"text":"of","start":96,"end":98,"id":16},{"text":"the","start":99,"end":102,"id":17},{"text":"Dirichlet","start":103,"end":112,"id":18},{"text":"Diffusion","start":113,"end":122,"id":19},{"text":"Tree","start":123,"end":127,"id":20},{"text":"(","start":128,"end":129,"id":21},{"text":"Neal","start":129,"end":133,"id":22},{"text":",","start":133,"end":134,"id":23},{"text":"2001","start":135,"end":139,"id":24},{"text":")","start":139,"end":140,"id":25},{"text":"which","start":141,"end":146,"id":26},{"text":"removes","start":147,"end":154,"id":27},{"text":"the","start":155,"end":158,"id":28},{"text":"restriction","start":159,"end":170,"id":29},{"text":"to","start":171,"end":173,"id":30},{"text":"binary","start":174,"end":180,"id":31},{"text":"branching","start":181,"end":190,"id":32},{"text":"structure","start":191,"end":200,"id":33},{"text":".","start":200,"end":201,"id":34}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":103,"end":127,"token_start":18,"token_end":20,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Through large-scale experiments on real-world image classification and audio tagging problems, the proposed method is shown to compare favorably with existing dimension reduction approaches.","_input_hash":-1802295659,"_task_hash":242394494,"tokens":[{"text":"Through","start":0,"end":7,"id":0},{"text":"large","start":8,"end":13,"id":1},{"text":"-","start":13,"end":14,"id":2},{"text":"scale","start":14,"end":19,"id":3},{"text":"experiments","start":20,"end":31,"id":4},{"text":"on","start":32,"end":34,"id":5},{"text":"real","start":35,"end":39,"id":6},{"text":"-","start":39,"end":40,"id":7},{"text":"world","start":40,"end":45,"id":8},{"text":"image","start":46,"end":51,"id":9},{"text":"classification","start":52,"end":66,"id":10},{"text":"and","start":67,"end":70,"id":11},{"text":"audio","start":71,"end":76,"id":12},{"text":"tagging","start":77,"end":84,"id":13},{"text":"problems","start":85,"end":93,"id":14},{"text":",","start":93,"end":94,"id":15},{"text":"the","start":95,"end":98,"id":16},{"text":"proposed","start":99,"end":107,"id":17},{"text":"method","start":108,"end":114,"id":18},{"text":"is","start":115,"end":117,"id":19},{"text":"shown","start":118,"end":123,"id":20},{"text":"to","start":124,"end":126,"id":21},{"text":"compare","start":127,"end":134,"id":22},{"text":"favorably","start":135,"end":144,"id":23},{"text":"with","start":145,"end":149,"id":24},{"text":"existing","start":150,"end":158,"id":25},{"text":"dimension","start":159,"end":168,"id":26},{"text":"reduction","start":169,"end":178,"id":27},{"text":"approaches","start":179,"end":189,"id":28},{"text":".","start":189,"end":190,"id":29}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"This model exemplifies a recent trend in statistical machine learning--the use of Bayesian nonparametric methods to infer distributions on flexible data structures.","_input_hash":1038966363,"_task_hash":204625339,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"model","start":5,"end":10,"id":1},{"text":"exemplifies","start":11,"end":22,"id":2},{"text":"a","start":23,"end":24,"id":3},{"text":"recent","start":25,"end":31,"id":4},{"text":"trend","start":32,"end":37,"id":5},{"text":"in","start":38,"end":40,"id":6},{"text":"statistical","start":41,"end":52,"id":7},{"text":"machine","start":53,"end":60,"id":8},{"text":"learning","start":61,"end":69,"id":9},{"text":"--","start":69,"end":71,"id":10},{"text":"the","start":71,"end":74,"id":11},{"text":"use","start":75,"end":78,"id":12},{"text":"of","start":79,"end":81,"id":13},{"text":"Bayesian","start":82,"end":90,"id":14},{"text":"nonparametric","start":91,"end":104,"id":15},{"text":"methods","start":105,"end":112,"id":16},{"text":"to","start":113,"end":115,"id":17},{"text":"infer","start":116,"end":121,"id":18},{"text":"distributions","start":122,"end":135,"id":19},{"text":"on","start":136,"end":138,"id":20},{"text":"flexible","start":139,"end":147,"id":21},{"text":"data","start":148,"end":152,"id":22},{"text":"structures","start":153,"end":163,"id":23},{"text":".","start":163,"end":164,"id":24}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":82,"end":104,"token_start":14,"token_end":15,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"The results hold universally for any optimization problem on a bounded domain and establish a connection between simulated annealing and up-to-date theory of convergence of Markov chain Monte Carlo methods on continuous domains.","_input_hash":-1859510931,"_task_hash":-1624116196,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"results","start":4,"end":11,"id":1},{"text":"hold","start":12,"end":16,"id":2},{"text":"universally","start":17,"end":28,"id":3},{"text":"for","start":29,"end":32,"id":4},{"text":"any","start":33,"end":36,"id":5},{"text":"optimization","start":37,"end":49,"id":6},{"text":"problem","start":50,"end":57,"id":7},{"text":"on","start":58,"end":60,"id":8},{"text":"a","start":61,"end":62,"id":9},{"text":"bounded","start":63,"end":70,"id":10},{"text":"domain","start":71,"end":77,"id":11},{"text":"and","start":78,"end":81,"id":12},{"text":"establish","start":82,"end":91,"id":13},{"text":"a","start":92,"end":93,"id":14},{"text":"connection","start":94,"end":104,"id":15},{"text":"between","start":105,"end":112,"id":16},{"text":"simulated","start":113,"end":122,"id":17},{"text":"annealing","start":123,"end":132,"id":18},{"text":"and","start":133,"end":136,"id":19},{"text":"up","start":137,"end":139,"id":20},{"text":"-","start":139,"end":140,"id":21},{"text":"to","start":140,"end":142,"id":22},{"text":"-","start":142,"end":143,"id":23},{"text":"date","start":143,"end":147,"id":24},{"text":"theory","start":148,"end":154,"id":25},{"text":"of","start":155,"end":157,"id":26},{"text":"convergence","start":158,"end":169,"id":27},{"text":"of","start":170,"end":172,"id":28},{"text":"Markov","start":173,"end":179,"id":29},{"text":"chain","start":180,"end":185,"id":30},{"text":"Monte","start":186,"end":191,"id":31},{"text":"Carlo","start":192,"end":197,"id":32},{"text":"methods","start":198,"end":205,"id":33},{"text":"on","start":206,"end":208,"id":34},{"text":"continuous","start":209,"end":219,"id":35},{"text":"domains","start":220,"end":227,"id":36},{"text":".","start":227,"end":228,"id":37}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":173,"end":197,"token_start":29,"token_end":32,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We propose a novel algorithm for KPLS which not only computes (a) the fit, but also (b) its approximate degrees of freedom and (c) error bars in quadratic runtime.","_input_hash":-1985083073,"_task_hash":-1116708158,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"propose","start":3,"end":10,"id":1},{"text":"a","start":11,"end":12,"id":2},{"text":"novel","start":13,"end":18,"id":3},{"text":"algorithm","start":19,"end":28,"id":4},{"text":"for","start":29,"end":32,"id":5},{"text":"KPLS","start":33,"end":37,"id":6},{"text":"which","start":38,"end":43,"id":7},{"text":"not","start":44,"end":47,"id":8},{"text":"only","start":48,"end":52,"id":9},{"text":"computes","start":53,"end":61,"id":10},{"text":"(","start":62,"end":63,"id":11},{"text":"a","start":63,"end":64,"id":12},{"text":")","start":64,"end":65,"id":13},{"text":"the","start":66,"end":69,"id":14},{"text":"fit","start":70,"end":73,"id":15},{"text":",","start":73,"end":74,"id":16},{"text":"but","start":75,"end":78,"id":17},{"text":"also","start":79,"end":83,"id":18},{"text":"(","start":84,"end":85,"id":19},{"text":"b","start":85,"end":86,"id":20},{"text":")","start":86,"end":87,"id":21},{"text":"its","start":88,"end":91,"id":22},{"text":"approximate","start":92,"end":103,"id":23},{"text":"degrees","start":104,"end":111,"id":24},{"text":"of","start":112,"end":114,"id":25},{"text":"freedom","start":115,"end":122,"id":26},{"text":"and","start":123,"end":126,"id":27},{"text":"(","start":127,"end":128,"id":28},{"text":"c","start":128,"end":129,"id":29},{"text":")","start":129,"end":130,"id":30},{"text":"error","start":131,"end":136,"id":31},{"text":"bars","start":137,"end":141,"id":32},{"text":"in","start":142,"end":144,"id":33},{"text":"quadratic","start":145,"end":154,"id":34},{"text":"runtime","start":155,"end":162,"id":35},{"text":".","start":162,"end":163,"id":36}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"The class of Schoenberg transformations, embedding Euclidean distances into higher dimensional Euclidean spaces, is presented, and derived from theorems on positive definite and conditionally negative definite matrices.","_input_hash":130472642,"_task_hash":1582280033,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"class","start":4,"end":9,"id":1},{"text":"of","start":10,"end":12,"id":2},{"text":"Schoenberg","start":13,"end":23,"id":3},{"text":"transformations","start":24,"end":39,"id":4},{"text":",","start":39,"end":40,"id":5},{"text":"embedding","start":41,"end":50,"id":6},{"text":"Euclidean","start":51,"end":60,"id":7},{"text":"distances","start":61,"end":70,"id":8},{"text":"into","start":71,"end":75,"id":9},{"text":"higher","start":76,"end":82,"id":10},{"text":"dimensional","start":83,"end":94,"id":11},{"text":"Euclidean","start":95,"end":104,"id":12},{"text":"spaces","start":105,"end":111,"id":13},{"text":",","start":111,"end":112,"id":14},{"text":"is","start":113,"end":115,"id":15},{"text":"presented","start":116,"end":125,"id":16},{"text":",","start":125,"end":126,"id":17},{"text":"and","start":127,"end":130,"id":18},{"text":"derived","start":131,"end":138,"id":19},{"text":"from","start":139,"end":143,"id":20},{"text":"theorems","start":144,"end":152,"id":21},{"text":"on","start":153,"end":155,"id":22},{"text":"positive","start":156,"end":164,"id":23},{"text":"definite","start":165,"end":173,"id":24},{"text":"and","start":174,"end":177,"id":25},{"text":"conditionally","start":178,"end":191,"id":26},{"text":"negative","start":192,"end":200,"id":27},{"text":"definite","start":201,"end":209,"id":28},{"text":"matrices","start":210,"end":218,"id":29},{"text":".","start":218,"end":219,"id":30}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"The unified view of this work leads to a more satisfactory treatment of concave high dimensional sparse estimation procedures, and serves as guideline for developing further numerical procedures for concave regularization.","_input_hash":1571267365,"_task_hash":-1360001340,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"unified","start":4,"end":11,"id":1},{"text":"view","start":12,"end":16,"id":2},{"text":"of","start":17,"end":19,"id":3},{"text":"this","start":20,"end":24,"id":4},{"text":"work","start":25,"end":29,"id":5},{"text":"leads","start":30,"end":35,"id":6},{"text":"to","start":36,"end":38,"id":7},{"text":"a","start":39,"end":40,"id":8},{"text":"more","start":41,"end":45,"id":9},{"text":"satisfactory","start":46,"end":58,"id":10},{"text":"treatment","start":59,"end":68,"id":11},{"text":"of","start":69,"end":71,"id":12},{"text":"concave","start":72,"end":79,"id":13},{"text":"high","start":80,"end":84,"id":14},{"text":"dimensional","start":85,"end":96,"id":15},{"text":"sparse","start":97,"end":103,"id":16},{"text":"estimation","start":104,"end":114,"id":17},{"text":"procedures","start":115,"end":125,"id":18},{"text":",","start":125,"end":126,"id":19},{"text":"and","start":127,"end":130,"id":20},{"text":"serves","start":131,"end":137,"id":21},{"text":"as","start":138,"end":140,"id":22},{"text":"guideline","start":141,"end":150,"id":23},{"text":"for","start":151,"end":154,"id":24},{"text":"developing","start":155,"end":165,"id":25},{"text":"further","start":166,"end":173,"id":26},{"text":"numerical","start":174,"end":183,"id":27},{"text":"procedures","start":184,"end":194,"id":28},{"text":"for","start":195,"end":198,"id":29},{"text":"concave","start":199,"end":206,"id":30},{"text":"regularization","start":207,"end":221,"id":31},{"text":".","start":221,"end":222,"id":32}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":72,"end":114,"token_start":13,"token_end":17,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Security issues are crucial in a number of machine learning applications, especially in scenarios dealing with human activity rather than natural phenomena (e.g., information ranking, spam detection, malware detection, etc.).","_input_hash":-669017834,"_task_hash":877841530,"tokens":[{"text":"Security","start":0,"end":8,"id":0},{"text":"issues","start":9,"end":15,"id":1},{"text":"are","start":16,"end":19,"id":2},{"text":"crucial","start":20,"end":27,"id":3},{"text":"in","start":28,"end":30,"id":4},{"text":"a","start":31,"end":32,"id":5},{"text":"number","start":33,"end":39,"id":6},{"text":"of","start":40,"end":42,"id":7},{"text":"machine","start":43,"end":50,"id":8},{"text":"learning","start":51,"end":59,"id":9},{"text":"applications","start":60,"end":72,"id":10},{"text":",","start":72,"end":73,"id":11},{"text":"especially","start":74,"end":84,"id":12},{"text":"in","start":85,"end":87,"id":13},{"text":"scenarios","start":88,"end":97,"id":14},{"text":"dealing","start":98,"end":105,"id":15},{"text":"with","start":106,"end":110,"id":16},{"text":"human","start":111,"end":116,"id":17},{"text":"activity","start":117,"end":125,"id":18},{"text":"rather","start":126,"end":132,"id":19},{"text":"than","start":133,"end":137,"id":20},{"text":"natural","start":138,"end":145,"id":21},{"text":"phenomena","start":146,"end":155,"id":22},{"text":"(","start":156,"end":157,"id":23},{"text":"e.g.","start":157,"end":161,"id":24},{"text":",","start":161,"end":162,"id":25},{"text":"information","start":163,"end":174,"id":26},{"text":"ranking","start":175,"end":182,"id":27},{"text":",","start":182,"end":183,"id":28},{"text":"spam","start":184,"end":188,"id":29},{"text":"detection","start":189,"end":198,"id":30},{"text":",","start":198,"end":199,"id":31},{"text":"malware","start":200,"end":207,"id":32},{"text":"detection","start":208,"end":217,"id":33},{"text":",","start":217,"end":218,"id":34},{"text":"etc","start":219,"end":222,"id":35},{"text":".","start":222,"end":223,"id":36},{"text":")","start":223,"end":224,"id":37},{"text":".","start":224,"end":225,"id":38}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"In this paper, we give a new sharp generalization bound of lp-MKL which is a generalized framework of multiple kernel learning (MKL) and imposes lp-mixed-norm regularization instead of l1-mixed-norm regularization.","_input_hash":1785306897,"_task_hash":-398936495,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"this","start":3,"end":7,"id":1},{"text":"paper","start":8,"end":13,"id":2},{"text":",","start":13,"end":14,"id":3},{"text":"we","start":15,"end":17,"id":4},{"text":"give","start":18,"end":22,"id":5},{"text":"a","start":23,"end":24,"id":6},{"text":"new","start":25,"end":28,"id":7},{"text":"sharp","start":29,"end":34,"id":8},{"text":"generalization","start":35,"end":49,"id":9},{"text":"bound","start":50,"end":55,"id":10},{"text":"of","start":56,"end":58,"id":11},{"text":"lp","start":59,"end":61,"id":12},{"text":"-","start":61,"end":62,"id":13},{"text":"MKL","start":62,"end":65,"id":14},{"text":"which","start":66,"end":71,"id":15},{"text":"is","start":72,"end":74,"id":16},{"text":"a","start":75,"end":76,"id":17},{"text":"generalized","start":77,"end":88,"id":18},{"text":"framework","start":89,"end":98,"id":19},{"text":"of","start":99,"end":101,"id":20},{"text":"multiple","start":102,"end":110,"id":21},{"text":"kernel","start":111,"end":117,"id":22},{"text":"learning","start":118,"end":126,"id":23},{"text":"(","start":127,"end":128,"id":24},{"text":"MKL","start":128,"end":131,"id":25},{"text":")","start":131,"end":132,"id":26},{"text":"and","start":133,"end":136,"id":27},{"text":"imposes","start":137,"end":144,"id":28},{"text":"lp","start":145,"end":147,"id":29},{"text":"-","start":147,"end":148,"id":30},{"text":"mixed","start":148,"end":153,"id":31},{"text":"-","start":153,"end":154,"id":32},{"text":"norm","start":154,"end":158,"id":33},{"text":"regularization","start":159,"end":173,"id":34},{"text":"instead","start":174,"end":181,"id":35},{"text":"of","start":182,"end":184,"id":36},{"text":"l1-mixed","start":185,"end":193,"id":37},{"text":"-","start":193,"end":194,"id":38},{"text":"norm","start":194,"end":198,"id":39},{"text":"regularization","start":199,"end":213,"id":40},{"text":".","start":213,"end":214,"id":41}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[{"start":102,"end":126,"token_start":21,"token_end":23,"label":"ALGO","answer":"accept"}]}
{"text":"We remove the smoothness assumption and generalize the analysis of graph Laplacians to include previously unstudied graphs including kNN graphs.","_input_hash":-473911311,"_task_hash":-1136260681,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"remove","start":3,"end":9,"id":1},{"text":"the","start":10,"end":13,"id":2},{"text":"smoothness","start":14,"end":24,"id":3},{"text":"assumption","start":25,"end":35,"id":4},{"text":"and","start":36,"end":39,"id":5},{"text":"generalize","start":40,"end":50,"id":6},{"text":"the","start":51,"end":54,"id":7},{"text":"analysis","start":55,"end":63,"id":8},{"text":"of","start":64,"end":66,"id":9},{"text":"graph","start":67,"end":72,"id":10},{"text":"Laplacians","start":73,"end":83,"id":11},{"text":"to","start":84,"end":86,"id":12},{"text":"include","start":87,"end":94,"id":13},{"text":"previously","start":95,"end":105,"id":14},{"text":"unstudied","start":106,"end":115,"id":15},{"text":"graphs","start":116,"end":122,"id":16},{"text":"including","start":123,"end":132,"id":17},{"text":"kNN","start":133,"end":136,"id":18},{"text":"graphs","start":137,"end":143,"id":19},{"text":".","start":143,"end":144,"id":20}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":133,"end":136,"token_start":18,"token_end":18,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Experiments on handwriting recognition and dependency parsing testify for the successfulness of the approach.","_input_hash":957913555,"_task_hash":-274463593,"tokens":[{"text":"Experiments","start":0,"end":11,"id":0},{"text":"on","start":12,"end":14,"id":1},{"text":"handwriting","start":15,"end":26,"id":2},{"text":"recognition","start":27,"end":38,"id":3},{"text":"and","start":39,"end":42,"id":4},{"text":"dependency","start":43,"end":53,"id":5},{"text":"parsing","start":54,"end":61,"id":6},{"text":"testify","start":62,"end":69,"id":7},{"text":"for","start":70,"end":73,"id":8},{"text":"the","start":74,"end":77,"id":9},{"text":"successfulness","start":78,"end":92,"id":10},{"text":"of","start":93,"end":95,"id":11},{"text":"the","start":96,"end":99,"id":12},{"text":"approach","start":100,"end":108,"id":13},{"text":".","start":108,"end":109,"id":14}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"After explaining the definitions, the concept of classifiers is discussed in the context of moduli spaces, and upper bounds for the number of hidden vertices in dendrograms are given.","_input_hash":1788297260,"_task_hash":1090886645,"tokens":[{"text":"After","start":0,"end":5,"id":0},{"text":"explaining","start":6,"end":16,"id":1},{"text":"the","start":17,"end":20,"id":2},{"text":"definitions","start":21,"end":32,"id":3},{"text":",","start":32,"end":33,"id":4},{"text":"the","start":34,"end":37,"id":5},{"text":"concept","start":38,"end":45,"id":6},{"text":"of","start":46,"end":48,"id":7},{"text":"classifiers","start":49,"end":60,"id":8},{"text":"is","start":61,"end":63,"id":9},{"text":"discussed","start":64,"end":73,"id":10},{"text":"in","start":74,"end":76,"id":11},{"text":"the","start":77,"end":80,"id":12},{"text":"context","start":81,"end":88,"id":13},{"text":"of","start":89,"end":91,"id":14},{"text":"moduli","start":92,"end":98,"id":15},{"text":"spaces","start":99,"end":105,"id":16},{"text":",","start":105,"end":106,"id":17},{"text":"and","start":107,"end":110,"id":18},{"text":"upper","start":111,"end":116,"id":19},{"text":"bounds","start":117,"end":123,"id":20},{"text":"for","start":124,"end":127,"id":21},{"text":"the","start":128,"end":131,"id":22},{"text":"number","start":132,"end":138,"id":23},{"text":"of","start":139,"end":141,"id":24},{"text":"hidden","start":142,"end":148,"id":25},{"text":"vertices","start":149,"end":157,"id":26},{"text":"in","start":158,"end":160,"id":27},{"text":"dendrograms","start":161,"end":172,"id":28},{"text":"are","start":173,"end":176,"id":29},{"text":"given","start":177,"end":182,"id":30},{"text":".","start":182,"end":183,"id":31}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We show that there are simple manifolds in which the necessary conditions are violated, and hence the algorithms cannot recover the underlying manifolds.","_input_hash":-1117391181,"_task_hash":-775241696,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"show","start":3,"end":7,"id":1},{"text":"that","start":8,"end":12,"id":2},{"text":"there","start":13,"end":18,"id":3},{"text":"are","start":19,"end":22,"id":4},{"text":"simple","start":23,"end":29,"id":5},{"text":"manifolds","start":30,"end":39,"id":6},{"text":"in","start":40,"end":42,"id":7},{"text":"which","start":43,"end":48,"id":8},{"text":"the","start":49,"end":52,"id":9},{"text":"necessary","start":53,"end":62,"id":10},{"text":"conditions","start":63,"end":73,"id":11},{"text":"are","start":74,"end":77,"id":12},{"text":"violated","start":78,"end":86,"id":13},{"text":",","start":86,"end":87,"id":14},{"text":"and","start":88,"end":91,"id":15},{"text":"hence","start":92,"end":97,"id":16},{"text":"the","start":98,"end":101,"id":17},{"text":"algorithms","start":102,"end":112,"id":18},{"text":"can","start":113,"end":116,"id":19},{"text":"not","start":116,"end":119,"id":20},{"text":"recover","start":120,"end":127,"id":21},{"text":"the","start":128,"end":131,"id":22},{"text":"underlying","start":132,"end":142,"id":23},{"text":"manifolds","start":143,"end":152,"id":24},{"text":".","start":152,"end":153,"id":25}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Reduced bias and enhanced generalization capability are attractive features of an improved estimator obtained after replacing the L0-(pseudo)norm with a nonconvex surrogate.","_input_hash":1377417693,"_task_hash":-31359632,"tokens":[{"text":"Reduced","start":0,"end":7,"id":0},{"text":"bias","start":8,"end":12,"id":1},{"text":"and","start":13,"end":16,"id":2},{"text":"enhanced","start":17,"end":25,"id":3},{"text":"generalization","start":26,"end":40,"id":4},{"text":"capability","start":41,"end":51,"id":5},{"text":"are","start":52,"end":55,"id":6},{"text":"attractive","start":56,"end":66,"id":7},{"text":"features","start":67,"end":75,"id":8},{"text":"of","start":76,"end":78,"id":9},{"text":"an","start":79,"end":81,"id":10},{"text":"improved","start":82,"end":90,"id":11},{"text":"estimator","start":91,"end":100,"id":12},{"text":"obtained","start":101,"end":109,"id":13},{"text":"after","start":110,"end":115,"id":14},{"text":"replacing","start":116,"end":125,"id":15},{"text":"the","start":126,"end":129,"id":16},{"text":"L0-(pseudo)norm","start":130,"end":145,"id":17},{"text":"with","start":146,"end":150,"id":18},{"text":"a","start":151,"end":152,"id":19},{"text":"nonconvex","start":153,"end":162,"id":20},{"text":"surrogate","start":163,"end":172,"id":21},{"text":".","start":172,"end":173,"id":22}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"  Furthermore, we show a simple way to choose k = k(x) locally at any x so as to nearly achieve the minimax rate at x in terms of the unknown intrinsic dimension in the vicinity of x. We also establish that the minimax rate does not depend on a particular choice of metric space or distribution, but rather that this minimax rate holds for any metric space and doubling measure.","_input_hash":174332260,"_task_hash":162025632,"tokens":[{"text":"  ","start":0,"end":2,"id":0},{"text":"Furthermore","start":2,"end":13,"id":1},{"text":",","start":13,"end":14,"id":2},{"text":"we","start":15,"end":17,"id":3},{"text":"show","start":18,"end":22,"id":4},{"text":"a","start":23,"end":24,"id":5},{"text":"simple","start":25,"end":31,"id":6},{"text":"way","start":32,"end":35,"id":7},{"text":"to","start":36,"end":38,"id":8},{"text":"choose","start":39,"end":45,"id":9},{"text":"k","start":46,"end":47,"id":10},{"text":"=","start":48,"end":49,"id":11},{"text":"k(x","start":50,"end":53,"id":12},{"text":")","start":53,"end":54,"id":13},{"text":"locally","start":55,"end":62,"id":14},{"text":"at","start":63,"end":65,"id":15},{"text":"any","start":66,"end":69,"id":16},{"text":"x","start":70,"end":71,"id":17},{"text":"so","start":72,"end":74,"id":18},{"text":"as","start":75,"end":77,"id":19},{"text":"to","start":78,"end":80,"id":20},{"text":"nearly","start":81,"end":87,"id":21},{"text":"achieve","start":88,"end":95,"id":22},{"text":"the","start":96,"end":99,"id":23},{"text":"minimax","start":100,"end":107,"id":24},{"text":"rate","start":108,"end":112,"id":25},{"text":"at","start":113,"end":115,"id":26},{"text":"x","start":116,"end":117,"id":27},{"text":"in","start":118,"end":120,"id":28},{"text":"terms","start":121,"end":126,"id":29},{"text":"of","start":127,"end":129,"id":30},{"text":"the","start":130,"end":133,"id":31},{"text":"unknown","start":134,"end":141,"id":32},{"text":"intrinsic","start":142,"end":151,"id":33},{"text":"dimension","start":152,"end":161,"id":34},{"text":"in","start":162,"end":164,"id":35},{"text":"the","start":165,"end":168,"id":36},{"text":"vicinity","start":169,"end":177,"id":37},{"text":"of","start":178,"end":180,"id":38},{"text":"x.","start":181,"end":183,"id":39},{"text":"We","start":184,"end":186,"id":40},{"text":"also","start":187,"end":191,"id":41},{"text":"establish","start":192,"end":201,"id":42},{"text":"that","start":202,"end":206,"id":43},{"text":"the","start":207,"end":210,"id":44},{"text":"minimax","start":211,"end":218,"id":45},{"text":"rate","start":219,"end":223,"id":46},{"text":"does","start":224,"end":228,"id":47},{"text":"not","start":229,"end":232,"id":48},{"text":"depend","start":233,"end":239,"id":49},{"text":"on","start":240,"end":242,"id":50},{"text":"a","start":243,"end":244,"id":51},{"text":"particular","start":245,"end":255,"id":52},{"text":"choice","start":256,"end":262,"id":53},{"text":"of","start":263,"end":265,"id":54},{"text":"metric","start":266,"end":272,"id":55},{"text":"space","start":273,"end":278,"id":56},{"text":"or","start":279,"end":281,"id":57},{"text":"distribution","start":282,"end":294,"id":58},{"text":",","start":294,"end":295,"id":59},{"text":"but","start":296,"end":299,"id":60},{"text":"rather","start":300,"end":306,"id":61},{"text":"that","start":307,"end":311,"id":62},{"text":"this","start":312,"end":316,"id":63},{"text":"minimax","start":317,"end":324,"id":64},{"text":"rate","start":325,"end":329,"id":65},{"text":"holds","start":330,"end":335,"id":66},{"text":"for","start":336,"end":339,"id":67},{"text":"any","start":340,"end":343,"id":68},{"text":"metric","start":344,"end":350,"id":69},{"text":"space","start":351,"end":356,"id":70},{"text":"and","start":357,"end":360,"id":71},{"text":"doubling","start":361,"end":369,"id":72},{"text":"measure","start":370,"end":377,"id":73},{"text":".","start":377,"end":378,"id":74}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Advanced plotting options are provided by the Rgraphviz package.","_input_hash":-2045713205,"_task_hash":123012275,"tokens":[{"text":"Advanced","start":0,"end":8,"id":0},{"text":"plotting","start":9,"end":17,"id":1},{"text":"options","start":18,"end":25,"id":2},{"text":"are","start":26,"end":29,"id":3},{"text":"provided","start":30,"end":38,"id":4},{"text":"by","start":39,"end":41,"id":5},{"text":"the","start":42,"end":45,"id":6},{"text":"Rgraphviz","start":46,"end":55,"id":7},{"text":"package","start":56,"end":63,"id":8},{"text":".","start":63,"end":64,"id":9}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"We provide rigorous theoretical guarantees on the convergence and complexity of our algorithm and demonstrate the effectiveness of our proposal via experiments.","_input_hash":901109941,"_task_hash":1319170134,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"provide","start":3,"end":10,"id":1},{"text":"rigorous","start":11,"end":19,"id":2},{"text":"theoretical","start":20,"end":31,"id":3},{"text":"guarantees","start":32,"end":42,"id":4},{"text":"on","start":43,"end":45,"id":5},{"text":"the","start":46,"end":49,"id":6},{"text":"convergence","start":50,"end":61,"id":7},{"text":"and","start":62,"end":65,"id":8},{"text":"complexity","start":66,"end":76,"id":9},{"text":"of","start":77,"end":79,"id":10},{"text":"our","start":80,"end":83,"id":11},{"text":"algorithm","start":84,"end":93,"id":12},{"text":"and","start":94,"end":97,"id":13},{"text":"demonstrate","start":98,"end":109,"id":14},{"text":"the","start":110,"end":113,"id":15},{"text":"effectiveness","start":114,"end":127,"id":16},{"text":"of","start":128,"end":130,"id":17},{"text":"our","start":131,"end":134,"id":18},{"text":"proposal","start":135,"end":143,"id":19},{"text":"via","start":144,"end":147,"id":20},{"text":"experiments","start":148,"end":159,"id":21},{"text":".","start":159,"end":160,"id":22}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Finally, in an appendix we present some new algorithm-independent results on the relationship between properness, convexity and robustness to misclassification noise for binary losses and show that all convex proper losses are non-robust to misclassification noise.","_input_hash":638507280,"_task_hash":-1270785360,"tokens":[{"text":"Finally","start":0,"end":7,"id":0},{"text":",","start":7,"end":8,"id":1},{"text":"in","start":9,"end":11,"id":2},{"text":"an","start":12,"end":14,"id":3},{"text":"appendix","start":15,"end":23,"id":4},{"text":"we","start":24,"end":26,"id":5},{"text":"present","start":27,"end":34,"id":6},{"text":"some","start":35,"end":39,"id":7},{"text":"new","start":40,"end":43,"id":8},{"text":"algorithm","start":44,"end":53,"id":9},{"text":"-","start":53,"end":54,"id":10},{"text":"independent","start":54,"end":65,"id":11},{"text":"results","start":66,"end":73,"id":12},{"text":"on","start":74,"end":76,"id":13},{"text":"the","start":77,"end":80,"id":14},{"text":"relationship","start":81,"end":93,"id":15},{"text":"between","start":94,"end":101,"id":16},{"text":"properness","start":102,"end":112,"id":17},{"text":",","start":112,"end":113,"id":18},{"text":"convexity","start":114,"end":123,"id":19},{"text":"and","start":124,"end":127,"id":20},{"text":"robustness","start":128,"end":138,"id":21},{"text":"to","start":139,"end":141,"id":22},{"text":"misclassification","start":142,"end":159,"id":23},{"text":"noise","start":160,"end":165,"id":24},{"text":"for","start":166,"end":169,"id":25},{"text":"binary","start":170,"end":176,"id":26},{"text":"losses","start":177,"end":183,"id":27},{"text":"and","start":184,"end":187,"id":28},{"text":"show","start":188,"end":192,"id":29},{"text":"that","start":193,"end":197,"id":30},{"text":"all","start":198,"end":201,"id":31},{"text":"convex","start":202,"end":208,"id":32},{"text":"proper","start":209,"end":215,"id":33},{"text":"losses","start":216,"end":222,"id":34},{"text":"are","start":223,"end":226,"id":35},{"text":"non","start":227,"end":230,"id":36},{"text":"-","start":230,"end":231,"id":37},{"text":"robust","start":231,"end":237,"id":38},{"text":"to","start":238,"end":240,"id":39},{"text":"misclassification","start":241,"end":258,"id":40},{"text":"noise","start":259,"end":264,"id":41},{"text":".","start":264,"end":265,"id":42}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We describe and study an alternative selection scheme based on relative bounds between estimators, and present a two step localization technique which can handle the selection of a parametric model from a family of those.","_input_hash":890575210,"_task_hash":-320249687,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"describe","start":3,"end":11,"id":1},{"text":"and","start":12,"end":15,"id":2},{"text":"study","start":16,"end":21,"id":3},{"text":"an","start":22,"end":24,"id":4},{"text":"alternative","start":25,"end":36,"id":5},{"text":"selection","start":37,"end":46,"id":6},{"text":"scheme","start":47,"end":53,"id":7},{"text":"based","start":54,"end":59,"id":8},{"text":"on","start":60,"end":62,"id":9},{"text":"relative","start":63,"end":71,"id":10},{"text":"bounds","start":72,"end":78,"id":11},{"text":"between","start":79,"end":86,"id":12},{"text":"estimators","start":87,"end":97,"id":13},{"text":",","start":97,"end":98,"id":14},{"text":"and","start":99,"end":102,"id":15},{"text":"present","start":103,"end":110,"id":16},{"text":"a","start":111,"end":112,"id":17},{"text":"two","start":113,"end":116,"id":18},{"text":"step","start":117,"end":121,"id":19},{"text":"localization","start":122,"end":134,"id":20},{"text":"technique","start":135,"end":144,"id":21},{"text":"which","start":145,"end":150,"id":22},{"text":"can","start":151,"end":154,"id":23},{"text":"handle","start":155,"end":161,"id":24},{"text":"the","start":162,"end":165,"id":25},{"text":"selection","start":166,"end":175,"id":26},{"text":"of","start":176,"end":178,"id":27},{"text":"a","start":179,"end":180,"id":28},{"text":"parametric","start":181,"end":191,"id":29},{"text":"model","start":192,"end":197,"id":30},{"text":"from","start":198,"end":202,"id":31},{"text":"a","start":203,"end":204,"id":32},{"text":"family","start":205,"end":211,"id":33},{"text":"of","start":212,"end":214,"id":34},{"text":"those","start":215,"end":220,"id":35},{"text":".","start":220,"end":221,"id":36}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":181,"end":191,"token_start":29,"token_end":29,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We consider the empirical risk minimization problem for linear supervised learning, with regularization by structured sparsity-inducing norms.","_input_hash":-1295944508,"_task_hash":-486257343,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"consider","start":3,"end":11,"id":1},{"text":"the","start":12,"end":15,"id":2},{"text":"empirical","start":16,"end":25,"id":3},{"text":"risk","start":26,"end":30,"id":4},{"text":"minimization","start":31,"end":43,"id":5},{"text":"problem","start":44,"end":51,"id":6},{"text":"for","start":52,"end":55,"id":7},{"text":"linear","start":56,"end":62,"id":8},{"text":"supervised","start":63,"end":73,"id":9},{"text":"learning","start":74,"end":82,"id":10},{"text":",","start":82,"end":83,"id":11},{"text":"with","start":84,"end":88,"id":12},{"text":"regularization","start":89,"end":103,"id":13},{"text":"by","start":104,"end":106,"id":14},{"text":"structured","start":107,"end":117,"id":15},{"text":"sparsity","start":118,"end":126,"id":16},{"text":"-","start":126,"end":127,"id":17},{"text":"inducing","start":127,"end":135,"id":18},{"text":"norms","start":136,"end":141,"id":19},{"text":".","start":141,"end":142,"id":20}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"To overcome both of these problems, we propose to compute the weight vectors using a low-dimensional neighborhood representation.","_input_hash":-371971599,"_task_hash":1006070794,"tokens":[{"text":"To","start":0,"end":2,"id":0},{"text":"overcome","start":3,"end":11,"id":1},{"text":"both","start":12,"end":16,"id":2},{"text":"of","start":17,"end":19,"id":3},{"text":"these","start":20,"end":25,"id":4},{"text":"problems","start":26,"end":34,"id":5},{"text":",","start":34,"end":35,"id":6},{"text":"we","start":36,"end":38,"id":7},{"text":"propose","start":39,"end":46,"id":8},{"text":"to","start":47,"end":49,"id":9},{"text":"compute","start":50,"end":57,"id":10},{"text":"the","start":58,"end":61,"id":11},{"text":"weight","start":62,"end":68,"id":12},{"text":"vectors","start":69,"end":76,"id":13},{"text":"using","start":77,"end":82,"id":14},{"text":"a","start":83,"end":84,"id":15},{"text":"low","start":85,"end":88,"id":16},{"text":"-","start":88,"end":89,"id":17},{"text":"dimensional","start":89,"end":100,"id":18},{"text":"neighborhood","start":101,"end":113,"id":19},{"text":"representation","start":114,"end":128,"id":20},{"text":".","start":128,"end":129,"id":21}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"A complementary challenge is to find codes that are invariant to irrelevant transformations of the data.","_input_hash":-1674053813,"_task_hash":1065610473,"tokens":[{"text":"A","start":0,"end":1,"id":0},{"text":"complementary","start":2,"end":15,"id":1},{"text":"challenge","start":16,"end":25,"id":2},{"text":"is","start":26,"end":28,"id":3},{"text":"to","start":29,"end":31,"id":4},{"text":"find","start":32,"end":36,"id":5},{"text":"codes","start":37,"end":42,"id":6},{"text":"that","start":43,"end":47,"id":7},{"text":"are","start":48,"end":51,"id":8},{"text":"invariant","start":52,"end":61,"id":9},{"text":"to","start":62,"end":64,"id":10},{"text":"irrelevant","start":65,"end":75,"id":11},{"text":"transformations","start":76,"end":91,"id":12},{"text":"of","start":92,"end":94,"id":13},{"text":"the","start":95,"end":98,"id":14},{"text":"data","start":99,"end":103,"id":15},{"text":".","start":103,"end":104,"id":16}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We introduce a precise notion of ``best'' and show there exist situations where two convex surrogate losses are incommensurable.","_input_hash":-1355809138,"_task_hash":713008655,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"introduce","start":3,"end":12,"id":1},{"text":"a","start":13,"end":14,"id":2},{"text":"precise","start":15,"end":22,"id":3},{"text":"notion","start":23,"end":29,"id":4},{"text":"of","start":30,"end":32,"id":5},{"text":"`","start":33,"end":34,"id":6},{"text":"`","start":34,"end":35,"id":7},{"text":"best","start":35,"end":39,"id":8},{"text":"'","start":39,"end":40,"id":9},{"text":"'","start":40,"end":41,"id":10},{"text":"and","start":42,"end":45,"id":11},{"text":"show","start":46,"end":50,"id":12},{"text":"there","start":51,"end":56,"id":13},{"text":"exist","start":57,"end":62,"id":14},{"text":"situations","start":63,"end":73,"id":15},{"text":"where","start":74,"end":79,"id":16},{"text":"two","start":80,"end":83,"id":17},{"text":"convex","start":84,"end":90,"id":18},{"text":"surrogate","start":91,"end":100,"id":19},{"text":"losses","start":101,"end":107,"id":20},{"text":"are","start":108,"end":111,"id":21},{"text":"incommensurable","start":112,"end":127,"id":22},{"text":".","start":127,"end":128,"id":23}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"In sparse Bayesian learning (SBL), Gaussian scale mixtures (GSMs) have been used to model sparsity-inducing priors that realize a class of concave penalty functions for the regression task in real-valued signal models.","_input_hash":-1051789616,"_task_hash":-642980501,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"sparse","start":3,"end":9,"id":1},{"text":"Bayesian","start":10,"end":18,"id":2},{"text":"learning","start":19,"end":27,"id":3},{"text":"(","start":28,"end":29,"id":4},{"text":"SBL","start":29,"end":32,"id":5},{"text":")","start":32,"end":33,"id":6},{"text":",","start":33,"end":34,"id":7},{"text":"Gaussian","start":35,"end":43,"id":8},{"text":"scale","start":44,"end":49,"id":9},{"text":"mixtures","start":50,"end":58,"id":10},{"text":"(","start":59,"end":60,"id":11},{"text":"GSMs","start":60,"end":64,"id":12},{"text":")","start":64,"end":65,"id":13},{"text":"have","start":66,"end":70,"id":14},{"text":"been","start":71,"end":75,"id":15},{"text":"used","start":76,"end":80,"id":16},{"text":"to","start":81,"end":83,"id":17},{"text":"model","start":84,"end":89,"id":18},{"text":"sparsity","start":90,"end":98,"id":19},{"text":"-","start":98,"end":99,"id":20},{"text":"inducing","start":99,"end":107,"id":21},{"text":"priors","start":108,"end":114,"id":22},{"text":"that","start":115,"end":119,"id":23},{"text":"realize","start":120,"end":127,"id":24},{"text":"a","start":128,"end":129,"id":25},{"text":"class","start":130,"end":135,"id":26},{"text":"of","start":136,"end":138,"id":27},{"text":"concave","start":139,"end":146,"id":28},{"text":"penalty","start":147,"end":154,"id":29},{"text":"functions","start":155,"end":164,"id":30},{"text":"for","start":165,"end":168,"id":31},{"text":"the","start":169,"end":172,"id":32},{"text":"regression","start":173,"end":183,"id":33},{"text":"task","start":184,"end":188,"id":34},{"text":"in","start":189,"end":191,"id":35},{"text":"real","start":192,"end":196,"id":36},{"text":"-","start":196,"end":197,"id":37},{"text":"valued","start":197,"end":203,"id":38},{"text":"signal","start":204,"end":210,"id":39},{"text":"models","start":211,"end":217,"id":40},{"text":".","start":217,"end":218,"id":41}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":35,"end":58,"token_start":8,"token_end":10,"label":"ALGO","answer":"accept"},{"start":192,"end":210,"token_start":36,"token_end":39,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"2].","_input_hash":-1137067089,"_task_hash":-93592937,"tokens":[{"text":"2","start":0,"end":1,"id":0},{"text":"]","start":1,"end":2,"id":1},{"text":".","start":2,"end":3,"id":2}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"This result extends to additive homoscedastic uniform noise around the subspaces (i.e., uniform distribution in a strip around them) and near recovery with an error proportional to the noise level.","_input_hash":-1900850711,"_task_hash":-1377835114,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"result","start":5,"end":11,"id":1},{"text":"extends","start":12,"end":19,"id":2},{"text":"to","start":20,"end":22,"id":3},{"text":"additive","start":23,"end":31,"id":4},{"text":"homoscedastic","start":32,"end":45,"id":5},{"text":"uniform","start":46,"end":53,"id":6},{"text":"noise","start":54,"end":59,"id":7},{"text":"around","start":60,"end":66,"id":8},{"text":"the","start":67,"end":70,"id":9},{"text":"subspaces","start":71,"end":80,"id":10},{"text":"(","start":81,"end":82,"id":11},{"text":"i.e.","start":82,"end":86,"id":12},{"text":",","start":86,"end":87,"id":13},{"text":"uniform","start":88,"end":95,"id":14},{"text":"distribution","start":96,"end":108,"id":15},{"text":"in","start":109,"end":111,"id":16},{"text":"a","start":112,"end":113,"id":17},{"text":"strip","start":114,"end":119,"id":18},{"text":"around","start":120,"end":126,"id":19},{"text":"them","start":127,"end":131,"id":20},{"text":")","start":131,"end":132,"id":21},{"text":"and","start":133,"end":136,"id":22},{"text":"near","start":137,"end":141,"id":23},{"text":"recovery","start":142,"end":150,"id":24},{"text":"with","start":151,"end":155,"id":25},{"text":"an","start":156,"end":158,"id":26},{"text":"error","start":159,"end":164,"id":27},{"text":"proportional","start":165,"end":177,"id":28},{"text":"to","start":178,"end":180,"id":29},{"text":"the","start":181,"end":184,"id":30},{"text":"noise","start":185,"end":190,"id":31},{"text":"level","start":191,"end":196,"id":32},{"text":".","start":196,"end":197,"id":33}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"Finally, we give a simple rule on how to subbag the predictor.","_input_hash":-67633537,"_task_hash":-936599844,"tokens":[{"text":"Finally","start":0,"end":7,"id":0},{"text":",","start":7,"end":8,"id":1},{"text":"we","start":9,"end":11,"id":2},{"text":"give","start":12,"end":16,"id":3},{"text":"a","start":17,"end":18,"id":4},{"text":"simple","start":19,"end":25,"id":5},{"text":"rule","start":26,"end":30,"id":6},{"text":"on","start":31,"end":33,"id":7},{"text":"how","start":34,"end":37,"id":8},{"text":"to","start":38,"end":40,"id":9},{"text":"subbag","start":41,"end":47,"id":10},{"text":"the","start":48,"end":51,"id":11},{"text":"predictor","start":52,"end":61,"id":12},{"text":".","start":61,"end":62,"id":13}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"More precisely, we show that the proximal operator associated with this norm is computable exactly via a dual approach that can be viewed as the composition of elementary proximal operators.","_input_hash":-1252767780,"_task_hash":-2041237220,"tokens":[{"text":"More","start":0,"end":4,"id":0},{"text":"precisely","start":5,"end":14,"id":1},{"text":",","start":14,"end":15,"id":2},{"text":"we","start":16,"end":18,"id":3},{"text":"show","start":19,"end":23,"id":4},{"text":"that","start":24,"end":28,"id":5},{"text":"the","start":29,"end":32,"id":6},{"text":"proximal","start":33,"end":41,"id":7},{"text":"operator","start":42,"end":50,"id":8},{"text":"associated","start":51,"end":61,"id":9},{"text":"with","start":62,"end":66,"id":10},{"text":"this","start":67,"end":71,"id":11},{"text":"norm","start":72,"end":76,"id":12},{"text":"is","start":77,"end":79,"id":13},{"text":"computable","start":80,"end":90,"id":14},{"text":"exactly","start":91,"end":98,"id":15},{"text":"via","start":99,"end":102,"id":16},{"text":"a","start":103,"end":104,"id":17},{"text":"dual","start":105,"end":109,"id":18},{"text":"approach","start":110,"end":118,"id":19},{"text":"that","start":119,"end":123,"id":20},{"text":"can","start":124,"end":127,"id":21},{"text":"be","start":128,"end":130,"id":22},{"text":"viewed","start":131,"end":137,"id":23},{"text":"as","start":138,"end":140,"id":24},{"text":"the","start":141,"end":144,"id":25},{"text":"composition","start":145,"end":156,"id":26},{"text":"of","start":157,"end":159,"id":27},{"text":"elementary","start":160,"end":170,"id":28},{"text":"proximal","start":171,"end":179,"id":29},{"text":"operators","start":180,"end":189,"id":30},{"text":".","start":189,"end":190,"id":31}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"In terms of predictive power, however, these regularized linear models are often slightly inferior to machine learning procedures like tree ensembles.","_input_hash":849111586,"_task_hash":359863619,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"terms","start":3,"end":8,"id":1},{"text":"of","start":9,"end":11,"id":2},{"text":"predictive","start":12,"end":22,"id":3},{"text":"power","start":23,"end":28,"id":4},{"text":",","start":28,"end":29,"id":5},{"text":"however","start":30,"end":37,"id":6},{"text":",","start":37,"end":38,"id":7},{"text":"these","start":39,"end":44,"id":8},{"text":"regularized","start":45,"end":56,"id":9},{"text":"linear","start":57,"end":63,"id":10},{"text":"models","start":64,"end":70,"id":11},{"text":"are","start":71,"end":74,"id":12},{"text":"often","start":75,"end":80,"id":13},{"text":"slightly","start":81,"end":89,"id":14},{"text":"inferior","start":90,"end":98,"id":15},{"text":"to","start":99,"end":101,"id":16},{"text":"machine","start":102,"end":109,"id":17},{"text":"learning","start":110,"end":118,"id":18},{"text":"procedures","start":119,"end":129,"id":19},{"text":"like","start":130,"end":134,"id":20},{"text":"tree","start":135,"end":139,"id":21},{"text":"ensembles","start":140,"end":149,"id":22},{"text":".","start":149,"end":150,"id":23}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":45,"end":63,"token_start":9,"token_end":10,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We show that the TC-RBM learns descriptive music features, such as underlying chords and typical melody transitions and dynamics.","_input_hash":1746979830,"_task_hash":-196159327,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"show","start":3,"end":7,"id":1},{"text":"that","start":8,"end":12,"id":2},{"text":"the","start":13,"end":16,"id":3},{"text":"TC","start":17,"end":19,"id":4},{"text":"-","start":19,"end":20,"id":5},{"text":"RBM","start":20,"end":23,"id":6},{"text":"learns","start":24,"end":30,"id":7},{"text":"descriptive","start":31,"end":42,"id":8},{"text":"music","start":43,"end":48,"id":9},{"text":"features","start":49,"end":57,"id":10},{"text":",","start":57,"end":58,"id":11},{"text":"such","start":59,"end":63,"id":12},{"text":"as","start":64,"end":66,"id":13},{"text":"underlying","start":67,"end":77,"id":14},{"text":"chords","start":78,"end":84,"id":15},{"text":"and","start":85,"end":88,"id":16},{"text":"typical","start":89,"end":96,"id":17},{"text":"melody","start":97,"end":103,"id":18},{"text":"transitions","start":104,"end":115,"id":19},{"text":"and","start":116,"end":119,"id":20},{"text":"dynamics","start":120,"end":128,"id":21},{"text":".","start":128,"end":129,"id":22}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"This paper reviews the Reproducing Kernel Hilbert Space structure that provides a finite-dimensional solution for a general minimization problem.","_input_hash":1503725807,"_task_hash":-427580733,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"paper","start":5,"end":10,"id":1},{"text":"reviews","start":11,"end":18,"id":2},{"text":"the","start":19,"end":22,"id":3},{"text":"Reproducing","start":23,"end":34,"id":4},{"text":"Kernel","start":35,"end":41,"id":5},{"text":"Hilbert","start":42,"end":49,"id":6},{"text":"Space","start":50,"end":55,"id":7},{"text":"structure","start":56,"end":65,"id":8},{"text":"that","start":66,"end":70,"id":9},{"text":"provides","start":71,"end":79,"id":10},{"text":"a","start":80,"end":81,"id":11},{"text":"finite","start":82,"end":88,"id":12},{"text":"-","start":88,"end":89,"id":13},{"text":"dimensional","start":89,"end":100,"id":14},{"text":"solution","start":101,"end":109,"id":15},{"text":"for","start":110,"end":113,"id":16},{"text":"a","start":114,"end":115,"id":17},{"text":"general","start":116,"end":123,"id":18},{"text":"minimization","start":124,"end":136,"id":19},{"text":"problem","start":137,"end":144,"id":20},{"text":".","start":144,"end":145,"id":21}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":35,"end":55,"token_start":5,"token_end":7,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Here we examine the case where the model parameters before and after the changepoint are independent and we derive an online algorithm for exact inference of the most recent changepoint.","_input_hash":852536986,"_task_hash":-585004257,"tokens":[{"text":"Here","start":0,"end":4,"id":0},{"text":"we","start":5,"end":7,"id":1},{"text":"examine","start":8,"end":15,"id":2},{"text":"the","start":16,"end":19,"id":3},{"text":"case","start":20,"end":24,"id":4},{"text":"where","start":25,"end":30,"id":5},{"text":"the","start":31,"end":34,"id":6},{"text":"model","start":35,"end":40,"id":7},{"text":"parameters","start":41,"end":51,"id":8},{"text":"before","start":52,"end":58,"id":9},{"text":"and","start":59,"end":62,"id":10},{"text":"after","start":63,"end":68,"id":11},{"text":"the","start":69,"end":72,"id":12},{"text":"changepoint","start":73,"end":84,"id":13},{"text":"are","start":85,"end":88,"id":14},{"text":"independent","start":89,"end":100,"id":15},{"text":"and","start":101,"end":104,"id":16},{"text":"we","start":105,"end":107,"id":17},{"text":"derive","start":108,"end":114,"id":18},{"text":"an","start":115,"end":117,"id":19},{"text":"online","start":118,"end":124,"id":20},{"text":"algorithm","start":125,"end":134,"id":21},{"text":"for","start":135,"end":138,"id":22},{"text":"exact","start":139,"end":144,"id":23},{"text":"inference","start":145,"end":154,"id":24},{"text":"of","start":155,"end":157,"id":25},{"text":"the","start":158,"end":161,"id":26},{"text":"most","start":162,"end":166,"id":27},{"text":"recent","start":167,"end":173,"id":28},{"text":"changepoint","start":174,"end":185,"id":29},{"text":".","start":185,"end":186,"id":30}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":118,"end":124,"token_start":20,"token_end":20,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We propose two extensions to the basic i.i.d.","_input_hash":449260399,"_task_hash":1618475934,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"propose","start":3,"end":10,"id":1},{"text":"two","start":11,"end":14,"id":2},{"text":"extensions","start":15,"end":25,"id":3},{"text":"to","start":26,"end":28,"id":4},{"text":"the","start":29,"end":32,"id":5},{"text":"basic","start":33,"end":38,"id":6},{"text":"i.i.d","start":39,"end":44,"id":7},{"text":".","start":44,"end":45,"id":8}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"If the bistatic systems are to replace the monostatic systems (at least par- tially), then all the existing usages of a monostatic system should be manageable in a bistatic system.","_input_hash":969981875,"_task_hash":2067844867,"tokens":[{"text":"If","start":0,"end":2,"id":0},{"text":"the","start":3,"end":6,"id":1},{"text":"bistatic","start":7,"end":15,"id":2},{"text":"systems","start":16,"end":23,"id":3},{"text":"are","start":24,"end":27,"id":4},{"text":"to","start":28,"end":30,"id":5},{"text":"replace","start":31,"end":38,"id":6},{"text":"the","start":39,"end":42,"id":7},{"text":"monostatic","start":43,"end":53,"id":8},{"text":"systems","start":54,"end":61,"id":9},{"text":"(","start":62,"end":63,"id":10},{"text":"at","start":63,"end":65,"id":11},{"text":"least","start":66,"end":71,"id":12},{"text":"par-","start":72,"end":76,"id":13},{"text":"tially","start":77,"end":83,"id":14},{"text":")","start":83,"end":84,"id":15},{"text":",","start":84,"end":85,"id":16},{"text":"then","start":86,"end":90,"id":17},{"text":"all","start":91,"end":94,"id":18},{"text":"the","start":95,"end":98,"id":19},{"text":"existing","start":99,"end":107,"id":20},{"text":"usages","start":108,"end":114,"id":21},{"text":"of","start":115,"end":117,"id":22},{"text":"a","start":118,"end":119,"id":23},{"text":"monostatic","start":120,"end":130,"id":24},{"text":"system","start":131,"end":137,"id":25},{"text":"should","start":138,"end":144,"id":26},{"text":"be","start":145,"end":147,"id":27},{"text":"manageable","start":148,"end":158,"id":28},{"text":"in","start":159,"end":161,"id":29},{"text":"a","start":162,"end":163,"id":30},{"text":"bistatic","start":164,"end":172,"id":31},{"text":"system","start":173,"end":179,"id":32},{"text":".","start":179,"end":180,"id":33}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We derive a method for estimating the nonparanormal, study the method's theoretical properties, and show that it works well in many examples.","_input_hash":639694872,"_task_hash":1466242928,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"derive","start":3,"end":9,"id":1},{"text":"a","start":10,"end":11,"id":2},{"text":"method","start":12,"end":18,"id":3},{"text":"for","start":19,"end":22,"id":4},{"text":"estimating","start":23,"end":33,"id":5},{"text":"the","start":34,"end":37,"id":6},{"text":"nonparanormal","start":38,"end":51,"id":7},{"text":",","start":51,"end":52,"id":8},{"text":"study","start":53,"end":58,"id":9},{"text":"the","start":59,"end":62,"id":10},{"text":"method","start":63,"end":69,"id":11},{"text":"'s","start":69,"end":71,"id":12},{"text":"theoretical","start":72,"end":83,"id":13},{"text":"properties","start":84,"end":94,"id":14},{"text":",","start":94,"end":95,"id":15},{"text":"and","start":96,"end":99,"id":16},{"text":"show","start":100,"end":104,"id":17},{"text":"that","start":105,"end":109,"id":18},{"text":"it","start":110,"end":112,"id":19},{"text":"works","start":113,"end":118,"id":20},{"text":"well","start":119,"end":123,"id":21},{"text":"in","start":124,"end":126,"id":22},{"text":"many","start":127,"end":131,"id":23},{"text":"examples","start":132,"end":140,"id":24},{"text":".","start":140,"end":141,"id":25}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":10,"end":11,"token_start":2,"token_end":2,"label":"ALGO","answer":"reject"},{"start":53,"end":58,"token_start":9,"token_end":9,"label":"ALGO","answer":"reject"}],"answer":"reject"}
{"text":"We study the problem of allocating stocks to dark pools.","_input_hash":8280324,"_task_hash":-1197115459,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"study","start":3,"end":8,"id":1},{"text":"the","start":9,"end":12,"id":2},{"text":"problem","start":13,"end":20,"id":3},{"text":"of","start":21,"end":23,"id":4},{"text":"allocating","start":24,"end":34,"id":5},{"text":"stocks","start":35,"end":41,"id":6},{"text":"to","start":42,"end":44,"id":7},{"text":"dark","start":45,"end":49,"id":8},{"text":"pools","start":50,"end":55,"id":9},{"text":".","start":55,"end":56,"id":10}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"As a demonstration of the benefits of the framework we derive an inference procedure for a state-space model over permutations.","_input_hash":-2037131638,"_task_hash":322361802,"tokens":[{"text":"As","start":0,"end":2,"id":0},{"text":"a","start":3,"end":4,"id":1},{"text":"demonstration","start":5,"end":18,"id":2},{"text":"of","start":19,"end":21,"id":3},{"text":"the","start":22,"end":25,"id":4},{"text":"benefits","start":26,"end":34,"id":5},{"text":"of","start":35,"end":37,"id":6},{"text":"the","start":38,"end":41,"id":7},{"text":"framework","start":42,"end":51,"id":8},{"text":"we","start":52,"end":54,"id":9},{"text":"derive","start":55,"end":61,"id":10},{"text":"an","start":62,"end":64,"id":11},{"text":"inference","start":65,"end":74,"id":12},{"text":"procedure","start":75,"end":84,"id":13},{"text":"for","start":85,"end":88,"id":14},{"text":"a","start":89,"end":90,"id":15},{"text":"state","start":91,"end":96,"id":16},{"text":"-","start":96,"end":97,"id":17},{"text":"space","start":97,"end":102,"id":18},{"text":"model","start":103,"end":108,"id":19},{"text":"over","start":109,"end":113,"id":20},{"text":"permutations","start":114,"end":126,"id":21},{"text":".","start":126,"end":127,"id":22}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":91,"end":102,"token_start":16,"token_end":18,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Deep belief networks are a powerful way to model complex probability distributions.","_input_hash":1007910041,"_task_hash":825608607,"tokens":[{"text":"Deep","start":0,"end":4,"id":0},{"text":"belief","start":5,"end":11,"id":1},{"text":"networks","start":12,"end":20,"id":2},{"text":"are","start":21,"end":24,"id":3},{"text":"a","start":25,"end":26,"id":4},{"text":"powerful","start":27,"end":35,"id":5},{"text":"way","start":36,"end":39,"id":6},{"text":"to","start":40,"end":42,"id":7},{"text":"model","start":43,"end":48,"id":8},{"text":"complex","start":49,"end":56,"id":9},{"text":"probability","start":57,"end":68,"id":10},{"text":"distributions","start":69,"end":82,"id":11},{"text":".","start":82,"end":83,"id":12}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":0,"end":20,"token_start":0,"token_end":2,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"1) finite sample bounds on prediction and estimation, and (2) asymptotic distribution and selection.","_input_hash":-472414264,"_task_hash":-1396763582,"tokens":[{"text":"1","start":0,"end":1,"id":0},{"text":")","start":1,"end":2,"id":1},{"text":"finite","start":3,"end":9,"id":2},{"text":"sample","start":10,"end":16,"id":3},{"text":"bounds","start":17,"end":23,"id":4},{"text":"on","start":24,"end":26,"id":5},{"text":"prediction","start":27,"end":37,"id":6},{"text":"and","start":38,"end":41,"id":7},{"text":"estimation","start":42,"end":52,"id":8},{"text":",","start":52,"end":53,"id":9},{"text":"and","start":54,"end":57,"id":10},{"text":"(","start":58,"end":59,"id":11},{"text":"2","start":59,"end":60,"id":12},{"text":")","start":60,"end":61,"id":13},{"text":"asymptotic","start":62,"end":72,"id":14},{"text":"distribution","start":73,"end":85,"id":15},{"text":"and","start":86,"end":89,"id":16},{"text":"selection","start":90,"end":99,"id":17},{"text":".","start":99,"end":100,"id":18}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Application examples are given for hidden model reconstruction, cellular automata filtering, and edge detection in images.","_input_hash":-23107903,"_task_hash":-1548598598,"tokens":[{"text":"Application","start":0,"end":11,"id":0},{"text":"examples","start":12,"end":20,"id":1},{"text":"are","start":21,"end":24,"id":2},{"text":"given","start":25,"end":30,"id":3},{"text":"for","start":31,"end":34,"id":4},{"text":"hidden","start":35,"end":41,"id":5},{"text":"model","start":42,"end":47,"id":6},{"text":"reconstruction","start":48,"end":62,"id":7},{"text":",","start":62,"end":63,"id":8},{"text":"cellular","start":64,"end":72,"id":9},{"text":"automata","start":73,"end":81,"id":10},{"text":"filtering","start":82,"end":91,"id":11},{"text":",","start":91,"end":92,"id":12},{"text":"and","start":93,"end":96,"id":13},{"text":"edge","start":97,"end":101,"id":14},{"text":"detection","start":102,"end":111,"id":15},{"text":"in","start":112,"end":114,"id":16},{"text":"images","start":115,"end":121,"id":17},{"text":".","start":121,"end":122,"id":18}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Photo album summarization is a typical application of CDOM, where photos are automatically aligned into a designed frame expressed in the Cartesian coordinate system.","_input_hash":1441536278,"_task_hash":-711176426,"tokens":[{"text":"Photo","start":0,"end":5,"id":0},{"text":"album","start":6,"end":11,"id":1},{"text":"summarization","start":12,"end":25,"id":2},{"text":"is","start":26,"end":28,"id":3},{"text":"a","start":29,"end":30,"id":4},{"text":"typical","start":31,"end":38,"id":5},{"text":"application","start":39,"end":50,"id":6},{"text":"of","start":51,"end":53,"id":7},{"text":"CDOM","start":54,"end":58,"id":8},{"text":",","start":58,"end":59,"id":9},{"text":"where","start":60,"end":65,"id":10},{"text":"photos","start":66,"end":72,"id":11},{"text":"are","start":73,"end":76,"id":12},{"text":"automatically","start":77,"end":90,"id":13},{"text":"aligned","start":91,"end":98,"id":14},{"text":"into","start":99,"end":103,"id":15},{"text":"a","start":104,"end":105,"id":16},{"text":"designed","start":106,"end":114,"id":17},{"text":"frame","start":115,"end":120,"id":18},{"text":"expressed","start":121,"end":130,"id":19},{"text":"in","start":131,"end":133,"id":20},{"text":"the","start":134,"end":137,"id":21},{"text":"Cartesian","start":138,"end":147,"id":22},{"text":"coordinate","start":148,"end":158,"id":23},{"text":"system","start":159,"end":165,"id":24},{"text":".","start":165,"end":166,"id":25}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"The second case is when the mean value is constant, but the variance can change.","_input_hash":1355478396,"_task_hash":-437160721,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"second","start":4,"end":10,"id":1},{"text":"case","start":11,"end":15,"id":2},{"text":"is","start":16,"end":18,"id":3},{"text":"when","start":19,"end":23,"id":4},{"text":"the","start":24,"end":27,"id":5},{"text":"mean","start":28,"end":32,"id":6},{"text":"value","start":33,"end":38,"id":7},{"text":"is","start":39,"end":41,"id":8},{"text":"constant","start":42,"end":50,"id":9},{"text":",","start":50,"end":51,"id":10},{"text":"but","start":52,"end":55,"id":11},{"text":"the","start":56,"end":59,"id":12},{"text":"variance","start":60,"end":68,"id":13},{"text":"can","start":69,"end":72,"id":14},{"text":"change","start":73,"end":79,"id":15},{"text":".","start":79,"end":80,"id":16}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"This paper studies the deviations of the regret in a stochastic multi-armed bandit problem.","_input_hash":1055947630,"_task_hash":271284137,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"paper","start":5,"end":10,"id":1},{"text":"studies","start":11,"end":18,"id":2},{"text":"the","start":19,"end":22,"id":3},{"text":"deviations","start":23,"end":33,"id":4},{"text":"of","start":34,"end":36,"id":5},{"text":"the","start":37,"end":40,"id":6},{"text":"regret","start":41,"end":47,"id":7},{"text":"in","start":48,"end":50,"id":8},{"text":"a","start":51,"end":52,"id":9},{"text":"stochastic","start":53,"end":63,"id":10},{"text":"multi","start":64,"end":69,"id":11},{"text":"-","start":69,"end":70,"id":12},{"text":"armed","start":70,"end":75,"id":13},{"text":"bandit","start":76,"end":82,"id":14},{"text":"problem","start":83,"end":90,"id":15},{"text":".","start":90,"end":91,"id":16}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"We show how to associate to any posterior distribution an effective temperature relating it to the Gibbs prior distribution with the same level of expected error rate, and how to estimate this effective temperature from data, resulting in an estimator whose expected error rate converges according to the best possible power of the sample size adaptively under any margin and parametric complexity assumptions.","_input_hash":1038190572,"_task_hash":655357634,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"show","start":3,"end":7,"id":1},{"text":"how","start":8,"end":11,"id":2},{"text":"to","start":12,"end":14,"id":3},{"text":"associate","start":15,"end":24,"id":4},{"text":"to","start":25,"end":27,"id":5},{"text":"any","start":28,"end":31,"id":6},{"text":"posterior","start":32,"end":41,"id":7},{"text":"distribution","start":42,"end":54,"id":8},{"text":"an","start":55,"end":57,"id":9},{"text":"effective","start":58,"end":67,"id":10},{"text":"temperature","start":68,"end":79,"id":11},{"text":"relating","start":80,"end":88,"id":12},{"text":"it","start":89,"end":91,"id":13},{"text":"to","start":92,"end":94,"id":14},{"text":"the","start":95,"end":98,"id":15},{"text":"Gibbs","start":99,"end":104,"id":16},{"text":"prior","start":105,"end":110,"id":17},{"text":"distribution","start":111,"end":123,"id":18},{"text":"with","start":124,"end":128,"id":19},{"text":"the","start":129,"end":132,"id":20},{"text":"same","start":133,"end":137,"id":21},{"text":"level","start":138,"end":143,"id":22},{"text":"of","start":144,"end":146,"id":23},{"text":"expected","start":147,"end":155,"id":24},{"text":"error","start":156,"end":161,"id":25},{"text":"rate","start":162,"end":166,"id":26},{"text":",","start":166,"end":167,"id":27},{"text":"and","start":168,"end":171,"id":28},{"text":"how","start":172,"end":175,"id":29},{"text":"to","start":176,"end":178,"id":30},{"text":"estimate","start":179,"end":187,"id":31},{"text":"this","start":188,"end":192,"id":32},{"text":"effective","start":193,"end":202,"id":33},{"text":"temperature","start":203,"end":214,"id":34},{"text":"from","start":215,"end":219,"id":35},{"text":"data","start":220,"end":224,"id":36},{"text":",","start":224,"end":225,"id":37},{"text":"resulting","start":226,"end":235,"id":38},{"text":"in","start":236,"end":238,"id":39},{"text":"an","start":239,"end":241,"id":40},{"text":"estimator","start":242,"end":251,"id":41},{"text":"whose","start":252,"end":257,"id":42},{"text":"expected","start":258,"end":266,"id":43},{"text":"error","start":267,"end":272,"id":44},{"text":"rate","start":273,"end":277,"id":45},{"text":"converges","start":278,"end":287,"id":46},{"text":"according","start":288,"end":297,"id":47},{"text":"to","start":298,"end":300,"id":48},{"text":"the","start":301,"end":304,"id":49},{"text":"best","start":305,"end":309,"id":50},{"text":"possible","start":310,"end":318,"id":51},{"text":"power","start":319,"end":324,"id":52},{"text":"of","start":325,"end":327,"id":53},{"text":"the","start":328,"end":331,"id":54},{"text":"sample","start":332,"end":338,"id":55},{"text":"size","start":339,"end":343,"id":56},{"text":"adaptively","start":344,"end":354,"id":57},{"text":"under","start":355,"end":360,"id":58},{"text":"any","start":361,"end":364,"id":59},{"text":"margin","start":365,"end":371,"id":60},{"text":"and","start":372,"end":375,"id":61},{"text":"parametric","start":376,"end":386,"id":62},{"text":"complexity","start":387,"end":397,"id":63},{"text":"assumptions","start":398,"end":409,"id":64},{"text":".","start":409,"end":410,"id":65}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"In this paper, we present a new method to jointly estimate multiple LiNGAMs under the assumption that the models share a causal ordering but may have different connection strengths and differently distributed variables.","_input_hash":-1141292986,"_task_hash":1514226206,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"this","start":3,"end":7,"id":1},{"text":"paper","start":8,"end":13,"id":2},{"text":",","start":13,"end":14,"id":3},{"text":"we","start":15,"end":17,"id":4},{"text":"present","start":18,"end":25,"id":5},{"text":"a","start":26,"end":27,"id":6},{"text":"new","start":28,"end":31,"id":7},{"text":"method","start":32,"end":38,"id":8},{"text":"to","start":39,"end":41,"id":9},{"text":"jointly","start":42,"end":49,"id":10},{"text":"estimate","start":50,"end":58,"id":11},{"text":"multiple","start":59,"end":67,"id":12},{"text":"LiNGAMs","start":68,"end":75,"id":13},{"text":"under","start":76,"end":81,"id":14},{"text":"the","start":82,"end":85,"id":15},{"text":"assumption","start":86,"end":96,"id":16},{"text":"that","start":97,"end":101,"id":17},{"text":"the","start":102,"end":105,"id":18},{"text":"models","start":106,"end":112,"id":19},{"text":"share","start":113,"end":118,"id":20},{"text":"a","start":119,"end":120,"id":21},{"text":"causal","start":121,"end":127,"id":22},{"text":"ordering","start":128,"end":136,"id":23},{"text":"but","start":137,"end":140,"id":24},{"text":"may","start":141,"end":144,"id":25},{"text":"have","start":145,"end":149,"id":26},{"text":"different","start":150,"end":159,"id":27},{"text":"connection","start":160,"end":170,"id":28},{"text":"strengths","start":171,"end":180,"id":29},{"text":"and","start":181,"end":184,"id":30},{"text":"differently","start":185,"end":196,"id":31},{"text":"distributed","start":197,"end":208,"id":32},{"text":"variables","start":209,"end":218,"id":33},{"text":".","start":218,"end":219,"id":34}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":68,"end":75,"token_start":13,"token_end":13,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Instead of minimizing an objective function involving the estimated cumulants, we show that by treating the cumulants as elements of the polynomial ring we can directly solve the problem, at a lower computational cost and with higher accuracy.","_input_hash":-1833178863,"_task_hash":-1301673129,"tokens":[{"text":"Instead","start":0,"end":7,"id":0},{"text":"of","start":8,"end":10,"id":1},{"text":"minimizing","start":11,"end":21,"id":2},{"text":"an","start":22,"end":24,"id":3},{"text":"objective","start":25,"end":34,"id":4},{"text":"function","start":35,"end":43,"id":5},{"text":"involving","start":44,"end":53,"id":6},{"text":"the","start":54,"end":57,"id":7},{"text":"estimated","start":58,"end":67,"id":8},{"text":"cumulants","start":68,"end":77,"id":9},{"text":",","start":77,"end":78,"id":10},{"text":"we","start":79,"end":81,"id":11},{"text":"show","start":82,"end":86,"id":12},{"text":"that","start":87,"end":91,"id":13},{"text":"by","start":92,"end":94,"id":14},{"text":"treating","start":95,"end":103,"id":15},{"text":"the","start":104,"end":107,"id":16},{"text":"cumulants","start":108,"end":117,"id":17},{"text":"as","start":118,"end":120,"id":18},{"text":"elements","start":121,"end":129,"id":19},{"text":"of","start":130,"end":132,"id":20},{"text":"the","start":133,"end":136,"id":21},{"text":"polynomial","start":137,"end":147,"id":22},{"text":"ring","start":148,"end":152,"id":23},{"text":"we","start":153,"end":155,"id":24},{"text":"can","start":156,"end":159,"id":25},{"text":"directly","start":160,"end":168,"id":26},{"text":"solve","start":169,"end":174,"id":27},{"text":"the","start":175,"end":178,"id":28},{"text":"problem","start":179,"end":186,"id":29},{"text":",","start":186,"end":187,"id":30},{"text":"at","start":188,"end":190,"id":31},{"text":"a","start":191,"end":192,"id":32},{"text":"lower","start":193,"end":198,"id":33},{"text":"computational","start":199,"end":212,"id":34},{"text":"cost","start":213,"end":217,"id":35},{"text":"and","start":218,"end":221,"id":36},{"text":"with","start":222,"end":226,"id":37},{"text":"higher","start":227,"end":233,"id":38},{"text":"accuracy","start":234,"end":242,"id":39},{"text":".","start":242,"end":243,"id":40}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"Multi-output regression;","_input_hash":1660835733,"_task_hash":-1789748449,"tokens":[{"text":"Multi","start":0,"end":5,"id":0},{"text":"-","start":5,"end":6,"id":1},{"text":"output","start":6,"end":12,"id":2},{"text":"regression","start":13,"end":23,"id":3},{"text":";","start":23,"end":24,"id":4}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":0,"end":23,"token_start":0,"token_end":3,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"This problem, often referred to as \\emph{PU learning}, differs from the standard supervised classification problem by the lack of negative examples in the training set.","_input_hash":-1905433351,"_task_hash":1673544025,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"problem","start":5,"end":12,"id":1},{"text":",","start":12,"end":13,"id":2},{"text":"often","start":14,"end":19,"id":3},{"text":"referred","start":20,"end":28,"id":4},{"text":"to","start":29,"end":31,"id":5},{"text":"as","start":32,"end":34,"id":6},{"text":"\\emph{PU","start":35,"end":43,"id":7},{"text":"learning","start":44,"end":52,"id":8},{"text":"}","start":52,"end":53,"id":9},{"text":",","start":53,"end":54,"id":10},{"text":"differs","start":55,"end":62,"id":11},{"text":"from","start":63,"end":67,"id":12},{"text":"the","start":68,"end":71,"id":13},{"text":"standard","start":72,"end":80,"id":14},{"text":"supervised","start":81,"end":91,"id":15},{"text":"classification","start":92,"end":106,"id":16},{"text":"problem","start":107,"end":114,"id":17},{"text":"by","start":115,"end":117,"id":18},{"text":"the","start":118,"end":121,"id":19},{"text":"lack","start":122,"end":126,"id":20},{"text":"of","start":127,"end":129,"id":21},{"text":"negative","start":130,"end":138,"id":22},{"text":"examples","start":139,"end":147,"id":23},{"text":"in","start":148,"end":150,"id":24},{"text":"the","start":151,"end":154,"id":25},{"text":"training","start":155,"end":163,"id":26},{"text":"set","start":164,"end":167,"id":27},{"text":".","start":167,"end":168,"id":28}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"can be reliably estimated;","_input_hash":1669662334,"_task_hash":605875294,"tokens":[{"text":"can","start":0,"end":3,"id":0},{"text":"be","start":4,"end":6,"id":1},{"text":"reliably","start":7,"end":15,"id":2},{"text":"estimated","start":16,"end":25,"id":3},{"text":";","start":25,"end":26,"id":4}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"In this paper, we propose a novel distribution-free SDR method called sufficient component analysis (SCA), which is computationally more efficient than existing methods.","_input_hash":1833305093,"_task_hash":-1024996848,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"this","start":3,"end":7,"id":1},{"text":"paper","start":8,"end":13,"id":2},{"text":",","start":13,"end":14,"id":3},{"text":"we","start":15,"end":17,"id":4},{"text":"propose","start":18,"end":25,"id":5},{"text":"a","start":26,"end":27,"id":6},{"text":"novel","start":28,"end":33,"id":7},{"text":"distribution","start":34,"end":46,"id":8},{"text":"-","start":46,"end":47,"id":9},{"text":"free","start":47,"end":51,"id":10},{"text":"SDR","start":52,"end":55,"id":11},{"text":"method","start":56,"end":62,"id":12},{"text":"called","start":63,"end":69,"id":13},{"text":"sufficient","start":70,"end":80,"id":14},{"text":"component","start":81,"end":90,"id":15},{"text":"analysis","start":91,"end":99,"id":16},{"text":"(","start":100,"end":101,"id":17},{"text":"SCA","start":101,"end":104,"id":18},{"text":")","start":104,"end":105,"id":19},{"text":",","start":105,"end":106,"id":20},{"text":"which","start":107,"end":112,"id":21},{"text":"is","start":113,"end":115,"id":22},{"text":"computationally","start":116,"end":131,"id":23},{"text":"more","start":132,"end":136,"id":24},{"text":"efficient","start":137,"end":146,"id":25},{"text":"than","start":147,"end":151,"id":26},{"text":"existing","start":152,"end":160,"id":27},{"text":"methods","start":161,"end":168,"id":28},{"text":".","start":168,"end":169,"id":29}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":70,"end":99,"token_start":14,"token_end":16,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Sparse coding consists in representing signals as sparse linear combinations of atoms selected from a dictionary.","_input_hash":689515512,"_task_hash":-1160202229,"tokens":[{"text":"Sparse","start":0,"end":6,"id":0},{"text":"coding","start":7,"end":13,"id":1},{"text":"consists","start":14,"end":22,"id":2},{"text":"in","start":23,"end":25,"id":3},{"text":"representing","start":26,"end":38,"id":4},{"text":"signals","start":39,"end":46,"id":5},{"text":"as","start":47,"end":49,"id":6},{"text":"sparse","start":50,"end":56,"id":7},{"text":"linear","start":57,"end":63,"id":8},{"text":"combinations","start":64,"end":76,"id":9},{"text":"of","start":77,"end":79,"id":10},{"text":"atoms","start":80,"end":85,"id":11},{"text":"selected","start":86,"end":94,"id":12},{"text":"from","start":95,"end":99,"id":13},{"text":"a","start":100,"end":101,"id":14},{"text":"dictionary","start":102,"end":112,"id":15},{"text":".","start":112,"end":113,"id":16}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":102,"end":112,"token_start":15,"token_end":15,"label":"ALGO","answer":"reject"}],"answer":"reject"}
{"text":"We provide polynomial time projections between the continuous hypersphere representation and the $n!$-element permutation space.","_input_hash":-1651165132,"_task_hash":944683808,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"provide","start":3,"end":10,"id":1},{"text":"polynomial","start":11,"end":21,"id":2},{"text":"time","start":22,"end":26,"id":3},{"text":"projections","start":27,"end":38,"id":4},{"text":"between","start":39,"end":46,"id":5},{"text":"the","start":47,"end":50,"id":6},{"text":"continuous","start":51,"end":61,"id":7},{"text":"hypersphere","start":62,"end":73,"id":8},{"text":"representation","start":74,"end":88,"id":9},{"text":"and","start":89,"end":92,"id":10},{"text":"the","start":93,"end":96,"id":11},{"text":"$","start":97,"end":98,"id":12},{"text":"n!$-element","start":98,"end":109,"id":13},{"text":"permutation","start":110,"end":121,"id":14},{"text":"space","start":122,"end":127,"id":15},{"text":".","start":127,"end":128,"id":16}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Numerical results show the superiority of the proposed estimator over these state-of-the-art estimators in terms of convergence speed, sparseness, reconstruction error, and robustness in low and medium signal-to-noise ratio regimes.","_input_hash":776948016,"_task_hash":1248297896,"tokens":[{"text":"Numerical","start":0,"end":9,"id":0},{"text":"results","start":10,"end":17,"id":1},{"text":"show","start":18,"end":22,"id":2},{"text":"the","start":23,"end":26,"id":3},{"text":"superiority","start":27,"end":38,"id":4},{"text":"of","start":39,"end":41,"id":5},{"text":"the","start":42,"end":45,"id":6},{"text":"proposed","start":46,"end":54,"id":7},{"text":"estimator","start":55,"end":64,"id":8},{"text":"over","start":65,"end":69,"id":9},{"text":"these","start":70,"end":75,"id":10},{"text":"state","start":76,"end":81,"id":11},{"text":"-","start":81,"end":82,"id":12},{"text":"of","start":82,"end":84,"id":13},{"text":"-","start":84,"end":85,"id":14},{"text":"the","start":85,"end":88,"id":15},{"text":"-","start":88,"end":89,"id":16},{"text":"art","start":89,"end":92,"id":17},{"text":"estimators","start":93,"end":103,"id":18},{"text":"in","start":104,"end":106,"id":19},{"text":"terms","start":107,"end":112,"id":20},{"text":"of","start":113,"end":115,"id":21},{"text":"convergence","start":116,"end":127,"id":22},{"text":"speed","start":128,"end":133,"id":23},{"text":",","start":133,"end":134,"id":24},{"text":"sparseness","start":135,"end":145,"id":25},{"text":",","start":145,"end":146,"id":26},{"text":"reconstruction","start":147,"end":161,"id":27},{"text":"error","start":162,"end":167,"id":28},{"text":",","start":167,"end":168,"id":29},{"text":"and","start":169,"end":172,"id":30},{"text":"robustness","start":173,"end":183,"id":31},{"text":"in","start":184,"end":186,"id":32},{"text":"low","start":187,"end":190,"id":33},{"text":"and","start":191,"end":194,"id":34},{"text":"medium","start":195,"end":201,"id":35},{"text":"signal","start":202,"end":208,"id":36},{"text":"-","start":208,"end":209,"id":37},{"text":"to","start":209,"end":211,"id":38},{"text":"-","start":211,"end":212,"id":39},{"text":"noise","start":212,"end":217,"id":40},{"text":"ratio","start":218,"end":223,"id":41},{"text":"regimes","start":224,"end":231,"id":42},{"text":".","start":231,"end":232,"id":43}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"In these applications, the variance of the noise scales linearly with the expectation of the observation.","_input_hash":2118010599,"_task_hash":1261307744,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"these","start":3,"end":8,"id":1},{"text":"applications","start":9,"end":21,"id":2},{"text":",","start":21,"end":22,"id":3},{"text":"the","start":23,"end":26,"id":4},{"text":"variance","start":27,"end":35,"id":5},{"text":"of","start":36,"end":38,"id":6},{"text":"the","start":39,"end":42,"id":7},{"text":"noise","start":43,"end":48,"id":8},{"text":"scales","start":49,"end":55,"id":9},{"text":"linearly","start":56,"end":64,"id":10},{"text":"with","start":65,"end":69,"id":11},{"text":"the","start":70,"end":73,"id":12},{"text":"expectation","start":74,"end":85,"id":13},{"text":"of","start":86,"end":88,"id":14},{"text":"the","start":89,"end":92,"id":15},{"text":"observation","start":93,"end":104,"id":16},{"text":".","start":104,"end":105,"id":17}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"the number of variables and n:","_input_hash":-1356592422,"_task_hash":1522437034,"tokens":[{"text":"the","start":0,"end":3,"id":0},{"text":"number","start":4,"end":10,"id":1},{"text":"of","start":11,"end":13,"id":2},{"text":"variables","start":14,"end":23,"id":3},{"text":"and","start":24,"end":27,"id":4},{"text":"n","start":28,"end":29,"id":5},{"text":":","start":29,"end":30,"id":6}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"This has been motivated partly by frameworks like multitask learning, multisensor networks or structured output data.","_input_hash":1009388369,"_task_hash":2124680217,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"has","start":5,"end":8,"id":1},{"text":"been","start":9,"end":13,"id":2},{"text":"motivated","start":14,"end":23,"id":3},{"text":"partly","start":24,"end":30,"id":4},{"text":"by","start":31,"end":33,"id":5},{"text":"frameworks","start":34,"end":44,"id":6},{"text":"like","start":45,"end":49,"id":7},{"text":"multitask","start":50,"end":59,"id":8},{"text":"learning","start":60,"end":68,"id":9},{"text":",","start":68,"end":69,"id":10},{"text":"multisensor","start":70,"end":81,"id":11},{"text":"networks","start":82,"end":90,"id":12},{"text":"or","start":91,"end":93,"id":13},{"text":"structured","start":94,"end":104,"id":14},{"text":"output","start":105,"end":111,"id":15},{"text":"data","start":112,"end":116,"id":16},{"text":".","start":116,"end":117,"id":17}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"Expectation propagation (EP) can be used for approximate inference.","_input_hash":-997774219,"_task_hash":-1162801788,"tokens":[{"text":"Expectation","start":0,"end":11,"id":0},{"text":"propagation","start":12,"end":23,"id":1},{"text":"(","start":24,"end":25,"id":2},{"text":"EP","start":25,"end":27,"id":3},{"text":")","start":27,"end":28,"id":4},{"text":"can","start":29,"end":32,"id":5},{"text":"be","start":33,"end":35,"id":6},{"text":"used","start":36,"end":40,"id":7},{"text":"for","start":41,"end":44,"id":8},{"text":"approximate","start":45,"end":56,"id":9},{"text":"inference","start":57,"end":66,"id":10},{"text":".","start":66,"end":67,"id":11}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":0,"end":23,"token_start":0,"token_end":1,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Viewing the tree size as a complexity parameter, we then select a forest using data splitting, and prove bounds on excess risk and structure selection consistency of the procedure.","_input_hash":-1610309224,"_task_hash":515018457,"tokens":[{"text":"Viewing","start":0,"end":7,"id":0},{"text":"the","start":8,"end":11,"id":1},{"text":"tree","start":12,"end":16,"id":2},{"text":"size","start":17,"end":21,"id":3},{"text":"as","start":22,"end":24,"id":4},{"text":"a","start":25,"end":26,"id":5},{"text":"complexity","start":27,"end":37,"id":6},{"text":"parameter","start":38,"end":47,"id":7},{"text":",","start":47,"end":48,"id":8},{"text":"we","start":49,"end":51,"id":9},{"text":"then","start":52,"end":56,"id":10},{"text":"select","start":57,"end":63,"id":11},{"text":"a","start":64,"end":65,"id":12},{"text":"forest","start":66,"end":72,"id":13},{"text":"using","start":73,"end":78,"id":14},{"text":"data","start":79,"end":83,"id":15},{"text":"splitting","start":84,"end":93,"id":16},{"text":",","start":93,"end":94,"id":17},{"text":"and","start":95,"end":98,"id":18},{"text":"prove","start":99,"end":104,"id":19},{"text":"bounds","start":105,"end":111,"id":20},{"text":"on","start":112,"end":114,"id":21},{"text":"excess","start":115,"end":121,"id":22},{"text":"risk","start":122,"end":126,"id":23},{"text":"and","start":127,"end":130,"id":24},{"text":"structure","start":131,"end":140,"id":25},{"text":"selection","start":141,"end":150,"id":26},{"text":"consistency","start":151,"end":162,"id":27},{"text":"of","start":163,"end":165,"id":28},{"text":"the","start":166,"end":169,"id":29},{"text":"procedure","start":170,"end":179,"id":30},{"text":".","start":179,"end":180,"id":31}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"When using the K-nearest neighbors method, one often ignores uncertainty in the choice of K. To account for such uncertainty, Holmes and Adams (2002) proposed a Bayesian framework for K-nearest neighbors (KNN).","_input_hash":434432237,"_task_hash":-133365338,"tokens":[{"text":"When","start":0,"end":4,"id":0},{"text":"using","start":5,"end":10,"id":1},{"text":"the","start":11,"end":14,"id":2},{"text":"K","start":15,"end":16,"id":3},{"text":"-","start":16,"end":17,"id":4},{"text":"nearest","start":17,"end":24,"id":5},{"text":"neighbors","start":25,"end":34,"id":6},{"text":"method","start":35,"end":41,"id":7},{"text":",","start":41,"end":42,"id":8},{"text":"one","start":43,"end":46,"id":9},{"text":"often","start":47,"end":52,"id":10},{"text":"ignores","start":53,"end":60,"id":11},{"text":"uncertainty","start":61,"end":72,"id":12},{"text":"in","start":73,"end":75,"id":13},{"text":"the","start":76,"end":79,"id":14},{"text":"choice","start":80,"end":86,"id":15},{"text":"of","start":87,"end":89,"id":16},{"text":"K.","start":90,"end":92,"id":17},{"text":"To","start":93,"end":95,"id":18},{"text":"account","start":96,"end":103,"id":19},{"text":"for","start":104,"end":107,"id":20},{"text":"such","start":108,"end":112,"id":21},{"text":"uncertainty","start":113,"end":124,"id":22},{"text":",","start":124,"end":125,"id":23},{"text":"Holmes","start":126,"end":132,"id":24},{"text":"and","start":133,"end":136,"id":25},{"text":"Adams","start":137,"end":142,"id":26},{"text":"(","start":143,"end":144,"id":27},{"text":"2002","start":144,"end":148,"id":28},{"text":")","start":148,"end":149,"id":29},{"text":"proposed","start":150,"end":158,"id":30},{"text":"a","start":159,"end":160,"id":31},{"text":"Bayesian","start":161,"end":169,"id":32},{"text":"framework","start":170,"end":179,"id":33},{"text":"for","start":180,"end":183,"id":34},{"text":"K","start":184,"end":185,"id":35},{"text":"-","start":185,"end":186,"id":36},{"text":"nearest","start":186,"end":193,"id":37},{"text":"neighbors","start":194,"end":203,"id":38},{"text":"(","start":204,"end":205,"id":39},{"text":"KNN","start":205,"end":208,"id":40},{"text":")","start":208,"end":209,"id":41},{"text":".","start":209,"end":210,"id":42}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":15,"end":34,"token_start":3,"token_end":6,"label":"ALGO","answer":"accept"},{"start":184,"end":203,"token_start":35,"token_end":38,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Completed by a point at infinity, they can be viewed as subtrees of the Bruhat-Tits tree associated to the $p$-adic projective line.","_input_hash":1227531399,"_task_hash":814001893,"tokens":[{"text":"Completed","start":0,"end":9,"id":0},{"text":"by","start":10,"end":12,"id":1},{"text":"a","start":13,"end":14,"id":2},{"text":"point","start":15,"end":20,"id":3},{"text":"at","start":21,"end":23,"id":4},{"text":"infinity","start":24,"end":32,"id":5},{"text":",","start":32,"end":33,"id":6},{"text":"they","start":34,"end":38,"id":7},{"text":"can","start":39,"end":42,"id":8},{"text":"be","start":43,"end":45,"id":9},{"text":"viewed","start":46,"end":52,"id":10},{"text":"as","start":53,"end":55,"id":11},{"text":"subtrees","start":56,"end":64,"id":12},{"text":"of","start":65,"end":67,"id":13},{"text":"the","start":68,"end":71,"id":14},{"text":"Bruhat","start":72,"end":78,"id":15},{"text":"-","start":78,"end":79,"id":16},{"text":"Tits","start":79,"end":83,"id":17},{"text":"tree","start":84,"end":88,"id":18},{"text":"associated","start":89,"end":99,"id":19},{"text":"to","start":100,"end":102,"id":20},{"text":"the","start":103,"end":106,"id":21},{"text":"$","start":107,"end":108,"id":22},{"text":"p$-adic","start":108,"end":115,"id":23},{"text":"projective","start":116,"end":126,"id":24},{"text":"line","start":127,"end":131,"id":25},{"text":".","start":131,"end":132,"id":26}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":72,"end":88,"token_start":15,"token_end":18,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"The measure also serves as a natural tool when choosing dimension-reduction parameters.","_input_hash":-1067289464,"_task_hash":2026893910,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"measure","start":4,"end":11,"id":1},{"text":"also","start":12,"end":16,"id":2},{"text":"serves","start":17,"end":23,"id":3},{"text":"as","start":24,"end":26,"id":4},{"text":"a","start":27,"end":28,"id":5},{"text":"natural","start":29,"end":36,"id":6},{"text":"tool","start":37,"end":41,"id":7},{"text":"when","start":42,"end":46,"id":8},{"text":"choosing","start":47,"end":55,"id":9},{"text":"dimension","start":56,"end":65,"id":10},{"text":"-","start":65,"end":66,"id":11},{"text":"reduction","start":66,"end":75,"id":12},{"text":"parameters","start":76,"end":86,"id":13},{"text":".","start":86,"end":87,"id":14}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"In this paper, we introduce the cascading Indian buffet process (CIBP), which provides a nonparametric prior on the structure of a layered, directed belief network that is unbounded in both depth and width, yet allows tractable inference.","_input_hash":1429126629,"_task_hash":-1662476059,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"this","start":3,"end":7,"id":1},{"text":"paper","start":8,"end":13,"id":2},{"text":",","start":13,"end":14,"id":3},{"text":"we","start":15,"end":17,"id":4},{"text":"introduce","start":18,"end":27,"id":5},{"text":"the","start":28,"end":31,"id":6},{"text":"cascading","start":32,"end":41,"id":7},{"text":"Indian","start":42,"end":48,"id":8},{"text":"buffet","start":49,"end":55,"id":9},{"text":"process","start":56,"end":63,"id":10},{"text":"(","start":64,"end":65,"id":11},{"text":"CIBP","start":65,"end":69,"id":12},{"text":")","start":69,"end":70,"id":13},{"text":",","start":70,"end":71,"id":14},{"text":"which","start":72,"end":77,"id":15},{"text":"provides","start":78,"end":86,"id":16},{"text":"a","start":87,"end":88,"id":17},{"text":"nonparametric","start":89,"end":102,"id":18},{"text":"prior","start":103,"end":108,"id":19},{"text":"on","start":109,"end":111,"id":20},{"text":"the","start":112,"end":115,"id":21},{"text":"structure","start":116,"end":125,"id":22},{"text":"of","start":126,"end":128,"id":23},{"text":"a","start":129,"end":130,"id":24},{"text":"layered","start":131,"end":138,"id":25},{"text":",","start":138,"end":139,"id":26},{"text":"directed","start":140,"end":148,"id":27},{"text":"belief","start":149,"end":155,"id":28},{"text":"network","start":156,"end":163,"id":29},{"text":"that","start":164,"end":168,"id":30},{"text":"is","start":169,"end":171,"id":31},{"text":"unbounded","start":172,"end":181,"id":32},{"text":"in","start":182,"end":184,"id":33},{"text":"both","start":185,"end":189,"id":34},{"text":"depth","start":190,"end":195,"id":35},{"text":"and","start":196,"end":199,"id":36},{"text":"width","start":200,"end":205,"id":37},{"text":",","start":205,"end":206,"id":38},{"text":"yet","start":207,"end":210,"id":39},{"text":"allows","start":211,"end":217,"id":40},{"text":"tractable","start":218,"end":227,"id":41},{"text":"inference","start":228,"end":237,"id":42},{"text":".","start":237,"end":238,"id":43}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":32,"end":55,"token_start":7,"token_end":9,"label":"ALGO","answer":"accept"},{"start":140,"end":163,"token_start":27,"token_end":29,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"The proposed kernels appear to be particularly convenient for analyzing the e ffect of each (group of) variable(s) and computing sensitivity indices without recursivity.","_input_hash":573596549,"_task_hash":-512475944,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"proposed","start":4,"end":12,"id":1},{"text":"kernels","start":13,"end":20,"id":2},{"text":"appear","start":21,"end":27,"id":3},{"text":"to","start":28,"end":30,"id":4},{"text":"be","start":31,"end":33,"id":5},{"text":"particularly","start":34,"end":46,"id":6},{"text":"convenient","start":47,"end":57,"id":7},{"text":"for","start":58,"end":61,"id":8},{"text":"analyzing","start":62,"end":71,"id":9},{"text":"the","start":72,"end":75,"id":10},{"text":"e","start":76,"end":77,"id":11},{"text":"ffect","start":78,"end":83,"id":12},{"text":"of","start":84,"end":86,"id":13},{"text":"each","start":87,"end":91,"id":14},{"text":"(","start":92,"end":93,"id":15},{"text":"group","start":93,"end":98,"id":16},{"text":"of","start":99,"end":101,"id":17},{"text":")","start":101,"end":102,"id":18},{"text":"variable(s","start":103,"end":113,"id":19},{"text":")","start":113,"end":114,"id":20},{"text":"and","start":115,"end":118,"id":21},{"text":"computing","start":119,"end":128,"id":22},{"text":"sensitivity","start":129,"end":140,"id":23},{"text":"indices","start":141,"end":148,"id":24},{"text":"without","start":149,"end":156,"id":25},{"text":"recursivity","start":157,"end":168,"id":26},{"text":".","start":168,"end":169,"id":27}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We present an extensive experimental evaluation on real world data showing the benefits of the proposed approach.","_input_hash":804669926,"_task_hash":-791853834,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"present","start":3,"end":10,"id":1},{"text":"an","start":11,"end":13,"id":2},{"text":"extensive","start":14,"end":23,"id":3},{"text":"experimental","start":24,"end":36,"id":4},{"text":"evaluation","start":37,"end":47,"id":5},{"text":"on","start":48,"end":50,"id":6},{"text":"real","start":51,"end":55,"id":7},{"text":"world","start":56,"end":61,"id":8},{"text":"data","start":62,"end":66,"id":9},{"text":"showing","start":67,"end":74,"id":10},{"text":"the","start":75,"end":78,"id":11},{"text":"benefits","start":79,"end":87,"id":12},{"text":"of","start":88,"end":90,"id":13},{"text":"the","start":91,"end":94,"id":14},{"text":"proposed","start":95,"end":103,"id":15},{"text":"approach","start":104,"end":112,"id":16},{"text":".","start":112,"end":113,"id":17}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Unlike previous methods, its quantization error depends only on the intrinsic dimension of the data distribution, rather than the apparent dimension of the space in which the data happen to lie.","_input_hash":1862424817,"_task_hash":351926450,"tokens":[{"text":"Unlike","start":0,"end":6,"id":0},{"text":"previous","start":7,"end":15,"id":1},{"text":"methods","start":16,"end":23,"id":2},{"text":",","start":23,"end":24,"id":3},{"text":"its","start":25,"end":28,"id":4},{"text":"quantization","start":29,"end":41,"id":5},{"text":"error","start":42,"end":47,"id":6},{"text":"depends","start":48,"end":55,"id":7},{"text":"only","start":56,"end":60,"id":8},{"text":"on","start":61,"end":63,"id":9},{"text":"the","start":64,"end":67,"id":10},{"text":"intrinsic","start":68,"end":77,"id":11},{"text":"dimension","start":78,"end":87,"id":12},{"text":"of","start":88,"end":90,"id":13},{"text":"the","start":91,"end":94,"id":14},{"text":"data","start":95,"end":99,"id":15},{"text":"distribution","start":100,"end":112,"id":16},{"text":",","start":112,"end":113,"id":17},{"text":"rather","start":114,"end":120,"id":18},{"text":"than","start":121,"end":125,"id":19},{"text":"the","start":126,"end":129,"id":20},{"text":"apparent","start":130,"end":138,"id":21},{"text":"dimension","start":139,"end":148,"id":22},{"text":"of","start":149,"end":151,"id":23},{"text":"the","start":152,"end":155,"id":24},{"text":"space","start":156,"end":161,"id":25},{"text":"in","start":162,"end":164,"id":26},{"text":"which","start":165,"end":170,"id":27},{"text":"the","start":171,"end":174,"id":28},{"text":"data","start":175,"end":179,"id":29},{"text":"happen","start":180,"end":186,"id":30},{"text":"to","start":187,"end":189,"id":31},{"text":"lie","start":190,"end":193,"id":32},{"text":".","start":193,"end":194,"id":33}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"Therefore, as a side effect we obtain a new training algorithm for learning sparse linear RLS predictors which can be used for large scale learning.","_input_hash":1252621343,"_task_hash":-757266465,"tokens":[{"text":"Therefore","start":0,"end":9,"id":0},{"text":",","start":9,"end":10,"id":1},{"text":"as","start":11,"end":13,"id":2},{"text":"a","start":14,"end":15,"id":3},{"text":"side","start":16,"end":20,"id":4},{"text":"effect","start":21,"end":27,"id":5},{"text":"we","start":28,"end":30,"id":6},{"text":"obtain","start":31,"end":37,"id":7},{"text":"a","start":38,"end":39,"id":8},{"text":"new","start":40,"end":43,"id":9},{"text":"training","start":44,"end":52,"id":10},{"text":"algorithm","start":53,"end":62,"id":11},{"text":"for","start":63,"end":66,"id":12},{"text":"learning","start":67,"end":75,"id":13},{"text":"sparse","start":76,"end":82,"id":14},{"text":"linear","start":83,"end":89,"id":15},{"text":"RLS","start":90,"end":93,"id":16},{"text":"predictors","start":94,"end":104,"id":17},{"text":"which","start":105,"end":110,"id":18},{"text":"can","start":111,"end":114,"id":19},{"text":"be","start":115,"end":117,"id":20},{"text":"used","start":118,"end":122,"id":21},{"text":"for","start":123,"end":126,"id":22},{"text":"large","start":127,"end":132,"id":23},{"text":"scale","start":133,"end":138,"id":24},{"text":"learning","start":139,"end":147,"id":25},{"text":".","start":147,"end":148,"id":26}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"This simple technique consists of performing exact line search optimization of the kurtosis contrast function.","_input_hash":1234467529,"_task_hash":-485536131,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"simple","start":5,"end":11,"id":1},{"text":"technique","start":12,"end":21,"id":2},{"text":"consists","start":22,"end":30,"id":3},{"text":"of","start":31,"end":33,"id":4},{"text":"performing","start":34,"end":44,"id":5},{"text":"exact","start":45,"end":50,"id":6},{"text":"line","start":51,"end":55,"id":7},{"text":"search","start":56,"end":62,"id":8},{"text":"optimization","start":63,"end":75,"id":9},{"text":"of","start":76,"end":78,"id":10},{"text":"the","start":79,"end":82,"id":11},{"text":"kurtosis","start":83,"end":91,"id":12},{"text":"contrast","start":92,"end":100,"id":13},{"text":"function","start":101,"end":109,"id":14},{"text":".","start":109,"end":110,"id":15}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Given a reproducing kernel Hilbert space H of real-valued functions and a suitable measure mu over the source space D (subset of R), we decompose H as the sum of a subspace of centered functions for mu and its orthogonal in H. This decomposition leads to a special case of ANOVA kernels, for which the functional ANOVA representation of the best predictor can be elegantly derived, either in an interpolation or regularization framework.","_input_hash":-833492554,"_task_hash":1131889486,"tokens":[{"text":"Given","start":0,"end":5,"id":0},{"text":"a","start":6,"end":7,"id":1},{"text":"reproducing","start":8,"end":19,"id":2},{"text":"kernel","start":20,"end":26,"id":3},{"text":"Hilbert","start":27,"end":34,"id":4},{"text":"space","start":35,"end":40,"id":5},{"text":"H","start":41,"end":42,"id":6},{"text":"of","start":43,"end":45,"id":7},{"text":"real","start":46,"end":50,"id":8},{"text":"-","start":50,"end":51,"id":9},{"text":"valued","start":51,"end":57,"id":10},{"text":"functions","start":58,"end":67,"id":11},{"text":"and","start":68,"end":71,"id":12},{"text":"a","start":72,"end":73,"id":13},{"text":"suitable","start":74,"end":82,"id":14},{"text":"measure","start":83,"end":90,"id":15},{"text":"mu","start":91,"end":93,"id":16},{"text":"over","start":94,"end":98,"id":17},{"text":"the","start":99,"end":102,"id":18},{"text":"source","start":103,"end":109,"id":19},{"text":"space","start":110,"end":115,"id":20},{"text":"D","start":116,"end":117,"id":21},{"text":"(","start":118,"end":119,"id":22},{"text":"subset","start":119,"end":125,"id":23},{"text":"of","start":126,"end":128,"id":24},{"text":"R","start":129,"end":130,"id":25},{"text":")","start":130,"end":131,"id":26},{"text":",","start":131,"end":132,"id":27},{"text":"we","start":133,"end":135,"id":28},{"text":"decompose","start":136,"end":145,"id":29},{"text":"H","start":146,"end":147,"id":30},{"text":"as","start":148,"end":150,"id":31},{"text":"the","start":151,"end":154,"id":32},{"text":"sum","start":155,"end":158,"id":33},{"text":"of","start":159,"end":161,"id":34},{"text":"a","start":162,"end":163,"id":35},{"text":"subspace","start":164,"end":172,"id":36},{"text":"of","start":173,"end":175,"id":37},{"text":"centered","start":176,"end":184,"id":38},{"text":"functions","start":185,"end":194,"id":39},{"text":"for","start":195,"end":198,"id":40},{"text":"mu","start":199,"end":201,"id":41},{"text":"and","start":202,"end":205,"id":42},{"text":"its","start":206,"end":209,"id":43},{"text":"orthogonal","start":210,"end":220,"id":44},{"text":"in","start":221,"end":223,"id":45},{"text":"H.","start":224,"end":226,"id":46},{"text":"This","start":227,"end":231,"id":47},{"text":"decomposition","start":232,"end":245,"id":48},{"text":"leads","start":246,"end":251,"id":49},{"text":"to","start":252,"end":254,"id":50},{"text":"a","start":255,"end":256,"id":51},{"text":"special","start":257,"end":264,"id":52},{"text":"case","start":265,"end":269,"id":53},{"text":"of","start":270,"end":272,"id":54},{"text":"ANOVA","start":273,"end":278,"id":55},{"text":"kernels","start":279,"end":286,"id":56},{"text":",","start":286,"end":287,"id":57},{"text":"for","start":288,"end":291,"id":58},{"text":"which","start":292,"end":297,"id":59},{"text":"the","start":298,"end":301,"id":60},{"text":"functional","start":302,"end":312,"id":61},{"text":"ANOVA","start":313,"end":318,"id":62},{"text":"representation","start":319,"end":333,"id":63},{"text":"of","start":334,"end":336,"id":64},{"text":"the","start":337,"end":340,"id":65},{"text":"best","start":341,"end":345,"id":66},{"text":"predictor","start":346,"end":355,"id":67},{"text":"can","start":356,"end":359,"id":68},{"text":"be","start":360,"end":362,"id":69},{"text":"elegantly","start":363,"end":372,"id":70},{"text":"derived","start":373,"end":380,"id":71},{"text":",","start":380,"end":381,"id":72},{"text":"either","start":382,"end":388,"id":73},{"text":"in","start":389,"end":391,"id":74},{"text":"an","start":392,"end":394,"id":75},{"text":"interpolation","start":395,"end":408,"id":76},{"text":"or","start":409,"end":411,"id":77},{"text":"regularization","start":412,"end":426,"id":78},{"text":"framework","start":427,"end":436,"id":79},{"text":".","start":436,"end":437,"id":80}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":20,"end":40,"token_start":3,"token_end":5,"label":"ALGO","answer":"accept"},{"start":273,"end":286,"token_start":55,"token_end":56,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We study the problem of learning a sparse linear regression vector under additional conditions on the structure of its sparsity pattern.","_input_hash":-1336290528,"_task_hash":-908265437,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"study","start":3,"end":8,"id":1},{"text":"the","start":9,"end":12,"id":2},{"text":"problem","start":13,"end":20,"id":3},{"text":"of","start":21,"end":23,"id":4},{"text":"learning","start":24,"end":32,"id":5},{"text":"a","start":33,"end":34,"id":6},{"text":"sparse","start":35,"end":41,"id":7},{"text":"linear","start":42,"end":48,"id":8},{"text":"regression","start":49,"end":59,"id":9},{"text":"vector","start":60,"end":66,"id":10},{"text":"under","start":67,"end":72,"id":11},{"text":"additional","start":73,"end":83,"id":12},{"text":"conditions","start":84,"end":94,"id":13},{"text":"on","start":95,"end":97,"id":14},{"text":"the","start":98,"end":101,"id":15},{"text":"structure","start":102,"end":111,"id":16},{"text":"of","start":112,"end":114,"id":17},{"text":"its","start":115,"end":118,"id":18},{"text":"sparsity","start":119,"end":127,"id":19},{"text":"pattern","start":128,"end":135,"id":20},{"text":".","start":135,"end":136,"id":21}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":35,"end":59,"token_start":7,"token_end":9,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"This class consists of Locally Linear Embedding (LLE), Laplacian Eigenmap, Local Tangent Space Alignment (LTSA), Hessian Eigenmaps (HLLE), and Diffusion maps.","_input_hash":86645528,"_task_hash":537964960,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"class","start":5,"end":10,"id":1},{"text":"consists","start":11,"end":19,"id":2},{"text":"of","start":20,"end":22,"id":3},{"text":"Locally","start":23,"end":30,"id":4},{"text":"Linear","start":31,"end":37,"id":5},{"text":"Embedding","start":38,"end":47,"id":6},{"text":"(","start":48,"end":49,"id":7},{"text":"LLE","start":49,"end":52,"id":8},{"text":")","start":52,"end":53,"id":9},{"text":",","start":53,"end":54,"id":10},{"text":"Laplacian","start":55,"end":64,"id":11},{"text":"Eigenmap","start":65,"end":73,"id":12},{"text":",","start":73,"end":74,"id":13},{"text":"Local","start":75,"end":80,"id":14},{"text":"Tangent","start":81,"end":88,"id":15},{"text":"Space","start":89,"end":94,"id":16},{"text":"Alignment","start":95,"end":104,"id":17},{"text":"(","start":105,"end":106,"id":18},{"text":"LTSA","start":106,"end":110,"id":19},{"text":")","start":110,"end":111,"id":20},{"text":",","start":111,"end":112,"id":21},{"text":"Hessian","start":113,"end":120,"id":22},{"text":"Eigenmaps","start":121,"end":130,"id":23},{"text":"(","start":131,"end":132,"id":24},{"text":"HLLE","start":132,"end":136,"id":25},{"text":")","start":136,"end":137,"id":26},{"text":",","start":137,"end":138,"id":27},{"text":"and","start":139,"end":142,"id":28},{"text":"Diffusion","start":143,"end":152,"id":29},{"text":"maps","start":153,"end":157,"id":30},{"text":".","start":157,"end":158,"id":31}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":23,"end":47,"token_start":4,"token_end":6,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"The algorithm, which we call greedy RLS, starts from the empty feature set, and on each iteration adds the feature whose addition provides the best leave-one-out cross-validation performance.","_input_hash":-432225788,"_task_hash":1717380557,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"algorithm","start":4,"end":13,"id":1},{"text":",","start":13,"end":14,"id":2},{"text":"which","start":15,"end":20,"id":3},{"text":"we","start":21,"end":23,"id":4},{"text":"call","start":24,"end":28,"id":5},{"text":"greedy","start":29,"end":35,"id":6},{"text":"RLS","start":36,"end":39,"id":7},{"text":",","start":39,"end":40,"id":8},{"text":"starts","start":41,"end":47,"id":9},{"text":"from","start":48,"end":52,"id":10},{"text":"the","start":53,"end":56,"id":11},{"text":"empty","start":57,"end":62,"id":12},{"text":"feature","start":63,"end":70,"id":13},{"text":"set","start":71,"end":74,"id":14},{"text":",","start":74,"end":75,"id":15},{"text":"and","start":76,"end":79,"id":16},{"text":"on","start":80,"end":82,"id":17},{"text":"each","start":83,"end":87,"id":18},{"text":"iteration","start":88,"end":97,"id":19},{"text":"adds","start":98,"end":102,"id":20},{"text":"the","start":103,"end":106,"id":21},{"text":"feature","start":107,"end":114,"id":22},{"text":"whose","start":115,"end":120,"id":23},{"text":"addition","start":121,"end":129,"id":24},{"text":"provides","start":130,"end":138,"id":25},{"text":"the","start":139,"end":142,"id":26},{"text":"best","start":143,"end":147,"id":27},{"text":"leave","start":148,"end":153,"id":28},{"text":"-","start":153,"end":154,"id":29},{"text":"one","start":154,"end":157,"id":30},{"text":"-","start":157,"end":158,"id":31},{"text":"out","start":158,"end":161,"id":32},{"text":"cross","start":162,"end":167,"id":33},{"text":"-","start":167,"end":168,"id":34},{"text":"validation","start":168,"end":178,"id":35},{"text":"performance","start":179,"end":190,"id":36},{"text":".","start":190,"end":191,"id":37}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":29,"end":39,"token_start":6,"token_end":7,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We also present an efficient active set algorithm, and analyze the consistency of variable selection for least-squares linear regression in low and high-dimensional settings.","_input_hash":1222720645,"_task_hash":-2028883158,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"also","start":3,"end":7,"id":1},{"text":"present","start":8,"end":15,"id":2},{"text":"an","start":16,"end":18,"id":3},{"text":"efficient","start":19,"end":28,"id":4},{"text":"active","start":29,"end":35,"id":5},{"text":"set","start":36,"end":39,"id":6},{"text":"algorithm","start":40,"end":49,"id":7},{"text":",","start":49,"end":50,"id":8},{"text":"and","start":51,"end":54,"id":9},{"text":"analyze","start":55,"end":62,"id":10},{"text":"the","start":63,"end":66,"id":11},{"text":"consistency","start":67,"end":78,"id":12},{"text":"of","start":79,"end":81,"id":13},{"text":"variable","start":82,"end":90,"id":14},{"text":"selection","start":91,"end":100,"id":15},{"text":"for","start":101,"end":104,"id":16},{"text":"least","start":105,"end":110,"id":17},{"text":"-","start":110,"end":111,"id":18},{"text":"squares","start":111,"end":118,"id":19},{"text":"linear","start":119,"end":125,"id":20},{"text":"regression","start":126,"end":136,"id":21},{"text":"in","start":137,"end":139,"id":22},{"text":"low","start":140,"end":143,"id":23},{"text":"and","start":144,"end":147,"id":24},{"text":"high","start":148,"end":152,"id":25},{"text":"-","start":152,"end":153,"id":26},{"text":"dimensional","start":153,"end":164,"id":27},{"text":"settings","start":165,"end":173,"id":28},{"text":".","start":173,"end":174,"id":29}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"This paper proposes an original approach to cluster multi-component data sets, including an estimation of the number of clusters.","_input_hash":-974865813,"_task_hash":1105404466,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"paper","start":5,"end":10,"id":1},{"text":"proposes","start":11,"end":19,"id":2},{"text":"an","start":20,"end":22,"id":3},{"text":"original","start":23,"end":31,"id":4},{"text":"approach","start":32,"end":40,"id":5},{"text":"to","start":41,"end":43,"id":6},{"text":"cluster","start":44,"end":51,"id":7},{"text":"multi","start":52,"end":57,"id":8},{"text":"-","start":57,"end":58,"id":9},{"text":"component","start":58,"end":67,"id":10},{"text":"data","start":68,"end":72,"id":11},{"text":"sets","start":73,"end":77,"id":12},{"text":",","start":77,"end":78,"id":13},{"text":"including","start":79,"end":88,"id":14},{"text":"an","start":89,"end":91,"id":15},{"text":"estimation","start":92,"end":102,"id":16},{"text":"of","start":103,"end":105,"id":17},{"text":"the","start":106,"end":109,"id":18},{"text":"number","start":110,"end":116,"id":19},{"text":"of","start":117,"end":119,"id":20},{"text":"clusters","start":120,"end":128,"id":21},{"text":".","start":128,"end":129,"id":22}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"This leads to a form of the covariance similar in spirit to the so called PITC and FITC approximations for a single output.","_input_hash":2080976227,"_task_hash":1854135906,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"leads","start":5,"end":10,"id":1},{"text":"to","start":11,"end":13,"id":2},{"text":"a","start":14,"end":15,"id":3},{"text":"form","start":16,"end":20,"id":4},{"text":"of","start":21,"end":23,"id":5},{"text":"the","start":24,"end":27,"id":6},{"text":"covariance","start":28,"end":38,"id":7},{"text":"similar","start":39,"end":46,"id":8},{"text":"in","start":47,"end":49,"id":9},{"text":"spirit","start":50,"end":56,"id":10},{"text":"to","start":57,"end":59,"id":11},{"text":"the","start":60,"end":63,"id":12},{"text":"so","start":64,"end":66,"id":13},{"text":"called","start":67,"end":73,"id":14},{"text":"PITC","start":74,"end":78,"id":15},{"text":"and","start":79,"end":82,"id":16},{"text":"FITC","start":83,"end":87,"id":17},{"text":"approximations","start":88,"end":102,"id":18},{"text":"for","start":103,"end":106,"id":19},{"text":"a","start":107,"end":108,"id":20},{"text":"single","start":109,"end":115,"id":21},{"text":"output","start":116,"end":122,"id":22},{"text":".","start":122,"end":123,"id":23}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"It corresponds to an ubiquitous situation in many applications such as information retrieval or gene ranking, when we have identified a set of data of interest sharing a particular property, and we wish to automatically retrieve additional data sharing the same property among a large and easily available pool of unlabeled data.","_input_hash":1701931546,"_task_hash":-732835768,"tokens":[{"text":"It","start":0,"end":2,"id":0},{"text":"corresponds","start":3,"end":14,"id":1},{"text":"to","start":15,"end":17,"id":2},{"text":"an","start":18,"end":20,"id":3},{"text":"ubiquitous","start":21,"end":31,"id":4},{"text":"situation","start":32,"end":41,"id":5},{"text":"in","start":42,"end":44,"id":6},{"text":"many","start":45,"end":49,"id":7},{"text":"applications","start":50,"end":62,"id":8},{"text":"such","start":63,"end":67,"id":9},{"text":"as","start":68,"end":70,"id":10},{"text":"information","start":71,"end":82,"id":11},{"text":"retrieval","start":83,"end":92,"id":12},{"text":"or","start":93,"end":95,"id":13},{"text":"gene","start":96,"end":100,"id":14},{"text":"ranking","start":101,"end":108,"id":15},{"text":",","start":108,"end":109,"id":16},{"text":"when","start":110,"end":114,"id":17},{"text":"we","start":115,"end":117,"id":18},{"text":"have","start":118,"end":122,"id":19},{"text":"identified","start":123,"end":133,"id":20},{"text":"a","start":134,"end":135,"id":21},{"text":"set","start":136,"end":139,"id":22},{"text":"of","start":140,"end":142,"id":23},{"text":"data","start":143,"end":147,"id":24},{"text":"of","start":148,"end":150,"id":25},{"text":"interest","start":151,"end":159,"id":26},{"text":"sharing","start":160,"end":167,"id":27},{"text":"a","start":168,"end":169,"id":28},{"text":"particular","start":170,"end":180,"id":29},{"text":"property","start":181,"end":189,"id":30},{"text":",","start":189,"end":190,"id":31},{"text":"and","start":191,"end":194,"id":32},{"text":"we","start":195,"end":197,"id":33},{"text":"wish","start":198,"end":202,"id":34},{"text":"to","start":203,"end":205,"id":35},{"text":"automatically","start":206,"end":219,"id":36},{"text":"retrieve","start":220,"end":228,"id":37},{"text":"additional","start":229,"end":239,"id":38},{"text":"data","start":240,"end":244,"id":39},{"text":"sharing","start":245,"end":252,"id":40},{"text":"the","start":253,"end":256,"id":41},{"text":"same","start":257,"end":261,"id":42},{"text":"property","start":262,"end":270,"id":43},{"text":"among","start":271,"end":276,"id":44},{"text":"a","start":277,"end":278,"id":45},{"text":"large","start":279,"end":284,"id":46},{"text":"and","start":285,"end":288,"id":47},{"text":"easily","start":289,"end":295,"id":48},{"text":"available","start":296,"end":305,"id":49},{"text":"pool","start":306,"end":310,"id":50},{"text":"of","start":311,"end":313,"id":51},{"text":"unlabeled","start":314,"end":323,"id":52},{"text":"data","start":324,"end":328,"id":53},{"text":".","start":328,"end":329,"id":54}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We derive an upper bound on the local Rademacher complexity of $\\ell_p$-norm multiple kernel learning, which yields a tighter excess risk bound than global approaches.","_input_hash":-629492065,"_task_hash":-1759924439,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"derive","start":3,"end":9,"id":1},{"text":"an","start":10,"end":12,"id":2},{"text":"upper","start":13,"end":18,"id":3},{"text":"bound","start":19,"end":24,"id":4},{"text":"on","start":25,"end":27,"id":5},{"text":"the","start":28,"end":31,"id":6},{"text":"local","start":32,"end":37,"id":7},{"text":"Rademacher","start":38,"end":48,"id":8},{"text":"complexity","start":49,"end":59,"id":9},{"text":"of","start":60,"end":62,"id":10},{"text":"$","start":63,"end":64,"id":11},{"text":"\\ell_p$-norm","start":64,"end":76,"id":12},{"text":"multiple","start":77,"end":85,"id":13},{"text":"kernel","start":86,"end":92,"id":14},{"text":"learning","start":93,"end":101,"id":15},{"text":",","start":101,"end":102,"id":16},{"text":"which","start":103,"end":108,"id":17},{"text":"yields","start":109,"end":115,"id":18},{"text":"a","start":116,"end":117,"id":19},{"text":"tighter","start":118,"end":125,"id":20},{"text":"excess","start":126,"end":132,"id":21},{"text":"risk","start":133,"end":137,"id":22},{"text":"bound","start":138,"end":143,"id":23},{"text":"than","start":144,"end":148,"id":24},{"text":"global","start":149,"end":155,"id":25},{"text":"approaches","start":156,"end":166,"id":26},{"text":".","start":166,"end":167,"id":27}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":77,"end":101,"token_start":13,"token_end":15,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"The network performance is determined by calculating the mean square error of the network prediction.","_input_hash":1246371537,"_task_hash":-64210889,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"network","start":4,"end":11,"id":1},{"text":"performance","start":12,"end":23,"id":2},{"text":"is","start":24,"end":26,"id":3},{"text":"determined","start":27,"end":37,"id":4},{"text":"by","start":38,"end":40,"id":5},{"text":"calculating","start":41,"end":52,"id":6},{"text":"the","start":53,"end":56,"id":7},{"text":"mean","start":57,"end":61,"id":8},{"text":"square","start":62,"end":68,"id":9},{"text":"error","start":69,"end":74,"id":10},{"text":"of","start":75,"end":77,"id":11},{"text":"the","start":78,"end":81,"id":12},{"text":"network","start":82,"end":89,"id":13},{"text":"prediction","start":90,"end":100,"id":14},{"text":".","start":100,"end":101,"id":15}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Additionally, we apply the PVM to a protein classification problem in which a kernel-based distance is used.","_input_hash":173881543,"_task_hash":-985074821,"tokens":[{"text":"Additionally","start":0,"end":12,"id":0},{"text":",","start":12,"end":13,"id":1},{"text":"we","start":14,"end":16,"id":2},{"text":"apply","start":17,"end":22,"id":3},{"text":"the","start":23,"end":26,"id":4},{"text":"PVM","start":27,"end":30,"id":5},{"text":"to","start":31,"end":33,"id":6},{"text":"a","start":34,"end":35,"id":7},{"text":"protein","start":36,"end":43,"id":8},{"text":"classification","start":44,"end":58,"id":9},{"text":"problem","start":59,"end":66,"id":10},{"text":"in","start":67,"end":69,"id":11},{"text":"which","start":70,"end":75,"id":12},{"text":"a","start":76,"end":77,"id":13},{"text":"kernel","start":78,"end":84,"id":14},{"text":"-","start":84,"end":85,"id":15},{"text":"based","start":85,"end":90,"id":16},{"text":"distance","start":91,"end":99,"id":17},{"text":"is","start":100,"end":102,"id":18},{"text":"used","start":103,"end":107,"id":19},{"text":".","start":107,"end":108,"id":20}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":27,"end":30,"token_start":5,"token_end":5,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Numerical results strongly validate the theory that is presented in this paper.","_input_hash":1709418211,"_task_hash":508913724,"tokens":[{"text":"Numerical","start":0,"end":9,"id":0},{"text":"results","start":10,"end":17,"id":1},{"text":"strongly","start":18,"end":26,"id":2},{"text":"validate","start":27,"end":35,"id":3},{"text":"the","start":36,"end":39,"id":4},{"text":"theory","start":40,"end":46,"id":5},{"text":"that","start":47,"end":51,"id":6},{"text":"is","start":52,"end":54,"id":7},{"text":"presented","start":55,"end":64,"id":8},{"text":"in","start":65,"end":67,"id":9},{"text":"this","start":68,"end":72,"id":10},{"text":"paper","start":73,"end":78,"id":11},{"text":".","start":78,"end":79,"id":12}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"A density ratio is defined by the ratio of two probability densities.","_input_hash":-428174293,"_task_hash":309688397,"tokens":[{"text":"A","start":0,"end":1,"id":0},{"text":"density","start":2,"end":9,"id":1},{"text":"ratio","start":10,"end":15,"id":2},{"text":"is","start":16,"end":18,"id":3},{"text":"defined","start":19,"end":26,"id":4},{"text":"by","start":27,"end":29,"id":5},{"text":"the","start":30,"end":33,"id":6},{"text":"ratio","start":34,"end":39,"id":7},{"text":"of","start":40,"end":42,"id":8},{"text":"two","start":43,"end":46,"id":9},{"text":"probability","start":47,"end":58,"id":10},{"text":"densities","start":59,"end":68,"id":11},{"text":".","start":68,"end":69,"id":12}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"In this paper, we propose alternative CDOM methods that can naturally address the model selection problem.","_input_hash":-1915100010,"_task_hash":-1879508361,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"this","start":3,"end":7,"id":1},{"text":"paper","start":8,"end":13,"id":2},{"text":",","start":13,"end":14,"id":3},{"text":"we","start":15,"end":17,"id":4},{"text":"propose","start":18,"end":25,"id":5},{"text":"alternative","start":26,"end":37,"id":6},{"text":"CDOM","start":38,"end":42,"id":7},{"text":"methods","start":43,"end":50,"id":8},{"text":"that","start":51,"end":55,"id":9},{"text":"can","start":56,"end":59,"id":10},{"text":"naturally","start":60,"end":69,"id":11},{"text":"address","start":70,"end":77,"id":12},{"text":"the","start":78,"end":81,"id":13},{"text":"model","start":82,"end":87,"id":14},{"text":"selection","start":88,"end":97,"id":15},{"text":"problem","start":98,"end":105,"id":16},{"text":".","start":105,"end":106,"id":17}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"We illustrate the application of the correlation screening methodology on a large scale gene-expression dataset, revealing a few influential variables that exhibit a significant amount of correlation over multiple treatments.","_input_hash":524528861,"_task_hash":1960929188,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"illustrate","start":3,"end":13,"id":1},{"text":"the","start":14,"end":17,"id":2},{"text":"application","start":18,"end":29,"id":3},{"text":"of","start":30,"end":32,"id":4},{"text":"the","start":33,"end":36,"id":5},{"text":"correlation","start":37,"end":48,"id":6},{"text":"screening","start":49,"end":58,"id":7},{"text":"methodology","start":59,"end":70,"id":8},{"text":"on","start":71,"end":73,"id":9},{"text":"a","start":74,"end":75,"id":10},{"text":"large","start":76,"end":81,"id":11},{"text":"scale","start":82,"end":87,"id":12},{"text":"gene","start":88,"end":92,"id":13},{"text":"-","start":92,"end":93,"id":14},{"text":"expression","start":93,"end":103,"id":15},{"text":"dataset","start":104,"end":111,"id":16},{"text":",","start":111,"end":112,"id":17},{"text":"revealing","start":113,"end":122,"id":18},{"text":"a","start":123,"end":124,"id":19},{"text":"few","start":125,"end":128,"id":20},{"text":"influential","start":129,"end":140,"id":21},{"text":"variables","start":141,"end":150,"id":22},{"text":"that","start":151,"end":155,"id":23},{"text":"exhibit","start":156,"end":163,"id":24},{"text":"a","start":164,"end":165,"id":25},{"text":"significant","start":166,"end":177,"id":26},{"text":"amount","start":178,"end":184,"id":27},{"text":"of","start":185,"end":187,"id":28},{"text":"correlation","start":188,"end":199,"id":29},{"text":"over","start":200,"end":204,"id":30},{"text":"multiple","start":205,"end":213,"id":31},{"text":"treatments","start":214,"end":224,"id":32},{"text":".","start":224,"end":225,"id":33}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We have used the techniques developed here for estimation of quantitative traits from markers, on the benchmark \"Bostob Housing\"data set and in some simulations.","_input_hash":-628692636,"_task_hash":-1247420650,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"have","start":3,"end":7,"id":1},{"text":"used","start":8,"end":12,"id":2},{"text":"the","start":13,"end":16,"id":3},{"text":"techniques","start":17,"end":27,"id":4},{"text":"developed","start":28,"end":37,"id":5},{"text":"here","start":38,"end":42,"id":6},{"text":"for","start":43,"end":46,"id":7},{"text":"estimation","start":47,"end":57,"id":8},{"text":"of","start":58,"end":60,"id":9},{"text":"quantitative","start":61,"end":73,"id":10},{"text":"traits","start":74,"end":80,"id":11},{"text":"from","start":81,"end":85,"id":12},{"text":"markers","start":86,"end":93,"id":13},{"text":",","start":93,"end":94,"id":14},{"text":"on","start":95,"end":97,"id":15},{"text":"the","start":98,"end":101,"id":16},{"text":"benchmark","start":102,"end":111,"id":17},{"text":"\"","start":112,"end":113,"id":18},{"text":"Bostob","start":113,"end":119,"id":19},{"text":"Housing\"data","start":120,"end":132,"id":20},{"text":"set","start":133,"end":136,"id":21},{"text":"and","start":137,"end":140,"id":22},{"text":"in","start":141,"end":143,"id":23},{"text":"some","start":144,"end":148,"id":24},{"text":"simulations","start":149,"end":160,"id":25},{"text":".","start":160,"end":161,"id":26}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Many statistical methods have been proposed to estimate causal models in classical situations with fewer variables than observations (p<n, p:","_input_hash":815758402,"_task_hash":38279444,"tokens":[{"text":"Many","start":0,"end":4,"id":0},{"text":"statistical","start":5,"end":16,"id":1},{"text":"methods","start":17,"end":24,"id":2},{"text":"have","start":25,"end":29,"id":3},{"text":"been","start":30,"end":34,"id":4},{"text":"proposed","start":35,"end":43,"id":5},{"text":"to","start":44,"end":46,"id":6},{"text":"estimate","start":47,"end":55,"id":7},{"text":"causal","start":56,"end":62,"id":8},{"text":"models","start":63,"end":69,"id":9},{"text":"in","start":70,"end":72,"id":10},{"text":"classical","start":73,"end":82,"id":11},{"text":"situations","start":83,"end":93,"id":12},{"text":"with","start":94,"end":98,"id":13},{"text":"fewer","start":99,"end":104,"id":14},{"text":"variables","start":105,"end":114,"id":15},{"text":"than","start":115,"end":119,"id":16},{"text":"observations","start":120,"end":132,"id":17},{"text":"(","start":133,"end":134,"id":18},{"text":"p","start":134,"end":135,"id":19},{"text":"<","start":135,"end":136,"id":20},{"text":"n","start":136,"end":137,"id":21},{"text":",","start":137,"end":138,"id":22},{"text":"p","start":139,"end":140,"id":23},{"text":":","start":140,"end":141,"id":24}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Among other properties, this kernel can be easily computed when the dimension d of the time series is much larger than the lengths of the considered time series x and x'.","_input_hash":248299645,"_task_hash":-1981813488,"tokens":[{"text":"Among","start":0,"end":5,"id":0},{"text":"other","start":6,"end":11,"id":1},{"text":"properties","start":12,"end":22,"id":2},{"text":",","start":22,"end":23,"id":3},{"text":"this","start":24,"end":28,"id":4},{"text":"kernel","start":29,"end":35,"id":5},{"text":"can","start":36,"end":39,"id":6},{"text":"be","start":40,"end":42,"id":7},{"text":"easily","start":43,"end":49,"id":8},{"text":"computed","start":50,"end":58,"id":9},{"text":"when","start":59,"end":63,"id":10},{"text":"the","start":64,"end":67,"id":11},{"text":"dimension","start":68,"end":77,"id":12},{"text":"d","start":78,"end":79,"id":13},{"text":"of","start":80,"end":82,"id":14},{"text":"the","start":83,"end":86,"id":15},{"text":"time","start":87,"end":91,"id":16},{"text":"series","start":92,"end":98,"id":17},{"text":"is","start":99,"end":101,"id":18},{"text":"much","start":102,"end":106,"id":19},{"text":"larger","start":107,"end":113,"id":20},{"text":"than","start":114,"end":118,"id":21},{"text":"the","start":119,"end":122,"id":22},{"text":"lengths","start":123,"end":130,"id":23},{"text":"of","start":131,"end":133,"id":24},{"text":"the","start":134,"end":137,"id":25},{"text":"considered","start":138,"end":148,"id":26},{"text":"time","start":149,"end":153,"id":27},{"text":"series","start":154,"end":160,"id":28},{"text":"x","start":161,"end":162,"id":29},{"text":"and","start":163,"end":166,"id":30},{"text":"x","start":167,"end":168,"id":31},{"text":"'","start":168,"end":169,"id":32},{"text":".","start":169,"end":170,"id":33}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"In addition to studying sufficient conditions for the sign consistency of the Lasso estimate, this paper also gives necessary conditions for sign consistency.","_input_hash":276825750,"_task_hash":1889503872,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"addition","start":3,"end":11,"id":1},{"text":"to","start":12,"end":14,"id":2},{"text":"studying","start":15,"end":23,"id":3},{"text":"sufficient","start":24,"end":34,"id":4},{"text":"conditions","start":35,"end":45,"id":5},{"text":"for","start":46,"end":49,"id":6},{"text":"the","start":50,"end":53,"id":7},{"text":"sign","start":54,"end":58,"id":8},{"text":"consistency","start":59,"end":70,"id":9},{"text":"of","start":71,"end":73,"id":10},{"text":"the","start":74,"end":77,"id":11},{"text":"Lasso","start":78,"end":83,"id":12},{"text":"estimate","start":84,"end":92,"id":13},{"text":",","start":92,"end":93,"id":14},{"text":"this","start":94,"end":98,"id":15},{"text":"paper","start":99,"end":104,"id":16},{"text":"also","start":105,"end":109,"id":17},{"text":"gives","start":110,"end":115,"id":18},{"text":"necessary","start":116,"end":125,"id":19},{"text":"conditions","start":126,"end":136,"id":20},{"text":"for","start":137,"end":140,"id":21},{"text":"sign","start":141,"end":145,"id":22},{"text":"consistency","start":146,"end":157,"id":23},{"text":".","start":157,"end":158,"id":24}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":78,"end":83,"token_start":12,"token_end":12,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We propose a procedure that minimizes the so-called TESLA loss (i.e., temporally smoothed L1 regularized regression), which allows jointly estimating the partition boundaries of the VCVS model and the coefficient of the sparse precision matrix on each block of the partition.","_input_hash":-662001291,"_task_hash":-331933964,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"propose","start":3,"end":10,"id":1},{"text":"a","start":11,"end":12,"id":2},{"text":"procedure","start":13,"end":22,"id":3},{"text":"that","start":23,"end":27,"id":4},{"text":"minimizes","start":28,"end":37,"id":5},{"text":"the","start":38,"end":41,"id":6},{"text":"so","start":42,"end":44,"id":7},{"text":"-","start":44,"end":45,"id":8},{"text":"called","start":45,"end":51,"id":9},{"text":"TESLA","start":52,"end":57,"id":10},{"text":"loss","start":58,"end":62,"id":11},{"text":"(","start":63,"end":64,"id":12},{"text":"i.e.","start":64,"end":68,"id":13},{"text":",","start":68,"end":69,"id":14},{"text":"temporally","start":70,"end":80,"id":15},{"text":"smoothed","start":81,"end":89,"id":16},{"text":"L1","start":90,"end":92,"id":17},{"text":"regularized","start":93,"end":104,"id":18},{"text":"regression","start":105,"end":115,"id":19},{"text":")","start":115,"end":116,"id":20},{"text":",","start":116,"end":117,"id":21},{"text":"which","start":118,"end":123,"id":22},{"text":"allows","start":124,"end":130,"id":23},{"text":"jointly","start":131,"end":138,"id":24},{"text":"estimating","start":139,"end":149,"id":25},{"text":"the","start":150,"end":153,"id":26},{"text":"partition","start":154,"end":163,"id":27},{"text":"boundaries","start":164,"end":174,"id":28},{"text":"of","start":175,"end":177,"id":29},{"text":"the","start":178,"end":181,"id":30},{"text":"VCVS","start":182,"end":186,"id":31},{"text":"model","start":187,"end":192,"id":32},{"text":"and","start":193,"end":196,"id":33},{"text":"the","start":197,"end":200,"id":34},{"text":"coefficient","start":201,"end":212,"id":35},{"text":"of","start":213,"end":215,"id":36},{"text":"the","start":216,"end":219,"id":37},{"text":"sparse","start":220,"end":226,"id":38},{"text":"precision","start":227,"end":236,"id":39},{"text":"matrix","start":237,"end":243,"id":40},{"text":"on","start":244,"end":246,"id":41},{"text":"each","start":247,"end":251,"id":42},{"text":"block","start":252,"end":257,"id":43},{"text":"of","start":258,"end":260,"id":44},{"text":"the","start":261,"end":264,"id":45},{"text":"partition","start":265,"end":274,"id":46},{"text":".","start":274,"end":275,"id":47}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":52,"end":57,"token_start":10,"token_end":10,"label":"ALGO","answer":"accept"},{"start":70,"end":115,"token_start":15,"token_end":19,"label":"ALGO","answer":"accept"},{"start":182,"end":186,"token_start":31,"token_end":31,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"The Indian buffet process has been used as a nonparametric Bayesian prior on the directed structure of a belief network with a single infinitely wide hidden layer.","_input_hash":-2052689914,"_task_hash":-1100611295,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"Indian","start":4,"end":10,"id":1},{"text":"buffet","start":11,"end":17,"id":2},{"text":"process","start":18,"end":25,"id":3},{"text":"has","start":26,"end":29,"id":4},{"text":"been","start":30,"end":34,"id":5},{"text":"used","start":35,"end":39,"id":6},{"text":"as","start":40,"end":42,"id":7},{"text":"a","start":43,"end":44,"id":8},{"text":"nonparametric","start":45,"end":58,"id":9},{"text":"Bayesian","start":59,"end":67,"id":10},{"text":"prior","start":68,"end":73,"id":11},{"text":"on","start":74,"end":76,"id":12},{"text":"the","start":77,"end":80,"id":13},{"text":"directed","start":81,"end":89,"id":14},{"text":"structure","start":90,"end":99,"id":15},{"text":"of","start":100,"end":102,"id":16},{"text":"a","start":103,"end":104,"id":17},{"text":"belief","start":105,"end":111,"id":18},{"text":"network","start":112,"end":119,"id":19},{"text":"with","start":120,"end":124,"id":20},{"text":"a","start":125,"end":126,"id":21},{"text":"single","start":127,"end":133,"id":22},{"text":"infinitely","start":134,"end":144,"id":23},{"text":"wide","start":145,"end":149,"id":24},{"text":"hidden","start":150,"end":156,"id":25},{"text":"layer","start":157,"end":162,"id":26},{"text":".","start":162,"end":163,"id":27}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":4,"end":17,"token_start":1,"token_end":2,"label":"ALGO","answer":"accept"},{"start":105,"end":119,"token_start":18,"token_end":19,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We discuss here the mean-field theory for a cellular automata model of meta-learning.","_input_hash":1230272902,"_task_hash":-131571153,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"discuss","start":3,"end":10,"id":1},{"text":"here","start":11,"end":15,"id":2},{"text":"the","start":16,"end":19,"id":3},{"text":"mean","start":20,"end":24,"id":4},{"text":"-","start":24,"end":25,"id":5},{"text":"field","start":25,"end":30,"id":6},{"text":"theory","start":31,"end":37,"id":7},{"text":"for","start":38,"end":41,"id":8},{"text":"a","start":42,"end":43,"id":9},{"text":"cellular","start":44,"end":52,"id":10},{"text":"automata","start":53,"end":61,"id":11},{"text":"model","start":62,"end":67,"id":12},{"text":"of","start":68,"end":70,"id":13},{"text":"meta","start":71,"end":75,"id":14},{"text":"-","start":75,"end":76,"id":15},{"text":"learning","start":76,"end":84,"id":16},{"text":".","start":84,"end":85,"id":17}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":44,"end":61,"token_start":10,"token_end":11,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We show that k-NN regression is also adaptive to intrinsic dimension.","_input_hash":-1631854893,"_task_hash":1826271386,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"show","start":3,"end":7,"id":1},{"text":"that","start":8,"end":12,"id":2},{"text":"k","start":13,"end":14,"id":3},{"text":"-","start":14,"end":15,"id":4},{"text":"NN","start":15,"end":17,"id":5},{"text":"regression","start":18,"end":28,"id":6},{"text":"is","start":29,"end":31,"id":7},{"text":"also","start":32,"end":36,"id":8},{"text":"adaptive","start":37,"end":45,"id":9},{"text":"to","start":46,"end":48,"id":10},{"text":"intrinsic","start":49,"end":58,"id":11},{"text":"dimension","start":59,"end":68,"id":12},{"text":".","start":68,"end":69,"id":13}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":13,"end":28,"token_start":3,"token_end":6,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"The utility function encodes some a priori knowledge external to the system, it quantifies how bad it is to make mistakes.","_input_hash":651532569,"_task_hash":-1655137091,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"utility","start":4,"end":11,"id":1},{"text":"function","start":12,"end":20,"id":2},{"text":"encodes","start":21,"end":28,"id":3},{"text":"some","start":29,"end":33,"id":4},{"text":"a","start":34,"end":35,"id":5},{"text":"priori","start":36,"end":42,"id":6},{"text":"knowledge","start":43,"end":52,"id":7},{"text":"external","start":53,"end":61,"id":8},{"text":"to","start":62,"end":64,"id":9},{"text":"the","start":65,"end":68,"id":10},{"text":"system","start":69,"end":75,"id":11},{"text":",","start":75,"end":76,"id":12},{"text":"it","start":77,"end":79,"id":13},{"text":"quantifies","start":80,"end":90,"id":14},{"text":"how","start":91,"end":94,"id":15},{"text":"bad","start":95,"end":98,"id":16},{"text":"it","start":99,"end":101,"id":17},{"text":"is","start":102,"end":104,"id":18},{"text":"to","start":105,"end":107,"id":19},{"text":"make","start":108,"end":112,"id":20},{"text":"mistakes","start":113,"end":121,"id":21},{"text":".","start":121,"end":122,"id":22}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Along with these results, suggestions were also made about how to stabilise the bistatic-ATR per- formance with changing bistatic angle.","_input_hash":-1519966039,"_task_hash":580927981,"tokens":[{"text":"Along","start":0,"end":5,"id":0},{"text":"with","start":6,"end":10,"id":1},{"text":"these","start":11,"end":16,"id":2},{"text":"results","start":17,"end":24,"id":3},{"text":",","start":24,"end":25,"id":4},{"text":"suggestions","start":26,"end":37,"id":5},{"text":"were","start":38,"end":42,"id":6},{"text":"also","start":43,"end":47,"id":7},{"text":"made","start":48,"end":52,"id":8},{"text":"about","start":53,"end":58,"id":9},{"text":"how","start":59,"end":62,"id":10},{"text":"to","start":63,"end":65,"id":11},{"text":"stabilise","start":66,"end":75,"id":12},{"text":"the","start":76,"end":79,"id":13},{"text":"bistatic","start":80,"end":88,"id":14},{"text":"-","start":88,"end":89,"id":15},{"text":"ATR","start":89,"end":92,"id":16},{"text":"per-","start":93,"end":97,"id":17},{"text":"formance","start":98,"end":106,"id":18},{"text":"with","start":107,"end":111,"id":19},{"text":"changing","start":112,"end":120,"id":20},{"text":"bistatic","start":121,"end":129,"id":21},{"text":"angle","start":130,"end":135,"id":22},{"text":".","start":135,"end":136,"id":23}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"The current paper fills this conceptual gap by presenting a general theoretical framework showing that under appropriate conditions, the global solution of nonconvex regularization leads to desirable recovery performance;","_input_hash":-1590783748,"_task_hash":1689666088,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"current","start":4,"end":11,"id":1},{"text":"paper","start":12,"end":17,"id":2},{"text":"fills","start":18,"end":23,"id":3},{"text":"this","start":24,"end":28,"id":4},{"text":"conceptual","start":29,"end":39,"id":5},{"text":"gap","start":40,"end":43,"id":6},{"text":"by","start":44,"end":46,"id":7},{"text":"presenting","start":47,"end":57,"id":8},{"text":"a","start":58,"end":59,"id":9},{"text":"general","start":60,"end":67,"id":10},{"text":"theoretical","start":68,"end":79,"id":11},{"text":"framework","start":80,"end":89,"id":12},{"text":"showing","start":90,"end":97,"id":13},{"text":"that","start":98,"end":102,"id":14},{"text":"under","start":103,"end":108,"id":15},{"text":"appropriate","start":109,"end":120,"id":16},{"text":"conditions","start":121,"end":131,"id":17},{"text":",","start":131,"end":132,"id":18},{"text":"the","start":133,"end":136,"id":19},{"text":"global","start":137,"end":143,"id":20},{"text":"solution","start":144,"end":152,"id":21},{"text":"of","start":153,"end":155,"id":22},{"text":"nonconvex","start":156,"end":165,"id":23},{"text":"regularization","start":166,"end":180,"id":24},{"text":"leads","start":181,"end":186,"id":25},{"text":"to","start":187,"end":189,"id":26},{"text":"desirable","start":190,"end":199,"id":27},{"text":"recovery","start":200,"end":208,"id":28},{"text":"performance","start":209,"end":220,"id":29},{"text":";","start":220,"end":221,"id":30}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"In other words, the corresponding linear transformation will be invertible if we restrict its domain to sufficiently sparse vectors.","_input_hash":-232438733,"_task_hash":-1957509887,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"other","start":3,"end":8,"id":1},{"text":"words","start":9,"end":14,"id":2},{"text":",","start":14,"end":15,"id":3},{"text":"the","start":16,"end":19,"id":4},{"text":"corresponding","start":20,"end":33,"id":5},{"text":"linear","start":34,"end":40,"id":6},{"text":"transformation","start":41,"end":55,"id":7},{"text":"will","start":56,"end":60,"id":8},{"text":"be","start":61,"end":63,"id":9},{"text":"invertible","start":64,"end":74,"id":10},{"text":"if","start":75,"end":77,"id":11},{"text":"we","start":78,"end":80,"id":12},{"text":"restrict","start":81,"end":89,"id":13},{"text":"its","start":90,"end":93,"id":14},{"text":"domain","start":94,"end":100,"id":15},{"text":"to","start":101,"end":103,"id":16},{"text":"sufficiently","start":104,"end":116,"id":17},{"text":"sparse","start":117,"end":123,"id":18},{"text":"vectors","start":124,"end":131,"id":19},{"text":".","start":131,"end":132,"id":20}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"In this example, the PVM outperforms the highly successful 1-NN with tangent distance, and does so retaining fewer than half of the data points.","_input_hash":702980345,"_task_hash":1874151832,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"this","start":3,"end":7,"id":1},{"text":"example","start":8,"end":15,"id":2},{"text":",","start":15,"end":16,"id":3},{"text":"the","start":17,"end":20,"id":4},{"text":"PVM","start":21,"end":24,"id":5},{"text":"outperforms","start":25,"end":36,"id":6},{"text":"the","start":37,"end":40,"id":7},{"text":"highly","start":41,"end":47,"id":8},{"text":"successful","start":48,"end":58,"id":9},{"text":"1-NN","start":59,"end":63,"id":10},{"text":"with","start":64,"end":68,"id":11},{"text":"tangent","start":69,"end":76,"id":12},{"text":"distance","start":77,"end":85,"id":13},{"text":",","start":85,"end":86,"id":14},{"text":"and","start":87,"end":90,"id":15},{"text":"does","start":91,"end":95,"id":16},{"text":"so","start":96,"end":98,"id":17},{"text":"retaining","start":99,"end":108,"id":18},{"text":"fewer","start":109,"end":114,"id":19},{"text":"than","start":115,"end":119,"id":20},{"text":"half","start":120,"end":124,"id":21},{"text":"of","start":125,"end":127,"id":22},{"text":"the","start":128,"end":131,"id":23},{"text":"data","start":132,"end":136,"id":24},{"text":"points","start":137,"end":143,"id":25},{"text":".","start":143,"end":144,"id":26}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":21,"end":24,"token_start":5,"token_end":5,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"it has two merits.","_input_hash":926998138,"_task_hash":350605449,"tokens":[{"text":"it","start":0,"end":2,"id":0},{"text":"has","start":3,"end":6,"id":1},{"text":"two","start":7,"end":10,"id":2},{"text":"merits","start":11,"end":17,"id":3},{"text":".","start":17,"end":18,"id":4}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"The prior and conditional probabilities are expressed in terms of RKHS functions of an empirical sample:","_input_hash":-2062731311,"_task_hash":-1793643208,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"prior","start":4,"end":9,"id":1},{"text":"and","start":10,"end":13,"id":2},{"text":"conditional","start":14,"end":25,"id":3},{"text":"probabilities","start":26,"end":39,"id":4},{"text":"are","start":40,"end":43,"id":5},{"text":"expressed","start":44,"end":53,"id":6},{"text":"in","start":54,"end":56,"id":7},{"text":"terms","start":57,"end":62,"id":8},{"text":"of","start":63,"end":65,"id":9},{"text":"RKHS","start":66,"end":70,"id":10},{"text":"functions","start":71,"end":80,"id":11},{"text":"of","start":81,"end":83,"id":12},{"text":"an","start":84,"end":86,"id":13},{"text":"empirical","start":87,"end":96,"id":14},{"text":"sample","start":97,"end":103,"id":15},{"text":":","start":103,"end":104,"id":16}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We also show that under a weak dependency condition the number of discoveries is dominated by a Poisson random variable giving an asymptotic expression for the false positive rate.","_input_hash":540828036,"_task_hash":-1405808145,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"also","start":3,"end":7,"id":1},{"text":"show","start":8,"end":12,"id":2},{"text":"that","start":13,"end":17,"id":3},{"text":"under","start":18,"end":23,"id":4},{"text":"a","start":24,"end":25,"id":5},{"text":"weak","start":26,"end":30,"id":6},{"text":"dependency","start":31,"end":41,"id":7},{"text":"condition","start":42,"end":51,"id":8},{"text":"the","start":52,"end":55,"id":9},{"text":"number","start":56,"end":62,"id":10},{"text":"of","start":63,"end":65,"id":11},{"text":"discoveries","start":66,"end":77,"id":12},{"text":"is","start":78,"end":80,"id":13},{"text":"dominated","start":81,"end":90,"id":14},{"text":"by","start":91,"end":93,"id":15},{"text":"a","start":94,"end":95,"id":16},{"text":"Poisson","start":96,"end":103,"id":17},{"text":"random","start":104,"end":110,"id":18},{"text":"variable","start":111,"end":119,"id":19},{"text":"giving","start":120,"end":126,"id":20},{"text":"an","start":127,"end":129,"id":21},{"text":"asymptotic","start":130,"end":140,"id":22},{"text":"expression","start":141,"end":151,"id":23},{"text":"for","start":152,"end":155,"id":24},{"text":"the","start":156,"end":159,"id":25},{"text":"false","start":160,"end":165,"id":26},{"text":"positive","start":166,"end":174,"id":27},{"text":"rate","start":175,"end":179,"id":28},{"text":".","start":179,"end":180,"id":29}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"The number of parameters will increase exponentially with the order considered.","_input_hash":539571614,"_task_hash":-552580186,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"number","start":4,"end":10,"id":1},{"text":"of","start":11,"end":13,"id":2},{"text":"parameters","start":14,"end":24,"id":3},{"text":"will","start":25,"end":29,"id":4},{"text":"increase","start":30,"end":38,"id":5},{"text":"exponentially","start":39,"end":52,"id":6},{"text":"with","start":53,"end":57,"id":7},{"text":"the","start":58,"end":61,"id":8},{"text":"order","start":62,"end":67,"id":9},{"text":"considered","start":68,"end":78,"id":10},{"text":".","start":78,"end":79,"id":11}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Finally, we discuss the potential application of our results to sample compression schemes.","_input_hash":551131111,"_task_hash":-674286629,"tokens":[{"text":"Finally","start":0,"end":7,"id":0},{"text":",","start":7,"end":8,"id":1},{"text":"we","start":9,"end":11,"id":2},{"text":"discuss","start":12,"end":19,"id":3},{"text":"the","start":20,"end":23,"id":4},{"text":"potential","start":24,"end":33,"id":5},{"text":"application","start":34,"end":45,"id":6},{"text":"of","start":46,"end":48,"id":7},{"text":"our","start":49,"end":52,"id":8},{"text":"results","start":53,"end":60,"id":9},{"text":"to","start":61,"end":63,"id":10},{"text":"sample","start":64,"end":70,"id":11},{"text":"compression","start":71,"end":82,"id":12},{"text":"schemes","start":83,"end":90,"id":13},{"text":".","start":90,"end":91,"id":14}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"Based on these deliberations we propose an efficient new algorithm that is able to distinguish between cause and effect for a finite sample of discrete variables.","_input_hash":-1203683082,"_task_hash":785981195,"tokens":[{"text":"Based","start":0,"end":5,"id":0},{"text":"on","start":6,"end":8,"id":1},{"text":"these","start":9,"end":14,"id":2},{"text":"deliberations","start":15,"end":28,"id":3},{"text":"we","start":29,"end":31,"id":4},{"text":"propose","start":32,"end":39,"id":5},{"text":"an","start":40,"end":42,"id":6},{"text":"efficient","start":43,"end":52,"id":7},{"text":"new","start":53,"end":56,"id":8},{"text":"algorithm","start":57,"end":66,"id":9},{"text":"that","start":67,"end":71,"id":10},{"text":"is","start":72,"end":74,"id":11},{"text":"able","start":75,"end":79,"id":12},{"text":"to","start":80,"end":82,"id":13},{"text":"distinguish","start":83,"end":94,"id":14},{"text":"between","start":95,"end":102,"id":15},{"text":"cause","start":103,"end":108,"id":16},{"text":"and","start":109,"end":112,"id":17},{"text":"effect","start":113,"end":119,"id":18},{"text":"for","start":120,"end":123,"id":19},{"text":"a","start":124,"end":125,"id":20},{"text":"finite","start":126,"end":132,"id":21},{"text":"sample","start":133,"end":139,"id":22},{"text":"of","start":140,"end":142,"id":23},{"text":"discrete","start":143,"end":151,"id":24},{"text":"variables","start":152,"end":161,"id":25},{"text":".","start":161,"end":162,"id":26}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"we use the fitted model to predict response values for new documents.","_input_hash":262643015,"_task_hash":440893708,"tokens":[{"text":"we","start":0,"end":2,"id":0},{"text":"use","start":3,"end":6,"id":1},{"text":"the","start":7,"end":10,"id":2},{"text":"fitted","start":11,"end":17,"id":3},{"text":"model","start":18,"end":23,"id":4},{"text":"to","start":24,"end":26,"id":5},{"text":"predict","start":27,"end":34,"id":6},{"text":"response","start":35,"end":43,"id":7},{"text":"values","start":44,"end":50,"id":8},{"text":"for","start":51,"end":54,"id":9},{"text":"new","start":55,"end":58,"id":10},{"text":"documents","start":59,"end":68,"id":11},{"text":".","start":68,"end":69,"id":12}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We present a new boosting algorithm, motivated by the large margins theory for boosting.","_input_hash":1649373943,"_task_hash":-1117424888,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"present","start":3,"end":10,"id":1},{"text":"a","start":11,"end":12,"id":2},{"text":"new","start":13,"end":16,"id":3},{"text":"boosting","start":17,"end":25,"id":4},{"text":"algorithm","start":26,"end":35,"id":5},{"text":",","start":35,"end":36,"id":6},{"text":"motivated","start":37,"end":46,"id":7},{"text":"by","start":47,"end":49,"id":8},{"text":"the","start":50,"end":53,"id":9},{"text":"large","start":54,"end":59,"id":10},{"text":"margins","start":60,"end":67,"id":11},{"text":"theory","start":68,"end":74,"id":12},{"text":"for","start":75,"end":78,"id":13},{"text":"boosting","start":79,"end":87,"id":14},{"text":".","start":87,"end":88,"id":15}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":17,"end":25,"token_start":4,"token_end":4,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Distributions over permutations arise in applications ranging from multi-object tracking to ranking of instances.","_input_hash":-578156914,"_task_hash":860254932,"tokens":[{"text":"Distributions","start":0,"end":13,"id":0},{"text":"over","start":14,"end":18,"id":1},{"text":"permutations","start":19,"end":31,"id":2},{"text":"arise","start":32,"end":37,"id":3},{"text":"in","start":38,"end":40,"id":4},{"text":"applications","start":41,"end":53,"id":5},{"text":"ranging","start":54,"end":61,"id":6},{"text":"from","start":62,"end":66,"id":7},{"text":"multi","start":67,"end":72,"id":8},{"text":"-","start":72,"end":73,"id":9},{"text":"object","start":73,"end":79,"id":10},{"text":"tracking","start":80,"end":88,"id":11},{"text":"to","start":89,"end":91,"id":12},{"text":"ranking","start":92,"end":99,"id":13},{"text":"of","start":100,"end":102,"id":14},{"text":"instances","start":103,"end":112,"id":15},{"text":".","start":112,"end":113,"id":16}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We analyze when different initializations lead to the same local optimum, and when they lead to different local optima.","_input_hash":1958327346,"_task_hash":-2001187100,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"analyze","start":3,"end":10,"id":1},{"text":"when","start":11,"end":15,"id":2},{"text":"different","start":16,"end":25,"id":3},{"text":"initializations","start":26,"end":41,"id":4},{"text":"lead","start":42,"end":46,"id":5},{"text":"to","start":47,"end":49,"id":6},{"text":"the","start":50,"end":53,"id":7},{"text":"same","start":54,"end":58,"id":8},{"text":"local","start":59,"end":64,"id":9},{"text":"optimum","start":65,"end":72,"id":10},{"text":",","start":72,"end":73,"id":11},{"text":"and","start":74,"end":77,"id":12},{"text":"when","start":78,"end":82,"id":13},{"text":"they","start":83,"end":87,"id":14},{"text":"lead","start":88,"end":92,"id":15},{"text":"to","start":93,"end":95,"id":16},{"text":"different","start":96,"end":105,"id":17},{"text":"local","start":106,"end":111,"id":18},{"text":"optima","start":112,"end":118,"id":19},{"text":".","start":118,"end":119,"id":20}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"However, a fast online approximation is possible.","_input_hash":-1349805728,"_task_hash":2002389883,"tokens":[{"text":"However","start":0,"end":7,"id":0},{"text":",","start":7,"end":8,"id":1},{"text":"a","start":9,"end":10,"id":2},{"text":"fast","start":11,"end":15,"id":3},{"text":"online","start":16,"end":22,"id":4},{"text":"approximation","start":23,"end":36,"id":5},{"text":"is","start":37,"end":39,"id":6},{"text":"possible","start":40,"end":48,"id":7},{"text":".","start":48,"end":49,"id":8}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"LLE first linearly reconstructs each input point from its nearest neighbors and then preserves these neighborhood relations in the low-dimensional embedding.","_input_hash":-258775660,"_task_hash":556806686,"tokens":[{"text":"LLE","start":0,"end":3,"id":0},{"text":"first","start":4,"end":9,"id":1},{"text":"linearly","start":10,"end":18,"id":2},{"text":"reconstructs","start":19,"end":31,"id":3},{"text":"each","start":32,"end":36,"id":4},{"text":"input","start":37,"end":42,"id":5},{"text":"point","start":43,"end":48,"id":6},{"text":"from","start":49,"end":53,"id":7},{"text":"its","start":54,"end":57,"id":8},{"text":"nearest","start":58,"end":65,"id":9},{"text":"neighbors","start":66,"end":75,"id":10},{"text":"and","start":76,"end":79,"id":11},{"text":"then","start":80,"end":84,"id":12},{"text":"preserves","start":85,"end":94,"id":13},{"text":"these","start":95,"end":100,"id":14},{"text":"neighborhood","start":101,"end":113,"id":15},{"text":"relations","start":114,"end":123,"id":16},{"text":"in","start":124,"end":126,"id":17},{"text":"the","start":127,"end":130,"id":18},{"text":"low","start":131,"end":134,"id":19},{"text":"-","start":134,"end":135,"id":20},{"text":"dimensional","start":135,"end":146,"id":21},{"text":"embedding","start":147,"end":156,"id":22},{"text":".","start":156,"end":157,"id":23}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":0,"end":3,"token_start":0,"token_end":0,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Second, we apply our results to the analysis of ERG models.","_input_hash":-1181044587,"_task_hash":215651923,"tokens":[{"text":"Second","start":0,"end":6,"id":0},{"text":",","start":6,"end":7,"id":1},{"text":"we","start":8,"end":10,"id":2},{"text":"apply","start":11,"end":16,"id":3},{"text":"our","start":17,"end":20,"id":4},{"text":"results","start":21,"end":28,"id":5},{"text":"to","start":29,"end":31,"id":6},{"text":"the","start":32,"end":35,"id":7},{"text":"analysis","start":36,"end":44,"id":8},{"text":"of","start":45,"end":47,"id":9},{"text":"ERG","start":48,"end":51,"id":10},{"text":"models","start":52,"end":58,"id":11},{"text":".","start":58,"end":59,"id":12}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We compute the probability distribution of the length of the current ``run,'' or time since the last changepoint, using a simple message-passing algorithm.","_input_hash":-2035327106,"_task_hash":2044654912,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"compute","start":3,"end":10,"id":1},{"text":"the","start":11,"end":14,"id":2},{"text":"probability","start":15,"end":26,"id":3},{"text":"distribution","start":27,"end":39,"id":4},{"text":"of","start":40,"end":42,"id":5},{"text":"the","start":43,"end":46,"id":6},{"text":"length","start":47,"end":53,"id":7},{"text":"of","start":54,"end":56,"id":8},{"text":"the","start":57,"end":60,"id":9},{"text":"current","start":61,"end":68,"id":10},{"text":"`","start":69,"end":70,"id":11},{"text":"`","start":70,"end":71,"id":12},{"text":"run","start":71,"end":74,"id":13},{"text":",","start":74,"end":75,"id":14},{"text":"'","start":75,"end":76,"id":15},{"text":"'","start":76,"end":77,"id":16},{"text":"or","start":78,"end":80,"id":17},{"text":"time","start":81,"end":85,"id":18},{"text":"since","start":86,"end":91,"id":19},{"text":"the","start":92,"end":95,"id":20},{"text":"last","start":96,"end":100,"id":21},{"text":"changepoint","start":101,"end":112,"id":22},{"text":",","start":112,"end":113,"id":23},{"text":"using","start":114,"end":119,"id":24},{"text":"a","start":120,"end":121,"id":25},{"text":"simple","start":122,"end":128,"id":26},{"text":"message","start":129,"end":136,"id":27},{"text":"-","start":136,"end":137,"id":28},{"text":"passing","start":137,"end":144,"id":29},{"text":"algorithm","start":145,"end":154,"id":30},{"text":".","start":154,"end":155,"id":31}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":129,"end":144,"token_start":27,"token_end":29,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"In most cases, the produced models had better prediction performance than, for example, the ones produced by the random forest or the rulefit algorithms.","_input_hash":-201210344,"_task_hash":-58260578,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"most","start":3,"end":7,"id":1},{"text":"cases","start":8,"end":13,"id":2},{"text":",","start":13,"end":14,"id":3},{"text":"the","start":15,"end":18,"id":4},{"text":"produced","start":19,"end":27,"id":5},{"text":"models","start":28,"end":34,"id":6},{"text":"had","start":35,"end":38,"id":7},{"text":"better","start":39,"end":45,"id":8},{"text":"prediction","start":46,"end":56,"id":9},{"text":"performance","start":57,"end":68,"id":10},{"text":"than","start":69,"end":73,"id":11},{"text":",","start":73,"end":74,"id":12},{"text":"for","start":75,"end":78,"id":13},{"text":"example","start":79,"end":86,"id":14},{"text":",","start":86,"end":87,"id":15},{"text":"the","start":88,"end":91,"id":16},{"text":"ones","start":92,"end":96,"id":17},{"text":"produced","start":97,"end":105,"id":18},{"text":"by","start":106,"end":108,"id":19},{"text":"the","start":109,"end":112,"id":20},{"text":"random","start":113,"end":119,"id":21},{"text":"forest","start":120,"end":126,"id":22},{"text":"or","start":127,"end":129,"id":23},{"text":"the","start":130,"end":133,"id":24},{"text":"rulefit","start":134,"end":141,"id":25},{"text":"algorithms","start":142,"end":152,"id":26},{"text":".","start":152,"end":153,"id":27}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":113,"end":126,"token_start":21,"token_end":22,"label":"ALGO","answer":"accept"},{"start":134,"end":141,"token_start":25,"token_end":25,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"This leads to a specific set of allowed nonzero patterns for the solutions of such problems.","_input_hash":-468147957,"_task_hash":-1296230655,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"leads","start":5,"end":10,"id":1},{"text":"to","start":11,"end":13,"id":2},{"text":"a","start":14,"end":15,"id":3},{"text":"specific","start":16,"end":24,"id":4},{"text":"set","start":25,"end":28,"id":5},{"text":"of","start":29,"end":31,"id":6},{"text":"allowed","start":32,"end":39,"id":7},{"text":"nonzero","start":40,"end":47,"id":8},{"text":"patterns","start":48,"end":56,"id":9},{"text":"for","start":57,"end":60,"id":10},{"text":"the","start":61,"end":64,"id":11},{"text":"solutions","start":65,"end":74,"id":12},{"text":"of","start":75,"end":77,"id":13},{"text":"such","start":78,"end":82,"id":14},{"text":"problems","start":83,"end":91,"id":15},{"text":".","start":91,"end":92,"id":16}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"Compared with the usual visualization, which simply lists the most probable topical terms, the multi-word expressions provide a better intuitive impression for what a topic is \"about.\"","_input_hash":-2034570161,"_task_hash":-885326438,"tokens":[{"text":"Compared","start":0,"end":8,"id":0},{"text":"with","start":9,"end":13,"id":1},{"text":"the","start":14,"end":17,"id":2},{"text":"usual","start":18,"end":23,"id":3},{"text":"visualization","start":24,"end":37,"id":4},{"text":",","start":37,"end":38,"id":5},{"text":"which","start":39,"end":44,"id":6},{"text":"simply","start":45,"end":51,"id":7},{"text":"lists","start":52,"end":57,"id":8},{"text":"the","start":58,"end":61,"id":9},{"text":"most","start":62,"end":66,"id":10},{"text":"probable","start":67,"end":75,"id":11},{"text":"topical","start":76,"end":83,"id":12},{"text":"terms","start":84,"end":89,"id":13},{"text":",","start":89,"end":90,"id":14},{"text":"the","start":91,"end":94,"id":15},{"text":"multi","start":95,"end":100,"id":16},{"text":"-","start":100,"end":101,"id":17},{"text":"word","start":101,"end":105,"id":18},{"text":"expressions","start":106,"end":117,"id":19},{"text":"provide","start":118,"end":125,"id":20},{"text":"a","start":126,"end":127,"id":21},{"text":"better","start":128,"end":134,"id":22},{"text":"intuitive","start":135,"end":144,"id":23},{"text":"impression","start":145,"end":155,"id":24},{"text":"for","start":156,"end":159,"id":25},{"text":"what","start":160,"end":164,"id":26},{"text":"a","start":165,"end":166,"id":27},{"text":"topic","start":167,"end":172,"id":28},{"text":"is","start":173,"end":175,"id":29},{"text":"\"","start":176,"end":177,"id":30},{"text":"about","start":177,"end":182,"id":31},{"text":".","start":182,"end":183,"id":32},{"text":"\"","start":183,"end":184,"id":33}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Second, on a more practical side, we check the behavior of the k-NN method when compared with a few other functional classifiers.","_input_hash":1170064707,"_task_hash":-315442763,"tokens":[{"text":"Second","start":0,"end":6,"id":0},{"text":",","start":6,"end":7,"id":1},{"text":"on","start":8,"end":10,"id":2},{"text":"a","start":11,"end":12,"id":3},{"text":"more","start":13,"end":17,"id":4},{"text":"practical","start":18,"end":27,"id":5},{"text":"side","start":28,"end":32,"id":6},{"text":",","start":32,"end":33,"id":7},{"text":"we","start":34,"end":36,"id":8},{"text":"check","start":37,"end":42,"id":9},{"text":"the","start":43,"end":46,"id":10},{"text":"behavior","start":47,"end":55,"id":11},{"text":"of","start":56,"end":58,"id":12},{"text":"the","start":59,"end":62,"id":13},{"text":"k","start":63,"end":64,"id":14},{"text":"-","start":64,"end":65,"id":15},{"text":"NN","start":65,"end":67,"id":16},{"text":"method","start":68,"end":74,"id":17},{"text":"when","start":75,"end":79,"id":18},{"text":"compared","start":80,"end":88,"id":19},{"text":"with","start":89,"end":93,"id":20},{"text":"a","start":94,"end":95,"id":21},{"text":"few","start":96,"end":99,"id":22},{"text":"other","start":100,"end":105,"id":23},{"text":"functional","start":106,"end":116,"id":24},{"text":"classifiers","start":117,"end":128,"id":25},{"text":".","start":128,"end":129,"id":26}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":63,"end":67,"token_start":14,"token_end":16,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"It is unclear whether this formal sure screening property is attainable when the response is a right-censored survival time.","_input_hash":2038335871,"_task_hash":-113303011,"tokens":[{"text":"It","start":0,"end":2,"id":0},{"text":"is","start":3,"end":5,"id":1},{"text":"unclear","start":6,"end":13,"id":2},{"text":"whether","start":14,"end":21,"id":3},{"text":"this","start":22,"end":26,"id":4},{"text":"formal","start":27,"end":33,"id":5},{"text":"sure","start":34,"end":38,"id":6},{"text":"screening","start":39,"end":48,"id":7},{"text":"property","start":49,"end":57,"id":8},{"text":"is","start":58,"end":60,"id":9},{"text":"attainable","start":61,"end":71,"id":10},{"text":"when","start":72,"end":76,"id":11},{"text":"the","start":77,"end":80,"id":12},{"text":"response","start":81,"end":89,"id":13},{"text":"is","start":90,"end":92,"id":14},{"text":"a","start":93,"end":94,"id":15},{"text":"right","start":95,"end":100,"id":16},{"text":"-","start":100,"end":101,"id":17},{"text":"censored","start":101,"end":109,"id":18},{"text":"survival","start":110,"end":118,"id":19},{"text":"time","start":119,"end":123,"id":20},{"text":".","start":123,"end":124,"id":21}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"The meta-learning is the process of combining outcomes of individual learning procedures in order to determine the final decision with higher accuracy than any single learning method.","_input_hash":-172294891,"_task_hash":707659187,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"meta","start":4,"end":8,"id":1},{"text":"-","start":8,"end":9,"id":2},{"text":"learning","start":9,"end":17,"id":3},{"text":"is","start":18,"end":20,"id":4},{"text":"the","start":21,"end":24,"id":5},{"text":"process","start":25,"end":32,"id":6},{"text":"of","start":33,"end":35,"id":7},{"text":"combining","start":36,"end":45,"id":8},{"text":"outcomes","start":46,"end":54,"id":9},{"text":"of","start":55,"end":57,"id":10},{"text":"individual","start":58,"end":68,"id":11},{"text":"learning","start":69,"end":77,"id":12},{"text":"procedures","start":78,"end":88,"id":13},{"text":"in","start":89,"end":91,"id":14},{"text":"order","start":92,"end":97,"id":15},{"text":"to","start":98,"end":100,"id":16},{"text":"determine","start":101,"end":110,"id":17},{"text":"the","start":111,"end":114,"id":18},{"text":"final","start":115,"end":120,"id":19},{"text":"decision","start":121,"end":129,"id":20},{"text":"with","start":130,"end":134,"id":21},{"text":"higher","start":135,"end":141,"id":22},{"text":"accuracy","start":142,"end":150,"id":23},{"text":"than","start":151,"end":155,"id":24},{"text":"any","start":156,"end":159,"id":25},{"text":"single","start":160,"end":166,"id":26},{"text":"learning","start":167,"end":175,"id":27},{"text":"method","start":176,"end":182,"id":28},{"text":".","start":182,"end":183,"id":29}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Using the Bessel K model, we derive a sparse estimator based on a modification of the expectation-maximization algorithm formulated for Type II estimation.","_input_hash":1921331652,"_task_hash":-1987007377,"tokens":[{"text":"Using","start":0,"end":5,"id":0},{"text":"the","start":6,"end":9,"id":1},{"text":"Bessel","start":10,"end":16,"id":2},{"text":"K","start":17,"end":18,"id":3},{"text":"model","start":19,"end":24,"id":4},{"text":",","start":24,"end":25,"id":5},{"text":"we","start":26,"end":28,"id":6},{"text":"derive","start":29,"end":35,"id":7},{"text":"a","start":36,"end":37,"id":8},{"text":"sparse","start":38,"end":44,"id":9},{"text":"estimator","start":45,"end":54,"id":10},{"text":"based","start":55,"end":60,"id":11},{"text":"on","start":61,"end":63,"id":12},{"text":"a","start":64,"end":65,"id":13},{"text":"modification","start":66,"end":78,"id":14},{"text":"of","start":79,"end":81,"id":15},{"text":"the","start":82,"end":85,"id":16},{"text":"expectation","start":86,"end":97,"id":17},{"text":"-","start":97,"end":98,"id":18},{"text":"maximization","start":98,"end":110,"id":19},{"text":"algorithm","start":111,"end":120,"id":20},{"text":"formulated","start":121,"end":131,"id":21},{"text":"for","start":132,"end":135,"id":22},{"text":"Type","start":136,"end":140,"id":23},{"text":"II","start":141,"end":143,"id":24},{"text":"estimation","start":144,"end":154,"id":25},{"text":".","start":154,"end":155,"id":26}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":10,"end":18,"token_start":2,"token_end":3,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"In this work, we argue that a preferable approach relies instead on a nonparametric guidance mechanism.","_input_hash":-1516692579,"_task_hash":-643555040,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"this","start":3,"end":7,"id":1},{"text":"work","start":8,"end":12,"id":2},{"text":",","start":12,"end":13,"id":3},{"text":"we","start":14,"end":16,"id":4},{"text":"argue","start":17,"end":22,"id":5},{"text":"that","start":23,"end":27,"id":6},{"text":"a","start":28,"end":29,"id":7},{"text":"preferable","start":30,"end":40,"id":8},{"text":"approach","start":41,"end":49,"id":9},{"text":"relies","start":50,"end":56,"id":10},{"text":"instead","start":57,"end":64,"id":11},{"text":"on","start":65,"end":67,"id":12},{"text":"a","start":68,"end":69,"id":13},{"text":"nonparametric","start":70,"end":83,"id":14},{"text":"guidance","start":84,"end":92,"id":15},{"text":"mechanism","start":93,"end":102,"id":16},{"text":".","start":102,"end":103,"id":17}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We improve recently published results about resources of Restricted Boltzmann Machines (RBM) and Deep Belief Networks (DBN) required to make them Universal Approximators.","_input_hash":-2080777763,"_task_hash":1828853774,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"improve","start":3,"end":10,"id":1},{"text":"recently","start":11,"end":19,"id":2},{"text":"published","start":20,"end":29,"id":3},{"text":"results","start":30,"end":37,"id":4},{"text":"about","start":38,"end":43,"id":5},{"text":"resources","start":44,"end":53,"id":6},{"text":"of","start":54,"end":56,"id":7},{"text":"Restricted","start":57,"end":67,"id":8},{"text":"Boltzmann","start":68,"end":77,"id":9},{"text":"Machines","start":78,"end":86,"id":10},{"text":"(","start":87,"end":88,"id":11},{"text":"RBM","start":88,"end":91,"id":12},{"text":")","start":91,"end":92,"id":13},{"text":"and","start":93,"end":96,"id":14},{"text":"Deep","start":97,"end":101,"id":15},{"text":"Belief","start":102,"end":108,"id":16},{"text":"Networks","start":109,"end":117,"id":17},{"text":"(","start":118,"end":119,"id":18},{"text":"DBN","start":119,"end":122,"id":19},{"text":")","start":122,"end":123,"id":20},{"text":"required","start":124,"end":132,"id":21},{"text":"to","start":133,"end":135,"id":22},{"text":"make","start":136,"end":140,"id":23},{"text":"them","start":141,"end":145,"id":24},{"text":"Universal","start":146,"end":155,"id":25},{"text":"Approximators","start":156,"end":169,"id":26},{"text":".","start":169,"end":170,"id":27}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":57,"end":86,"token_start":8,"token_end":10,"label":"ALGO","answer":"accept"},{"start":97,"end":117,"token_start":15,"token_end":17,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"While their approach behaves well in simulations, it could not be used to bound the generalization risk of classifiers, because there were no bounds for the estimation error of the VC dimension itself.","_input_hash":769902862,"_task_hash":1523034090,"tokens":[{"text":"While","start":0,"end":5,"id":0},{"text":"their","start":6,"end":11,"id":1},{"text":"approach","start":12,"end":20,"id":2},{"text":"behaves","start":21,"end":28,"id":3},{"text":"well","start":29,"end":33,"id":4},{"text":"in","start":34,"end":36,"id":5},{"text":"simulations","start":37,"end":48,"id":6},{"text":",","start":48,"end":49,"id":7},{"text":"it","start":50,"end":52,"id":8},{"text":"could","start":53,"end":58,"id":9},{"text":"not","start":59,"end":62,"id":10},{"text":"be","start":63,"end":65,"id":11},{"text":"used","start":66,"end":70,"id":12},{"text":"to","start":71,"end":73,"id":13},{"text":"bound","start":74,"end":79,"id":14},{"text":"the","start":80,"end":83,"id":15},{"text":"generalization","start":84,"end":98,"id":16},{"text":"risk","start":99,"end":103,"id":17},{"text":"of","start":104,"end":106,"id":18},{"text":"classifiers","start":107,"end":118,"id":19},{"text":",","start":118,"end":119,"id":20},{"text":"because","start":120,"end":127,"id":21},{"text":"there","start":128,"end":133,"id":22},{"text":"were","start":134,"end":138,"id":23},{"text":"no","start":139,"end":141,"id":24},{"text":"bounds","start":142,"end":148,"id":25},{"text":"for","start":149,"end":152,"id":26},{"text":"the","start":153,"end":156,"id":27},{"text":"estimation","start":157,"end":167,"id":28},{"text":"error","start":168,"end":173,"id":29},{"text":"of","start":174,"end":176,"id":30},{"text":"the","start":177,"end":180,"id":31},{"text":"VC","start":181,"end":183,"id":32},{"text":"dimension","start":184,"end":193,"id":33},{"text":"itself","start":194,"end":200,"id":34},{"text":".","start":200,"end":201,"id":35}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":181,"end":193,"token_start":32,"token_end":33,"label":"ALGO","answer":"reject"}],"answer":"reject"}
{"text":"Given some of the strategic advantages for a bistatic configuration, and tech- nological advances in the past few years, large-scale implementation of the bistatic systems is a scope for the near future.","_input_hash":95905853,"_task_hash":-1625320139,"tokens":[{"text":"Given","start":0,"end":5,"id":0},{"text":"some","start":6,"end":10,"id":1},{"text":"of","start":11,"end":13,"id":2},{"text":"the","start":14,"end":17,"id":3},{"text":"strategic","start":18,"end":27,"id":4},{"text":"advantages","start":28,"end":38,"id":5},{"text":"for","start":39,"end":42,"id":6},{"text":"a","start":43,"end":44,"id":7},{"text":"bistatic","start":45,"end":53,"id":8},{"text":"configuration","start":54,"end":67,"id":9},{"text":",","start":67,"end":68,"id":10},{"text":"and","start":69,"end":72,"id":11},{"text":"tech-","start":73,"end":78,"id":12},{"text":"nological","start":79,"end":88,"id":13},{"text":"advances","start":89,"end":97,"id":14},{"text":"in","start":98,"end":100,"id":15},{"text":"the","start":101,"end":104,"id":16},{"text":"past","start":105,"end":109,"id":17},{"text":"few","start":110,"end":113,"id":18},{"text":"years","start":114,"end":119,"id":19},{"text":",","start":119,"end":120,"id":20},{"text":"large","start":121,"end":126,"id":21},{"text":"-","start":126,"end":127,"id":22},{"text":"scale","start":127,"end":132,"id":23},{"text":"implementation","start":133,"end":147,"id":24},{"text":"of","start":148,"end":150,"id":25},{"text":"the","start":151,"end":154,"id":26},{"text":"bistatic","start":155,"end":163,"id":27},{"text":"systems","start":164,"end":171,"id":28},{"text":"is","start":172,"end":174,"id":29},{"text":"a","start":175,"end":176,"id":30},{"text":"scope","start":177,"end":182,"id":31},{"text":"for","start":183,"end":186,"id":32},{"text":"the","start":187,"end":190,"id":33},{"text":"near","start":191,"end":195,"id":34},{"text":"future","start":196,"end":202,"id":35},{"text":".","start":202,"end":203,"id":36}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"A visualization tool of this kind is useful in signal processing and machine learning whenever learning/adaptation algorithms insist on high-dimensional parameter manifolds.","_input_hash":-475828779,"_task_hash":1723077357,"tokens":[{"text":"A","start":0,"end":1,"id":0},{"text":"visualization","start":2,"end":15,"id":1},{"text":"tool","start":16,"end":20,"id":2},{"text":"of","start":21,"end":23,"id":3},{"text":"this","start":24,"end":28,"id":4},{"text":"kind","start":29,"end":33,"id":5},{"text":"is","start":34,"end":36,"id":6},{"text":"useful","start":37,"end":43,"id":7},{"text":"in","start":44,"end":46,"id":8},{"text":"signal","start":47,"end":53,"id":9},{"text":"processing","start":54,"end":64,"id":10},{"text":"and","start":65,"end":68,"id":11},{"text":"machine","start":69,"end":76,"id":12},{"text":"learning","start":77,"end":85,"id":13},{"text":"whenever","start":86,"end":94,"id":14},{"text":"learning","start":95,"end":103,"id":15},{"text":"/","start":103,"end":104,"id":16},{"text":"adaptation","start":104,"end":114,"id":17},{"text":"algorithms","start":115,"end":125,"id":18},{"text":"insist","start":126,"end":132,"id":19},{"text":"on","start":133,"end":135,"id":20},{"text":"high","start":136,"end":140,"id":21},{"text":"-","start":140,"end":141,"id":22},{"text":"dimensional","start":141,"end":152,"id":23},{"text":"parameter","start":153,"end":162,"id":24},{"text":"manifolds","start":163,"end":172,"id":25},{"text":".","start":172,"end":173,"id":26}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Numerical simulations highlight the benefit of structured sparsity and the advantage offered by our approach over the Lasso method and other related methods.","_input_hash":-1360195889,"_task_hash":-786543207,"tokens":[{"text":"Numerical","start":0,"end":9,"id":0},{"text":"simulations","start":10,"end":21,"id":1},{"text":"highlight","start":22,"end":31,"id":2},{"text":"the","start":32,"end":35,"id":3},{"text":"benefit","start":36,"end":43,"id":4},{"text":"of","start":44,"end":46,"id":5},{"text":"structured","start":47,"end":57,"id":6},{"text":"sparsity","start":58,"end":66,"id":7},{"text":"and","start":67,"end":70,"id":8},{"text":"the","start":71,"end":74,"id":9},{"text":"advantage","start":75,"end":84,"id":10},{"text":"offered","start":85,"end":92,"id":11},{"text":"by","start":93,"end":95,"id":12},{"text":"our","start":96,"end":99,"id":13},{"text":"approach","start":100,"end":108,"id":14},{"text":"over","start":109,"end":113,"id":15},{"text":"the","start":114,"end":117,"id":16},{"text":"Lasso","start":118,"end":123,"id":17},{"text":"method","start":124,"end":130,"id":18},{"text":"and","start":131,"end":134,"id":19},{"text":"other","start":135,"end":140,"id":20},{"text":"related","start":141,"end":148,"id":21},{"text":"methods","start":149,"end":156,"id":22},{"text":".","start":156,"end":157,"id":23}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":118,"end":123,"token_start":17,"token_end":17,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"To our knowledge, this is the first attempt to utilize relatedness of multiple outputs to perform fast screening of relevant variables.","_input_hash":-238012842,"_task_hash":1453907067,"tokens":[{"text":"To","start":0,"end":2,"id":0},{"text":"our","start":3,"end":6,"id":1},{"text":"knowledge","start":7,"end":16,"id":2},{"text":",","start":16,"end":17,"id":3},{"text":"this","start":18,"end":22,"id":4},{"text":"is","start":23,"end":25,"id":5},{"text":"the","start":26,"end":29,"id":6},{"text":"first","start":30,"end":35,"id":7},{"text":"attempt","start":36,"end":43,"id":8},{"text":"to","start":44,"end":46,"id":9},{"text":"utilize","start":47,"end":54,"id":10},{"text":"relatedness","start":55,"end":66,"id":11},{"text":"of","start":67,"end":69,"id":12},{"text":"multiple","start":70,"end":78,"id":13},{"text":"outputs","start":79,"end":86,"id":14},{"text":"to","start":87,"end":89,"id":15},{"text":"perform","start":90,"end":97,"id":16},{"text":"fast","start":98,"end":102,"id":17},{"text":"screening","start":103,"end":112,"id":18},{"text":"of","start":113,"end":115,"id":19},{"text":"relevant","start":116,"end":124,"id":20},{"text":"variables","start":125,"end":134,"id":21},{"text":".","start":134,"end":135,"id":22}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We model the contribution of each source to all mixture channels in the time-frequency domain as a zero-mean Gaussian random variable whose covariance encodes the spatial characteristics of the source.","_input_hash":-1097143795,"_task_hash":-1929968510,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"model","start":3,"end":8,"id":1},{"text":"the","start":9,"end":12,"id":2},{"text":"contribution","start":13,"end":25,"id":3},{"text":"of","start":26,"end":28,"id":4},{"text":"each","start":29,"end":33,"id":5},{"text":"source","start":34,"end":40,"id":6},{"text":"to","start":41,"end":43,"id":7},{"text":"all","start":44,"end":47,"id":8},{"text":"mixture","start":48,"end":55,"id":9},{"text":"channels","start":56,"end":64,"id":10},{"text":"in","start":65,"end":67,"id":11},{"text":"the","start":68,"end":71,"id":12},{"text":"time","start":72,"end":76,"id":13},{"text":"-","start":76,"end":77,"id":14},{"text":"frequency","start":77,"end":86,"id":15},{"text":"domain","start":87,"end":93,"id":16},{"text":"as","start":94,"end":96,"id":17},{"text":"a","start":97,"end":98,"id":18},{"text":"zero","start":99,"end":103,"id":19},{"text":"-","start":103,"end":104,"id":20},{"text":"mean","start":104,"end":108,"id":21},{"text":"Gaussian","start":109,"end":117,"id":22},{"text":"random","start":118,"end":124,"id":23},{"text":"variable","start":125,"end":133,"id":24},{"text":"whose","start":134,"end":139,"id":25},{"text":"covariance","start":140,"end":150,"id":26},{"text":"encodes","start":151,"end":158,"id":27},{"text":"the","start":159,"end":162,"id":28},{"text":"spatial","start":163,"end":170,"id":29},{"text":"characteristics","start":171,"end":186,"id":30},{"text":"of","start":187,"end":189,"id":31},{"text":"the","start":190,"end":193,"id":32},{"text":"source","start":194,"end":200,"id":33},{"text":".","start":200,"end":201,"id":34}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Existing approaches to analyzing the asymptotics of graph Laplacians typically assume a well-behaved kernel function with smoothness assumptions.","_input_hash":1648810887,"_task_hash":216872705,"tokens":[{"text":"Existing","start":0,"end":8,"id":0},{"text":"approaches","start":9,"end":19,"id":1},{"text":"to","start":20,"end":22,"id":2},{"text":"analyzing","start":23,"end":32,"id":3},{"text":"the","start":33,"end":36,"id":4},{"text":"asymptotics","start":37,"end":48,"id":5},{"text":"of","start":49,"end":51,"id":6},{"text":"graph","start":52,"end":57,"id":7},{"text":"Laplacians","start":58,"end":68,"id":8},{"text":"typically","start":69,"end":78,"id":9},{"text":"assume","start":79,"end":85,"id":10},{"text":"a","start":86,"end":87,"id":11},{"text":"well","start":88,"end":92,"id":12},{"text":"-","start":92,"end":93,"id":13},{"text":"behaved","start":93,"end":100,"id":14},{"text":"kernel","start":101,"end":107,"id":15},{"text":"function","start":108,"end":116,"id":16},{"text":"with","start":117,"end":121,"id":17},{"text":"smoothness","start":122,"end":132,"id":18},{"text":"assumptions","start":133,"end":144,"id":19},{"text":".","start":144,"end":145,"id":20}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We consider the problem of learning a forest of nonlinear decision rules with general loss functions.","_input_hash":1963655993,"_task_hash":304801715,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"consider","start":3,"end":11,"id":1},{"text":"the","start":12,"end":15,"id":2},{"text":"problem","start":16,"end":23,"id":3},{"text":"of","start":24,"end":26,"id":4},{"text":"learning","start":27,"end":35,"id":5},{"text":"a","start":36,"end":37,"id":6},{"text":"forest","start":38,"end":44,"id":7},{"text":"of","start":45,"end":47,"id":8},{"text":"nonlinear","start":48,"end":57,"id":9},{"text":"decision","start":58,"end":66,"id":10},{"text":"rules","start":67,"end":72,"id":11},{"text":"with","start":73,"end":77,"id":12},{"text":"general","start":78,"end":85,"id":13},{"text":"loss","start":86,"end":90,"id":14},{"text":"functions","start":91,"end":100,"id":15},{"text":".","start":100,"end":101,"id":16}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":38,"end":44,"token_start":7,"token_end":7,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Our method achieves higher accuracy and smaller models than gradient boosting (and Adaboost with exponential loss) on many datasets.","_input_hash":2000539278,"_task_hash":1964994087,"tokens":[{"text":"Our","start":0,"end":3,"id":0},{"text":"method","start":4,"end":10,"id":1},{"text":"achieves","start":11,"end":19,"id":2},{"text":"higher","start":20,"end":26,"id":3},{"text":"accuracy","start":27,"end":35,"id":4},{"text":"and","start":36,"end":39,"id":5},{"text":"smaller","start":40,"end":47,"id":6},{"text":"models","start":48,"end":54,"id":7},{"text":"than","start":55,"end":59,"id":8},{"text":"gradient","start":60,"end":68,"id":9},{"text":"boosting","start":69,"end":77,"id":10},{"text":"(","start":78,"end":79,"id":11},{"text":"and","start":79,"end":82,"id":12},{"text":"Adaboost","start":83,"end":91,"id":13},{"text":"with","start":92,"end":96,"id":14},{"text":"exponential","start":97,"end":108,"id":15},{"text":"loss","start":109,"end":113,"id":16},{"text":")","start":113,"end":114,"id":17},{"text":"on","start":115,"end":117,"id":18},{"text":"many","start":118,"end":122,"id":19},{"text":"datasets","start":123,"end":131,"id":20},{"text":".","start":131,"end":132,"id":21}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":60,"end":77,"token_start":9,"token_end":10,"label":"ALGO","answer":"accept"},{"start":83,"end":91,"token_start":13,"token_end":13,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Last, the polarimetric data from bistatic scattering were found to have distinct information, contrary to expert opinions.","_input_hash":430156641,"_task_hash":-1829141188,"tokens":[{"text":"Last","start":0,"end":4,"id":0},{"text":",","start":4,"end":5,"id":1},{"text":"the","start":6,"end":9,"id":2},{"text":"polarimetric","start":10,"end":22,"id":3},{"text":"data","start":23,"end":27,"id":4},{"text":"from","start":28,"end":32,"id":5},{"text":"bistatic","start":33,"end":41,"id":6},{"text":"scattering","start":42,"end":52,"id":7},{"text":"were","start":53,"end":57,"id":8},{"text":"found","start":58,"end":63,"id":9},{"text":"to","start":64,"end":66,"id":10},{"text":"have","start":67,"end":71,"id":11},{"text":"distinct","start":72,"end":80,"id":12},{"text":"information","start":81,"end":92,"id":13},{"text":",","start":92,"end":93,"id":14},{"text":"contrary","start":94,"end":102,"id":15},{"text":"to","start":103,"end":105,"id":16},{"text":"expert","start":106,"end":112,"id":17},{"text":"opinions","start":113,"end":121,"id":18},{"text":".","start":121,"end":122,"id":19}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We prove that this method is consistent for assigning nodes to blocks, as only a negligible number of nodes will be mis-assigned.","_input_hash":-234554623,"_task_hash":-1643202703,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"prove","start":3,"end":8,"id":1},{"text":"that","start":9,"end":13,"id":2},{"text":"this","start":14,"end":18,"id":3},{"text":"method","start":19,"end":25,"id":4},{"text":"is","start":26,"end":28,"id":5},{"text":"consistent","start":29,"end":39,"id":6},{"text":"for","start":40,"end":43,"id":7},{"text":"assigning","start":44,"end":53,"id":8},{"text":"nodes","start":54,"end":59,"id":9},{"text":"to","start":60,"end":62,"id":10},{"text":"blocks","start":63,"end":69,"id":11},{"text":",","start":69,"end":70,"id":12},{"text":"as","start":71,"end":73,"id":13},{"text":"only","start":74,"end":78,"id":14},{"text":"a","start":79,"end":80,"id":15},{"text":"negligible","start":81,"end":91,"id":16},{"text":"number","start":92,"end":98,"id":17},{"text":"of","start":99,"end":101,"id":18},{"text":"nodes","start":102,"end":107,"id":19},{"text":"will","start":108,"end":112,"id":20},{"text":"be","start":113,"end":115,"id":21},{"text":"mis","start":116,"end":119,"id":22},{"text":"-","start":119,"end":120,"id":23},{"text":"assigned","start":120,"end":128,"id":24},{"text":".","start":128,"end":129,"id":25}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We have developed an efficient algorithm for the maximum likelihood joint tracking and association problem in a strong clutter for GMTI data.","_input_hash":2013596882,"_task_hash":-1223038722,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"have","start":3,"end":7,"id":1},{"text":"developed","start":8,"end":17,"id":2},{"text":"an","start":18,"end":20,"id":3},{"text":"efficient","start":21,"end":30,"id":4},{"text":"algorithm","start":31,"end":40,"id":5},{"text":"for","start":41,"end":44,"id":6},{"text":"the","start":45,"end":48,"id":7},{"text":"maximum","start":49,"end":56,"id":8},{"text":"likelihood","start":57,"end":67,"id":9},{"text":"joint","start":68,"end":73,"id":10},{"text":"tracking","start":74,"end":82,"id":11},{"text":"and","start":83,"end":86,"id":12},{"text":"association","start":87,"end":98,"id":13},{"text":"problem","start":99,"end":106,"id":14},{"text":"in","start":107,"end":109,"id":15},{"text":"a","start":110,"end":111,"id":16},{"text":"strong","start":112,"end":118,"id":17},{"text":"clutter","start":119,"end":126,"id":18},{"text":"for","start":127,"end":130,"id":19},{"text":"GMTI","start":131,"end":135,"id":20},{"text":"data","start":136,"end":140,"id":21},{"text":".","start":140,"end":141,"id":22}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":21,"end":30,"token_start":4,"token_end":4,"label":"ALGO","answer":"reject"},{"start":31,"end":40,"token_start":5,"token_end":5,"label":"ALGO","answer":"reject"}],"answer":"reject"}
{"text":"Still, the fundamental relationship between these solutions such as whether they are identical or their relationship to the global minimizer of the underlying nonconvex formulation is unknown.","_input_hash":1587376415,"_task_hash":127042126,"tokens":[{"text":"Still","start":0,"end":5,"id":0},{"text":",","start":5,"end":6,"id":1},{"text":"the","start":7,"end":10,"id":2},{"text":"fundamental","start":11,"end":22,"id":3},{"text":"relationship","start":23,"end":35,"id":4},{"text":"between","start":36,"end":43,"id":5},{"text":"these","start":44,"end":49,"id":6},{"text":"solutions","start":50,"end":59,"id":7},{"text":"such","start":60,"end":64,"id":8},{"text":"as","start":65,"end":67,"id":9},{"text":"whether","start":68,"end":75,"id":10},{"text":"they","start":76,"end":80,"id":11},{"text":"are","start":81,"end":84,"id":12},{"text":"identical","start":85,"end":94,"id":13},{"text":"or","start":95,"end":97,"id":14},{"text":"their","start":98,"end":103,"id":15},{"text":"relationship","start":104,"end":116,"id":16},{"text":"to","start":117,"end":119,"id":17},{"text":"the","start":120,"end":123,"id":18},{"text":"global","start":124,"end":130,"id":19},{"text":"minimizer","start":131,"end":140,"id":20},{"text":"of","start":141,"end":143,"id":21},{"text":"the","start":144,"end":147,"id":22},{"text":"underlying","start":148,"end":158,"id":23},{"text":"nonconvex","start":159,"end":168,"id":24},{"text":"formulation","start":169,"end":180,"id":25},{"text":"is","start":181,"end":183,"id":26},{"text":"unknown","start":184,"end":191,"id":27},{"text":".","start":191,"end":192,"id":28}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"RobustICA's capabilities in processing real-world data involving noncircular complex strongly super-Gaussian sources are illustrated by the biomedical problem of atrial activity (AA) extraction in atrial fibrillation (AF) electrocardiograms (ECGs), where it outperforms an alternative ICA-based technique.","_input_hash":237247266,"_task_hash":257349884,"tokens":[{"text":"RobustICA","start":0,"end":9,"id":0},{"text":"'s","start":9,"end":11,"id":1},{"text":"capabilities","start":12,"end":24,"id":2},{"text":"in","start":25,"end":27,"id":3},{"text":"processing","start":28,"end":38,"id":4},{"text":"real","start":39,"end":43,"id":5},{"text":"-","start":43,"end":44,"id":6},{"text":"world","start":44,"end":49,"id":7},{"text":"data","start":50,"end":54,"id":8},{"text":"involving","start":55,"end":64,"id":9},{"text":"noncircular","start":65,"end":76,"id":10},{"text":"complex","start":77,"end":84,"id":11},{"text":"strongly","start":85,"end":93,"id":12},{"text":"super","start":94,"end":99,"id":13},{"text":"-","start":99,"end":100,"id":14},{"text":"Gaussian","start":100,"end":108,"id":15},{"text":"sources","start":109,"end":116,"id":16},{"text":"are","start":117,"end":120,"id":17},{"text":"illustrated","start":121,"end":132,"id":18},{"text":"by","start":133,"end":135,"id":19},{"text":"the","start":136,"end":139,"id":20},{"text":"biomedical","start":140,"end":150,"id":21},{"text":"problem","start":151,"end":158,"id":22},{"text":"of","start":159,"end":161,"id":23},{"text":"atrial","start":162,"end":168,"id":24},{"text":"activity","start":169,"end":177,"id":25},{"text":"(","start":178,"end":179,"id":26},{"text":"AA","start":179,"end":181,"id":27},{"text":")","start":181,"end":182,"id":28},{"text":"extraction","start":183,"end":193,"id":29},{"text":"in","start":194,"end":196,"id":30},{"text":"atrial","start":197,"end":203,"id":31},{"text":"fibrillation","start":204,"end":216,"id":32},{"text":"(","start":217,"end":218,"id":33},{"text":"AF","start":218,"end":220,"id":34},{"text":")","start":220,"end":221,"id":35},{"text":"electrocardiograms","start":222,"end":240,"id":36},{"text":"(","start":241,"end":242,"id":37},{"text":"ECGs","start":242,"end":246,"id":38},{"text":")","start":246,"end":247,"id":39},{"text":",","start":247,"end":248,"id":40},{"text":"where","start":249,"end":254,"id":41},{"text":"it","start":255,"end":257,"id":42},{"text":"outperforms","start":258,"end":269,"id":43},{"text":"an","start":270,"end":272,"id":44},{"text":"alternative","start":273,"end":284,"id":45},{"text":"ICA","start":285,"end":288,"id":46},{"text":"-","start":288,"end":289,"id":47},{"text":"based","start":289,"end":294,"id":48},{"text":"technique","start":295,"end":304,"id":49},{"text":".","start":304,"end":305,"id":50}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":0,"end":9,"token_start":0,"token_end":0,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"The standard methods employ boosted decision trees such as Adaboost for exponential loss and Friedman's gradient boosting for general loss.","_input_hash":-1811174226,"_task_hash":1576081543,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"standard","start":4,"end":12,"id":1},{"text":"methods","start":13,"end":20,"id":2},{"text":"employ","start":21,"end":27,"id":3},{"text":"boosted","start":28,"end":35,"id":4},{"text":"decision","start":36,"end":44,"id":5},{"text":"trees","start":45,"end":50,"id":6},{"text":"such","start":51,"end":55,"id":7},{"text":"as","start":56,"end":58,"id":8},{"text":"Adaboost","start":59,"end":67,"id":9},{"text":"for","start":68,"end":71,"id":10},{"text":"exponential","start":72,"end":83,"id":11},{"text":"loss","start":84,"end":88,"id":12},{"text":"and","start":89,"end":92,"id":13},{"text":"Friedman","start":93,"end":101,"id":14},{"text":"'s","start":101,"end":103,"id":15},{"text":"gradient","start":104,"end":112,"id":16},{"text":"boosting","start":113,"end":121,"id":17},{"text":"for","start":122,"end":125,"id":18},{"text":"general","start":126,"end":133,"id":19},{"text":"loss","start":134,"end":138,"id":20},{"text":".","start":138,"end":139,"id":21}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":28,"end":50,"token_start":4,"token_end":6,"label":"ALGO","answer":"accept"},{"start":59,"end":67,"token_start":9,"token_end":9,"label":"ALGO","answer":"accept"},{"start":93,"end":121,"token_start":14,"token_end":17,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We introduce an efficient method for training the linear ranking support vector machine.","_input_hash":-385118110,"_task_hash":-1798325579,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"introduce","start":3,"end":12,"id":1},{"text":"an","start":13,"end":15,"id":2},{"text":"efficient","start":16,"end":25,"id":3},{"text":"method","start":26,"end":32,"id":4},{"text":"for","start":33,"end":36,"id":5},{"text":"training","start":37,"end":45,"id":6},{"text":"the","start":46,"end":49,"id":7},{"text":"linear","start":50,"end":56,"id":8},{"text":"ranking","start":57,"end":64,"id":9},{"text":"support","start":65,"end":72,"id":10},{"text":"vector","start":73,"end":79,"id":11},{"text":"machine","start":80,"end":87,"id":12},{"text":".","start":87,"end":88,"id":13}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":50,"end":87,"token_start":8,"token_end":12,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"From a Gaussian processes perspective, the problem reduces to specifying an appropriate covariance function that, whilst being positive semi-definite, captures the dependencies between all the data points and across all the outputs.","_input_hash":-2121883516,"_task_hash":393969558,"tokens":[{"text":"From","start":0,"end":4,"id":0},{"text":"a","start":5,"end":6,"id":1},{"text":"Gaussian","start":7,"end":15,"id":2},{"text":"processes","start":16,"end":25,"id":3},{"text":"perspective","start":26,"end":37,"id":4},{"text":",","start":37,"end":38,"id":5},{"text":"the","start":39,"end":42,"id":6},{"text":"problem","start":43,"end":50,"id":7},{"text":"reduces","start":51,"end":58,"id":8},{"text":"to","start":59,"end":61,"id":9},{"text":"specifying","start":62,"end":72,"id":10},{"text":"an","start":73,"end":75,"id":11},{"text":"appropriate","start":76,"end":87,"id":12},{"text":"covariance","start":88,"end":98,"id":13},{"text":"function","start":99,"end":107,"id":14},{"text":"that","start":108,"end":112,"id":15},{"text":",","start":112,"end":113,"id":16},{"text":"whilst","start":114,"end":120,"id":17},{"text":"being","start":121,"end":126,"id":18},{"text":"positive","start":127,"end":135,"id":19},{"text":"semi","start":136,"end":140,"id":20},{"text":"-","start":140,"end":141,"id":21},{"text":"definite","start":141,"end":149,"id":22},{"text":",","start":149,"end":150,"id":23},{"text":"captures","start":151,"end":159,"id":24},{"text":"the","start":160,"end":163,"id":25},{"text":"dependencies","start":164,"end":176,"id":26},{"text":"between","start":177,"end":184,"id":27},{"text":"all","start":185,"end":188,"id":28},{"text":"the","start":189,"end":192,"id":29},{"text":"data","start":193,"end":197,"id":30},{"text":"points","start":198,"end":204,"id":31},{"text":"and","start":205,"end":208,"id":32},{"text":"across","start":209,"end":215,"id":33},{"text":"all","start":216,"end":219,"id":34},{"text":"the","start":220,"end":223,"id":35},{"text":"outputs","start":224,"end":231,"id":36},{"text":".","start":231,"end":232,"id":37}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"The generative process is described and shown to result in an exchangeable distribution over data points.","_input_hash":-1517521904,"_task_hash":-1297885229,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"generative","start":4,"end":14,"id":1},{"text":"process","start":15,"end":22,"id":2},{"text":"is","start":23,"end":25,"id":3},{"text":"described","start":26,"end":35,"id":4},{"text":"and","start":36,"end":39,"id":5},{"text":"shown","start":40,"end":45,"id":6},{"text":"to","start":46,"end":48,"id":7},{"text":"result","start":49,"end":55,"id":8},{"text":"in","start":56,"end":58,"id":9},{"text":"an","start":59,"end":61,"id":10},{"text":"exchangeable","start":62,"end":74,"id":11},{"text":"distribution","start":75,"end":87,"id":12},{"text":"over","start":88,"end":92,"id":13},{"text":"data","start":93,"end":97,"id":14},{"text":"points","start":98,"end":104,"id":15},{"text":".","start":104,"end":105,"id":16}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We test our algorithm on real data acquired to study the mental representation of objects, and we show that the proposed algorithm not only delineates meaningful brain regions but yields as well better prediction accuracy than reference methods.","_input_hash":-1783154804,"_task_hash":186721418,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"test","start":3,"end":7,"id":1},{"text":"our","start":8,"end":11,"id":2},{"text":"algorithm","start":12,"end":21,"id":3},{"text":"on","start":22,"end":24,"id":4},{"text":"real","start":25,"end":29,"id":5},{"text":"data","start":30,"end":34,"id":6},{"text":"acquired","start":35,"end":43,"id":7},{"text":"to","start":44,"end":46,"id":8},{"text":"study","start":47,"end":52,"id":9},{"text":"the","start":53,"end":56,"id":10},{"text":"mental","start":57,"end":63,"id":11},{"text":"representation","start":64,"end":78,"id":12},{"text":"of","start":79,"end":81,"id":13},{"text":"objects","start":82,"end":89,"id":14},{"text":",","start":89,"end":90,"id":15},{"text":"and","start":91,"end":94,"id":16},{"text":"we","start":95,"end":97,"id":17},{"text":"show","start":98,"end":102,"id":18},{"text":"that","start":103,"end":107,"id":19},{"text":"the","start":108,"end":111,"id":20},{"text":"proposed","start":112,"end":120,"id":21},{"text":"algorithm","start":121,"end":130,"id":22},{"text":"not","start":131,"end":134,"id":23},{"text":"only","start":135,"end":139,"id":24},{"text":"delineates","start":140,"end":150,"id":25},{"text":"meaningful","start":151,"end":161,"id":26},{"text":"brain","start":162,"end":167,"id":27},{"text":"regions","start":168,"end":175,"id":28},{"text":"but","start":176,"end":179,"id":29},{"text":"yields","start":180,"end":186,"id":30},{"text":"as","start":187,"end":189,"id":31},{"text":"well","start":190,"end":194,"id":32},{"text":"better","start":195,"end":201,"id":33},{"text":"prediction","start":202,"end":212,"id":34},{"text":"accuracy","start":213,"end":221,"id":35},{"text":"than","start":222,"end":226,"id":36},{"text":"reference","start":227,"end":236,"id":37},{"text":"methods","start":237,"end":244,"id":38},{"text":".","start":244,"end":245,"id":39}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Concerning design of kernel density estimators, one essential issue is how fast the pointwise mean square error (MSE) and/or the integrated mean square error (IMSE) diminish as the number of sampling instances increases.","_input_hash":2132921784,"_task_hash":233238422,"tokens":[{"text":"Concerning","start":0,"end":10,"id":0},{"text":"design","start":11,"end":17,"id":1},{"text":"of","start":18,"end":20,"id":2},{"text":"kernel","start":21,"end":27,"id":3},{"text":"density","start":28,"end":35,"id":4},{"text":"estimators","start":36,"end":46,"id":5},{"text":",","start":46,"end":47,"id":6},{"text":"one","start":48,"end":51,"id":7},{"text":"essential","start":52,"end":61,"id":8},{"text":"issue","start":62,"end":67,"id":9},{"text":"is","start":68,"end":70,"id":10},{"text":"how","start":71,"end":74,"id":11},{"text":"fast","start":75,"end":79,"id":12},{"text":"the","start":80,"end":83,"id":13},{"text":"pointwise","start":84,"end":93,"id":14},{"text":"mean","start":94,"end":98,"id":15},{"text":"square","start":99,"end":105,"id":16},{"text":"error","start":106,"end":111,"id":17},{"text":"(","start":112,"end":113,"id":18},{"text":"MSE","start":113,"end":116,"id":19},{"text":")","start":116,"end":117,"id":20},{"text":"and/or","start":118,"end":124,"id":21},{"text":"the","start":125,"end":128,"id":22},{"text":"integrated","start":129,"end":139,"id":23},{"text":"mean","start":140,"end":144,"id":24},{"text":"square","start":145,"end":151,"id":25},{"text":"error","start":152,"end":157,"id":26},{"text":"(","start":158,"end":159,"id":27},{"text":"IMSE","start":159,"end":163,"id":28},{"text":")","start":163,"end":164,"id":29},{"text":"diminish","start":165,"end":173,"id":30},{"text":"as","start":174,"end":176,"id":31},{"text":"the","start":177,"end":180,"id":32},{"text":"number","start":181,"end":187,"id":33},{"text":"of","start":188,"end":190,"id":34},{"text":"sampling","start":191,"end":199,"id":35},{"text":"instances","start":200,"end":209,"id":36},{"text":"increases","start":210,"end":219,"id":37},{"text":".","start":219,"end":220,"id":38}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"Our approach is based on a language model of arbitrary length expressions, for which we develop a new methodology based on nested permutation tests to find significant phrases.","_input_hash":1958017045,"_task_hash":2136400857,"tokens":[{"text":"Our","start":0,"end":3,"id":0},{"text":"approach","start":4,"end":12,"id":1},{"text":"is","start":13,"end":15,"id":2},{"text":"based","start":16,"end":21,"id":3},{"text":"on","start":22,"end":24,"id":4},{"text":"a","start":25,"end":26,"id":5},{"text":"language","start":27,"end":35,"id":6},{"text":"model","start":36,"end":41,"id":7},{"text":"of","start":42,"end":44,"id":8},{"text":"arbitrary","start":45,"end":54,"id":9},{"text":"length","start":55,"end":61,"id":10},{"text":"expressions","start":62,"end":73,"id":11},{"text":",","start":73,"end":74,"id":12},{"text":"for","start":75,"end":78,"id":13},{"text":"which","start":79,"end":84,"id":14},{"text":"we","start":85,"end":87,"id":15},{"text":"develop","start":88,"end":95,"id":16},{"text":"a","start":96,"end":97,"id":17},{"text":"new","start":98,"end":101,"id":18},{"text":"methodology","start":102,"end":113,"id":19},{"text":"based","start":114,"end":119,"id":20},{"text":"on","start":120,"end":122,"id":21},{"text":"nested","start":123,"end":129,"id":22},{"text":"permutation","start":130,"end":141,"id":23},{"text":"tests","start":142,"end":147,"id":24},{"text":"to","start":148,"end":150,"id":25},{"text":"find","start":151,"end":155,"id":26},{"text":"significant","start":156,"end":167,"id":27},{"text":"phrases","start":168,"end":175,"id":28},{"text":".","start":175,"end":176,"id":29}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Finally, we suggest a simple iterative method that can be used to improve the output of existing algorithms.","_input_hash":-402573250,"_task_hash":-2136247451,"tokens":[{"text":"Finally","start":0,"end":7,"id":0},{"text":",","start":7,"end":8,"id":1},{"text":"we","start":9,"end":11,"id":2},{"text":"suggest","start":12,"end":19,"id":3},{"text":"a","start":20,"end":21,"id":4},{"text":"simple","start":22,"end":28,"id":5},{"text":"iterative","start":29,"end":38,"id":6},{"text":"method","start":39,"end":45,"id":7},{"text":"that","start":46,"end":50,"id":8},{"text":"can","start":51,"end":54,"id":9},{"text":"be","start":55,"end":57,"id":10},{"text":"used","start":58,"end":62,"id":11},{"text":"to","start":63,"end":65,"id":12},{"text":"improve","start":66,"end":73,"id":13},{"text":"the","start":74,"end":77,"id":14},{"text":"output","start":78,"end":84,"id":15},{"text":"of","start":85,"end":87,"id":16},{"text":"existing","start":88,"end":96,"id":17},{"text":"algorithms","start":97,"end":107,"id":18},{"text":".","start":107,"end":108,"id":19}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Modeling data with linear combinations of a few elements from a learned dictionary has been the focus of much recent research in machine learning, neuroscience and signal processing.","_input_hash":1318071673,"_task_hash":2133471701,"tokens":[{"text":"Modeling","start":0,"end":8,"id":0},{"text":"data","start":9,"end":13,"id":1},{"text":"with","start":14,"end":18,"id":2},{"text":"linear","start":19,"end":25,"id":3},{"text":"combinations","start":26,"end":38,"id":4},{"text":"of","start":39,"end":41,"id":5},{"text":"a","start":42,"end":43,"id":6},{"text":"few","start":44,"end":47,"id":7},{"text":"elements","start":48,"end":56,"id":8},{"text":"from","start":57,"end":61,"id":9},{"text":"a","start":62,"end":63,"id":10},{"text":"learned","start":64,"end":71,"id":11},{"text":"dictionary","start":72,"end":82,"id":12},{"text":"has","start":83,"end":86,"id":13},{"text":"been","start":87,"end":91,"id":14},{"text":"the","start":92,"end":95,"id":15},{"text":"focus","start":96,"end":101,"id":16},{"text":"of","start":102,"end":104,"id":17},{"text":"much","start":105,"end":109,"id":18},{"text":"recent","start":110,"end":116,"id":19},{"text":"research","start":117,"end":125,"id":20},{"text":"in","start":126,"end":128,"id":21},{"text":"machine","start":129,"end":136,"id":22},{"text":"learning","start":137,"end":145,"id":23},{"text":",","start":145,"end":146,"id":24},{"text":"neuroscience","start":147,"end":159,"id":25},{"text":"and","start":160,"end":163,"id":26},{"text":"signal","start":164,"end":170,"id":27},{"text":"processing","start":171,"end":181,"id":28},{"text":".","start":181,"end":182,"id":29}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Using the Kullback-Leibler divergence as a measure of generalisation error we draw learning curves in simplified situations.","_input_hash":1258501792,"_task_hash":-828622494,"tokens":[{"text":"Using","start":0,"end":5,"id":0},{"text":"the","start":6,"end":9,"id":1},{"text":"Kullback","start":10,"end":18,"id":2},{"text":"-","start":18,"end":19,"id":3},{"text":"Leibler","start":19,"end":26,"id":4},{"text":"divergence","start":27,"end":37,"id":5},{"text":"as","start":38,"end":40,"id":6},{"text":"a","start":41,"end":42,"id":7},{"text":"measure","start":43,"end":50,"id":8},{"text":"of","start":51,"end":53,"id":9},{"text":"generalisation","start":54,"end":68,"id":10},{"text":"error","start":69,"end":74,"id":11},{"text":"we","start":75,"end":77,"id":12},{"text":"draw","start":78,"end":82,"id":13},{"text":"learning","start":83,"end":91,"id":14},{"text":"curves","start":92,"end":98,"id":15},{"text":"in","start":99,"end":101,"id":16},{"text":"simplified","start":102,"end":112,"id":17},{"text":"situations","start":113,"end":123,"id":18},{"text":".","start":123,"end":124,"id":19}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"The embedding associates each node with a vector;","_input_hash":-367439027,"_task_hash":-200207598,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"embedding","start":4,"end":13,"id":1},{"text":"associates","start":14,"end":24,"id":2},{"text":"each","start":25,"end":29,"id":3},{"text":"node","start":30,"end":34,"id":4},{"text":"with","start":35,"end":39,"id":5},{"text":"a","start":40,"end":41,"id":6},{"text":"vector","start":42,"end":48,"id":7},{"text":";","start":48,"end":49,"id":8}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We extend multi-way, multivariate ANOVA-type analysis to cases where one covariate is the view, with features of each view coming from different, high-dimensional domains.","_input_hash":-1034263990,"_task_hash":-1703169187,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"extend","start":3,"end":9,"id":1},{"text":"multi","start":10,"end":15,"id":2},{"text":"-","start":15,"end":16,"id":3},{"text":"way","start":16,"end":19,"id":4},{"text":",","start":19,"end":20,"id":5},{"text":"multivariate","start":21,"end":33,"id":6},{"text":"ANOVA","start":34,"end":39,"id":7},{"text":"-","start":39,"end":40,"id":8},{"text":"type","start":40,"end":44,"id":9},{"text":"analysis","start":45,"end":53,"id":10},{"text":"to","start":54,"end":56,"id":11},{"text":"cases","start":57,"end":62,"id":12},{"text":"where","start":63,"end":68,"id":13},{"text":"one","start":69,"end":72,"id":14},{"text":"covariate","start":73,"end":82,"id":15},{"text":"is","start":83,"end":85,"id":16},{"text":"the","start":86,"end":89,"id":17},{"text":"view","start":90,"end":94,"id":18},{"text":",","start":94,"end":95,"id":19},{"text":"with","start":96,"end":100,"id":20},{"text":"features","start":101,"end":109,"id":21},{"text":"of","start":110,"end":112,"id":22},{"text":"each","start":113,"end":117,"id":23},{"text":"view","start":118,"end":122,"id":24},{"text":"coming","start":123,"end":129,"id":25},{"text":"from","start":130,"end":134,"id":26},{"text":"different","start":135,"end":144,"id":27},{"text":",","start":144,"end":145,"id":28},{"text":"high","start":146,"end":150,"id":29},{"text":"-","start":150,"end":151,"id":30},{"text":"dimensional","start":151,"end":162,"id":31},{"text":"domains","start":163,"end":170,"id":32},{"text":".","start":170,"end":171,"id":33}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"The idea is to use an asymmetry between the distributions of cause and effect that occurs if both the covariance matrix of the cause and the structure matrix mapping cause to the effect are independently chosen.","_input_hash":1554371149,"_task_hash":-110771222,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"idea","start":4,"end":8,"id":1},{"text":"is","start":9,"end":11,"id":2},{"text":"to","start":12,"end":14,"id":3},{"text":"use","start":15,"end":18,"id":4},{"text":"an","start":19,"end":21,"id":5},{"text":"asymmetry","start":22,"end":31,"id":6},{"text":"between","start":32,"end":39,"id":7},{"text":"the","start":40,"end":43,"id":8},{"text":"distributions","start":44,"end":57,"id":9},{"text":"of","start":58,"end":60,"id":10},{"text":"cause","start":61,"end":66,"id":11},{"text":"and","start":67,"end":70,"id":12},{"text":"effect","start":71,"end":77,"id":13},{"text":"that","start":78,"end":82,"id":14},{"text":"occurs","start":83,"end":89,"id":15},{"text":"if","start":90,"end":92,"id":16},{"text":"both","start":93,"end":97,"id":17},{"text":"the","start":98,"end":101,"id":18},{"text":"covariance","start":102,"end":112,"id":19},{"text":"matrix","start":113,"end":119,"id":20},{"text":"of","start":120,"end":122,"id":21},{"text":"the","start":123,"end":126,"id":22},{"text":"cause","start":127,"end":132,"id":23},{"text":"and","start":133,"end":136,"id":24},{"text":"the","start":137,"end":140,"id":25},{"text":"structure","start":141,"end":150,"id":26},{"text":"matrix","start":151,"end":157,"id":27},{"text":"mapping","start":158,"end":165,"id":28},{"text":"cause","start":166,"end":171,"id":29},{"text":"to","start":172,"end":174,"id":30},{"text":"the","start":175,"end":178,"id":31},{"text":"effect","start":179,"end":185,"id":32},{"text":"are","start":186,"end":189,"id":33},{"text":"independently","start":190,"end":203,"id":34},{"text":"chosen","start":204,"end":210,"id":35},{"text":".","start":210,"end":211,"id":36}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"For graph estimation, we consider the problem of estimating forests with restricted tree sizes.","_input_hash":-2038417911,"_task_hash":-1279752279,"tokens":[{"text":"For","start":0,"end":3,"id":0},{"text":"graph","start":4,"end":9,"id":1},{"text":"estimation","start":10,"end":20,"id":2},{"text":",","start":20,"end":21,"id":3},{"text":"we","start":22,"end":24,"id":4},{"text":"consider","start":25,"end":33,"id":5},{"text":"the","start":34,"end":37,"id":6},{"text":"problem","start":38,"end":45,"id":7},{"text":"of","start":46,"end":48,"id":8},{"text":"estimating","start":49,"end":59,"id":9},{"text":"forests","start":60,"end":67,"id":10},{"text":"with","start":68,"end":72,"id":11},{"text":"restricted","start":73,"end":83,"id":12},{"text":"tree","start":84,"end":88,"id":13},{"text":"sizes","start":89,"end":94,"id":14},{"text":".","start":94,"end":95,"id":15}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"This confirms a conjecture presented by Le Roux and Bengio 2010.","_input_hash":-75785123,"_task_hash":-204126453,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"confirms","start":5,"end":13,"id":1},{"text":"a","start":14,"end":15,"id":2},{"text":"conjecture","start":16,"end":26,"id":3},{"text":"presented","start":27,"end":36,"id":4},{"text":"by","start":37,"end":39,"id":5},{"text":"Le","start":40,"end":42,"id":6},{"text":"Roux","start":43,"end":47,"id":7},{"text":"and","start":48,"end":51,"id":8},{"text":"Bengio","start":52,"end":58,"id":9},{"text":"2010","start":59,"end":63,"id":10},{"text":".","start":63,"end":64,"id":11}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Results on vowel recognition data, Parkinson's disease data, and microarray data are also given.","_input_hash":1269753913,"_task_hash":-861504645,"tokens":[{"text":"Results","start":0,"end":7,"id":0},{"text":"on","start":8,"end":10,"id":1},{"text":"vowel","start":11,"end":16,"id":2},{"text":"recognition","start":17,"end":28,"id":3},{"text":"data","start":29,"end":33,"id":4},{"text":",","start":33,"end":34,"id":5},{"text":"Parkinson","start":35,"end":44,"id":6},{"text":"'s","start":44,"end":46,"id":7},{"text":"disease","start":47,"end":54,"id":8},{"text":"data","start":55,"end":59,"id":9},{"text":",","start":59,"end":60,"id":10},{"text":"and","start":61,"end":64,"id":11},{"text":"microarray","start":65,"end":75,"id":12},{"text":"data","start":76,"end":80,"id":13},{"text":"are","start":81,"end":84,"id":14},{"text":"also","start":85,"end":89,"id":15},{"text":"given","start":90,"end":95,"id":16},{"text":".","start":95,"end":96,"id":17}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Motivated by an existing graph partitioning framework, we derive relationships between optimizing relevance information, as defined in the Information Bottleneck method, and the regularized cut in a K-partitioned graph.","_input_hash":909894230,"_task_hash":1831309840,"tokens":[{"text":"Motivated","start":0,"end":9,"id":0},{"text":"by","start":10,"end":12,"id":1},{"text":"an","start":13,"end":15,"id":2},{"text":"existing","start":16,"end":24,"id":3},{"text":"graph","start":25,"end":30,"id":4},{"text":"partitioning","start":31,"end":43,"id":5},{"text":"framework","start":44,"end":53,"id":6},{"text":",","start":53,"end":54,"id":7},{"text":"we","start":55,"end":57,"id":8},{"text":"derive","start":58,"end":64,"id":9},{"text":"relationships","start":65,"end":78,"id":10},{"text":"between","start":79,"end":86,"id":11},{"text":"optimizing","start":87,"end":97,"id":12},{"text":"relevance","start":98,"end":107,"id":13},{"text":"information","start":108,"end":119,"id":14},{"text":",","start":119,"end":120,"id":15},{"text":"as","start":121,"end":123,"id":16},{"text":"defined","start":124,"end":131,"id":17},{"text":"in","start":132,"end":134,"id":18},{"text":"the","start":135,"end":138,"id":19},{"text":"Information","start":139,"end":150,"id":20},{"text":"Bottleneck","start":151,"end":161,"id":21},{"text":"method","start":162,"end":168,"id":22},{"text":",","start":168,"end":169,"id":23},{"text":"and","start":170,"end":173,"id":24},{"text":"the","start":174,"end":177,"id":25},{"text":"regularized","start":178,"end":189,"id":26},{"text":"cut","start":190,"end":193,"id":27},{"text":"in","start":194,"end":196,"id":28},{"text":"a","start":197,"end":198,"id":29},{"text":"K","start":199,"end":200,"id":30},{"text":"-","start":200,"end":201,"id":31},{"text":"partitioned","start":201,"end":212,"id":32},{"text":"graph","start":213,"end":218,"id":33},{"text":".","start":218,"end":219,"id":34}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":139,"end":161,"token_start":20,"token_end":21,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Variable selection","_input_hash":459880386,"_task_hash":-1337870547,"tokens":[{"text":"Variable","start":0,"end":8,"id":0},{"text":"selection","start":9,"end":18,"id":1}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"A detailed investigation of the possibilities of an automatic target recognition (ATR) facil- ity in a bistatic radar system is presented.","_input_hash":391590519,"_task_hash":-69783910,"tokens":[{"text":"A","start":0,"end":1,"id":0},{"text":"detailed","start":2,"end":10,"id":1},{"text":"investigation","start":11,"end":24,"id":2},{"text":"of","start":25,"end":27,"id":3},{"text":"the","start":28,"end":31,"id":4},{"text":"possibilities","start":32,"end":45,"id":5},{"text":"of","start":46,"end":48,"id":6},{"text":"an","start":49,"end":51,"id":7},{"text":"automatic","start":52,"end":61,"id":8},{"text":"target","start":62,"end":68,"id":9},{"text":"recognition","start":69,"end":80,"id":10},{"text":"(","start":81,"end":82,"id":11},{"text":"ATR","start":82,"end":85,"id":12},{"text":")","start":85,"end":86,"id":13},{"text":"facil-","start":87,"end":93,"id":14},{"text":"ity","start":94,"end":97,"id":15},{"text":"in","start":98,"end":100,"id":16},{"text":"a","start":101,"end":102,"id":17},{"text":"bistatic","start":103,"end":111,"id":18},{"text":"radar","start":112,"end":117,"id":19},{"text":"system","start":118,"end":124,"id":20},{"text":"is","start":125,"end":127,"id":21},{"text":"presented","start":128,"end":137,"id":22},{"text":".","start":137,"end":138,"id":23}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":52,"end":80,"token_start":8,"token_end":10,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We analyze the performance of a class of manifold-learning algorithms that find their output by minimizing a quadratic form under some normalization constraints.","_input_hash":1489635198,"_task_hash":1268568208,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"analyze","start":3,"end":10,"id":1},{"text":"the","start":11,"end":14,"id":2},{"text":"performance","start":15,"end":26,"id":3},{"text":"of","start":27,"end":29,"id":4},{"text":"a","start":30,"end":31,"id":5},{"text":"class","start":32,"end":37,"id":6},{"text":"of","start":38,"end":40,"id":7},{"text":"manifold","start":41,"end":49,"id":8},{"text":"-","start":49,"end":50,"id":9},{"text":"learning","start":50,"end":58,"id":10},{"text":"algorithms","start":59,"end":69,"id":11},{"text":"that","start":70,"end":74,"id":12},{"text":"find","start":75,"end":79,"id":13},{"text":"their","start":80,"end":85,"id":14},{"text":"output","start":86,"end":92,"id":15},{"text":"by","start":93,"end":95,"id":16},{"text":"minimizing","start":96,"end":106,"id":17},{"text":"a","start":107,"end":108,"id":18},{"text":"quadratic","start":109,"end":118,"id":19},{"text":"form","start":119,"end":123,"id":20},{"text":"under","start":124,"end":129,"id":21},{"text":"some","start":130,"end":134,"id":22},{"text":"normalization","start":135,"end":148,"id":23},{"text":"constraints","start":149,"end":160,"id":24},{"text":".","start":160,"end":161,"id":25}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":41,"end":58,"token_start":8,"token_end":10,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"In this paper, we present a general formulation for supervised dictionary learning adapted to a wide variety of tasks, and present an efficient algorithm for solving the corresponding optimization problem.","_input_hash":1983508494,"_task_hash":-1638656457,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"this","start":3,"end":7,"id":1},{"text":"paper","start":8,"end":13,"id":2},{"text":",","start":13,"end":14,"id":3},{"text":"we","start":15,"end":17,"id":4},{"text":"present","start":18,"end":25,"id":5},{"text":"a","start":26,"end":27,"id":6},{"text":"general","start":28,"end":35,"id":7},{"text":"formulation","start":36,"end":47,"id":8},{"text":"for","start":48,"end":51,"id":9},{"text":"supervised","start":52,"end":62,"id":10},{"text":"dictionary","start":63,"end":73,"id":11},{"text":"learning","start":74,"end":82,"id":12},{"text":"adapted","start":83,"end":90,"id":13},{"text":"to","start":91,"end":93,"id":14},{"text":"a","start":94,"end":95,"id":15},{"text":"wide","start":96,"end":100,"id":16},{"text":"variety","start":101,"end":108,"id":17},{"text":"of","start":109,"end":111,"id":18},{"text":"tasks","start":112,"end":117,"id":19},{"text":",","start":117,"end":118,"id":20},{"text":"and","start":119,"end":122,"id":21},{"text":"present","start":123,"end":130,"id":22},{"text":"an","start":131,"end":133,"id":23},{"text":"efficient","start":134,"end":143,"id":24},{"text":"algorithm","start":144,"end":153,"id":25},{"text":"for","start":154,"end":157,"id":26},{"text":"solving","start":158,"end":165,"id":27},{"text":"the","start":166,"end":169,"id":28},{"text":"corresponding","start":170,"end":183,"id":29},{"text":"optimization","start":184,"end":196,"id":30},{"text":"problem","start":197,"end":204,"id":31},{"text":".","start":204,"end":205,"id":32}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"The PVM selects a relatively small number of representative points which can then be used for classification.","_input_hash":1942868972,"_task_hash":-664145965,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"PVM","start":4,"end":7,"id":1},{"text":"selects","start":8,"end":15,"id":2},{"text":"a","start":16,"end":17,"id":3},{"text":"relatively","start":18,"end":28,"id":4},{"text":"small","start":29,"end":34,"id":5},{"text":"number","start":35,"end":41,"id":6},{"text":"of","start":42,"end":44,"id":7},{"text":"representative","start":45,"end":59,"id":8},{"text":"points","start":60,"end":66,"id":9},{"text":"which","start":67,"end":72,"id":10},{"text":"can","start":73,"end":76,"id":11},{"text":"then","start":77,"end":81,"id":12},{"text":"be","start":82,"end":84,"id":13},{"text":"used","start":85,"end":89,"id":14},{"text":"for","start":90,"end":93,"id":15},{"text":"classification","start":94,"end":108,"id":16},{"text":".","start":108,"end":109,"id":17}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":4,"end":7,"token_start":1,"token_end":1,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"The additional reduction in dimension when compared to bounds found in the literature, is at least $13\\%$, and, in some cases, up to $30\\%$ additional reduction is achieved.","_input_hash":-2144446175,"_task_hash":-232732974,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"additional","start":4,"end":14,"id":1},{"text":"reduction","start":15,"end":24,"id":2},{"text":"in","start":25,"end":27,"id":3},{"text":"dimension","start":28,"end":37,"id":4},{"text":"when","start":38,"end":42,"id":5},{"text":"compared","start":43,"end":51,"id":6},{"text":"to","start":52,"end":54,"id":7},{"text":"bounds","start":55,"end":61,"id":8},{"text":"found","start":62,"end":67,"id":9},{"text":"in","start":68,"end":70,"id":10},{"text":"the","start":71,"end":74,"id":11},{"text":"literature","start":75,"end":85,"id":12},{"text":",","start":85,"end":86,"id":13},{"text":"is","start":87,"end":89,"id":14},{"text":"at","start":90,"end":92,"id":15},{"text":"least","start":93,"end":98,"id":16},{"text":"$","start":99,"end":100,"id":17},{"text":"13\\%$","start":100,"end":105,"id":18},{"text":",","start":105,"end":106,"id":19},{"text":"and","start":107,"end":110,"id":20},{"text":",","start":110,"end":111,"id":21},{"text":"in","start":112,"end":114,"id":22},{"text":"some","start":115,"end":119,"id":23},{"text":"cases","start":120,"end":125,"id":24},{"text":",","start":125,"end":126,"id":25},{"text":"up","start":127,"end":129,"id":26},{"text":"to","start":130,"end":132,"id":27},{"text":"$","start":133,"end":134,"id":28},{"text":"30\\%$","start":134,"end":139,"id":29},{"text":"additional","start":140,"end":150,"id":30},{"text":"reduction","start":151,"end":160,"id":31},{"text":"is","start":161,"end":163,"id":32},{"text":"achieved","start":164,"end":172,"id":33},{"text":".","start":172,"end":173,"id":34}],"_session_id":null,"_view_id":"ner_manual","answer":"accept","spans":[]}
{"text":"We consider a standard binary classification problem.","_input_hash":1330900170,"_task_hash":1231965395,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"consider","start":3,"end":11,"id":1},{"text":"a","start":12,"end":13,"id":2},{"text":"standard","start":14,"end":22,"id":3},{"text":"binary","start":23,"end":29,"id":4},{"text":"classification","start":30,"end":44,"id":5},{"text":"problem","start":45,"end":52,"id":6},{"text":".","start":52,"end":53,"id":7}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":23,"end":44,"token_start":4,"token_end":5,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"In Chapter 2, we propose a Bayesian method to avoid this selection bias, with application to naive Bayes models and mixture models.","_input_hash":-134608525,"_task_hash":269575710,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"Chapter","start":3,"end":10,"id":1},{"text":"2","start":11,"end":12,"id":2},{"text":",","start":12,"end":13,"id":3},{"text":"we","start":14,"end":16,"id":4},{"text":"propose","start":17,"end":24,"id":5},{"text":"a","start":25,"end":26,"id":6},{"text":"Bayesian","start":27,"end":35,"id":7},{"text":"method","start":36,"end":42,"id":8},{"text":"to","start":43,"end":45,"id":9},{"text":"avoid","start":46,"end":51,"id":10},{"text":"this","start":52,"end":56,"id":11},{"text":"selection","start":57,"end":66,"id":12},{"text":"bias","start":67,"end":71,"id":13},{"text":",","start":71,"end":72,"id":14},{"text":"with","start":73,"end":77,"id":15},{"text":"application","start":78,"end":89,"id":16},{"text":"to","start":90,"end":92,"id":17},{"text":"naive","start":93,"end":98,"id":18},{"text":"Bayes","start":99,"end":104,"id":19},{"text":"models","start":105,"end":111,"id":20},{"text":"and","start":112,"end":115,"id":21},{"text":"mixture","start":116,"end":123,"id":22},{"text":"models","start":124,"end":130,"id":23},{"text":".","start":130,"end":131,"id":24}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":27,"end":35,"token_start":7,"token_end":7,"label":"ALGO","answer":"accept"},{"start":93,"end":104,"token_start":18,"token_end":19,"label":"ALGO","answer":"accept"},{"start":116,"end":123,"token_start":22,"token_end":22,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We propose Dirichlet Process mixtures of Generalized Linear Models (DP-GLM), a new method of nonparametric regression that accommodates continuous and categorical inputs, and responses that can be modeled by a generalized linear model.","_input_hash":-1048931128,"_task_hash":796525325,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"propose","start":3,"end":10,"id":1},{"text":"Dirichlet","start":11,"end":20,"id":2},{"text":"Process","start":21,"end":28,"id":3},{"text":"mixtures","start":29,"end":37,"id":4},{"text":"of","start":38,"end":40,"id":5},{"text":"Generalized","start":41,"end":52,"id":6},{"text":"Linear","start":53,"end":59,"id":7},{"text":"Models","start":60,"end":66,"id":8},{"text":"(","start":67,"end":68,"id":9},{"text":"DP","start":68,"end":70,"id":10},{"text":"-","start":70,"end":71,"id":11},{"text":"GLM","start":71,"end":74,"id":12},{"text":")","start":74,"end":75,"id":13},{"text":",","start":75,"end":76,"id":14},{"text":"a","start":77,"end":78,"id":15},{"text":"new","start":79,"end":82,"id":16},{"text":"method","start":83,"end":89,"id":17},{"text":"of","start":90,"end":92,"id":18},{"text":"nonparametric","start":93,"end":106,"id":19},{"text":"regression","start":107,"end":117,"id":20},{"text":"that","start":118,"end":122,"id":21},{"text":"accommodates","start":123,"end":135,"id":22},{"text":"continuous","start":136,"end":146,"id":23},{"text":"and","start":147,"end":150,"id":24},{"text":"categorical","start":151,"end":162,"id":25},{"text":"inputs","start":163,"end":169,"id":26},{"text":",","start":169,"end":170,"id":27},{"text":"and","start":171,"end":174,"id":28},{"text":"responses","start":175,"end":184,"id":29},{"text":"that","start":185,"end":189,"id":30},{"text":"can","start":190,"end":193,"id":31},{"text":"be","start":194,"end":196,"id":32},{"text":"modeled","start":197,"end":204,"id":33},{"text":"by","start":205,"end":207,"id":34},{"text":"a","start":208,"end":209,"id":35},{"text":"generalized","start":210,"end":221,"id":36},{"text":"linear","start":222,"end":228,"id":37},{"text":"model","start":229,"end":234,"id":38},{"text":".","start":234,"end":235,"id":39}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":11,"end":59,"token_start":2,"token_end":7,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Some representative applications of the kernel Bayes' rule are presented, including Baysian computation without likelihood and filtering with a nonparametric state-space model.","_input_hash":-728004888,"_task_hash":-619912438,"tokens":[{"text":"Some","start":0,"end":4,"id":0},{"text":"representative","start":5,"end":19,"id":1},{"text":"applications","start":20,"end":32,"id":2},{"text":"of","start":33,"end":35,"id":3},{"text":"the","start":36,"end":39,"id":4},{"text":"kernel","start":40,"end":46,"id":5},{"text":"Bayes","start":47,"end":52,"id":6},{"text":"'","start":52,"end":53,"id":7},{"text":"rule","start":54,"end":58,"id":8},{"text":"are","start":59,"end":62,"id":9},{"text":"presented","start":63,"end":72,"id":10},{"text":",","start":72,"end":73,"id":11},{"text":"including","start":74,"end":83,"id":12},{"text":"Baysian","start":84,"end":91,"id":13},{"text":"computation","start":92,"end":103,"id":14},{"text":"without","start":104,"end":111,"id":15},{"text":"likelihood","start":112,"end":122,"id":16},{"text":"and","start":123,"end":126,"id":17},{"text":"filtering","start":127,"end":136,"id":18},{"text":"with","start":137,"end":141,"id":19},{"text":"a","start":142,"end":143,"id":20},{"text":"nonparametric","start":144,"end":157,"id":21},{"text":"state","start":158,"end":163,"id":22},{"text":"-","start":163,"end":164,"id":23},{"text":"space","start":164,"end":169,"id":24},{"text":"model","start":170,"end":175,"id":25},{"text":".","start":175,"end":176,"id":26}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":144,"end":169,"token_start":21,"token_end":24,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Due to its simplicity, training LapSVM in the primal can be the starting point for additional enhancements of the original LapSVM formulation, such as those for dealing with large datasets.","_input_hash":-1273538646,"_task_hash":-1233073403,"tokens":[{"text":"Due","start":0,"end":3,"id":0},{"text":"to","start":4,"end":6,"id":1},{"text":"its","start":7,"end":10,"id":2},{"text":"simplicity","start":11,"end":21,"id":3},{"text":",","start":21,"end":22,"id":4},{"text":"training","start":23,"end":31,"id":5},{"text":"LapSVM","start":32,"end":38,"id":6},{"text":"in","start":39,"end":41,"id":7},{"text":"the","start":42,"end":45,"id":8},{"text":"primal","start":46,"end":52,"id":9},{"text":"can","start":53,"end":56,"id":10},{"text":"be","start":57,"end":59,"id":11},{"text":"the","start":60,"end":63,"id":12},{"text":"starting","start":64,"end":72,"id":13},{"text":"point","start":73,"end":78,"id":14},{"text":"for","start":79,"end":82,"id":15},{"text":"additional","start":83,"end":93,"id":16},{"text":"enhancements","start":94,"end":106,"id":17},{"text":"of","start":107,"end":109,"id":18},{"text":"the","start":110,"end":113,"id":19},{"text":"original","start":114,"end":122,"id":20},{"text":"LapSVM","start":123,"end":129,"id":21},{"text":"formulation","start":130,"end":141,"id":22},{"text":",","start":141,"end":142,"id":23},{"text":"such","start":143,"end":147,"id":24},{"text":"as","start":148,"end":150,"id":25},{"text":"those","start":151,"end":156,"id":26},{"text":"for","start":157,"end":160,"id":27},{"text":"dealing","start":161,"end":168,"id":28},{"text":"with","start":169,"end":173,"id":29},{"text":"large","start":174,"end":179,"id":30},{"text":"datasets","start":180,"end":188,"id":31},{"text":".","start":188,"end":189,"id":32}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":32,"end":38,"token_start":6,"token_end":6,"label":"ALGO","answer":"accept"},{"start":123,"end":129,"token_start":21,"token_end":21,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Graph construction is an important aspect of graph-based learning.","_input_hash":-682490540,"_task_hash":1173103109,"tokens":[{"text":"Graph","start":0,"end":5,"id":0},{"text":"construction","start":6,"end":18,"id":1},{"text":"is","start":19,"end":21,"id":2},{"text":"an","start":22,"end":24,"id":3},{"text":"important","start":25,"end":34,"id":4},{"text":"aspect","start":35,"end":41,"id":5},{"text":"of","start":42,"end":44,"id":6},{"text":"graph","start":45,"end":50,"id":7},{"text":"-","start":50,"end":51,"id":8},{"text":"based","start":51,"end":56,"id":9},{"text":"learning","start":57,"end":65,"id":10},{"text":".","start":65,"end":66,"id":11}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"In response to a 1997 problem of M. Vidyasagar, we state a criterion for PAC learnability of a concept class $\\mathscr C$ under the family of all non-atomic (diffuse) measures on the domain $\\Omega$. The uniform Glivenko--Cantelli property with respect to non-atomic measures is no longer a necessary condition, and consistent learnability cannot in general be expected.","_input_hash":1405529642,"_task_hash":-126507442,"tokens":[{"text":"In","start":0,"end":2,"id":0},{"text":"response","start":3,"end":11,"id":1},{"text":"to","start":12,"end":14,"id":2},{"text":"a","start":15,"end":16,"id":3},{"text":"1997","start":17,"end":21,"id":4},{"text":"problem","start":22,"end":29,"id":5},{"text":"of","start":30,"end":32,"id":6},{"text":"M.","start":33,"end":35,"id":7},{"text":"Vidyasagar","start":36,"end":46,"id":8},{"text":",","start":46,"end":47,"id":9},{"text":"we","start":48,"end":50,"id":10},{"text":"state","start":51,"end":56,"id":11},{"text":"a","start":57,"end":58,"id":12},{"text":"criterion","start":59,"end":68,"id":13},{"text":"for","start":69,"end":72,"id":14},{"text":"PAC","start":73,"end":76,"id":15},{"text":"learnability","start":77,"end":89,"id":16},{"text":"of","start":90,"end":92,"id":17},{"text":"a","start":93,"end":94,"id":18},{"text":"concept","start":95,"end":102,"id":19},{"text":"class","start":103,"end":108,"id":20},{"text":"$","start":109,"end":110,"id":21},{"text":"\\mathscr","start":110,"end":118,"id":22},{"text":"C$","start":119,"end":121,"id":23},{"text":"under","start":122,"end":127,"id":24},{"text":"the","start":128,"end":131,"id":25},{"text":"family","start":132,"end":138,"id":26},{"text":"of","start":139,"end":141,"id":27},{"text":"all","start":142,"end":145,"id":28},{"text":"non","start":146,"end":149,"id":29},{"text":"-","start":149,"end":150,"id":30},{"text":"atomic","start":150,"end":156,"id":31},{"text":"(","start":157,"end":158,"id":32},{"text":"diffuse","start":158,"end":165,"id":33},{"text":")","start":165,"end":166,"id":34},{"text":"measures","start":167,"end":175,"id":35},{"text":"on","start":176,"end":178,"id":36},{"text":"the","start":179,"end":182,"id":37},{"text":"domain","start":183,"end":189,"id":38},{"text":"$","start":190,"end":191,"id":39},{"text":"\\Omega$.","start":191,"end":199,"id":40},{"text":"The","start":200,"end":203,"id":41},{"text":"uniform","start":204,"end":211,"id":42},{"text":"Glivenko","start":212,"end":220,"id":43},{"text":"--","start":220,"end":222,"id":44},{"text":"Cantelli","start":222,"end":230,"id":45},{"text":"property","start":231,"end":239,"id":46},{"text":"with","start":240,"end":244,"id":47},{"text":"respect","start":245,"end":252,"id":48},{"text":"to","start":253,"end":255,"id":49},{"text":"non","start":256,"end":259,"id":50},{"text":"-","start":259,"end":260,"id":51},{"text":"atomic","start":260,"end":266,"id":52},{"text":"measures","start":267,"end":275,"id":53},{"text":"is","start":276,"end":278,"id":54},{"text":"no","start":279,"end":281,"id":55},{"text":"longer","start":282,"end":288,"id":56},{"text":"a","start":289,"end":290,"id":57},{"text":"necessary","start":291,"end":300,"id":58},{"text":"condition","start":301,"end":310,"id":59},{"text":",","start":310,"end":311,"id":60},{"text":"and","start":312,"end":315,"id":61},{"text":"consistent","start":316,"end":326,"id":62},{"text":"learnability","start":327,"end":339,"id":63},{"text":"can","start":340,"end":343,"id":64},{"text":"not","start":343,"end":346,"id":65},{"text":"in","start":347,"end":349,"id":66},{"text":"general","start":350,"end":357,"id":67},{"text":"be","start":358,"end":360,"id":68},{"text":"expected","start":361,"end":369,"id":69},{"text":".","start":369,"end":370,"id":70}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"reject"}
{"text":"The novelty of correlation screening is that it identifies a smaller number of variables which are highly correlated with others, as compared to identifying a number of correlation parameters.","_input_hash":-1948674786,"_task_hash":-437653757,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"novelty","start":4,"end":11,"id":1},{"text":"of","start":12,"end":14,"id":2},{"text":"correlation","start":15,"end":26,"id":3},{"text":"screening","start":27,"end":36,"id":4},{"text":"is","start":37,"end":39,"id":5},{"text":"that","start":40,"end":44,"id":6},{"text":"it","start":45,"end":47,"id":7},{"text":"identifies","start":48,"end":58,"id":8},{"text":"a","start":59,"end":60,"id":9},{"text":"smaller","start":61,"end":68,"id":10},{"text":"number","start":69,"end":75,"id":11},{"text":"of","start":76,"end":78,"id":12},{"text":"variables","start":79,"end":88,"id":13},{"text":"which","start":89,"end":94,"id":14},{"text":"are","start":95,"end":98,"id":15},{"text":"highly","start":99,"end":105,"id":16},{"text":"correlated","start":106,"end":116,"id":17},{"text":"with","start":117,"end":121,"id":18},{"text":"others","start":122,"end":128,"id":19},{"text":",","start":128,"end":129,"id":20},{"text":"as","start":130,"end":132,"id":21},{"text":"compared","start":133,"end":141,"id":22},{"text":"to","start":142,"end":144,"id":23},{"text":"identifying","start":145,"end":156,"id":24},{"text":"a","start":157,"end":158,"id":25},{"text":"number","start":159,"end":165,"id":26},{"text":"of","start":166,"end":168,"id":27},{"text":"correlation","start":169,"end":180,"id":28},{"text":"parameters","start":181,"end":191,"id":29},{"text":".","start":191,"end":192,"id":30}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We present experiments with artificial data and real-world gene expression data to evaluate the method.","_input_hash":-875733824,"_task_hash":1270460339,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"present","start":3,"end":10,"id":1},{"text":"experiments","start":11,"end":22,"id":2},{"text":"with","start":23,"end":27,"id":3},{"text":"artificial","start":28,"end":38,"id":4},{"text":"data","start":39,"end":43,"id":5},{"text":"and","start":44,"end":47,"id":6},{"text":"real","start":48,"end":52,"id":7},{"text":"-","start":52,"end":53,"id":8},{"text":"world","start":53,"end":58,"id":9},{"text":"gene","start":59,"end":63,"id":10},{"text":"expression","start":64,"end":74,"id":11},{"text":"data","start":75,"end":79,"id":12},{"text":"to","start":80,"end":82,"id":13},{"text":"evaluate","start":83,"end":91,"id":14},{"text":"the","start":92,"end":95,"id":15},{"text":"method","start":96,"end":102,"id":16},{"text":".","start":102,"end":103,"id":17}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"Following the manifold regularization approach, Laplacian Support Vector Machines (LapSVMs) have shown the state of the art performance in semi--supervised classification.","_input_hash":1370397883,"_task_hash":1015134833,"tokens":[{"text":"Following","start":0,"end":9,"id":0},{"text":"the","start":10,"end":13,"id":1},{"text":"manifold","start":14,"end":22,"id":2},{"text":"regularization","start":23,"end":37,"id":3},{"text":"approach","start":38,"end":46,"id":4},{"text":",","start":46,"end":47,"id":5},{"text":"Laplacian","start":48,"end":57,"id":6},{"text":"Support","start":58,"end":65,"id":7},{"text":"Vector","start":66,"end":72,"id":8},{"text":"Machines","start":73,"end":81,"id":9},{"text":"(","start":82,"end":83,"id":10},{"text":"LapSVMs","start":83,"end":90,"id":11},{"text":")","start":90,"end":91,"id":12},{"text":"have","start":92,"end":96,"id":13},{"text":"shown","start":97,"end":102,"id":14},{"text":"the","start":103,"end":106,"id":15},{"text":"state","start":107,"end":112,"id":16},{"text":"of","start":113,"end":115,"id":17},{"text":"the","start":116,"end":119,"id":18},{"text":"art","start":120,"end":123,"id":19},{"text":"performance","start":124,"end":135,"id":20},{"text":"in","start":136,"end":138,"id":21},{"text":"semi","start":139,"end":143,"id":22},{"text":"--","start":143,"end":145,"id":23},{"text":"supervised","start":145,"end":155,"id":24},{"text":"classification","start":156,"end":170,"id":25},{"text":".","start":170,"end":171,"id":26}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":48,"end":81,"token_start":6,"token_end":9,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We show how to extend systematically all the results obtained in the inductive setting to transductive learning, and use this to improve Vapnik's generalization bounds, extending them to the case when the sample is made of independent non-identically distributed pairs of patterns and labels.","_input_hash":-979179916,"_task_hash":399736153,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"show","start":3,"end":7,"id":1},{"text":"how","start":8,"end":11,"id":2},{"text":"to","start":12,"end":14,"id":3},{"text":"extend","start":15,"end":21,"id":4},{"text":"systematically","start":22,"end":36,"id":5},{"text":"all","start":37,"end":40,"id":6},{"text":"the","start":41,"end":44,"id":7},{"text":"results","start":45,"end":52,"id":8},{"text":"obtained","start":53,"end":61,"id":9},{"text":"in","start":62,"end":64,"id":10},{"text":"the","start":65,"end":68,"id":11},{"text":"inductive","start":69,"end":78,"id":12},{"text":"setting","start":79,"end":86,"id":13},{"text":"to","start":87,"end":89,"id":14},{"text":"transductive","start":90,"end":102,"id":15},{"text":"learning","start":103,"end":111,"id":16},{"text":",","start":111,"end":112,"id":17},{"text":"and","start":113,"end":116,"id":18},{"text":"use","start":117,"end":120,"id":19},{"text":"this","start":121,"end":125,"id":20},{"text":"to","start":126,"end":128,"id":21},{"text":"improve","start":129,"end":136,"id":22},{"text":"Vapnik","start":137,"end":143,"id":23},{"text":"'s","start":143,"end":145,"id":24},{"text":"generalization","start":146,"end":160,"id":25},{"text":"bounds","start":161,"end":167,"id":26},{"text":",","start":167,"end":168,"id":27},{"text":"extending","start":169,"end":178,"id":28},{"text":"them","start":179,"end":183,"id":29},{"text":"to","start":184,"end":186,"id":30},{"text":"the","start":187,"end":190,"id":31},{"text":"case","start":191,"end":195,"id":32},{"text":"when","start":196,"end":200,"id":33},{"text":"the","start":201,"end":204,"id":34},{"text":"sample","start":205,"end":211,"id":35},{"text":"is","start":212,"end":214,"id":36},{"text":"made","start":215,"end":219,"id":37},{"text":"of","start":220,"end":222,"id":38},{"text":"independent","start":223,"end":234,"id":39},{"text":"non","start":235,"end":238,"id":40},{"text":"-","start":238,"end":239,"id":41},{"text":"identically","start":239,"end":250,"id":42},{"text":"distributed","start":251,"end":262,"id":43},{"text":"pairs","start":263,"end":268,"id":44},{"text":"of","start":269,"end":271,"id":45},{"text":"patterns","start":272,"end":280,"id":46},{"text":"and","start":281,"end":284,"id":47},{"text":"labels","start":285,"end":291,"id":48},{"text":".","start":291,"end":292,"id":49}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"We address the task of modeling melodic sequences from the same music genre.","_input_hash":1207741908,"_task_hash":963602810,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"address","start":3,"end":10,"id":1},{"text":"the","start":11,"end":14,"id":2},{"text":"task","start":15,"end":19,"id":3},{"text":"of","start":20,"end":22,"id":4},{"text":"modeling","start":23,"end":31,"id":5},{"text":"melodic","start":32,"end":39,"id":6},{"text":"sequences","start":40,"end":49,"id":7},{"text":"from","start":50,"end":54,"id":8},{"text":"the","start":55,"end":58,"id":9},{"text":"same","start":59,"end":63,"id":10},{"text":"music","start":64,"end":69,"id":11},{"text":"genre","start":70,"end":75,"id":12},{"text":".","start":75,"end":76,"id":13}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"This approach was motivated by the Divisive Information-Theoretic Feature Clustering model in probabilistic space with Kullback-Leibler divergence which may be regarded as a special case within the Clustering Minimisation framework.","_input_hash":109449691,"_task_hash":-373914296,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"approach","start":5,"end":13,"id":1},{"text":"was","start":14,"end":17,"id":2},{"text":"motivated","start":18,"end":27,"id":3},{"text":"by","start":28,"end":30,"id":4},{"text":"the","start":31,"end":34,"id":5},{"text":"Divisive","start":35,"end":43,"id":6},{"text":"Information","start":44,"end":55,"id":7},{"text":"-","start":55,"end":56,"id":8},{"text":"Theoretic","start":56,"end":65,"id":9},{"text":"Feature","start":66,"end":73,"id":10},{"text":"Clustering","start":74,"end":84,"id":11},{"text":"model","start":85,"end":90,"id":12},{"text":"in","start":91,"end":93,"id":13},{"text":"probabilistic","start":94,"end":107,"id":14},{"text":"space","start":108,"end":113,"id":15},{"text":"with","start":114,"end":118,"id":16},{"text":"Kullback","start":119,"end":127,"id":17},{"text":"-","start":127,"end":128,"id":18},{"text":"Leibler","start":128,"end":135,"id":19},{"text":"divergence","start":136,"end":146,"id":20},{"text":"which","start":147,"end":152,"id":21},{"text":"may","start":153,"end":156,"id":22},{"text":"be","start":157,"end":159,"id":23},{"text":"regarded","start":160,"end":168,"id":24},{"text":"as","start":169,"end":171,"id":25},{"text":"a","start":172,"end":173,"id":26},{"text":"special","start":174,"end":181,"id":27},{"text":"case","start":182,"end":186,"id":28},{"text":"within","start":187,"end":193,"id":29},{"text":"the","start":194,"end":197,"id":30},{"text":"Clustering","start":198,"end":208,"id":31},{"text":"Minimisation","start":209,"end":221,"id":32},{"text":"framework","start":222,"end":231,"id":33},{"text":".","start":231,"end":232,"id":34}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":35,"end":84,"token_start":6,"token_end":11,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"The present contribution suggests the use of a multidimensional scaling (MDS) algorithm as a visualization tool for manifold-valued elements.","_input_hash":-27624822,"_task_hash":-80399198,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"present","start":4,"end":11,"id":1},{"text":"contribution","start":12,"end":24,"id":2},{"text":"suggests","start":25,"end":33,"id":3},{"text":"the","start":34,"end":37,"id":4},{"text":"use","start":38,"end":41,"id":5},{"text":"of","start":42,"end":44,"id":6},{"text":"a","start":45,"end":46,"id":7},{"text":"multidimensional","start":47,"end":63,"id":8},{"text":"scaling","start":64,"end":71,"id":9},{"text":"(","start":72,"end":73,"id":10},{"text":"MDS","start":73,"end":76,"id":11},{"text":")","start":76,"end":77,"id":12},{"text":"algorithm","start":78,"end":87,"id":13},{"text":"as","start":88,"end":90,"id":14},{"text":"a","start":91,"end":92,"id":15},{"text":"visualization","start":93,"end":106,"id":16},{"text":"tool","start":107,"end":111,"id":17},{"text":"for","start":112,"end":115,"id":18},{"text":"manifold","start":116,"end":124,"id":19},{"text":"-","start":124,"end":125,"id":20},{"text":"valued","start":125,"end":131,"id":21},{"text":"elements","start":132,"end":140,"id":22},{"text":".","start":140,"end":141,"id":23}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":47,"end":71,"token_start":8,"token_end":9,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We introduce supervised latent Dirichlet allocation (sLDA), a statistical model of labelled documents.","_input_hash":1571434723,"_task_hash":-159136033,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"introduce","start":3,"end":12,"id":1},{"text":"supervised","start":13,"end":23,"id":2},{"text":"latent","start":24,"end":30,"id":3},{"text":"Dirichlet","start":31,"end":40,"id":4},{"text":"allocation","start":41,"end":51,"id":5},{"text":"(","start":52,"end":53,"id":6},{"text":"sLDA","start":53,"end":57,"id":7},{"text":")","start":57,"end":58,"id":8},{"text":",","start":58,"end":59,"id":9},{"text":"a","start":60,"end":61,"id":10},{"text":"statistical","start":62,"end":73,"id":11},{"text":"model","start":74,"end":79,"id":12},{"text":"of","start":80,"end":82,"id":13},{"text":"labelled","start":83,"end":91,"id":14},{"text":"documents","start":92,"end":101,"id":15},{"text":".","start":101,"end":102,"id":16}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":13,"end":51,"token_start":2,"token_end":5,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"Dependence estimation is analytically carried out by recently-proposed least-squares mutual information (LSMI), and dependence maximization is also analytically carried out by utilizing the Epanechnikov kernel.","_input_hash":1010392247,"_task_hash":-731117921,"tokens":[{"text":"Dependence","start":0,"end":10,"id":0},{"text":"estimation","start":11,"end":21,"id":1},{"text":"is","start":22,"end":24,"id":2},{"text":"analytically","start":25,"end":37,"id":3},{"text":"carried","start":38,"end":45,"id":4},{"text":"out","start":46,"end":49,"id":5},{"text":"by","start":50,"end":52,"id":6},{"text":"recently","start":53,"end":61,"id":7},{"text":"-","start":61,"end":62,"id":8},{"text":"proposed","start":62,"end":70,"id":9},{"text":"least","start":71,"end":76,"id":10},{"text":"-","start":76,"end":77,"id":11},{"text":"squares","start":77,"end":84,"id":12},{"text":"mutual","start":85,"end":91,"id":13},{"text":"information","start":92,"end":103,"id":14},{"text":"(","start":104,"end":105,"id":15},{"text":"LSMI","start":105,"end":109,"id":16},{"text":")","start":109,"end":110,"id":17},{"text":",","start":110,"end":111,"id":18},{"text":"and","start":112,"end":115,"id":19},{"text":"dependence","start":116,"end":126,"id":20},{"text":"maximization","start":127,"end":139,"id":21},{"text":"is","start":140,"end":142,"id":22},{"text":"also","start":143,"end":147,"id":23},{"text":"analytically","start":148,"end":160,"id":24},{"text":"carried","start":161,"end":168,"id":25},{"text":"out","start":169,"end":172,"id":26},{"text":"by","start":173,"end":175,"id":27},{"text":"utilizing","start":176,"end":185,"id":28},{"text":"the","start":186,"end":189,"id":29},{"text":"Epanechnikov","start":190,"end":202,"id":30},{"text":"kernel","start":203,"end":209,"id":31},{"text":".","start":209,"end":210,"id":32}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":190,"end":209,"token_start":30,"token_end":31,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"From the construction of a minimal spanning tree with Prim's algorithm, and the assumption that the vertices are approximately distributed according to a Poisson distribution, the number of clusters is estimated by thresholding the Prim's trajectory.","_input_hash":-1335032842,"_task_hash":-1698450164,"tokens":[{"text":"From","start":0,"end":4,"id":0},{"text":"the","start":5,"end":8,"id":1},{"text":"construction","start":9,"end":21,"id":2},{"text":"of","start":22,"end":24,"id":3},{"text":"a","start":25,"end":26,"id":4},{"text":"minimal","start":27,"end":34,"id":5},{"text":"spanning","start":35,"end":43,"id":6},{"text":"tree","start":44,"end":48,"id":7},{"text":"with","start":49,"end":53,"id":8},{"text":"Prim","start":54,"end":58,"id":9},{"text":"'s","start":58,"end":60,"id":10},{"text":"algorithm","start":61,"end":70,"id":11},{"text":",","start":70,"end":71,"id":12},{"text":"and","start":72,"end":75,"id":13},{"text":"the","start":76,"end":79,"id":14},{"text":"assumption","start":80,"end":90,"id":15},{"text":"that","start":91,"end":95,"id":16},{"text":"the","start":96,"end":99,"id":17},{"text":"vertices","start":100,"end":108,"id":18},{"text":"are","start":109,"end":112,"id":19},{"text":"approximately","start":113,"end":126,"id":20},{"text":"distributed","start":127,"end":138,"id":21},{"text":"according","start":139,"end":148,"id":22},{"text":"to","start":149,"end":151,"id":23},{"text":"a","start":152,"end":153,"id":24},{"text":"Poisson","start":154,"end":161,"id":25},{"text":"distribution","start":162,"end":174,"id":26},{"text":",","start":174,"end":175,"id":27},{"text":"the","start":176,"end":179,"id":28},{"text":"number","start":180,"end":186,"id":29},{"text":"of","start":187,"end":189,"id":30},{"text":"clusters","start":190,"end":198,"id":31},{"text":"is","start":199,"end":201,"id":32},{"text":"estimated","start":202,"end":211,"id":33},{"text":"by","start":212,"end":214,"id":34},{"text":"thresholding","start":215,"end":227,"id":35},{"text":"the","start":228,"end":231,"id":36},{"text":"Prim","start":232,"end":236,"id":37},{"text":"'s","start":236,"end":238,"id":38},{"text":"trajectory","start":239,"end":249,"id":39},{"text":".","start":249,"end":250,"id":40}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":54,"end":60,"token_start":9,"token_end":10,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"The embedding function is then determined by solving a semidefinite program which has an interesting connection to the soft-margin linear binary support vector machine classifier.","_input_hash":1743355789,"_task_hash":1754270326,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"embedding","start":4,"end":13,"id":1},{"text":"function","start":14,"end":22,"id":2},{"text":"is","start":23,"end":25,"id":3},{"text":"then","start":26,"end":30,"id":4},{"text":"determined","start":31,"end":41,"id":5},{"text":"by","start":42,"end":44,"id":6},{"text":"solving","start":45,"end":52,"id":7},{"text":"a","start":53,"end":54,"id":8},{"text":"semidefinite","start":55,"end":67,"id":9},{"text":"program","start":68,"end":75,"id":10},{"text":"which","start":76,"end":81,"id":11},{"text":"has","start":82,"end":85,"id":12},{"text":"an","start":86,"end":88,"id":13},{"text":"interesting","start":89,"end":100,"id":14},{"text":"connection","start":101,"end":111,"id":15},{"text":"to","start":112,"end":114,"id":16},{"text":"the","start":115,"end":118,"id":17},{"text":"soft","start":119,"end":123,"id":18},{"text":"-","start":123,"end":124,"id":19},{"text":"margin","start":124,"end":130,"id":20},{"text":"linear","start":131,"end":137,"id":21},{"text":"binary","start":138,"end":144,"id":22},{"text":"support","start":145,"end":152,"id":23},{"text":"vector","start":153,"end":159,"id":24},{"text":"machine","start":160,"end":167,"id":25},{"text":"classifier","start":168,"end":178,"id":26},{"text":".","start":178,"end":179,"id":27}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":119,"end":178,"token_start":18,"token_end":26,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"A continuing challenge, however, is guiding an autoencoder toward representations that are useful for particular tasks.","_input_hash":-2067075707,"_task_hash":-2059569207,"tokens":[{"text":"A","start":0,"end":1,"id":0},{"text":"continuing","start":2,"end":12,"id":1},{"text":"challenge","start":13,"end":22,"id":2},{"text":",","start":22,"end":23,"id":3},{"text":"however","start":24,"end":31,"id":4},{"text":",","start":31,"end":32,"id":5},{"text":"is","start":33,"end":35,"id":6},{"text":"guiding","start":36,"end":43,"id":7},{"text":"an","start":44,"end":46,"id":8},{"text":"autoencoder","start":47,"end":58,"id":9},{"text":"toward","start":59,"end":65,"id":10},{"text":"representations","start":66,"end":81,"id":11},{"text":"that","start":82,"end":86,"id":12},{"text":"are","start":87,"end":90,"id":13},{"text":"useful","start":91,"end":97,"id":14},{"text":"for","start":98,"end":101,"id":15},{"text":"particular","start":102,"end":112,"id":16},{"text":"tasks","start":113,"end":118,"id":17},{"text":".","start":118,"end":119,"id":18}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":47,"end":58,"token_start":9,"token_end":9,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We show here that it is possible for SFA to detect a component which is even slower than the driving force itself (e.g. the envelope of a modulated sine wave).","_input_hash":-1116922908,"_task_hash":570675549,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"show","start":3,"end":7,"id":1},{"text":"here","start":8,"end":12,"id":2},{"text":"that","start":13,"end":17,"id":3},{"text":"it","start":18,"end":20,"id":4},{"text":"is","start":21,"end":23,"id":5},{"text":"possible","start":24,"end":32,"id":6},{"text":"for","start":33,"end":36,"id":7},{"text":"SFA","start":37,"end":40,"id":8},{"text":"to","start":41,"end":43,"id":9},{"text":"detect","start":44,"end":50,"id":10},{"text":"a","start":51,"end":52,"id":11},{"text":"component","start":53,"end":62,"id":12},{"text":"which","start":63,"end":68,"id":13},{"text":"is","start":69,"end":71,"id":14},{"text":"even","start":72,"end":76,"id":15},{"text":"slower","start":77,"end":83,"id":16},{"text":"than","start":84,"end":88,"id":17},{"text":"the","start":89,"end":92,"id":18},{"text":"driving","start":93,"end":100,"id":19},{"text":"force","start":101,"end":106,"id":20},{"text":"itself","start":107,"end":113,"id":21},{"text":"(","start":114,"end":115,"id":22},{"text":"e.g.","start":115,"end":119,"id":23},{"text":"the","start":120,"end":123,"id":24},{"text":"envelope","start":124,"end":132,"id":25},{"text":"of","start":133,"end":135,"id":26},{"text":"a","start":136,"end":137,"id":27},{"text":"modulated","start":138,"end":147,"id":28},{"text":"sine","start":148,"end":152,"id":29},{"text":"wave","start":153,"end":157,"id":30},{"text":")","start":157,"end":158,"id":31},{"text":".","start":158,"end":159,"id":32}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":37,"end":40,"token_start":8,"token_end":8,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"For the case where the optimal kernel combination is not exactly sparse, we prove that elastic-net MKL can achieve a faster convergence rate than the block-l1 and block-l2 MKL methods by carefully controlling the balance between the block-l1and block-l2 regularizers.","_input_hash":240417848,"_task_hash":-249577191,"tokens":[{"text":"For","start":0,"end":3,"id":0},{"text":"the","start":4,"end":7,"id":1},{"text":"case","start":8,"end":12,"id":2},{"text":"where","start":13,"end":18,"id":3},{"text":"the","start":19,"end":22,"id":4},{"text":"optimal","start":23,"end":30,"id":5},{"text":"kernel","start":31,"end":37,"id":6},{"text":"combination","start":38,"end":49,"id":7},{"text":"is","start":50,"end":52,"id":8},{"text":"not","start":53,"end":56,"id":9},{"text":"exactly","start":57,"end":64,"id":10},{"text":"sparse","start":65,"end":71,"id":11},{"text":",","start":71,"end":72,"id":12},{"text":"we","start":73,"end":75,"id":13},{"text":"prove","start":76,"end":81,"id":14},{"text":"that","start":82,"end":86,"id":15},{"text":"elastic","start":87,"end":94,"id":16},{"text":"-","start":94,"end":95,"id":17},{"text":"net","start":95,"end":98,"id":18},{"text":"MKL","start":99,"end":102,"id":19},{"text":"can","start":103,"end":106,"id":20},{"text":"achieve","start":107,"end":114,"id":21},{"text":"a","start":115,"end":116,"id":22},{"text":"faster","start":117,"end":123,"id":23},{"text":"convergence","start":124,"end":135,"id":24},{"text":"rate","start":136,"end":140,"id":25},{"text":"than","start":141,"end":145,"id":26},{"text":"the","start":146,"end":149,"id":27},{"text":"block","start":150,"end":155,"id":28},{"text":"-","start":155,"end":156,"id":29},{"text":"l1","start":156,"end":158,"id":30},{"text":"and","start":159,"end":162,"id":31},{"text":"block","start":163,"end":168,"id":32},{"text":"-","start":168,"end":169,"id":33},{"text":"l2","start":169,"end":171,"id":34},{"text":"MKL","start":172,"end":175,"id":35},{"text":"methods","start":176,"end":183,"id":36},{"text":"by","start":184,"end":186,"id":37},{"text":"carefully","start":187,"end":196,"id":38},{"text":"controlling","start":197,"end":208,"id":39},{"text":"the","start":209,"end":212,"id":40},{"text":"balance","start":213,"end":220,"id":41},{"text":"between","start":221,"end":228,"id":42},{"text":"the","start":229,"end":232,"id":43},{"text":"block","start":233,"end":238,"id":44},{"text":"-","start":238,"end":239,"id":45},{"text":"l1and","start":239,"end":244,"id":46},{"text":"block","start":245,"end":250,"id":47},{"text":"-","start":250,"end":251,"id":48},{"text":"l2","start":251,"end":253,"id":49},{"text":"regularizers","start":254,"end":266,"id":50},{"text":".","start":266,"end":267,"id":51}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"The algorithm is robust to local extrema and shows a very high convergence speed in terms of the computational cost required to reach a given source extraction quality, particularly for short data records.","_input_hash":1984769510,"_task_hash":-1720541801,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"algorithm","start":4,"end":13,"id":1},{"text":"is","start":14,"end":16,"id":2},{"text":"robust","start":17,"end":23,"id":3},{"text":"to","start":24,"end":26,"id":4},{"text":"local","start":27,"end":32,"id":5},{"text":"extrema","start":33,"end":40,"id":6},{"text":"and","start":41,"end":44,"id":7},{"text":"shows","start":45,"end":50,"id":8},{"text":"a","start":51,"end":52,"id":9},{"text":"very","start":53,"end":57,"id":10},{"text":"high","start":58,"end":62,"id":11},{"text":"convergence","start":63,"end":74,"id":12},{"text":"speed","start":75,"end":80,"id":13},{"text":"in","start":81,"end":83,"id":14},{"text":"terms","start":84,"end":89,"id":15},{"text":"of","start":90,"end":92,"id":16},{"text":"the","start":93,"end":96,"id":17},{"text":"computational","start":97,"end":110,"id":18},{"text":"cost","start":111,"end":115,"id":19},{"text":"required","start":116,"end":124,"id":20},{"text":"to","start":125,"end":127,"id":21},{"text":"reach","start":128,"end":133,"id":22},{"text":"a","start":134,"end":135,"id":23},{"text":"given","start":136,"end":141,"id":24},{"text":"source","start":142,"end":148,"id":25},{"text":"extraction","start":149,"end":159,"id":26},{"text":"quality","start":160,"end":167,"id":27},{"text":",","start":167,"end":168,"id":28},{"text":"particularly","start":169,"end":181,"id":29},{"text":"for","start":182,"end":185,"id":30},{"text":"short","start":186,"end":191,"id":31},{"text":"data","start":192,"end":196,"id":32},{"text":"records","start":197,"end":204,"id":33},{"text":".","start":204,"end":205,"id":34}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"The performance for learning drifting concepts of one of the presented algorithms is analysed and compared with the Baldi-Chauvin algorithm in the same situations.","_input_hash":1678323752,"_task_hash":1298138938,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"performance","start":4,"end":15,"id":1},{"text":"for","start":16,"end":19,"id":2},{"text":"learning","start":20,"end":28,"id":3},{"text":"drifting","start":29,"end":37,"id":4},{"text":"concepts","start":38,"end":46,"id":5},{"text":"of","start":47,"end":49,"id":6},{"text":"one","start":50,"end":53,"id":7},{"text":"of","start":54,"end":56,"id":8},{"text":"the","start":57,"end":60,"id":9},{"text":"presented","start":61,"end":70,"id":10},{"text":"algorithms","start":71,"end":81,"id":11},{"text":"is","start":82,"end":84,"id":12},{"text":"analysed","start":85,"end":93,"id":13},{"text":"and","start":94,"end":97,"id":14},{"text":"compared","start":98,"end":106,"id":15},{"text":"with","start":107,"end":111,"id":16},{"text":"the","start":112,"end":115,"id":17},{"text":"Baldi","start":116,"end":121,"id":18},{"text":"-","start":121,"end":122,"id":19},{"text":"Chauvin","start":122,"end":129,"id":20},{"text":"algorithm","start":130,"end":139,"id":21},{"text":"in","start":140,"end":142,"id":22},{"text":"the","start":143,"end":146,"id":23},{"text":"same","start":147,"end":151,"id":24},{"text":"situations","start":152,"end":162,"id":25},{"text":".","start":162,"end":163,"id":26}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":116,"end":129,"token_start":18,"token_end":20,"label":"ALGO","answer":"accept"}],"answer":"accept"}
{"text":"We give a unified overview of models previously introduced in the literature, which is simpler and more comprehensive than previous descriptions and enables us to highlight commonalities and differences among models that were not observed in the past.","_input_hash":-1270427432,"_task_hash":2081150045,"tokens":[{"text":"We","start":0,"end":2,"id":0},{"text":"give","start":3,"end":7,"id":1},{"text":"a","start":8,"end":9,"id":2},{"text":"unified","start":10,"end":17,"id":3},{"text":"overview","start":18,"end":26,"id":4},{"text":"of","start":27,"end":29,"id":5},{"text":"models","start":30,"end":36,"id":6},{"text":"previously","start":37,"end":47,"id":7},{"text":"introduced","start":48,"end":58,"id":8},{"text":"in","start":59,"end":61,"id":9},{"text":"the","start":62,"end":65,"id":10},{"text":"literature","start":66,"end":76,"id":11},{"text":",","start":76,"end":77,"id":12},{"text":"which","start":78,"end":83,"id":13},{"text":"is","start":84,"end":86,"id":14},{"text":"simpler","start":87,"end":94,"id":15},{"text":"and","start":95,"end":98,"id":16},{"text":"more","start":99,"end":103,"id":17},{"text":"comprehensive","start":104,"end":117,"id":18},{"text":"than","start":118,"end":122,"id":19},{"text":"previous","start":123,"end":131,"id":20},{"text":"descriptions","start":132,"end":144,"id":21},{"text":"and","start":145,"end":148,"id":22},{"text":"enables","start":149,"end":156,"id":23},{"text":"us","start":157,"end":159,"id":24},{"text":"to","start":160,"end":162,"id":25},{"text":"highlight","start":163,"end":172,"id":26},{"text":"commonalities","start":173,"end":186,"id":27},{"text":"and","start":187,"end":190,"id":28},{"text":"differences","start":191,"end":202,"id":29},{"text":"among","start":203,"end":208,"id":30},{"text":"models","start":209,"end":215,"id":31},{"text":"that","start":216,"end":220,"id":32},{"text":"were","start":221,"end":225,"id":33},{"text":"not","start":226,"end":229,"id":34},{"text":"observed","start":230,"end":238,"id":35},{"text":"in","start":239,"end":241,"id":36},{"text":"the","start":242,"end":245,"id":37},{"text":"past","start":246,"end":250,"id":38},{"text":".","start":250,"end":251,"id":39}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":0,"end":2,"token_start":0,"token_end":0,"label":"ALGO","answer":"reject"}],"answer":"reject"}
{"text":"When data comes from a dynamic stochastic process, a single static network model cannot adequately capture transient dependencies, such as, gene regulatory dependencies throughout a developmental cycle of an organism.","_input_hash":-236160815,"_task_hash":-1156441500,"tokens":[{"text":"When","start":0,"end":4,"id":0},{"text":"data","start":5,"end":9,"id":1},{"text":"comes","start":10,"end":15,"id":2},{"text":"from","start":16,"end":20,"id":3},{"text":"a","start":21,"end":22,"id":4},{"text":"dynamic","start":23,"end":30,"id":5},{"text":"stochastic","start":31,"end":41,"id":6},{"text":"process","start":42,"end":49,"id":7},{"text":",","start":49,"end":50,"id":8},{"text":"a","start":51,"end":52,"id":9},{"text":"single","start":53,"end":59,"id":10},{"text":"static","start":60,"end":66,"id":11},{"text":"network","start":67,"end":74,"id":12},{"text":"model","start":75,"end":80,"id":13},{"text":"can","start":81,"end":84,"id":14},{"text":"not","start":84,"end":87,"id":15},{"text":"adequately","start":88,"end":98,"id":16},{"text":"capture","start":99,"end":106,"id":17},{"text":"transient","start":107,"end":116,"id":18},{"text":"dependencies","start":117,"end":129,"id":19},{"text":",","start":129,"end":130,"id":20},{"text":"such","start":131,"end":135,"id":21},{"text":"as","start":136,"end":138,"id":22},{"text":",","start":138,"end":139,"id":23},{"text":"gene","start":140,"end":144,"id":24},{"text":"regulatory","start":145,"end":155,"id":25},{"text":"dependencies","start":156,"end":168,"id":26},{"text":"throughout","start":169,"end":179,"id":27},{"text":"a","start":180,"end":181,"id":28},{"text":"developmental","start":182,"end":195,"id":29},{"text":"cycle","start":196,"end":201,"id":30},{"text":"of","start":202,"end":204,"id":31},{"text":"an","start":205,"end":207,"id":32},{"text":"organism","start":208,"end":216,"id":33},{"text":".","start":216,"end":217,"id":34}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"This rank-modulated degree(RMD) scheme is able to significantly sparsify the graph near valleys and provides an adaptive way to cope with unbalanced data.","_input_hash":-827263233,"_task_hash":-1818114478,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"rank","start":5,"end":9,"id":1},{"text":"-","start":9,"end":10,"id":2},{"text":"modulated","start":10,"end":19,"id":3},{"text":"degree(RMD","start":20,"end":30,"id":4},{"text":")","start":30,"end":31,"id":5},{"text":"scheme","start":32,"end":38,"id":6},{"text":"is","start":39,"end":41,"id":7},{"text":"able","start":42,"end":46,"id":8},{"text":"to","start":47,"end":49,"id":9},{"text":"significantly","start":50,"end":63,"id":10},{"text":"sparsify","start":64,"end":72,"id":11},{"text":"the","start":73,"end":76,"id":12},{"text":"graph","start":77,"end":82,"id":13},{"text":"near","start":83,"end":87,"id":14},{"text":"valleys","start":88,"end":95,"id":15},{"text":"and","start":96,"end":99,"id":16},{"text":"provides","start":100,"end":108,"id":17},{"text":"an","start":109,"end":111,"id":18},{"text":"adaptive","start":112,"end":120,"id":19},{"text":"way","start":121,"end":124,"id":20},{"text":"to","start":125,"end":127,"id":21},{"text":"cope","start":128,"end":132,"id":22},{"text":"with","start":133,"end":137,"id":23},{"text":"unbalanced","start":138,"end":148,"id":24},{"text":"data","start":149,"end":153,"id":25},{"text":".","start":153,"end":154,"id":26}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"This article proposes a method for finding global maxima if the joint distribution is modeled by a kernel density estimation.","_input_hash":232411880,"_task_hash":735084196,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"article","start":5,"end":12,"id":1},{"text":"proposes","start":13,"end":21,"id":2},{"text":"a","start":22,"end":23,"id":3},{"text":"method","start":24,"end":30,"id":4},{"text":"for","start":31,"end":34,"id":5},{"text":"finding","start":35,"end":42,"id":6},{"text":"global","start":43,"end":49,"id":7},{"text":"maxima","start":50,"end":56,"id":8},{"text":"if","start":57,"end":59,"id":9},{"text":"the","start":60,"end":63,"id":10},{"text":"joint","start":64,"end":69,"id":11},{"text":"distribution","start":70,"end":82,"id":12},{"text":"is","start":83,"end":85,"id":13},{"text":"modeled","start":86,"end":93,"id":14},{"text":"by","start":94,"end":96,"id":15},{"text":"a","start":97,"end":98,"id":16},{"text":"kernel","start":99,"end":105,"id":17},{"text":"density","start":106,"end":113,"id":18},{"text":"estimation","start":114,"end":124,"id":19},{"text":".","start":124,"end":125,"id":20}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"This work is inspired by the concept of finite-time learning with known accuracy and confidence developed in statistical learning theory.","_input_hash":-619534829,"_task_hash":1297335163,"tokens":[{"text":"This","start":0,"end":4,"id":0},{"text":"work","start":5,"end":9,"id":1},{"text":"is","start":10,"end":12,"id":2},{"text":"inspired","start":13,"end":21,"id":3},{"text":"by","start":22,"end":24,"id":4},{"text":"the","start":25,"end":28,"id":5},{"text":"concept","start":29,"end":36,"id":6},{"text":"of","start":37,"end":39,"id":7},{"text":"finite","start":40,"end":46,"id":8},{"text":"-","start":46,"end":47,"id":9},{"text":"time","start":47,"end":51,"id":10},{"text":"learning","start":52,"end":60,"id":11},{"text":"with","start":61,"end":65,"id":12},{"text":"known","start":66,"end":71,"id":13},{"text":"accuracy","start":72,"end":80,"id":14},{"text":"and","start":81,"end":84,"id":15},{"text":"confidence","start":85,"end":95,"id":16},{"text":"developed","start":96,"end":105,"id":17},{"text":"in","start":106,"end":108,"id":18},{"text":"statistical","start":109,"end":120,"id":19},{"text":"learning","start":121,"end":129,"id":20},{"text":"theory","start":130,"end":136,"id":21},{"text":".","start":136,"end":137,"id":22}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"The reduction from minimizing over an infinite dimensional space to minimizing over a finite dimensional space occurs for more general objective functions:","_input_hash":1396707951,"_task_hash":-1792952711,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"reduction","start":4,"end":13,"id":1},{"text":"from","start":14,"end":18,"id":2},{"text":"minimizing","start":19,"end":29,"id":3},{"text":"over","start":30,"end":34,"id":4},{"text":"an","start":35,"end":37,"id":5},{"text":"infinite","start":38,"end":46,"id":6},{"text":"dimensional","start":47,"end":58,"id":7},{"text":"space","start":59,"end":64,"id":8},{"text":"to","start":65,"end":67,"id":9},{"text":"minimizing","start":68,"end":78,"id":10},{"text":"over","start":79,"end":83,"id":11},{"text":"a","start":84,"end":85,"id":12},{"text":"finite","start":86,"end":92,"id":13},{"text":"dimensional","start":93,"end":104,"id":14},{"text":"space","start":105,"end":110,"id":15},{"text":"occurs","start":111,"end":117,"id":16},{"text":"for","start":118,"end":121,"id":17},{"text":"more","start":122,"end":126,"id":18},{"text":"general","start":127,"end":134,"id":19},{"text":"objective","start":135,"end":144,"id":20},{"text":"functions","start":145,"end":154,"id":21},{"text":":","start":154,"end":155,"id":22}],"_session_id":null,"_view_id":"ner_manual","spans":[],"answer":"accept"}
{"text":"The method works for both stochastic and deterministic causal relations, provided that the dimensionality is sufficiently high (in some experiments, 5 was enough).","_input_hash":-325916622,"_task_hash":977270369,"tokens":[{"text":"The","start":0,"end":3,"id":0},{"text":"method","start":4,"end":10,"id":1},{"text":"works","start":11,"end":16,"id":2},{"text":"for","start":17,"end":20,"id":3},{"text":"both","start":21,"end":25,"id":4},{"text":"stochastic","start":26,"end":36,"id":5},{"text":"and","start":37,"end":40,"id":6},{"text":"deterministic","start":41,"end":54,"id":7},{"text":"causal","start":55,"end":61,"id":8},{"text":"relations","start":62,"end":71,"id":9},{"text":",","start":71,"end":72,"id":10},{"text":"provided","start":73,"end":81,"id":11},{"text":"that","start":82,"end":86,"id":12},{"text":"the","start":87,"end":90,"id":13},{"text":"dimensionality","start":91,"end":105,"id":14},{"text":"is","start":106,"end":108,"id":15},{"text":"sufficiently","start":109,"end":121,"id":16},{"text":"high","start":122,"end":126,"id":17},{"text":"(","start":127,"end":128,"id":18},{"text":"in","start":128,"end":130,"id":19},{"text":"some","start":131,"end":135,"id":20},{"text":"experiments","start":136,"end":147,"id":21},{"text":",","start":147,"end":148,"id":22},{"text":"5","start":149,"end":150,"id":23},{"text":"was","start":151,"end":154,"id":24},{"text":"enough","start":155,"end":161,"id":25},{"text":")","start":161,"end":162,"id":26},{"text":".","start":162,"end":163,"id":27}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":0,"end":3,"token_start":0,"token_end":0,"label":"ALGO","answer":"reject"}],"answer":"reject"}
{"text":"A linear non-Gaussian structural equation model called LiNGAM is an identifiable model for exploratory causal analysis.","_input_hash":-2030681503,"_task_hash":-353768703,"tokens":[{"text":"A","start":0,"end":1,"id":0},{"text":"linear","start":2,"end":8,"id":1},{"text":"non","start":9,"end":12,"id":2},{"text":"-","start":12,"end":13,"id":3},{"text":"Gaussian","start":13,"end":21,"id":4},{"text":"structural","start":22,"end":32,"id":5},{"text":"equation","start":33,"end":41,"id":6},{"text":"model","start":42,"end":47,"id":7},{"text":"called","start":48,"end":54,"id":8},{"text":"LiNGAM","start":55,"end":61,"id":9},{"text":"is","start":62,"end":64,"id":10},{"text":"an","start":65,"end":67,"id":11},{"text":"identifiable","start":68,"end":80,"id":12},{"text":"model","start":81,"end":86,"id":13},{"text":"for","start":87,"end":90,"id":14},{"text":"exploratory","start":91,"end":102,"id":15},{"text":"causal","start":103,"end":109,"id":16},{"text":"analysis","start":110,"end":118,"id":17},{"text":".","start":118,"end":119,"id":18}],"_session_id":null,"_view_id":"ner_manual","spans":[{"start":2,"end":41,"token_start":1,"token_end":6,"label":"ALGO","answer":"accept"},{"start":55,"end":61,"token_start":9,"token_end":9,"label":"ALGO","answer":"accept"}],"answer":"accept"}
